{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb26350-3e60-4fcc-97fa-483663a798b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stp 1 Dependecis instal\n",
    "#!pip install transformers==4.41.2\n",
    "# Important to install that transformer version, unles it occur error\n",
    "#See link: https://github.com/THUDM/CogVLM2/issues/181#issuecomment-2381807778\n",
    "# !pip install transformers sentencepiece datasets evaluate\n",
    "# !pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bc4697-f92f-4838-92ab-a9dd306dd236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "#from LLMLingua.experiments.llmlingua2.evaluation.metrics import qa_f1_score\n",
    "from metrics import qa_f1_score, f1_score\n",
    "if 'model' in globals():\n",
    "    del model \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#######################################\n",
    "print('cuda is',torch.cuda.is_available()) \n",
    "#################################################\n",
    "# 3 model & dataset \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcaafff-a6de-4b15-b4ef-be30536c8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is True\n"
     ]
    }
   ],
   "source": [
    "print('cuda is',torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41da0400-1574-4695-8eb0-4a8d2910d847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3291093cfb2d4bdaba32822b80578775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ChatGLM-6B\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"THUDM/chatglm2-6b-32k\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True\n",
    ").to(device)\n",
    "\n",
    "model=model.eval()\n",
    "#print('model config is:',model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b03872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b-32k\", trust_remote_code=True)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36cd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patch the tokenizer's _pad method to handle the padding_side argument\n",
    "original_pad = tokenizer._pad\n",
    "def new_pad(self, encoded_inputs, max_length=None, padding_strategy=\"longest\", pad_to_multiple_of=None, return_attention_mask=None, **kwargs):\n",
    "    return original_pad(encoded_inputs, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask)\n",
    "tokenizer._pad = new_pad.__get__(tokenizer, type(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e2ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  four datasets\n",
    "datasets = [\"multifieldqa_en\", \"hotpotqa\", \"triviaqa\", \"narrativeqa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e2f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongjae/miniconda3/envs/seongjae_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LINGUA COMPRESSION\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "compressor = PromptCompressor(\n",
    "    model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n",
    "    device_map=\"cuda\",  # Use GPU\n",
    "    use_llmlingua2=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8fe26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, ratio=None):\n",
    "    f1_scores = []\n",
    "    latencies = []\n",
    "    memory_usages = []\n",
    "    peak_memory_usages = []\n",
    "    \n",
    "    # Model constraints\n",
    "    max_context_length = 32768  # ChatGLM2-6B-32k's limit\n",
    "    max_new_tokens = 256  # Allow generating up to 256 tokens\n",
    "    max_input_length = max_context_length - max_new_tokens  # 32768 - 256 = 32512\n",
    "    \n",
    "    #for idx in range(20):  # Test on 10 examples\n",
    "    for idx in range(len(data)):  # Test on all examples\n",
    "        example = data[idx] \n",
    "        #print(type(example))\n",
    "        #print(example)\n",
    "        context = example[\"context\"]\n",
    "        question = example[\"input\"]\n",
    "        answers =example[\"answers\"][0]\n",
    "\n",
    "        compressed_context = compressor.compress_prompt(\n",
    "            context,\n",
    "            rate=ratio,           # Keep 50% tokens (compression ratio)\n",
    "        )[\"compressed_prompt\"]\n",
    "        #print('compressed_context:',compressed_context)\n",
    "\n",
    "        tokenized = tokenizer.encode(\n",
    "            compressed_context, \n",
    "            truncation=True, \n",
    "            max_length=max_input_length\n",
    "        )\n",
    "        truncated_context = tokenizer.decode(tokenized, skip_special_tokens=True)\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()        \n",
    "        start_time = time.time()\n",
    "        response, _ = model.chat(\n",
    "            tokenizer,\n",
    "            question,\n",
    "            history=[(compressed_context, \"Answer:\")],\n",
    "            max_new_tokens=max_new_tokens  # Critical parameter\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Measure memory\n",
    "        memory = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
    "        \n",
    "        #Measure peak memory?\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = qa_f1_score(prediction=response, ground_truth=answers)\n",
    "        f1_scores.append(f1)\n",
    "        latencies.append(latency)\n",
    "        peak_memory_usages.append(peak_memory)\n",
    "    \n",
    "    return {\n",
    "        \"avg_f1\": np.mean(f1_scores),\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_peak_memory\": np.mean(peak_memory_usages)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f58e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable function to print and save results\n",
    "        \n",
    "def log_result_LLMLingua(message):\n",
    "    #print(message)\n",
    "    with open(\"all_results_LLMLingua.txt\", \"a\") as f:  # 'a' = append mode\n",
    "        f.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22776e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===multifieldqa_en  llmlingua compression (25%) ===\n",
      "F1: 0.20, Latency: 3.44s,  Peak Memory: 14415.18 MB\n",
      "\n",
      "===multifieldqa_en  llmlingua compression (50%) ===\n",
      "F1: 0.30, Latency: 2.11s,  Peak Memory: 14738.71 MB\n",
      "\n",
      "===multifieldqa_en  llmlingua compression (75%) ===\n",
      "F1: 0.36, Latency: 2.37s,  Peak Memory: 15068.07 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[0]  #multifiledqa_en\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a98b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===hotpotqa  llmlingua compression (25%) ===\n",
      "F1: 0.15, Latency: 1.14s,  Peak Memory: 14740.71 MB\n",
      "\n",
      "===hotpotqa  llmlingua compression (50%) ===\n",
      "F1: 0.21, Latency: 1.77s,  Peak Memory: 15337.76 MB\n",
      "\n",
      "===hotpotqa  llmlingua compression (75%) ===\n",
      "F1: 0.26, Latency: 2.28s,  Peak Memory: 15926.03 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[1]  #hotpotqa\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6556788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===triviaqa  llmlingua compression (25%) ===\n",
      "F1: 0.41, Latency: 0.72s,  Peak Memory: 14767.27 MB\n",
      "\n",
      "===triviaqa  llmlingua compression (50%) ===\n",
      "F1: 0.40, Latency: 1.22s,  Peak Memory: 15279.17 MB\n",
      "\n",
      "===triviaqa  llmlingua compression (75%) ===\n",
      "F1: 0.42, Latency: 1.85s,  Peak Memory: 15787.47 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[2]  #triviaqa\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[3]  #narrativeqa\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "for ratio in pruning_ratios:\n",
    "    # lingua_results = evaluate(data, compress_with_llmlingua, ratio=ratio)\n",
    "    lingua_results = evaluate(data, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_LLMLingua(f\"\\n===20 samples {dataset_name}  llmlingua compression ({int(ratio*100)}%) ===\")\n",
    "    log_result_LLMLingua(f\"F1: {lingua_results['avg_f1']:.2f}, Latency: {lingua_results['avg_latency']:.2f}s,  Peak Memory: {lingua_results['avg_peak_memory']:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seongjae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
