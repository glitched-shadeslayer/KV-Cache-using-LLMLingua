input,context,answers,length,dataset,language,all_classes,_id,summary
What is the name of the most active fan club?,"Football Club Urartu (, translated Futbolayin Akumb Urartu), commonly known as Urartu, is an Armenian professional football team based in the capital Yerevan that currently plays in the Armenian Premier League. The club won the Armenian Cup three times, in 1992, 2007 and 2016. In 2013–2014, they won the Armenian Premier League for the first time in their history.

In early 2016, the Russia-based Armenian businessman Dzhevan Cheloyants became a co-owner of the club after purchasing the major part of the club shares. The club was known as FC Banants until 1 August 2019, when it was officially renamed FC Urartu.

History

Kotayk
Urartu FC were founded as FC Banants by Sarkis Israelyan on 21 January 1992 in the village of Kotayk, representing the Kotayk Province. He named the club after his native village of Banants (currently known as Bayan). Between 1992 and 1995, the club was commonly referred to as Banants Kotayk. During the 1992 season, the club won the first Armenian Cup. At the end of the 1995 transitional season, Banants suffered a financial crisis. The club owners decided that it was better to merge the club with FC Kotayk of Abovyan, rather than disband it. In 2001, Banants demerged from FC Kotayk, and was moved from Abovyan to the capital Yerevan.

Yerevan

FC Banants was relocated to Yerevan in 2001. At the beginning of 2003, Banants merged with FC Spartak Yerevan, but was able to limit the name of the new merger to FC Banants. Spartak became Banants's youth academy and later changed the name to Banants-2. Because of the merger, Banants acquired many players from Spartak Yerevan, including Samvel Melkonyan. After the merger, Banants took a more serious approach and have finished highly in the league table ever since. The club managed to lift the Armenian Cup in 2007.
Experience is making way for youth for the 2008 and 2009 seasons. The departures of most of the experienced players have left the club's future to the youth. Along with two Ukrainian players, Ugandan international, Noah Kasule, has been signed.

The club headquarters are located on Jivani Street 2 of the Malatia-Sebastia District, Yerevan.

Domestic

European

Stadium

The construction of the Banants Stadium was launched in 2006 in the Malatia-Sebastia District of Yerevan, with the assistance of the FIFA goal programme. It was officially opened in 2008 with a capacity of 3,600 seats. Further developments were implemented later in 2011, when the playing pitch was modernized and the capacity of the stadium was increased up to 4,860 seats (2,760 at the northern stand, 1,500 at the southern stand and 600 at the western stand).

Training centre/academy
Banants Training Centre is the club's academy base located in the Malatia-Sebastia District of Yerevan. In addition to the main stadium, the centre houses 3 full-size training pitches, mini football pitches as well as an indoor facility. The current technical director of the academy is the former Russian footballer Ilshat Faizulin.

Fans
The most active group of fans is the South West Ultras fan club, mainly composed of residents from several neighbourhoods within the Malatia-Sebastia District of Yerevan, since the club is a de facto representer of the district. Members of the fan club benefit from events organized by the club and many facilities of the Banants training centre, such as the mini football pitch, the club store and other entertainments.

Achievements
 Armenian Premier League
 Winner (1): 2013–14.
 Runner-up (5): 2003, 2006, 2007, 2010, 2018.

 Armenian Cup
 Winner (3): 1992, 2007, 2016.
 Runner-up (6): 2003, 2004, 2008, 2009, 2010, 2021–22

 Armenian Supercup
 Winner (1): 2014.
 Runner-up (5): 2004, 2007, 2009, 2010, 2016.

Current squad

Out on loan

Personnel

Technical staff

Management

Urartu-2

FC Banants' reserve squad play as FC Banants-2 in the Armenian First League. They play their home games at the training field with artificial turf of the Urartu Training Centre.

Managerial history
 Varuzhan Sukiasyan (1992–94)
 Poghos Galstyan (July 1, 1996 – June 30, 1998)
 Oganes Zanazanyan (2001–05)
 Ashot Barseghyan (2005–06)
 Nikolay Kiselyov (2006–07)
 Jan Poštulka (2007)
 Nikolay Kostov (July 1, 2007 – April 8, 2008)
 Nedelcho Matushev (April 8, 2008 – June 30, 2008)
 Kim Splidsboel (2008)
 Armen Gyulbudaghyants (Jan 1, 2009 – Dec 1, 2009)
 Ashot Barseghyan (interim) (2009)
 Stevica Kuzmanovski (Jan 1, 2010 – Dec 31, 2010)
 Rafael Nazaryan (Jan 1, 2011 – Jan 15, 2012)
 Volodymyr Pyatenko (Jan 17, 2013 – June 30, 2013)
 Zsolt Hornyák (July 1, 2013 – May 30, 2015)
 Aram Voskanyan (July 1, 2015 – Oct 11, 2015)
 Tito Ramallo (Oct 12, 2015 – Oct 3, 2016)
 Artur Voskanyan (Oct 3, 2016 – Aug 11, 2018)
 Ilshat Faizulin (Aug 12, 2018 –Nov 24, 2019)
 Aleksandr Grigoryan (Nov 25, 2019 –Mar 10, 2021)
 Robert Arzumanyan (10 March 2021–24 June 2022)
 Dmitri Gunko (27 June 2022–)

References

External links
 Official website 
 Banants at Weltfussball.de  

 
Urartu
Urartu
Urartu
Urartu",['South West Ultras fan club.'],819,multifieldqa_en,en,,c29e95ab6195406aceecf3874186150cb1b8b26db5bcd0e4," FC Urartu is an Armenian professional football team based in the capital Yerevan. They currently play in the Armenian Premier League. The club won the Armenian Cup three times, in 1992, 2007 and 2016. In early 2016, the Russia-based Armenian businessman Dzhevan Cheloyants became a co-owner of the club after purchasing the major part of the shares. The team was known as FC Banants until 1 August 2019, when it was officially renamed FC Urartsu. The stadium was officially opened in 2008 with a capacity of 3,600 seats. In 2011, the playing pitch was modernized and the capacity of the stadium was increased up to 4,860 seats (2,760 at the northern stand, 1,500 at the southern stand and 600 at the western stand). Banants Training Centre is the club's academy base located in the Malatia-Sebastia District. The most active group of fans is the South West Ultras fan club. Members of the fan club benefit from events organized by the club and many facilities of the Banants training centre, such as the mini football pitch, the club store and other entertainments. In addition to the main stadium, the centre houses 3 full-size training pitches, mini football pitches as well as an indoor facility. The current technical director of the academy is the former Russian footballer Ilshat Faizulin. They play their home games at the training field with artificial turf of the training centre. They also have a reserve squad play asFC Banants-2 in the First Armenian League. In 2013–2014, they won the Armenian Premier League for the first time in their history. In 2010, they were runners-up in the Armenian Supercup. They were runner-up on five occasions, in 2003, 2004, 2006, 2007, 2010, and 2014. They are currently playing their first season in the Premier League, having won the league three times in their previous four seasons. They have signed two Ukrainian players, Ugandan international, Noah Kasule, has been signed. In 2012, they signed a Ukrainian international, Samvel Melkonyan, who has played for FC Spartak Yerevans and FC Kotayk of Abovyan. In 2014, they also signed a Russian international, Alexei Kuznetsov, who played for Spartak in the Russian Premier League and the Armenian Super Cup. In 2007, Banants managed to lift the Armenian League Cup. Banants were also winners of the Armenian Champions League in 2007, 2008, 2009, 2010 and 2012. In 2016, they became the first Armenian club to win the Champions League. They will play their first game in the Europa League this season. The first game will be on Saturday, September 14, at the home ground of Ataturk, in the city of Tbilisi. The match will be played at the Stadion of the Tbilisian Stadium. The game is scheduled to start at 7pm local time (8pm BST) and will be followed by a Europa League match on Sunday, September 17, at 7.30pm local (7.30am BST). The match is scheduled for 8pm BST (8.30 local time) and 9pm GMT (9.30 am BST) The team will also play a friendly match in the same city on September 14. zhan Sukiasyan (1992–94) Poghos Galstyan (July 1, 1996 – June 30, 1998) Oganes Zanazanyan (2001 –05) Ashot Barseghyan (2005–06) Nikolay Kiselyov (2006–07) Jan Poštulka (2007) Stevica Kuzmanovski (Jan 1, 2010 – Dec 31, 2010) Rafael Nazaryan (Jan. 1, 2011 – Jan 15, 2012) Volodymyr Pyatenko (Jan 17, 2013 –June 30, 2013) Aleksandr Grigoryan (Nov 25, 2019 –Mar 10, 2021) Robert Arzumanyan  (10 March 2021–24 June 2022) Dmitri Gunko (27 June 2022–) Huldrych Voskanyan  (27 March 2021 –) Urartu (Urartu) (UrARTU) is an abbreviation of Urartus, an Armenian word for ""footballer"" or ""player"" and used to refer to a number of players in the national team's history. UrARTU is also the name of a football club in the country, Banants (Banants) is a club in which the players are known as ""Banants"" (pronounced ""banants"") or ""Banant"""
Is the ISR necessary for transgene reactivation?,"Current address: Division of Brain Sciences, Department of Medicine, Imperial College London, London, United Kingdom.
In a variety of species, reduced food intake, and in particular protein or amino acid (AA) restriction, extends lifespan and healthspan. However, the underlying epigenetic and/or transcriptional mechanisms are largely unknown, and dissection of specific pathways in cultured cells may contribute to filling this gap. We have previously shown that, in mammalian cells, deprivation of essential AAs (methionine/cysteine or tyrosine) leads to the transcriptional reactivation of integrated silenced transgenes, including plasmid and retroviral vectors and latent HIV-1 provirus, by a process involving epigenetic chromatic remodeling and histone acetylation. Here we show that the deprivation of methionine/cysteine also leads to the transcriptional upregulation of endogenous retroviruses, suggesting that essential AA starvation affects the expression not only of exogenous non-native DNA sequences, but also of endogenous anciently-integrated and silenced parasitic elements of the genome. Moreover, we show that the transgene reactivation response is highly conserved in different mammalian cell types, and it is reproducible with deprivation of most essential AAs. The General Control Non-derepressible 2 (GCN2) kinase and the downstream integrated stress response represent the best candidates mediating this process; however, by pharmacological approaches, RNA interference and genomic editing, we demonstrate that they are not implicated. Instead, the response requires MEK/ERK and/or JNK activity and is reproduced by ribosomal inhibitors, suggesting that it is triggered by a novel nutrient-sensing and signaling pathway, initiated by translational block at the ribosome, and independent of mTOR and GCN2. Overall, these findings point to a general transcriptional response to essential AA deprivation, which affects the expression of non-native genomic sequences, with relevant implications for the epigenetic/transcriptional effects of AA restriction in health and disease.
Copyright: © 2018 De Vito et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Data Availability: All relevant data are within the paper and its Supporting Information files. RNAseq data are available in the ArrayExpress database under the accession number E-MTAB-6452.
Funding: This study was funded by the Ajinomoto Innovation Alliance Program, (AIAP; https://www.ajinomoto.com/en/rd/AIAP/index.html#aiap) (to M.V.S and D.G), which is a joint research initiative of Ajinomoto Co., Inc., Japan. One of the authors [M.B.] is an employee of Ajinomoto Co., and his specific roles are articulated in the ‘author contributions’ section. The commercial funder provided support in the form of salary for author [M.B.] and some of the necessary research materials (medium for cell culture), but did not have any additional role in the study design, data collection and analysis, or preparation of the manuscript, and the authors had unrestricted access to the data. Due to a confidentiality agreement, the commercial funder participated only in the decision to publish the data obtained during the study, without any restriction.
Competing interests: This study was funded by Ajinomoto Co., Inc., Japan and one of the authors [M.B.] is an employee of this commercial funder. No other employment or consultancy relationships exist with the commercial funder, and no patents, products in development, or marketed products result from this study. The authors declare that no competing interests exist and that the commercial affiliation of one of the authors does not alter the adherence of authors to all PLOS ONE policies on sharing data and materials.
In animals, excessive, insufficient, or imbalanced nutrient availability is known to strongly impact on phenotype and health, both short and long-term, and across generations [1, 2]. In particular, studies in yeast, animal models and humans have shown that reduced food intake, reducing either overall calories, or only sugars, proteins, or even single amino acids (AA), such as Methionine (Met), may extend lifespan and healthspan, and reduce the risk of cancer and other age-related diseases [3–9]. In addition, fasting or specific AA deprivation have shown potential therapeutic applications, owing to their ability to directly reduce the growth of some tumor types [10, 11], sensitize cancer cells to chemo- or immunotherapy [12, 13], and allow efficient hematopoietic stem cell engraftment . However, little is known about the specific processes and molecular mechanisms mediating the roles of nutrient restriction in human health and longevity.
A properly balanced diet in metazoans contains optimal amounts of a subset of AA, which cannot be synthetized de novo and are therefore named essential amino acids (EAAs). In humans these include Met, Histidine (His), Isoleucine (Ile), Leucine (Leu), Lysine (Lys), Phenylalanine (Phe), Threonine (Thr), Tryptophan (Trp), and Valine (Val), while a few others are considered as semi-essential, such as Glutamine (Gln) and Tyrosine (Tyr) [15, 16]. Consistently, EAA deprivation triggers a cell-autonomous adaptive response, characterized by extensive metabolic and gene expression modifications, implementing biosynthetic, catabolic, and plasma membrane transport processes, aimed at reconstituting the full AA complement [17, 18]. The best known and conserved pathways responding to AA deprivation are triggered by mechanistic Target of Rapamycin Complex 1 (mTORC1) and General amino acid Control Non-derepressible 2 (GCN2) protein kinases [15, 19, 20]. Activation of mTORC1 requires in particular the presence of Gln, Arg and Leu, but also Met , which activate the kinase through sensors mainly acting upstream of Rag GTPases at lysosomal membranes . In turn, mTORC1 promotes cell growth, proliferation and anabolism upon activation, and translational attenuation and autophagy upon inhibition [19, 20].
By contrast, GCN2 is activated by deprivation of any individual EAA, by means of its histidyl-tRNA synthetase-related domain, which binds uncharged tRNAs accumulating during AA limitation [23, 24]. Upon activation, GCN2 phosphorylates and inhibits its only known downstream target, namely the eukaryotic Initiation Factor 2 α (eIF2α), thereby initiating the Integrated Stress Response (ISR). This leads to attenuation of general translation, and induction of a transcriptional/translational program, aimed at increasing stress resistance and restoring cell homeostasis, by upregulating a specific subset of genes, including Activating Transcription Factor 4 (ATF4) and C/EBP-Homologous Protein (CHOP) [25–27]. Thus, inhibition of mTORC1 and activation of GCN2 by AA restriction cooperate to attenuate general translation at the initiation step, increase catabolism and turnover, and enhance stress resistance to promote adaptation . However, how these processes eventually induce protective mechanisms against the alterations associated with aging, which include pervasive epigenetic and transcriptional changes [28, 29], remains largely unknown.
We previously reported the unexpected observation that prolonged deprivation of either Tyr, or of both Methionine and Cysteine (Met/Cys), triggers the selective and reversible reactivation of exogenous transcriptional units, including plasmids, retroviral vectors and proviruses, integrated into the genome and transcriptionally repressed by defensive mechanisms against non-native DNA sequences [30, 31]. This phenomenon was observed both in HeLa epithelial and ACH-2 lymphocytic human cells, and was independent of the transgene or provirus (Ocular Albinism type 1, OA1; Green Fluorescent Protein, GFP; Lysosomal-Associated Membrane Protein 1, LAMP1; Human Immunodeficiency Virus-1, HIV-1), or of the exogenous promoter driving their transcription, either viral (cytomegalovirus, CMV; Long Terminal Repeat, LTR) or human (Phospho-Glycerate Kinase 1, PGK1; Elongation Factor-1α, EF-1α) . Furthermore, this transgene reactivation response was not reproduced by serum starvation, activation of p38, or pharmacological inhibitors of mTOR (PP242 or rapamycin), sirtuins and DNA methylation. By contrast, it was induced by pan histone deacetylase (HDAC) inhibitors, and by selective inhibitors of class II HDACs . Consistently, we found that the mechanism responsible involves epigenetic modifications at the transgene promoter, including reduced nucleosome occupancy and increased histone acetylation, and is mediated in part by reduced expression of a class II HDAC, namely HDAC4 .
These findings indicate that AA deprivation induces a specific epigenetic and transcriptional response, affecting the expression of newly-integrated exogenous transgenes and proviruses, and suggesting that endogenous sequences sharing similar structural and functional features may represent a transcriptional target as well [30, 31]. In particular, transposable elements, such as LTR-retrotransposons (or endogenous retroviruses, ERVs), are genomic “parasites” anciently-integrated into the genome, and silenced by epigenetic mechanisms of mammalian cells against the spreading of mobile elements, eventually becoming ""endogenized"" during evolution [32, 33]. This raises the question of whether their expression is also sensitive to AA restriction. In addition, it remains unclear whether or not the transgene reactivation response is related to specific AA deprivations, and most importantly which is the AA sensing/signaling pathway involved, in particular whether the GCN2 kinase is implicated. Thus, here we used the reactivation of silenced transgenes in cultured cells, as a model to investigate a novel molecular pathway induced by imbalanced EAA starvation, implicated in the epigenetic/transcriptional regulation of exogenous non-native DNA sequences and possibly of other endogenous anciently-integrated genomic elements.
HeLa human epithelial carcinoma, HepG2 human hepatocellular carcinoma and C2C12 mouse skeletal muscle cells were maintained in DMEM containing glutaMAX (Invitrogen) and supplemented with 10% FBS (Sigma), 100 U/ml penicillin G (Invitrogen), 100 mg/ml streptomycin (Invitrogen), at 37°C in a 5% CO2 humidified atmosphere. Cell lines carrying integrated and partially silenced transgenes were also maintained in 600–1000 μg/ml G418.
The C2C12 cell line was provided by ATCC. HeLa and HepG2 cells were obtained by Drs. F. Blasi and G. Tonon at San Raffaele Scientific Institute, Milan, Italy, respectively, and were authenticated by Short Tandem Repeat (STR) profiling, using the Cell ID System kit (Promega), according to the manufacturer’s instructions. Briefly, STR-based multiplex PCR was carried out in a final volume of 25 μL/reaction, including 5 μL Cell ID Enzyme Mix 5X, 2.5 μL Cell ID Primer Mix 10X and 3 ng of template DNA. The thermal cycling conditions were: 1 cycle at 96°C for 2 min, followed by 32 cycles at 94°C for 30 sec, 62°C for 90 sec, and 72°C for 90 sec, and 1 cycle at 60°C for 45 sec. The following STR loci were amplified: AMEL, CSF1PO, D13S317, D16S539, D21S11, D5S818, D7S820, TH01, TPOX, vWA. Fragment length analysis of STR-PCR products was performed by Eurofins Genomics, using standard procedures of capillary electrophoresis on the Applied Biosystems 3130 XL sequencing machine, and assessment of the STR profile was performed at the online STR matching analysis service provided at http://www.dsmz.de/fp/cgi-bin/str.html.
Stable cell clones, expressing myc-tagged human OA1 (GPR143) or GFP transcripts, were generated using pcDNA3.1/OA1myc-His or pcDNA3.1/EGFP vectors . Briefly, HeLa, HepG2 and C2C12 cells were transfected using FuGENE 6 (Roche) and selected with 800, 1000, and 650 μg/ml of G418 (Sigma), respectively, which was maintained thereafter to avoid loss of plasmid integration. G418-resistant clones were isolated and analyzed for protein expression by epifluorescence and/or immunoblotting.
Full DMEM-based medium, carrying the entire AA complement, and media deprived of Met/Cys (both AAs), Met (only), Cys (only), Alanine (Ala), Thr, Gln, Val, Leu, Tyr, Trp, Lys and His were prepared using the Nutrition free DMEM (cat.#09077–05, from Nacalai Tesque, Inc., Kyoto, Japan), by adding Glucose, NaHCO3, and either all 20 AAs (for full medium) or 18–19 AAs only (for deprivations of two-one AAs). Single AAs, Glucose, and NaHCO3 were from Sigma. Further details and amounts utilized are indicated in S1 Table. All media were supplemented with 10% dialyzed FBS (Invitrogen), 100 U/ml penicillin G (Invitrogen), 100 mg/ml streptomycin (Invitrogen), and G418 as required. HBSS was from Invitrogen. Cells were seeded at 10–30% of confluency; cells to be starved for 48 h were plated 2–3 times more confluent compared to the control. The following day, cells were washed and cultured in the appropriate medium, with or without EAA, for 24–48 h.
L-Histidinol (HisOH), PP242, Integrated Stress Response Inhibitor (ISRIB), SP600125, Cycloheximide (CHX) were from Sigma; Salubrinal was from Tocris Bioscience; U0126 was from Promega. Drugs were used at the following final concentrations: HisOH at 4–16 mM; PP242 at 1–3 μM; ISRIB at 100 nM; SP600125 at 20 μM in HepG2 cells and 50 μM in HeLa cells; Cycloheximide (CHX) at 50 ug/ml in HepG2 cells and 100 ug/ml in HeLa cells; Salubrinal at 75 μM; U0126 at 50 μM. Vehicle was used as mock control. Treatments with drugs to be tested for their ability to inhibit transgene reactivation (ISRIB, SP600125 and U0126) were initiated 1h before the subsequent addition of L-Histidinol (ISRIB) or the subsequent depletion of Met/Cys (SP600125 and U0126).
Total RNA was purified using the RNeasy Mini kit (Qiagen), according to manufacturer’s instructions. RNA concentration was determined by Nanodrop 8000 Spectrophotometer (Thermo Scientific). Equal amount (1 μg) of RNA from HeLa, HepG2 and C2C12 cells was reverse transcribed using the SuperScript First-Strand Synthesis System for RT-PCR (Invitrogen) using oligo-dT as primers, and diluted to 5 ng/μl. The cDNA (2 μl) was amplified by real-time PCR using SYBR green Master Mix on a Light Cycler 480 (Roche), according to manufacturer’s instructions. The thermal cycling conditions were: 1 cycle at 95°C for 5 min, followed by 40–45 cycles at 95° for 20 sec, 56° for 20 sec and 72° for 20 sec. The sequences, efficiencies and annealing temperatures of the primers are provided in S2 Table. Data were analyzed with Microsoft Excel using the formula EtargetΔct target (control-sample) /EreferenceΔct reference (control-sample) . Reference genes for normalizations were ARPC2 (actin-related protein 2/3 complex, subunit 2) for HeLa and HepG2 cells; and Actb (actin beta) for C2C12 cells, unless otherwise indicated.
siRNA (Mission esiRNA, 200 ng/μL; Sigma) against ATF4 and GCN2 were designed against the targeted sequences NM_001675 and NM_001013703, respectively. Cells seeded in 6-well plates were transfected with 1 μg of siRNAs and 5 μL of Lipofectamine 2000 (Invitrogen), following manufacturer’s instructions, at day 1 post-plating for ATF4 and at day 1 and 2 post-plating for GCN2. At day 2 (ATF4) or 3 (GCN2) post-plating, cells were washed and cultured in medium in the absence or presence of HisOH 4 mM for 6 h. siRNAs against RLuc (Sigma), targeting Renilla Luciferase, were used as negative control. For CRISPR/Cas9 experiments, we used the “all-in-one Cas9-reporter” vector, expressing GFP (Sigma), which is characterized by a single vector format including the Cas9 protein expression cassette and gRNA (guide RNA). GFP is co-expressed from the same mRNA as the Cas9 protein, enabling tracking of transfection efficiency and enrichment of transfected cells by fluorescence activated cell sorting (FACS). The human U6 promoter drives gRNA expression, and the CMV promoter drives Cas9 and GFP expression. The oligonucleotide sequences for the three gRNAs targeting GCN2 exon 1 or 6 are listed in S2 Table. We transfected HeLa and HepG2 cells with these plasmids individually (one plasmid one guide) and sorted the GFP-positive, transfected cells by FACS. Screening GCN2-KO clones was performed by western blotting. In the case of HepG2-OA1 cells, two rounds of selection were necessary to obtain three GCN2-KO clones by using a guide RNA against exon 1. Compared to the original HepG2-OA1 cell line and to the clone resulting from the first round of selection (185#27), the selected clones E23, F22 and F27 showed a very low amount—if any—of residual GCN2 protein (see results).
Genomic DNA of HeLa and HepG2 cells was purified using DNeasy Blood and Tissue kit (Qiagen), according to the manufacturer’s instructions. DNA concentration was determined by Nanodrop 8000 Spectrophotometer (Thermo Scientific). PCR conditions for amplification of GCN2 exon 1 and 6 were as follows: 1 cycle at 94°C for 5 min, followed by 35 cycles at 94°C for 40 sec, 56°C for 40 sec, and 72°C for 40 sec; and a final extension step of 5 min at 72°C. The primer sequences are provided in S2 Table.
For OA1, western immunoblotting was carried out as described . For GCN2, cells were lysed in RIPA buffer, boiled at 95°C for 5 min and resolved on a 7.5% polyacrylamide gel; immunoblotting was then performed following standard procedures. Primary Abs were as follows: anti-human OA1, previously developed by our group in rabbits ; anti-GCN2 (Cell Signaling, Cat. #3302).
Statistical analyses were performed using Microsoft Excel for Mac (version 15.32, Microsoft) for Student’s t-test; or GraphPad Prism (version 5.0d for Mac, GraphPad Software, Inc.) for one-way analysis of variance (ANOVA), followed by Dunnett’s or Tukey’s multiple comparisons post-tests. T-test was used when only two means, typically sample versus control, were compared, as specified in the figure legends. One way ANOVA was used for multiple comparisons, followed by either a Dunnett’s (to compare every mean to a control mean), or a Tukey’s (to compare every mean with every other mean) post-test, by setting the significance level at 0.05 (95% confidence intervals). Both tests compare the difference between means to the amount of scatter, quantified using information from all the groups. Specifically, Prism computes the Tukey-Kramer test, allowing unequal sample sizes. P values in Figures are generally referred to comparison between a sample and the control (full medium/mock), and are indicated as follows: *P<0.05, **P<0.01, ***P<0.001. Comparisons not involving the control are similarly indicated, by a horizontal line at the top of the graphs, encompassing the two samples under analysis. Additional details regarding the specific experiments are reported in the Figure Legends.
To examine the expression behavior of genomic repeats upon AA starvation, we performed a transcriptomic analysis taking advantage of an intramural sequencing facility. HeLa-OA1 cells were cultured in normal medium (for 6-30-120 hours) or in absence of Met/Cys (for 6-15-30-72-120 hours). Total RNA was prepared using Trizol (Sigma) to preserve transcripts of both small and long sizes (from Alu, of about 0.3 kb, to Long Interspersed Nuclear Elements, LINEs, and ERVs, up to 6–8 kb long), DNase treated to avoid contamination of genomic DNA, and processed for NGS sequencing by Ovation RNA-Seq System V2 protocol and HiSeq 2000 apparatus. Raw sequence data (10–20 M reads/sample) were aligned to the human genome (build hg19) with SOAPSplice . Read counts over repeated regions, defined by RepeatMasker track from UCSC genome browser , were obtained using bedtools suite . Normalization factors and read dispersion (d) were estimated with edgeR , variation of abundance during time was analyzed using maSigPro package , fitting with a negative binomial distribution (Θ = 1/d, Q = 0.01), with a cutoff on stepwise regression fit r2 = 0.7. Read counts were transformed to RPKM for visualization purposes. The expression of the OA1 transgene and HDAC4, which are progressively up- and down-regulated during starvation, respectively , were used as internal controls.
For genomic repeat analysis, reads belonging to repetitive elements were classified according to RepeatMasker and assigned to repeat classes (total number in the genome = 21), families (total number in the genome = 56) and finally subfamilies (total number in the genome = 1396), each including a variable number of genomic loci (from a few hundred for endogenous retroviruses, up to several thousand for Alu). Repeat subfamilies were then clustered according to their expression pattern in starved vs control cells, by maSigPro using default parameters, and repeats classes or families that are significantly enriched in each cluster, compared to all genomic repeats, were identified by applying a Fisher Exact test (using scipy.stats, a statistical module of Python). Alternatively, differentially expressed repeat subfamilies were identified by averaging three time points of starvation (15-30-72 h) and controls. Repeats significantly up- or downregulated (104 and 77, respectively) were selected based on a P value <0.05 (unpaired two-tailed Student’s t-test, assuming equal variance), and analyzed for their class enrichment by a Fisher Exact test as described above.
For gene set enrichment analysis of Met/Cys deprived vs control HeLa cells, differentially expressed genes were selected considering three time points of starvation (15-30-72 h) and controls, based on a P value <0.05 (unpaired two-tailed Student’s t-test, assuming equal variance) and a fold change >2. This led to a total of 2033 differentially expressed genes, 996 upregulated and 1037 downregulated. The enrichment analysis was performed separately for up and down regulated genes, or with all differentially expressed genes together (both), using the KEGG database. The analysis was performed with correction for the background of all expressed genes (about 13600 genes showing an average expression over 3 starvation and 3 control samples of at least 5 counts) and by using default parameters (adjusted P value and q-value cut-off of <0.05 and 0.2, respectively). Differentially expressed genes were also selected considering all starvation time points, as with genomic repeats, by maSigPro using default parameters, and a fold change of at least 1.5, leading to similar enrichment results (not shown). RNAseq gene expression data are available in the ArrayExpress database under the accession number E-MTAB-6452.
To provide proof-of-principle that AA starvation may affect the expression of transposable elements, we performed an RNAseq analysis of the previously described HeLa-OA1 cells, carrying an integrated and partially silenced OA1 transgene . Since the reactivation of the transgene by starvation is a progressive phenomenon , we performed a time-course experiment, where each time point represents one biological sample, rather than a biological triplicate of a single time point. To this aim, cells were cultured either in normal medium, or in absence of Met/Cys for different time points (6-15-30-72-120 hours), resulting in the progressive upregulation of the OA1 transgene during starvation (Fig 1A and 1B), consistent with previously published results . The expression of genomic repeats was determined according to RepeatMasker annotation and classification into classes, families, and subfamilies. Repeat species were then subjected to differential expression and enrichment analyses in starved vs control conditions. Out of 1396 annotated repeat subfamilies, 172 species displayed a differential expression profile during starvation.
Fig 1. Exogenous transgene and endogenous retroviruses are upregulated in Met/Cys-deprived HeLa cells.
(A,B) Exogenous integrated transgene (OA1) mRNA abundance in HeLa-OA1 cells, cultured in Met/Cys-deprived medium for the indicated time points, and analyzed by RNAseq (A), or RT-qPCR (B), compared to full medium. Data represent RPKM (A), or mean ± SD of 2 technical replicates, expressed as fold change vs. control (full medium at 6 h = 1) (B). (C) Clustering of 172 genomic repeat subfamilies, differentially expressed upon starvation, according to their expression profile. (D) Class distribution of repeat subfamilies belonging to differential expression clusters, compared to all genomic repeat subfamilies (first column). Class DNA includes DNA transposons; SINE includes Alu; LINE includes L1 an L2; LTR includes endogenous retroviruses and solitary LTRs; Satellite includes centromeric acrosomal and telomeric satellites; Others includes SVA, simple repeats, snRNA, and tRNAs. LTR-retroelements are significantly enriched among repeats that are upregulated upon starvation, while LINEs are significantly enriched among repeats that are downregulated. *P<0.05, ***P<0.001 (Fisher exact test).
As shown in Fig 1C, the clustering of differentially expressed repeats, according to their expression pattern, reveals profiles comparable to the behavior of the transgene in the same conditions, i.e. upregulation upon starvation and no change in regular medium (Cluster 1 and 2). In particular, Cluster 1 contains sequences that, similarly to the OA1 transgene, are progressively upregulated upon starvation (Fig 1A and 1C) , while Cluster 2 contains sequences that are upregulated at early time points. Interestingly, repeat families that are significantly enriched in these two clusters belong mostly to the group of LTR-retrotransposons, including ERV1, ERVK, ERVL, ERVL-MaLR and other LTR sequences (Fig 1D; S1A and S2A Figs). By contrast, DNA transposons (such as TcMar-Tigger) and L1 non-LTR retrotransposons are enriched among repeats that are downregulated during starvation, particularly at late time points (Clusters 3 and 4) (Fig 1D; S1A and S2A Figs). Consistent results were obtained by selecting significantly up- or downregulated genomic repeats (overall 181 species), based on their average expression out of three time points of starvation (15-30-72 h, when the transgene upregulation is more homogeneous) and controls, and on a P value <0.05 (S1B and S2B Figs). These findings suggest that EAA starvation induces genome-wide effects involving repetitive elements, and that—among major repeat classes—it upregulates in particular the expression of ERVs.
In addition, to obtain a general overview of main gene pathways changing their expression together with the transgene during AA starvation, we performed gene expression and enrichment analyses of regular genes, by considering three time points of starvation (15-30-72 h) and controls. Differentially expressed genes were selected based on a P value <0.05 and a fold change between means of at least 2, and analyzed with the EnrichR tool . As shown in Fig 2 and S1 File, enrichment analyses against the KEGG and Reactome databases reveals a predominance of downregulated pathways, namely ribosome and translation, proteasome, AA metabolism, oxidative phosphorylation and other pathways related to mitochondrial functions, which are affected in Huntington, Alzheimer and Parkinson diseases (http://www.genome.jp/kegg/pathway.html). In particular, a large fraction of ribosomal protein mRNAs is downregulated upon Met/Cys starvation (Fig 2A and 2C; S1 File), consistent with the notion that their genes–despite being scattered throughout the genome—are coordinately expressed in a variety of conditions . This reduced expression may depend on multiple pathways that control ribosome biogenesis in response to external stimuli, including the downregulation of Myc activity , the downregulation of mTORC1 [42, 44], or possibly the activation of the ISR, as described in yeast . By contrast, upregulated genes show a significant enrichment for transcription and gene expression (Fig 2B). Similar results were obtained by the Gene Ontology Biological Process (GO-BP) database (S1 File), overall indicating a general downregulation of translation and metabolism, and upregulation of transcription, during the time interval of Met/Cys starvation corresponding to the transgene upregulation.
Fig 2. Gene set enrichment analysis of Met/Cys-deprived HeLa cells.
Differentially expressed genes between three time points of starvation (15-30-72 h) and controls were selected based on a P value <0.05 and a fold change of at least 2, leading to a total of 996 upregulated, and 1037 downregulated genes. The enrichment analysis was performed separately for up and down regulated genes, using the EnrichR tool and the KEGG (A) and REACTOME (B, C) databases. Ranking is based on the combined score provided by EnrichR, and categories are displayed up to 20 items with an Adjusted P value <0.05. No significant categories were found with upregulated genes against the KEGG database. All data are shown in S1 File. The enrichment analysis using all differentially expressed genes together did not reveal any additional enriched process.
To characterize the pathway leading to the reactivation of silenced transgenes, we used HeLa-OA1 and HeLa-GFP cells, as described . In addition, to test cell types relevant for AA metabolism, such as liver and muscle, we generated clones of HepG2 human hepatoma and C2C12 mouse skeletal muscle cells, stably transfected with plasmids for OA1 and GFP transgenes, respectively (HepG2-OA1 and C2C12-GFP cells; endogenous OA1 is not expressed in any of these cell types). In all cases, the integrated transgenes are under the control of the CMV promoter in the context of a pcDNA3.1 plasmid, are partially silenced, and can be efficiently upregulated by HDAC inhibitors (trichostatin A, TSA; ref.  and S3A, S3B and S4A Figs), indicating that their expression is controlled at least in part by epigenetic mechanisms, as previously described .
To establish whether the reactivation response results from the shortage of specific AAs only, such as Met/Cys, or it is triggered by any AA deprivations, we cultured HeLa-OA1, HeLa-GFP, HepG2-OA1 and C2C12-GFP cells for 24–48 hours with a battery of media deprived of EAAs or semi-EAAs, including Met/Cys, Thr, Gln, Val, Leu, Tyr, Trp, Lys, and His. As negative controls, cells were cultured in full medium, carrying the entire AA complement, and in a medium deprived of Ala, a non-essential AA. The expression of the transgene transcript was then evaluated by RT-qPCR. As shown in Fig 3, and in S3C and S4B Figs, most EAA-deficiencies induced reactivation of the OA1 or GFP transgenes in all four cell lines, with the notable exception of Trp deprivation, which consistently resulted in no or minimal reactivation of the transgenes. Indeed, despite some variability, Met/Cys deficiency, but also Thr, Val, Tyr, and His deprivation always gave an efficient response, while Leu, Gln and Lys elicited evident responses in some cases, but not in others. Depletion of Phe gave results comparable to Tyr deprivation, however it significantly altered multiple reference genes used for normalization and therefore was eventually omitted from the analysis (not shown). Finally, in the above experiments we used a combined Met/Cys deficiency, to avoid the potential sparing of Met by Cys  and for consistency with our previous studies . Nevertheless, the analysis of single Met or Cys starvation, both at the protein and transcript levels, revealed an exclusive role of Met deprivation in transgene reactivation, consistent with the notion that Cys is not an EAA (S3D and S3E Fig).
Fig 3. EAA deprivation induces reactivation of silent transgenes in HeLa and HepG2 cells.
Relative transgene (OA1) and CHOP mRNA abundance in HeLa-OA1 (A) and HepG2-OA1 (B) cells, cultured in various AA-deprived media for 48 h and 24 h, respectively, compared to full medium. Mean ± SEM of 3 independent experiments. Data are expressed as fold change vs. control (full medium = 1). *P<0.05, **P<0.01, ***P<0.001 (one way ANOVA, followed by Dunnett’s post-test vs. full medium).
Collectively, these results indicate that transgene reactivation by EAA starvation is reproducible with most EAAs, shared by different cell types (epithelium, liver, and skeletal muscle), and conserved in different mammalian species (human, mouse).
mTORC1 inhibition and GCN2 activation trigger the best-known signaling pathways responding to AA starvation . We previously showed that inhibition of mTORC1 is not sufficient to reproduce transgene reactivation in HeLa cells . By contrast, the involvement of GCN2 and the ISR, including the downstream effectors ATF4 and CHOP, has never been tested. In addition, this pathway has been typically assessed in transient assays, lasting for a few hours, which may not be comparable with the prolonged starvation conditions necessary to reactivate the transgene expression (at least 15–24 h). Thus, we tested whether CHOP expression was upregulated upon incubation of HeLa-OA1, HepG2-OA1 and C2C12-GFP cells in media deprived of different EAAs for 24–48 h.
As shown in Fig 3 and S4B Fig, we found that CHOP expression is increased in all EAA-starvation conditions, but not in the absence of Ala, in all tested cell lines. Similar, yet less pronounced, results were obtained with ATF4, consistent with the notion that activation of this transcription factor is mainly mediated by translational upregulation (not shown) [15, 26]. However, the upregulation of CHOP does not parallel quantitatively that of the transgene, neither appears sufficient to induce it. In fact, CHOP is highly upregulated even upon Trp starvation, which consistently results in no or minimal reactivation of the transgenes (compare CHOP with OA1 or GFP expression; Fig 3 and S4B Fig). Thus, while the ISR appears widely activated upon EAA starvation, the upregulation of its downstream effector CHOP only partly correlates with transgene reactivation and may not be sufficient to induce it.
The activation of the ISR upon AA starvation suggests that GCN2 may be involved in the transgene reactivation response. Therefore, we tested whether direct pharmacological activation of this kinase is sufficient to trigger the transgene reactivation similarly to starvation. In addition, we used pharmacological inhibitors of mTOR to corroborate previous negative results in HeLa cells  in the other cell lines under study. To this aim, HeLa-OA1 or GFP, HepG2-OA1 and C2C12-GFP cells were cultured in the presence of different concentrations of PP242 (mTOR inhibitor) or L-Histidinol (GCN2 activator, inhibiting tRNAHis charging by histidyl-tRNA synthetase), either alone or in combination for 24 h, compared to Met/Cys-deprived and full medium. As shown in Fig 4 and S5 Fig, while inhibition of mTORC1 consistently leads to minor or no effects, in agreement with previous findings , treatment with L-Histidinol results in efficient reactivation of the transgene in HepG2-OA1 and C2C12-GFP cells, but not in HeLa cells.
Fig 4. mTOR inhibition and GCN2 activation differently affect transgene expression in HeLa and HepG2 cells.
Relative transgene (OA1) and CHOP mRNA abundance in HeLa-OA1 (A) and HepG2-OA1 (B) cells, cultured in Met/Cys-deprived medium, or in the presence of PP242 (mTOR inhibitor; 1–3 μM) or L-Histidinol (HisOH, GCN2 activator; 4–16 mM), either alone or in combination for 24–48 h, compared to full medium. Mean ± SEM of 4 (A) or 3 (B) independent experiments. Data are expressed as fold change vs. control (full medium = 1). *P<0.05, **P<0.01, ***P<0.001 (one way ANOVA, followed by Dunnett’s post-test vs. full medium). PP-1 and PP-3, PP242 at 1 and 3 μM, respectively; HisOH-4 and HisOH-16, L-Histidinol at 4 and 16 mM, respectively.
Specifically, L-Histidinol is not effective in HeLa-OA1 and HeLa-GFP cells, either alone or in combination with PP242 (Fig 4A and S5A Fig), or by using different concentrations of the drug, with or without serum (not shown). In these cells, L-Histidinol appears also unable to trigger the ISR, as indicated by lack of CHOP upregulation, possibly due to their different sensitivity to the drug. These findings are consistent with previous reports, describing the use of L-Histidinol in HeLa cells in conditions of low His concentration in the culture medium , which would resemble AA starvation in our system and therefore may not be applicable. Thus, even though the amount of the amino alcohol was adapted to exceed 20 to 80 times that of the amino acid, as described , HeLa cells may be resistant or able to compensate.
In contrast, in other cell types, L-Histidinol has been utilized in regular DMEM, to mimic the AA response triggered by DMEM lacking His [48, 49]. Consistently, in HepG2-OA1 cells, L-Histidinol is sufficient to elicit extremely high levels of transgene reactivation, and its combination with PP242 results in additive or even synergistic effects, possibly due to an indirect effect of mTOR inhibition on GCN2 activity (Fig 4B) [50, 51]. Similarly, C2C12-GFP cells efficiently reactivate the transgene upon treatment with L-Histidinol, but not PP242 (S5B Fig). However, differently from HepG2-OA1 cells, simultaneous treatment of C2C12-GFP cells with L-Histidinol and PP242 does not lead to synergistic effects. Consistent with stimulation of the ISR, CHOP and to a minor extent ATF4 are upregulated by L-Histidinol in both cell lines, yet their expression levels show only an incomplete correlation with those of the transgene (Fig 4B, S5B Fig, and not shown).
The finding that GCN2 activation by L-Histidinol is sufficient to reactivate the transgenes in both HepG2-OA1 and C2C12-GFP cells pointed to this kinase, and to the downstream ISR, as the pathway possibly involved in the EAA starvation response. Thus, we investigated whether the ISR is sufficient to trigger upregulation of the OA1 transgene in HepG2-OA1 cells by pharmacological means. As CHOP expression does not correspond quantitatively and is not sufficient to induce transgene reactivation, we tested the role of the core upstream event of the ISR, namely the phosphorylation of eIF2α , which can be induced by pharmacological treatments, independent of GCN2 (Fig 5A). To this aim, we used Salubrinal, a specific phosphatase inhibitor that blocks both constitutive and ER stress-induced phosphatase complexes against eIF2α, thereby increasing its phosphorylation . We found that, while the ISR is activated upon Salubrinal treatment, as shown by increased CHOP expression, it does not induce OA1 transgene reactivation (Fig 5B).
Fig 5. The ISR is neither sufficient nor necessary to induce transgene reactivation in HepG2 cells.
(A) Schematic representation of GCN2 activation by AA starvation, resulting in phosphorylation of eIF2a and initiation of the downstream ISR. In addition to GCN2, the ISR may be activated by other eIF2a kinases (PKR, HRI and PERK; not shown in the picture). (B) Relative transgene (OA1) and CHOP mRNA abundance in HepG2-OA1 cells treated for 24 h with Salubrinal (a drug that induces the ISR by inhibiting the dephosphorylation of eIF2α; 75 μM), compared to full medium. Mean ± range of two experiments. Data are expressed as fold change vs. control (DMEM = 1). *P<0.05 (paired two-tailed Student’s t-test vs. control). (C) Relative transgene (OA1) and CHOP mRNA abundance in HepG2-OA1 cells treated for 6 h with L-Histidinol (HisOH, GCN2 activator; 4 mM), in the absence or presence of ISRIB (a drug that bypasses the phosphorylation of eIF2α, inhibiting triggering of the ISR; 100 nM). Mean ± range of two experiments. Data are expressed as fold change vs. control (DMEM = 1). **P<0.01, ***P<0.001 (one way ANOVA, followed by Tukey’s post-test; P values refer to comparisons vs. control, unless otherwise indicated). (D) Relative transgene (OA1) and ATF4 mRNA abundance in HepG2-OA1 cells transfected with control (CTRL) or anti-ATF4 siRNAs, and incubated in the presence or absence of L-Histidinol (HisOH, GCN2 activator; 4 mM) for 6 h. Mean ± range of two experiments. Data are expressed as fold change vs. control (w/o HisOH = 1, top; control siRNA = 1, bottom). *P<0.05 (one way ANOVA, followed by Tukey’s post-test; P values refer to comparisons vs. control, unless otherwise indicated).
To test whether the ISR is necessary to trigger the transgene response to L-Histidinol, we used the chemical compound ISRIB, which inhibits the activation of the ISR, even in the presence of phosphorylated eIF2α, likely by boosting the activity of the guanine-nucleotide exchange factor (GEF) for eIF2α, namely eIF2B [53, 54]. HepG2-OA1 cells were stimulated with L-Histidinol, either in the presence or absence of ISRIB. As shown in Fig 5C, while the expression of CHOP is inhibited by ISRIB, as expected, the reactivation of the OA1 transgene is not affected. In addition, knockdown of the closest eIF2α downstream effector ATF4 by siRNAs does not interfere with the reactivation of the OA1 transgene by L-Histidinol (Fig 5D). Together, these data suggest that eIF2α phosphorylation and the downstream ISR pathway are neither sufficient nor necessary to induce transgene reactivation.
To definitively establish if GCN2 is necessary to trigger the transgene reactivation response to EAA starvation, we directly suppressed its expression by CRISPR/Cas9-mediated knock-out (KO). We generated three independent GCN2-KO clones from the parental HeLa-OA1 cell line, by using three different guide RNAs, two against exon 1 (clones 183#11 and 185#5), and one against exon 6 (clone 239#1) of the GCN2 gene. Genomic characterization confirmed the presence of mutations on both alleles of exon 1 of the GCN2 gene in clone 183#11, and on both alleles of exon 6 in clone 239#1; by contrast, clone 185#5 showed multiple alleles in exon 1, consistent with the presence of two cell populations, and was not characterized further at the genomic level (S6 Fig). None of these clones express GCN2 at the protein level, as shown by immunoblotting (Fig 6A). To test the GCN2-KO cells for their ability to respond to EAA starvation, parental HeLa-OA1 cells and the three GCN2-KO clones were cultured in media deprived of Met/Cys or Thr (corresponding to the most effective treatments in this cell line; see Fig 3A) for 24–48 h and transgene expression was assessed by RT-qPCR. We found that the reactivation of the OA1 transgene is neither abolished, nor reduced by KO of GCN2, thus excluding that this kinase is necessary for the response to EAA starvation in HeLa-OA1 cells (Fig 6B and 6C).
Fig 6. GCN2 knockout does not interfere with transgene reactivation in HeLa cells.
(A) Immunoblotting of protein extracts from the HeLa-OA1 parental cell line and GCN2-KO clones 183#11, 185#5 and 239#1, immunodecorated with anti-GCN2 antibody. Arrow, GCN2 specific band. Ponceau staining was used as loading control. (B, C) Relative transgene (OA1) mRNA abundance in HeLa-OA1 cells and GCN2-KO clones, cultured in Met/Cys (B) or Thr (C) deprived medium for 24 h or 48 h, respectively, compared to full medium. Mean ± SD of 3 technical replicates from 1 experiment. Data are expressed as fold change vs. control (full medium = 1). Since independent clones may display variable reactivation responses (e.g. due to different levels of transgene expression in basal conditions), the results are not shown as means of the three clones, but as separate replicates.
Similarly, we generated GCN2-KO clones from the parental HepG2-OA1 cell line by the same strategy. By using a guide RNA against exon 1 of the GCN2 gene, we obtained three independent GCN2-KO clones, namely E23, F22 and F27. Genomic characterization confirmed the presence of mutations on both alleles of exon 1 of the GCN2 gene in clone F27 (S7 Fig) and all three clones showed a very low amount—if any—of residual GCN2 protein, compared to the original HepG2-OA1 cell line (Fig 7A). To assess the ability of GCN2-KO cells to reactivate the transgene upon starvation, we cultured parental HepG2-OA1 cells and the three GCN2-KO clones in media deprived of Met/Cys or His (corresponding to the most effective treatments in this cell line; see Fig 3B) for 24 h, and evaluated the transgene expression by RT-qPCR. As shown in Fig 7B and 7C, we found that the reactivation of the OA1 transgene is neither abolished, nor reduced by KO of GCN2, as in HeLa cells. To further confirm this result, we knocked-down GCN2 by RNA interference (RNAi), and incubated the cells with or without L-Histidinol for 6 h. As shown in Fig 8, treatment of HepG2-OA1 cells with L-Histidinol results in efficient transgene reactivation, even upon significant GCN2 downregulation, both at the mRNA and protein levels. Taken together, these data strongly support the conclusion that GCN2 is not necessary for transgene reactivation in response to EAA starvation, either in HeLa or in HepG2 cells.
Fig 7. GCN2 knockout does not interfere with transgene reactivation in HepG2 cells.
(A) Immunoblotting of protein extracts from the HepG2-OA1 parental cell line and GCN2-KO clones 185#27, E23, F22, F27, immunodecorated with anti-GCN2 antibody. Clone 185#27 results from the first round of selection, and was used to generate clones E23, F22, F27. Arrow, GCN2 specific band. For GCN2 protein quantification, Ponceau staining was used as loading control and data are expressed as fold change vs. parental cell line (= 1). (B, C) Relative transgene (OA1) mRNA abundance in HepG2-OA1 cells and GCN2-KO clones, cultured in Met/Cys (B) or His (C) deprived medium for 24 h, compared to full medium. Mean ± SD of 3 technical replicates from 1 experiment.","['No, it is not necessary.']",6900,multifieldqa_en,en,,1d46294ee8fcc0a64828778b04198fcbe4f75841775e1205," In a variety of species, reduced food intake, and in particular protein or amino acid (AA) restriction, extends lifespan and healthspan. However, the underlying epigenetic and/or transcriptional mechanisms are largely unknown, and dissection of specific pathways in cultured cells may contribute to filling this gap. We show that the deprivation of methionine/cysteine also leads to the transcriptional upregulation of endogenous retroviruses, suggesting that essential AA starvation affects the expression not only of exogenous non-native DNA sequences, but also of. anciently-integrated and silenced parasitic elements of the genome. Overall, these findings point to a general transcriptional response to essential AA deprivation, which affects the. expression of non- native genomic sequences, with relevant implications for the. epigenetic/transcriptional effects of AA restriction in health and disease. The authors declare that no competing interests exist and that the commercial affiliation of one of the authors does not alter the adherence of authors to all PLOS ONE policies on sharing data and materials. The study was funded by the Ajinomoto Innovation Alliance Program, (AIAP; https://www.ajinomoto.com/en/rd/AIAP/index.html#aiap) (to M.V.S and D.G), which is a joint research initiative of A Jinomoto Co., Inc., Japan. The author had unrestricted access to the data. Due to the confidentiality agreement, the commercial funder participated only in the decision to publish the data obtained during the study, without any restriction. No other employment or consultancy relationships exist with the commercial. funder, and no patents, products in development, or marketed products result from this study. This is an open access article distributed under the terms of the Creative Commons License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. The research was conducted by the Department of Medicine, London, United Kingdom, and the author is the author of the book. The book was published by the University of Cambridge, with the author’s name on the cover and the publication date on the back. It is published under the ‘author contributions’ section of the “Research by Authors’ “ section. It was written by the authors for the purposes of ‘research by authors’ and “researchers”, and has been published by MIT Press under the name “MIT Press.” The author has no affiliation with any of the companies involved in the study. It has been released under the open access license of MIT Press, and is available for download from the MIT Press website. The manuscript is available in three versions: the English, the French and the German versions, and it is available on the MIT press website. It can be downloaded by clicking here. ability is known to strongly impact on phenotype and health, both short and long-term, and across generations. Little is known about the specific processes and molecular mechanisms mediating the roles of nutrient restriction in human health and longevity. The best known and conserved pathways responding to AA deprivation are triggered by mechanistic Target of Rapamycin Complex 1 (mTORC1) and General amino acid Control Non-derepressible 2 (GCN2) protein kinases. We previously reported the unexpected observation that prolonged deprivation of either Tyr, or of both Methionine and Cysteine (Met/Cys), triggers the selective and reversible reactivation of exogenous transcriptional units, including plasmids, retroviral vectors and proviruses. This phenomenon was observed both in HeLa epithelial and ACH-2 lymphtic human cells. It was independent of the transgene or provirus (OA type 1, OA Fluorescent Protein, and OA fluorescent Protein 1, LAMP) that was integrated into the genome and transcriptionally repressed by defensive mechanisms against non-native DNA sequences [30, 31]. We found that the phenomenon was also observed in human cells that were infected with HIV and other viruses. We conclude that AA deprivation triggers a cell-autonomous adaptive response, characterized by extensive metabolic and gene expression modifications, implementing biosynthetic, catabolic, and plasma membrane transport processes, aimed at reconstituting the full AA complement [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 74, 75, 80, 78, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 111, 116 and 113, 112 and 113 are all essential amino acids (EAAs) in humans. In humans these include Met, Histidine (His), Isoleucine (Ile), Leukine (Leu), Lysine (Lys), Phenylalanine (Phe), Threonine (Thr), Tryptophan (Trp), and Valine (Val), while a few others are considered as semi-essential, such as Glutamine (Gln) and Tyrosine (Tyr) [15, 16]. Consistently, EAA deprivation triggers the induction of a transcriptional/translational program aimed at increasing stress resistance and restoring cell homeostasis, by upregulating a specific subset of genes, including Activating Transcription Factor 4 (ATF4) and C/EBP-Homologous Protein (CHOP) This transgene reactivation response was not reproduced by serum starvation, activation of p38, or pharmacological inhibitors of mTOR (PP242 or rapamycin), sirtuins and DNA methylation. By contrast, it was induced by pan histone deacetylase (HDAC) inhibitors, and by selective inhibitors of class II HDACs. We found that the mechanism responsible involves epigenetic modifications at the transGene promoter, including reduced nucleosome occupancy and increased histone acetylation. These findings indicate that AA deprivation induces a specific epigenetic and transcriptional response, affecting the expression of newly-integrated exogenous transgenes and proviruses. This suggests that endogenous sequences sharing similar structural and functional features may represent a transcriptional target as well [30, 31]. In particular, transposable elements, such as LTR-retrotransposons (or endogenous retroviruses, ERVs), are genomic “parasites” anciently-integration into the genome, and silenced by epigenetic mechanisms of mammalian cells against the spreading of mobile elements, eventually becoming ""endogenized"" during evolution. This raises the question of whether their expression is also sensitive to AA restriction. In addition, it remains unclear whether or not the trans gene reactivated response is related to specific AA deprivations, and most importantly which is the AA sensing/signaling pathway involved, in particular whether the GCN2 kinase is implicated. We used the reactivation of silenced transGenes in cultured cells, as a model to investigate a novel molecular pathway induced by imbalanced EAA starvation, implicated in the epigenetic/transcriptional regulation of exogenous non-native DNA sequences and possibly of other endogenous genomic elements. The following STR loci were amplified: AMEL, CSF1PO, D13S317, D16S539, D21S11, D5S818, D7S8 18, TH01, TPOX, vWA. The C2C12 cell line was provided by ATCC. HeLa and HepG2 cells were obtained by Drs. F. Blasi and G. Tonon at San Raffaele Scientific Institute, Milan, Italy, respectively, and were authenticated by Short Tandem Repeat (STR) profiling, using the Cell ID System kit (Promega), according to the manufacturer’s instructions. The thermal cycling conditions were: 1 cycle at 96°C for 2 min, followed by 32 cycles at 94°C. for 30 sec, 62°Cfor 90 sec, and 72°c for 90 sec. and 1 cycle for 45 sec. The cell lines were maintained in DMEM containing glutaMAX (Invitrogen) and supplemented with 10% FBS (Sigma), 100 mg/ml streptomycin, at 37°C in a 5% CO2 humidified atmosphere. They were also maintained in 600–1000 μg/ml G418 (G418) or 650/Sigma (GPR143) or GPR1/EGFP3/His/His (HeLa, HepG 2 and HepC2) cells. The cells were selected using FuGENE 6 (Roche) and selected with 800, 1000, 650, and 650ml of G418, respectively. The STR profile was performed at the Applied Biosystems Genomics service, and an assessment of capillary electrophoresis was performed. Full DMEM-based medium, carrying the entire AA complement, was used. Media deprived of Met/Cys (both AAs), Met (only), Cys ( only), Alanine (Ala), Thr, Gln, Val, Leu, Tyr, Trp, Lys and His were prepared using the Nutrition free DMEM (cat.#09077–05, from Nacalai Tesque, Inc., Kyoto, Japan) All media were supplemented with 10% dialyzed FBS, 100 U/ml penicillin G, 100 mg/ml streptomycin, and G418 as required. Cells were seeded at 10–30% of confluency; cells to be starved for 48 h were plated 2–3 times more confluent compared to the control. Treatments with drugs to be tested for their ability to inhibit transgene reactivation (ISRIB, SP600125 and U0126) were initiated 1h before the subsequent addition of L-Histidinol. RNA was purified using the RNeasy Mini kit (Qiagen), according to manufacturer’s instructions. RNA concentration was determined by Nanodrop 8000 Spectrophotometer (Thermo Scientific) Equal amount (1 μg) of RNA from HeLa, HepG2 and C2C12 cells was reverse transcribed using the SuperScript First-Strand Synthesis System for RT-PCR (Invitrogen) using oligo-dT as primers. The cDNA (2 μl) was amplified by real-time PCR using SYBR green Master Mix on a Light Cycler 480 (Roche) according to manufacturers’ instructions. The sequences, efficiencies and annealing temperatures of the primers are provided in S2 Table. Data were analyzed with Microsoft Excel using the formula EtargetΔct target (control-sample) /EreferenceΓct reference ( control- sample) . Reference genes for normalizations were ARPC2 (actin-related protein 2/3 complex, subunit 2) for HeLa and Hep G2 cells; and Actb (act in beta) for C2 C12 cells, unless otherwise indicated. ATF and GCN2 sequences were designed against NM_001675_001013703 and NM4N2, respectively (Mission esiRNA, 200 ng/L; Sigma) against the targeted and otherwise otherwise. Cells seeded in 6-well plates were transfected with 1 μg of siRNAs and 5 μM of Lipofectamine (Lipofamine 2000), following instructions, at day 1-plating for ATF and 2 post-plates forGCN2. At day 2- Plating, cells were washed and cultured in the absence or presence of HisOH (ATF4) or 3F4 (atF4 or 3L) At day 3-plated for GCNN2- At 2-Plating for GCG2-GCN4, at 4 h and 1 h after the first plasmid transfer, the cells were isolated and analyzed for protein expression by epifluorescence and/or immunoblotting. Vehicle was used as mock control. Drugs were used at the following final concentrations: HisOH at 4–16 mM; PP242 at 1–3 μM; ISRIB at 100 nM; SP600 125 at 20 μM in HepG 2 cells and 50 μM. Salubrinal at 75 μM, U01 26 at 50 μm; U0124 at 75 nM. Genomic DNA of HeLa and HepG2 cells was purified using DNeasy Blood and Tissue kit (Qiagen), according to the manufacturer’s instructions. Primer sequences for the three gRNAs targeting GCN2 exon 1 or 6 are listed in S2 Table. U6 promoter drives gRNA expression, and the CMV promoter drives Cas9 and GFP expression. Primary Abs were as follows: anti-human OA1, previously developed by our group in rabbits ; anti-GCN2 (Cell Signaling, Cat. #3302). P values in Figures are generally referred to comparison between a sample and the control (full medium/mock) and are indicated as follows. Additional details regarding the specific experiments are reported in the Figure Legends. Repeat counts over repeated regions, defined by SOAPer, were analyzed using maSig package. Repeat variation of abundance during time of starvation was analyzed using edgeR , dR , and edgeR package. T-test was used when only two means, typically sample versus control, were compared, as specified in the figure legends. P values were indicated by a horizontal line at the top of the graphs, encompassing the two samples under analysis. The analysis was carried out using Microsoft Excel for Mac (version 15.32, Microsoft) for Student's t-test; or GraphPad Prism (version 5.0d for Mac, GraphPad Software, Inc.) for one-way analysis of variance (ANOVA), followed by Dunnett's or Tukey’S multiple comparisons post-tests. The results were published in the online edition of the journal Genome Research, which is available on Amazon.com for $99.99 (Amazon.com.com). For confidential support, call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support on suicide matters call the Samaritans on 08457 90 90 90 or visit  www.samaritans.org. For confidential help on suicide issues, call the Samaritans on 08457 909090 or  visit http:// www.s Samaritans.com/. For confidential advice on suicide,  call the National Suicide Prevention lifeline on  08457 909090 or http www.saritans-org on- behalf of the British Samaritans, on behalf of the University of London, “The Samaritans”, “The University of London” or ”The University of London,” “‘‘,’ ‘or’, ‘’ or ‘the University of London’’. For information on how to donate to the cause of suicide, see the Samaritans’ ‘Online’ section of this article. For more details, see www.Saritans ’Home’ page’ and ‘The Online Site’ . For information about donating to the causes of suicide in the U.S., see the ‘Online site’ tab. For details on donating to cancer research in the United States, visit the  American Cancer Society (ACS) or the  American College of Medicine (ACM) . The expression of the OA1 transgene and HDAC4, which are progressively up- and down-regulated during starvation, respectively, were used as internal controls. Repeats significantly up- or downregulated (104 and 77, respectively) were selected based on a P value <0.05 (unpaired two-tailed Student’s t-test, assuming equal variance) and analyzed for their class enrichment by a Fisher Exact test. Differentially expressed repeat subfamilies were identified by averaging three time points of starvation (15-30-72 h) and controls. The enrichment analysis was performed separately for up and down regulated genes, or with all differentially expressed genes together (both), using the KEGG database. Out of 1396 annotated subfam families, 172 species displayed a differential expression profile during starvation. The analysis was done with correction for the background of all expressed genes (about 13600 genes showing an average expression over 3 starvation and 3 control samples of at least 5 counts) and by using default parameters (adjusted P value and q-value cut-off of < 0.05 and 0.2, respectively). Differently expressed genes were also selected considering all starvation time points, as with genomic repeats, by maSigPro usingdefault parameters, and a fold change of at at least 1.5, leading to similar enrichment results (not shown). RNAseq gene expression data are available in the ArrayExpress database under the accession number E-MTAB-6452. Exogenous transgenes and endogenous retroviruses are upregulated in Met/Cys-deprived HeLa cells. The expression of genomic repeats was determined according to RepeatMasker annotation and classification into classes, families, and subfam Families. The results were consistent with previously published results . The analysis of the previously described HeLa-OA1 cells, carrying an integrated and partially silenced Oa1 transGene, showed a progressive upregulation of the trans Gene during starvation (Fig 1A and 1B), consistent with previous results. The authors conclude that AA starvation may affect the expression of transposable elements, and that it may be a ‘progressive’ phenomenon. They conclude that the results of their study are consistent with the findings of previous studies on the effects of AA starvation on gene expression and gene expression in humans. The study was published in the open-access journal The Journal of the Royal Society of Medicine (J.S.M.I.C.E.R.D. and the Journal of Cell Biology and Chemistry) (http://www.j.sj.org/jc/2013/01/13/14/article/jcsm.html#storylink=cpy.html) The authors also suggest that the findings could be applied to other animal models of starvation, such as the ‘starvation-induced gene expression’ (‘Starvation in mice’) or ‘Star starvation in humans’. Clustering of differentially expressed repeats, according to their expression pattern, reveals profiles comparable to the behavior of the transgene in the same conditions. LTR-retroelements are significantly enriched among repeats that are upregulated upon starvation, while LINEs are significantly enrichment among repeats That are downregulated. These findings suggest that EAA starvation induces genome-wide effects involving repetitive elements, and that—among major repeat classes—it upregulates in particular the expression of ERVs. In particular, a large fraction of ribosomal protein mRNAs is downregulated upon Met/Cys starvation (Fig 2A and 2C; S1 File), consistent with the notion that their genes–despite being scattered throughout the genome—are coordinately expressed in a variety of conditions. This reduced expression may depend on multiple pathways that control ribosome biogenesis in response to external stimuli, including downregulation of mTORC1, 44], the activation activity of the Mycc R gene, or the activation of the ISR gene, possibly as described in Myc c. Myc and ISR, as well as the oc c o o o r o, 44, 44 and 44, 42, 44. In addition, we performed gene expression and enrichment analyses of regular genes, by considering three time points of starvation (15-30-72 h) and controls. Differentially expressed genes were selected based on a P value <0.05 and a fold change between means of at least 2, and analyzed with the EnrichR tool. By contrast,regulated genes show a significant enrichment for transcription and expression for transcription, oc o o c Myc, 44 [42], 44], 44, 43, 45, 42 and 42, as shown in Fig 2 and S1 file. The results were obtained by selecting significantly up- or downregulated genomic repeats (overall 181 species), based on their average expression out of three timepoints of starvation, and controls, and on aP value < 0.05 (S1B and S2B Figs) (Fig 1D, S2A, and S3 File) (Figs 1A, 1B, 1C, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 28 and 29, 31,. 32, 33, 34, 34,. 34, 35, 36, 37, 38, 39, 40, 41, 41 and 43, 44,. 41, 43,. 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 72, 73, 74, 78, 79, 80, 80 and 81, 84, 81, 82, 83, 84 and 84, 83,. 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94 and 94, 95, 96, 97, 98, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 111, 113 and 114, 114 and 115, 111 and 113, respectively. Differentially expressed genes between three time points of starvation (15-30-72 h) and controls were selected based on a P value <0.05 and a fold change of at least 2, leading to a total of 996 upregulated, and 1037 downregulated genes. Most EAA-deficiencies induced reactivation of the OA1 or GFP transgenes in all four cell lines, with the notable exception of Trp deprivation, which consistently resulted in no or minimal reactivation. The analysis of single Met or Cys starvation, both at the protein and transcript levels, revealed an exclusive role of Met deprivation in transgene reactivation, consistent with the notion that Cys is not an EAA (S3D and S3E Fig). EAA deprivation in HeLa and HepG2 cells induces reactivating of silent HeLa-OA1 (HeLa-A1) and Hep2-A2 (HepG2-G2) transgenES. EAA depriving cells of Thr, Val, Tyr, and His always gave an efficient response, while Leu, Gln and Lys elicited evident responses in some cases, but not in others. Depletion of Phe gave results comparable to Tyr deprivation, however it significantly altered multiple reference genes used for normalization and therefore was eventually omitted from the analysis (not shown). Finally, in the above experiments we used a combined Met/Cys deficiency, to avoid the potential sparing of Met by Cys  and for consistency with our previous studies . Nevertheless, the analysis ofsingle Met orCys starvation revealed anexclusive role ofmet deprivation intransgene reactionivation, as shown in Fig 3, and in S3C and S4B Figs. All data are shown in S1 File, overall indicating a general downregulation of translation and metabolism, and upregulation of transcription, during the time interval of Met/ cys starvation corresponding to the transg Gene Ontology Biological Process (GO-BP) upregulation. The enrichment analysis using all differentially. expressed genes together did not reveal any additional enriched process. The expression of the transGene transcript was then evaluated by RT-qPCR. In all cases, the integrated transGenes are under the control of the CMV promoter in the context of a pcDNA3.1 plasmid, are partially silenced, and can be efficiently upregulated by HDAC inhibitors (trichostatin A, TSA; ref.   and S 3A, S3B and S 4A Figs), indicating that their expression is controlled at least in part by epigenetic mechanisms, as previously described . In the experiments, cells were cultured in full medium, carrying the entire AA complement. and in a medium deprived of Ala, a non-essential AA. As negative controls, the cells were Cultured in full. medium, with a battery of media deprived of EAAs or semi-EAAs, including Met/cys, Thr,. Gln, Val,. Val, Leu,. Tyr, Trp, Lys, and his. The cells were then cultured in. full medium carrying the full AA complement, and. in amedium deprived of AA, aNon- essential AA. The cell types relevant for AA metabolism, such as liver and muscle, were stably transfected with plasmids for O a1 and GFP. mTORC1 inhibition and GCN2 activation trigger the best-known signaling pathways responding to AA starvation. We tested whether direct pharmacological activation of this kinase is sufficient to trigger the transgene reactivation similarly to starvation. In addition, we used pharmacological inhibitors of mTOR to corroborate previous negative results in HeLa cells  in the other cell lines under study. We found that CHOP expression is increased in all EAA-starvation conditions, but not in the absence of Ala, in all tested cell lines. Treatment with L-Histidinol results in efficient reactivation of the transGene in HepG2-OA1 and C2C12-GFP cells,but not in He La cells. The upregulation of CHOP does not parallel quantitatively that of theTransgene, neither appears sufficient to induce it. While the ISR appears widely activated upon EAA starvation, the up regulation of its downstream effector CHOP only partly correlates with transgenes reactivation and may not be sufficient to induced it. The activation of the IsR upon AA starvation suggests that GCN 2 may be involved in the reactivation response. We previously showed that inhibition of m TORC1 is not sufficient to reproduce transg Gene reactivation in He la cells. We also found that treatment with PP242 (mTOR inhibitor; 1–3 μM) or L-histidinols (GCN2 activator; 4–16 mM), either alone or in combination for 24 h, can trigger the reactivating of transgenes in Hela cells. These results indicate that transgGene reactivation by EAAs is reproducible with most EAAs, shared by different cell types (epithelium, liver, and skeletal muscle) and conserved in different mammalian species (human, mouse) The results are consistent with previous studies showing that the reaction is efficient with different EAAs and that it is not dependent on the type of EA as well as the amount of EAAs used. The results were published in the Journal of Cell Biology and Molecular Biology (JCLB) (http://www.jclb.org/content/early/2013/01/07/13/chop-transgene-recovery-by-EAAs-in-heLa- cells-and-mTOR-inhibitor-PP242-L-HistIDinol-HisOH.html) and in the New England Journal of Medicine (NEMM) ( http:// www.jelm.com/news/2013-01/08/chOP-transgenes-recycle-by EAAs in-HeLa-cells-and mTOR inhibiting MTOR and-MTOR-Inhibitor HisOH-His OH.html). We are happy to clarify that the results of these studies were based on the NEMM test, which was conducted on cells cultured in Met/Cys-deprived medium, or in the presence of PP242, HisOH, and full medium. We are also happy to point out that mTOR is not effective in inhibiting mTOR in any of the cell lines we tested. The study was published in JCLB, which is a journal of the American Society for Cell Biology (N.E.M.C.I.S.) (http www.nemm.gov/jlm/ 2013/06/09/chops/Chop.html), but we have no plans to change the name of the study. In HepG2-OA1 cells, L-Histidinol is sufficient to elicit extremely high levels of transgene reactivation, possibly due to an indirect effect of mTOR inhibition on GCN2 activity. C2C12-GFP cells efficiently reactivate thetransgene upon treatment with L- historicinol, but not PP242 (S5B Fig) The ISR is neither sufficient nor necessary to induce transgenes reactivation in HepG1 cells. We tested the role of the core upstream event of the ISR, namely the phosphorylation of eIF2α , which can be induced by pharmacological treatments. Salubrinal, a specific phosphatase inhibitor, blocks both constitutive and ER stress-induced phosphat enzyme complexes against eif2α. We found that, while the IsR is activated upon Salub rinal treatment, as shown by increased CHOP expression, it does not induce OA1 transgne reactivation (Fig 5B). The ISr is neither necessary nor sufficient to induce O a1 reactivation. We also found that L- historicalinol does not trigger upregulation of the O aa1 transGene in Hep G2-oa1 cells or ATF4 in the absence of the drug. These findings are consistent with previous reports describing the use of L-historicinol in HeLa cells in conditions of low His concentration in the culture medium, which would resemble AA starvation in our system and therefore may not be applicable. The amount of amino alcohol adapted to exceed 20 to 80 times that of the amino acid, as described , may be resistant or able to compensate. We conclude that the EAA starvation response may be involved in the response to DMEM lacking His, and that this response should not be used in the treatment of cancer patients in DMEM with His-deficient patients, as this response is thought to be more likely to be due to the presence of other amino acids in the DMEM medium, such as selenium, thiamine, guanidine, nor do we know if this is the case in cancer patients with AA starvation. The results of this study are published in the current issue of the Journal of the American Society of Clinical Pharmacology and Experimental Biology (JASCO) (Fig 4B, S5A Fig) and the next issue of JASCO will be published in June 2014. The embargo on the publication of these results has been lifted at the request of the journal’s editorial board. For confidential support, call the National Institute of Allergy and Infectious Diseases on 1-800-273-8255 or visit http://www.nacks.org/. For confidential. support, contact the National Institutes of Health (NICE) at 1-866-856-9090 or http:// www.NICE.org/NICE/NIC/NEC/NTC/NSC/NCC/NAC/NOC/NNC/NCE/NLC/NCL/NCl.NEC.NCC.NLC.NCL.NTC. NCC. NCL.NC.NCT. NLC. NC.NAC. NNC. NEC. NSC. NAC.NCCC. NCHOP. NGCN2 activation by L-histidinOL is sufficient. to reactivates the transgenes in both HepG3-Oa1 and C2 C12- GFP cells. The expression levels show only an incomplete correlation with those of the transghene. To test whether the ISR is necessary to trigger the transgene response to L-Histidinol, we used the chemical compound ISRIB, which inhibits the activation of ISR. We found that the reactivation of the OA1 transgenes is neither abolished, nor reduced by KO of GCN2, thus excluding that this kinase is necessary for the response to EAA starvation in HeLa-OA1 cells (Fig 6B and 6C).GCN2 knockout does not interfere with transg Gene reactivation in Hela cells. None of these clones express GCN 2 at the protein level, as shown by immunoblotting. All three clones showed a very low amount of residual protein—of any kind—of the original GC2 gene, compared to the original HepG2 gene in clone E23, F22 and F27. Genomic characterization confirmed the presence of mutations on both alleles of exon 1 in clone 183#11, and on bothalleles ofexon 6 in clone 239#1. The clones showed multiple alleles in exon1 in clone 185#5, which was not characterized further at the genomic level (S6 Fig). The clones were cultured in media deprived of Met/Cys or Thr (corresponding to the most effective treatments in this cell line; see Fig 3A) for 24–48 h and transgGene expression was assessed by RT-qPCR. Since independent clones may display variable reactivation responses (e.g. due to different levels of expression in basal conditions), the results are not shown as separate means of the three clones, but as separate replicates from the parental HepG1-KO cell line by the same strategy by using a guide RNAs. The results are shown as fold change vs. control (w/o HisOH = 1, top; control siRNA = 1,. bottom). *P<0.05 (one way ANOVA, followed by Tukey’s post-test; P values refer to comparisons vs.control, unless otherwise indicated). The results of the experiments are shown in Fig 5C, as expected, the expression of CHOP is inhibited by ISriB, as well as the reactivating of the oA1transgene is not affected. The experiments were carried out on HepG3-Oo1 cells, which have been shown to be resistant to Eaa starvation and to the use of EAA as a food source in the past. The data suggest that eIF2α phosphorylation and the downstream ISR pathway are neither sufficient nor necessary to induce transGene reactivation. The findings are consistent with the findings of previous studies [53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 83, 86, 84 and 86, which also found that eif2α is not necessary to activate the transG Gene. The study was carried out in mice and the results were published in the Journal of the American Society for Cell Biology. The authors did not use mice as a control, but rather as a model for the study of the effects of CRISPR/Cas9-mediated knock-out (KO) of the GCN1 gene on the human GCN gene. The experiment was conducted in mice that had been infected with EAA. We found that the reactivation of the OA1 transgene is neither abolished, nor reduced by KO of GCN2, as in HeLa cells. To further confirm this result, we knocked-downGCN2 by RNA interference (RNAi) and incubated the cells with or without L-Histidinol for 6 h. As shown in Fig 8, treatment of HepG2-OA1 cells with L-histidinols results in efficient transg Gene reactivation. Taken together, these data strongly support the conclusion that GCN 2 is not necessary for transgGene reactivation in response to EAA starvation, either in HeLA or in HepG 2 cells. The data are expressed as fold change vs. parental cell line (= 1) and mean ± SD of 3 technical replicates from 1 experiment. The results are published in the online version of this article. We are happy to provide a copy of the technical report for the purposes of this study. We would like to point out that the results of the study are based on a single experiment, and that the data are not representative of the whole study. For more information, please visit www.ncbi.nlm.uk/pub/2013/01/07/hg2-oa1- cells-and-GCN-KO-clones-in-met/Cys-or-his- deprived-medium-for-24-h-treatment-of-OA1-transgene-synthesized-cells."
What experimental techniques were used to study the quantum dot structures in this research?,"\section{Introduction}

Despite the rise of graphene and other 2D materials, semiconducting single-walled carbon nanotubes (SWNT) are still regarded as strong candidates for the next generation of high-performance ultrascaled transistors~\cite{Cao_IBM_2015,IBM_2017,3D_CNT_FET} as well as for opto-electronic devices~\cite{Review_Avouris,CNT_photonics} such as chip-scale electronic-photonic platforms~\cite{Pernice_2016} or low-threshold near-infrared tunable micro-lasers~\cite{Graf_2017}. 
Engineering a quantum dot (QD) along a (suspended) semiconducting SWNT foreshadows promising opportunities in the field of quantum information processing and sensing through recently proposed schemes such as detection and manipulation of single spins via coupling to vibrational motion~\cite{Palyi_2012}, optomechanical cooling~\cite{Wilson_Rae_2012} as well as all optical manipulation of electron spins~\cite{Galland_all_optical_2008}. Furthermore, the quasi one-dimensional geometry of SWNTs allows for defining tunable p-n junctions induced by electrostatic doping through local gates~\cite{Buchs_JAP,tunable_pn_2011}. Combining a well-defined QD within such a p-n junction structure could constitute a crucial building-block for the realization of highly desirable electrically driven, on-demand single photon emitters operating at telecom wavelength, based $e.g.$ on a turnstile device architecture~\cite{turnstile_1994,turnstile_1999}.
In practice, QDs in carbon nanotubes have been reported predominantly for two different confinement structures: i) Engineered tunneling barriers at metal-nanotube contacts~\cite{Pablo04nat} and/or by gate electrodes, used \emph{e.g.} to manipulate single electron spins~\cite{Laird:2015}, ii) Unintentional localization potentials stemming from environmental disorder~\cite{Hofmann_2016}, allowing for single-photon emission mediated by localization of band-edge excitons to QD states~\cite{CNT_photonics,Hoegele_2008,Walden_Newman_2012,Hofmann_2013,Pernice_2016_2}. Both types of structures are usually operated at cryogenic temperature due to small energy scales ranging from a few to few tens of millielectronvolts.
\\
\indent Another technique for achieving confinement in SWNTs makes use of artificial defects such as covalently bound oxygen or aryl functionalization groups on the side walls of semiconducting SWNTs, inducing deep exciton trap states allowing for single-photon emission at room temperature~\cite{Htoon_2015,tunable_QD_defects}. Also, carrier confinement between defect pairs acting as strong scattering centers has been reported for mechanically induced defects~\cite{Postma_SET} as well as for ion-induced defects with reported level spacings up to 200 meV in metallic SWNTs~\cite{Buchs_PRL}. The latter technique, combined with recent progress in controlling defects structure and localization~\cite{Robertson_2012,Yoon_2016,Laser_writing_2017} offers a high potential for engineering a broad set of SWNT-based quantum devices operating at room temperature. 
\\
\indent Here, we demonstrate confinement of electrons and holes in sub-10 nm QD structures defined by ion-induced defect pairs along the axis of semiconducting SWNTs. Using low temperature scanning tunneling microscopy and spectroscopy (STM/STS), bound states with level spacings of the order of 100 meV and larger are resolved in energy and space. By solving the one-dimensional Schr\""odinger equation over a piecewise constant potential model, the effects of asymmetric defect scattering strength as well as the influence of the Au(111) substrate such as terrace edges on the bound states structure are remarkably well reproduced. By means of ab-initio calculations based on density functional theory and Green's functions, we find that single (SV) and double vacancies (DV) as well as chemisorbed nitrogen ad-atoms are good candidates to produce QDs with the experimentally observed features. These simulations also allow to study the scattering profile as a function of energy for different defect combinations.

\section{Experimental section}

The experiments have been performed in a commercial (Omicron) low temperature STM setup operating at $\sim5$~K in ultra high vacuum. Topography images have been recorded in constant current mode with a grounded sample, using mechanically cut Pt/Ir tips. Differential conductance $dI/dV$ spectra, proportional in first approximation to the local density of states (LDOS)~\cite{Tersoff85} have been recorded using a lock-in amplifier technique. The LDOS spatial evolution along a nanotube axis is obtained by $dI/dV(x,V)$ maps built by a series of equidistant $dI/dV$ spectra. Spatial extent mismatches between topography images and consecutive $dI/dV(x,V)$ maps have been systematically corrected~\cite{Buchs_Ar}, and the metallic nature of the tip has been systematically checked on the gold substrate to prevent any tip artefacts before recording STM or/and STS data sets. 
\\
\indent Nanotube samples were made of extremely pure high-pressure CO conversion (HiPCo) SWNTs~\cite{Smalley01} with a diameter distribution centered around 1 nm, FWHM $\sim$ 0.3 nm~\cite{Buchs_conf}. The measured intrinsic defect density was below one defect every 200 nm. SWNTs were deposited on atomically flat Au(111) surfaces from a 1,2-dichloroethane suspension, followed by an in-situ annealing process~\cite{Buchs_APL_07,Buchs_Ar}.
\\
\indent Local defects in SWNTs have been created in-situ by exposure to: (i) Medium energy $\sim$ 200 eV argon ions (Ar$^{+}$) produced by an ion gun \cite{Buchs_Ar,Buchs_PRL}, (ii) Low energy (few eV's) nitrogen ions (N$^{+}$) produced by a 2.45 GHz ECR plasma source~\cite{Buchs_APL_07,Buchs_NJP_07}. In both cases, the exposure parameters have been calibrated to reach an average defect separation along the SWNTs of about 10 nm~\cite{Buchs_Ar,Buchs_APL_07}.

\section{Results and discussion}
\subsection{Experimental LDOS patterns}
\begin{figure}
  \includegraphics[width=8cm]{Figure_1.pdf}
  \caption{\label{exp_data_1} (a)-(b) 3D topography images (processed with WSXM~\cite{WSXM}) of SWNT I with Ar$^{+}$ ions-induced defects, with sample-tip bias voltage ($V_\mathrm{S}$) 1 V and tunneling current $I_\mathrm{S}$ 0.1 nA. (c) Corresponding $dI/dV(x,V)$ map recorded along the horizontal dashed lines in (b), with $V_\mathrm{S}=1$ V, $I_\mathrm{S}=0.2$ nA. Spatial resolution $\sim$ 0.3 nm. (d) 3D topography image of SWNT II with N$^{+}$ ions-induced defects, with $V_\mathrm{S}=1$ V, $I_\mathrm{S}=128$ pA. (e) Corresponding $dI/dV(x,V)$ map recorded along the horizontal dashed lines in (d), with $V_\mathrm{S}=1.5$ V, $I_\mathrm{S}=0.3$ nA. Spatial resolution $\sim$ 0.2 nm.}
\end{figure}
In Fig.~\ref{exp_data_1} (a) and (b), we show 3D STM images of the same semiconducting SWNT (referred as SWNT I in the following) with Ar$^{+}$ ions-induced defect sites labeled $d1-d5$ . Panel (d) shows a 3D STM image of a second semiconducting SWNT (referred as SWNT II) with N$^{+}$ ions-induced defect sites labeled $d6-d7$. In both cases, defect sites typically appear as hillock-like protrusions with an apparent height ranging from 0.5~{\AA} to 4~{\AA} and an apparent lateral extension varying between 5~{\AA} and 30~{\AA}~\cite{Buchs_NJP_07,Buchs_Ar,Thesis_Buchs}. 
\\
\indent The resulting $dI/dV(x,V)$ maps recorded along the horizontal dashed line drawn in the STM images (b) and (d) are displayed in panels (c) and (e) in Fig.~\ref{exp_data_1}, respectively. Defect signatures in the LDOS in both cases are characterized by deep in-gap states at the defects positions. This is consistent with the expected defect structures, $i.e.$ mainly SVs, DVs and combinations thereof for collisions with Ar$^{+}$ ions~\cite{Buchs_Ar} and bridgelike N ad-atom for collisions with N$^{+}$ ions~\cite{Thesis_Buchs,Nitrogen_prb_07}. Note that gap states at energy levels $\sim$~0.2 eV and $\sim$~0.05 eV in panels (c) and (e), respectively, are shifted to the right from $d3$ by about 1 nm and to the right from $d6$ by about 2 nm. This indicates the presence of intrinsic or ion-induced defects on the lateral or bottom side wall of the SWNTs~\cite{Kra01prb}, not visible in the topographic images. These defects are labelled $d3'$ and $d6'$, respectively.  
\\
\begin{figure}
  \includegraphics[width=12cm]{Figure_2.pdf}
  \caption{\label{exp_data_Ar} (a)-(b) QD I detailed $dI/dV(x,V)$ maps in conduction and valence bands. Lower subpanels contain QD states linecut profiles and stationary wave-like fits in left and right QD parts. Right subpanels contain experimental energy dispersion relation data sets $k_\mathrm{n}(E_\mathrm{n})$ and tight-binding calculations. (c)-(d) Resulting LDOS calculated from a one-dimensional piecewise constant potential model featuring potential barriers and a potential step (gray area), with position of the potential step: 5.09 nm from the right barrier's center, potential step heigth: $U_\mathrm{C}=V_\mathrm{L}-V_\mathrm{R}=60$ meV, barrier heights: $V_\mathrm{d3'}=1$ eV, $V_\mathrm{d4}=0.85$ eV, barrier widths: $a_\mathrm{d3'}=a_\mathrm{d4}=3.4$ nm.  Valence band: $V_\mathrm{d3'}=-0.4$ eV, $a_\mathrm{d3'}=a_\mathrm{d4}=2.5$ nm, $V_\mathrm{d4}=-0.4$ eV. $E_\mathrm{g}$ stands for bandgap energy.}
\end{figure}
\begin{figure}
  \includegraphics[width=12cm]{Figure_3.pdf}
  \caption{\label{exp_data_N} (a) QD II detailed $dI/dV(x,V)$ map. Lower subpanels contain QD states linecut profiles and stationary wave-like fits in the left and right QD parts. Right subpanel contains experimental energy dispersion relation data sets $k_\mathrm{n}(E_\mathrm{n})$ and tight-binding calculations. (b) Resulting LDOS calculated from a one-dimensional piecewise constant potential model featuring potential barriers and a potential step (gray area) with position of the potential step: 4.7 nm from the right barrier's center, potential step heigth: $U_\mathrm{C}=V_\mathrm{L}-V_\mathrm{R}=60$ meV, barrier heights: $V_\mathrm{d6'}=0.6$ eV, $V_\mathrm{d7}=0.6$ eV, barrier widths: $a_\mathrm{d6'}=1.5$ nm, $a_\mathrm{d7}=2.6$ nm.}
\end{figure}
\indent Remarkably, the $dI/dV(x,V)$ maps in Fig.~\ref{exp_data_1} exhibit several broad discrete states in the conduction bands of SWNT I, II (white dashed boxes in panel (c) and (e), respectively) and in the valence band of SWNT I (white dashed box in panel (c)), characterized by a modulation of the $dI/dV$ signals in the spatial direction between pairs of consecutive defect sites $d3'-d4$ and $d6'-d7$. Enlarged plots of these boxed regions are displayed in Fig.~\ref{exp_data_Ar}(a)-(b) and Fig.~\ref{exp_data_N}(a) for SWNTs I and II, respectively. In the conduction bands, cross-sectional curves recorded along the black horizontal dashed lines labelled m1--m3 in Fig.~\ref{exp_data_Ar}(a) and m1--m4 in Fig.~\ref{exp_data_N}(a) are plotted below the LDOS panels. These clearly reveal one to three and respectively one to four spatially equidistant maxima. The number of maxima increases for increasing $\left|V_\mathrm{bias}\right|$ and the measured level spacings between consecutive discrete states is of the order of 100 meV and larger for both cases. This indicates that defect sites $d3'-d4$ and $d6'-d7$, respectively separated by 12.1 nm and 11.2 nm, act as strong scattering centers able to confine carriers in semiconducting SWNTs~\cite{Buchs_PRL,Bercioux_prb_2011}. Such intrananotube QD structures will be referred as QD I (in SWNT I) and QD II (in SWNT II) in the following. We estimated the level spacings in the conduction band of QD I to 98 meV (m1-m2) and 116 meV (m2-m3). For QD II, we measured 122 meV (m1-m2), 185 meV (m2-m3) and 210 meV (m3-m4).
\\
\indent In the valence band of SWNT I, discrete states with level spacings of the order of 80-90 meV, with one clear maximum at the level m-1, can also be distinguished between defect sites $d3'-d4$ in Fig.~\ref{exp_data_Ar}(b). The discretization of the states indicates that this QD structure also confines holes. Discrete states starting from m-2 and lower show less well defined structures compared to the conduction band states. In the case of SWNT II, no clear discrete states are observed in the valence band (see supplementary information). These observations are most probably the result of an energy dependent scattering strength of the defects, respectively $d3'$-$d4$ and $d6'$-$d7$, leading here to a weaker confinement in the valence band. Such energy dependence is well known for metallic SWNTs~\cite{Chico96,vac_2007,mayrhofer:2011,Bockrath_Science01} and is corroborated by our ab-initio calculations. Note that mixing effects with defect states and substrate-induced effects~\cite{substrate_effects} cannot be ruled out.
\\
\indent Another remarkable feature in the LDOS is the strong spatial asymmetry of the lowest energy states m1 and m-1 in QD I and m1 in QD II. In QD I, m1 is shifted to the right side of the dot while m-1 is shifted to the left side. Higher states m2 and m3 show more symmetry in terms of position of the maxima relative to the center of the QD. In QD II, m1 is shifted to the right side of the QD. We attribute the observed lowest energy states asymmetry (for electrons as well as for holes) in part to their strong sensitivity to weak potential modulations within the QD structure (as we will show in section \ref{1D}). For QD I, this assertion is supported by the observation of a 0.25 nm high Au(111) terrace edge located around the center of the QD, leading to a supported-suspended interface (see white dashed lines in Fig.~\ref{exp_data_1}(b) and more topographic details in Fig.~S2(a)-(d) in supplementary information). Such configurations have been reported to induce a rigid shift in the SWNT bands~\cite{Clair_2011}, for instance here a down-shift in the right side of QD I corresponding to the ""suspended"" portion between two terraces. In QD II, we attribute the spatial shift of m1 to a potential modulation induced by a layer of disordered impurities, most probably residua from the 1,2-dichloroethane suspension, lying between the gold substrate and the SWNT (see Fig.~\ref{exp_data_1}(d) and Fig.~S2(e)-(h) in supplementary information). 
\\
\indent Also, the LDOS in QD I and II (Fig.~\ref{exp_data_Ar}(a) and Fig.~\ref{exp_data_N}(a), respectively) reveals asymmetric patterns with curved stripes oriented from top left to bottom right for QD I and from bottom left to top right for QD II. These are characteristic signatures for defect pairs with different scattering strengths~\cite{Bercioux_prb_2011,Buchs_PRL}. For instance here, the left defect in QD I ($d3'$) has a larger scattering strength than the right one ($d4$), while the right defect in QD II ($d7$) has a larger scattering strength than the left one ($d6'$). 
\\
\indent The exact atomic structure of the defects could in principle be determined from a comparison of $dI/dV$ spectra with simulated first-principle LDOS signatures of expected defect types. In reality, this is hampered by the large number of possible geometries to simulate, including complex multiple defect structures~\cite{Buchs_Ar}, together with the large unit cells of the semiconducting chiral SWNTs studied here.
\\
\subsection{1D piecewise constant potential model}
\label{1D}
To better understand the physical origins of the non-trivial signatures of the quantized states, we model the experimental $dI/dV$ maps by solving the time independent one-dimensional Schr\""odinger equation over a piecewise constant potential model of QD I and QD II. The scattering centers are approximated by semi-transparent rectangular tunneling barriers leading to a square confinement potential~\cite{Laird:2015}. This is supported by previous results on defect-induced confinement in metallic SWNTs using the same experimental conditions~\cite{Buchs_PRL} and is consistent with ab-initio simulations presented later in this work. The potential modulation within the QD is approximated by a potential step. The resulting potential geometries are illustrated with gray shaded areas in Fig.~\ref{exp_data_Ar} (c) and (d) and Fig.~\ref{exp_data_N}(b). Dispersion relations $E(k)$ can be extracted experimentally from the quantized states wavefunctions by measuring the energy and corresponding momenta in the left and right sides of the QDs. The wavevectors $k$ are determined using stationary wave-like fitting functions~\cite{Buchs_PRL} displayed with dashed red curves in Figs.~\ref{exp_data_Ar}(a)-(b) and ~\ref{exp_data_N}(a)). From this procedure, the potential step height and position can be estimated (see supplementary information). The experimental data sets $E(k)$ are plotted in the right panels of Figs.~\ref{exp_data_Ar}(a) and \ref{exp_data_N}(a) together with dispersion relations from a third-nearest neighbor tight-binding calculation closely approximating ab-initio results~\cite{Reich_TB_2002}. These chirality-dependent tight-binding dispersion relations, calculated within an extended Brillouin zone resulting from the defect-induced breaking of the translation invariance~\cite{Bercioux_prb_2011}, are used in the Hamiltonian of our one-dimensional model. Taking into account the measured chiral angle, diameter distribution~\cite{Buchs_conf} and measured bandgaps, we find the best match with chiralities $(7,6)$ for QD I and $(11,1)$ for QD II (see supplementary information). 
\\
\indent Once chiralities together with potential step heights and positions are optimized, one can fit the height and width of the rectangular tunneling barriers in order to reproduce the experimental level spacings and general LDOS patterns. On a qualitative ground, a symmetric double barrier system results in the formation of spatially symmetric discrete bound states. Increasing both barrier heights simultaneously shifts the bound state energy levels and level spacings up. This leads to sharper bound states as the confinement in the QD is made stronger thus increasing the lifetime of the confined electrons. Increasing the barrier thickness with constant inner edge separation does not affect much the level spacings but further sharpens the bound states. Any asymmetry introduced by a change in the width or height of one single barrier leads to broader bound states. The presence of a potential step modifies the LDOS in lifting the levels of the bound states, with a more pronounced effect on the lower states. In QD I and II, the center of each barrier is aligned with the center of the gap states ($d3'$-$d4$ for QD I and $d6'$-$d7$ in QD II) and the width ratio is kept proportional to the ratio of the spatial extent of the gap states. Thus, by increasing the width of the barriers, we decrease the length of the QD leading to higher level spacings, and vice versa. The experimental level spacings can then be approximated by tuning both barrier widths in the same ratio and the heights individually, knowing that the scattering strength of $d3'$ ($d7$) is larger than $d4$ ($d6'$) according to the observed asymmetry in the LDOS described above \footnote{The transmission probability through a rectangular tunneling barrier is given by $T=\left( 1+\frac{V^{2}\sinh^{2}\left( a \cdot \sqrt{2m^{*}(V-E)}/\hbar \right)}{4E(V-E)} \right)^{-1}$, where $V$ and $a$ are respectively the barrier height and width. For the argument in the $\sinh$ sufficiently small such that $\sinh(x)\simeq x$, it can be shown that $a$ and $V$ can be coupled such that the transmission probability becomes a function of the area under the barrier $A=a\cdot V$, with $T=\left( 1+ \frac{m^{*}A^{2}}{2\hbar^{2}E} \right)^{-1}$. In our case, this condition is not satisfied and thus the barrier geometries are tuned empirically to fit the experimental level spacings.}. 
\\
\indent For QD I, we find a good match in the conduction band for the barrier heights $V_\mathrm{d3'}=1$ eV and $V_\mathrm{d4}=0.85$ eV, widths $a_\mathrm{d3'}=a_\mathrm{d4}=$ 3.4 nm,  and potential step $V_\mathrm{L}-V_\mathrm{R}=60$ meV. With these parameters, the spatial profile of the obtained quantized states (see lower subpanels in Fig.~\ref{exp_data_Ar}(a) and (c)) reproduces the experimental modulation features remarkably well. Also, the simulated LDOS displays a pattern with curved stripes oriented from top left to bottom right, as observed experimentally, due to a left barrier with a larger scattering strength. In the valence band, although modes m-2 and lower do not show a well defined structure in the spatial direction, thinner barriers with dimensions $a_\mathrm{d3'/d4}=2.5$ nm, $V_\mathrm{d3'/d4}=-0.4$ eV, leading to a slightly longer QD length (9.6 nm compared to 8.7 nm in the conduction band) can reproduce the measured level spacings very well. 
\\
\indent For QD II, we observed that the measured energy levels are overestimated by a factor $\alpha\sim1.29$, presumably due to a voltage division effect induced by the impurity layer mentioned above (see details in supplementary information). We find a good agreement with the experimental LDOS with the parameters: $V_{d3'}=V_{d4}\simeq$ 0.47 eV, $a_\mathrm{d6'}=1.5$ nm, $a_\mathrm{d7}=2.6$ nm and $U_\mathrm{C}=V_\mathrm{L}-V_\mathrm{R}\simeq 47$ meV. Note that in Fig.~\ref{exp_data_N}(b) the barrier and potential heights are multiplied by $\alpha$ to allow a direct comparison with the experimental LDOS. The simulated LDOS shows a pattern with curved stripes oriented from bottom left to top right, as observed experimentally, due to a right barrier exhibiting a larger scattering strength. Also, the spatial profile of the obtained bound states (see lower subpanels in Fig.~\ref{exp_data_N}(a) and (b)) reproduces the experimental features quite well. Note also that one can distinguish an isolated state in the experimental LDOS at an energy level between m1 and m2, about in the middle of the QD. This state that prevented an accurate fit of the state m2 in the right QD part is attributed to a spatial feature visible in the STM topography image in Fig.~\ref{exp_data_Ar}(d) (see also supplementary information, Fig.S2(f)), probably a physisorbed impurity which does not affect the LDOS significantly.
\\
\subsection{Ab-initio calculations}
\begin{figure}
  \includegraphics[width=16cm]{Figure_4.pdf}
  \caption{\label{num_data} (a)-(c) LDOS ab-initio simulations of a semiconducting $(16,0)$ SWNT with combinations of vacancies defects separated by 11.1 nm. Subpanels display QD state linecut profiles. (d) Tight-binding (black curve) and ab-initio dispersion relations (green circles) for a pristine $(16,0)$ SWNT with $E_\mathrm{n}(k_\mathrm{n})$ data sets extracted from (a)-(c). (e)-(g) LDOS ab-initio simulations of a semiconducting $(17,0)$ SWNT with combinations of N ad-atoms and vacancies defects separated by 10.7 nm. (h) Tight-binding (black curve) and ab-initio dispersion relations (green circles) for a pristine $(17,0)$ SWNT with $E_\mathrm{n}(k_\mathrm{n})$ data sets extracted from (e)-(g).}
\end{figure}
In order to elucidate the physical nature of the electron/hole confining scattering centers, we performed ab-initio simulations based on a combination of density functional theory~\cite{pbe,paw,vasp_paw,VASP2}, maximally localized Wannier orbitals~\cite{transportwannier90} and Green's functions (see supplementary information). Without loss of generality, we have simulated short unit cell semiconducting zigzag SWNTs with different combinations of the most probable defect structures. Results for vacancy defects likely being induced by 200 eV Ar$^{+}$ ions, separated by about 11 nm in a $(16,0)$ SWNT are shown in Fig.~\ref{num_data}(a)-(c) with DV-DV, DV-SV and SV-SV pairs, respectively. The LDOS displays midgap states at the defect positions as expected as well as defect states in the valence band~\cite{Buchs_Ar}. Most importantly, clear quantized states with a number of maxima increasing with energy are observed between the defects in the conduction band, emphasizing the ability of SVs and DVs to confine carriers. For the asymmetric configuration DV-SV, one can distinguish faint curved stripe patterns oriented from top left to bottom right, indicating a larger scattering strength for DVs compared to SVs. This is consistent with observations in transport experiments~\cite{Gomez05nm}. On the other hand, the patterns in the valence band strongly depend on the defect types. Discrete states can be distinguished for the DV-DV case, with m-2 being mixed with defect states. For the DV-SV case, clear curved stripe patterns oriented from bottom left to top right indicate again a stronger scattering strength for DV. Also, broader states are observed, indicating that the scattering strength of DVs and SVs is weaker in the valence band compared to the conduction band.
\\
\indent More insight on the energy dependent scattering strength for each defect pair configuration can be obtained by extracting the wavevector $k_\mathrm{n}(E_\mathrm{n})$ for each resonant state. This data set is plotted in Fig.~\ref{num_data}(d) for the conduction and valence bands together with the $(16,0)$ dispersion relations calculated from the third-nearest neighbor TB model and from the ab-initio calculation for the pristine nanotube. A first observation is the excellent agreement between TB and ab-initio results, further validating the method used in Figs.~\ref{exp_data_Ar}(a)-(b) and ~\ref{exp_data_N}(a). The vertical dashed lines indicate the limiting $k_\mathrm{n,\infty}=\frac{\pi \cdot n}{L}$ values corresponding to the closed system (infinite hard walls potential) with $L=11.1$ nm being the defect-defect distance. In the conduction band, we find that $k_\mathrm{n}(E_\mathrm{n})=\frac{\pi \cdot n}{L_\mathrm{eff}(n)} < k_\mathrm{n,\infty}$, indicating that the effective lengths $L_\mathrm{eff}(n)$ of the QD are larger than $L$ ($i.e.$ the resonant states wavefunctions are characterized by penetrating evanescent modes inside the defect scattering potential), as expected for an open system. The shortest $L_\mathrm{eff}(n)$ are obtained for the DV-DV configuration with 12.1 nm (m1), 13.1 nm (m2) and 12.9 nm (m3), which we attribute to wider scattering potential profiles for DVs compared to SVs. In the valence band, we find that $k_\mathrm{n}(E_\mathrm{n})=\frac{\pi \cdot n}{L_\mathrm{eff}(n)} > k_\mathrm{n,\infty}$, with $L_\mathrm{eff}(n)$ values between 7.9 nm (DV-DV, m-1) and 9.66 nm (DV-SV, m-2). We attribute this pronounced QD shortening to wider scattering potential profiles of both DVs and SVs in the valence band, probably due to mixing with wide spread defect states in the valence band.
\\
\indent Ab-initio calculations for different defect pairs combinations containing at least one N ad-atom, $i.e.$ N-DV, N-SV and N-N, are presented in Fig.~\ref{num_data}(e)-(h) for a $(17,0)$ SWNT, along with details on the defects geometries. Remarkably, clear QD states are generated for all three configurations, underlining the potential of N ad-atoms to confine carriers in semiconducting SWNTs and thus to generate intrananotube QDs. 
\\
\indent In order to demonstrate the scattering strengths of the different defects, we calculated the energy dependent conductance in addition to the LDOS for the different combinations of the QD defining scattering defects on the $(16,0)$ and $(17,0)$ SWNTs, see supplementary information. Generally we can observe strong conductance modulation of the order of 30-40\% with regard to the pristine CNT for all three tested defects (double vacancies DV, single vacancies SV and chemisorbed C-N) with the DVs having the largest scattering strength in the CB and VB.  
\\
\indent Note that the choice of the zigzag SWNT chiralities in the two different ab-initio scenarios is motivated by the different effective masses of both chiralities ($m^{*}_{(17,0)}>m^{*}_{(16,0)}$) which is typical for chirality families $(3n-1,0)$ and $(3n-2,0)$~\cite{ZZ_families}. Taking advantage of recent reports on SWNT chirality control~\cite{chirality_control_EMPA,chirality_control_chinese,chirality_chemistry}, this property could be used in practice to design QDs with different level spacings for the same QD length. From an application point of view, however, QDs generated by DVs will have far superior stability at room temperature due to their high migration barrier above 5 eV ($\sim$~1 eV for single vacancy)~\cite{Kra06vm}. This value drops down by at least 2 eV for N ad-atoms depending on their chemisorption configuration~\cite{Nitrogen_prb_07,Yma05nitr}.
\\
\indent Our ab-initio simulations do not take into account any substrate effect. In the experimental case, the carriers can decay through the substrate, thus limiting their lifetime. This leads to state broadening, measured between about 60 meV up to 120 meV in QD I and II, while the quantized states widths in ab-initio simulations vary between about 5 meV and 45 meV. This suggests that a better contrast of the experimental quantized states, especially in the valence band, could be achieved by lowering the nanotubes-substrate interaction through $e.g.$ the insertion of atomically thin insulating NaCl films~\cite{Ruffieux_Nature_2016}. This would allow to gain more insight on the electronic structure of the QDs as well as in the associated scattering physics at the confining defects~\cite{Buchs_PRL}. 

\section{Conclusions and outlook}
In summary, using low-temperature STM/STS measurements supported by an analytical model and ab-initio simulations, we have demonstrated that intrananotube quantum dots with confined electron and hole states characterized by energy level spacings well above thermal broadening at room temperature can be generated in semiconducting SWNTs by structural defects such as vacancies and di-vacancies, as well as nitrogen ad-atoms. These results, combined with recent progresses in type and spatial control in the formation of defects~\cite{Robertson_2012,Yoon_2016,Laser_writing_2017} as well as chirality control~\cite{tunable_QD_defects}, hold a high potential for applications in the design of SWNT based quantum devices. These include $e.g.$ electrically driven single-photon emitters operating at room temperature and telecom wavelength. In this context, the observation of quantum confinement effects in the emitted light of cut, sub-10 nm, semiconducting SWNTs~\cite{Dai_2008} shall be seen as an additional motivation for investigating the optical properties of our ""QD with leads"" building-blocks. These would include $e.g.$ studying optical transitions selection rules for different types and configurations of defect pairs~\cite{sel_rules_2006} associated with experimental studies such as photoluminescence~\cite{Lefebvre06} combined to $g^{(2)}$ correlation measurements~\cite{Hofmann_2013} in suspended SWNT devices as well as photocurrent imaging~\cite{Buchs_Nat_comm} and spectroscopy~\cite{Gabor_2009}.

\section*{Acknowledgements}
The authors thank Ethan Minot, Lee Aspitarte, Jhon Gonzalez, Andres Ayuela, Omjoti Dutta and Arkady Krasheninnikov for fruitful discussions.
The work of DB is supported by Spanish Ministerio de Econom\'ia y Competitividad (MINECO) through the project  FIS2014-55987-P and by the (LTC) QuantumChemPhys. LM acknowledges support from the BMBF-project WireControl (FKZ16ES0294) and computing time for the supercomputers JUROPA and JURECA at the J\""ulich Supercomputer Centre (JSC).


\clearpage

\section*{References}


",['Low temperature scanning tunneling microscopy and spectroscopy (STM/STS).'],4297,multifieldqa_en,en,,5b978c8d5792b07ad99da0fc2639c6046051e9c29825ad25," Despite the rise of graphene and other 2D materials, semiconducting single-walled carbon nanotubes (SWNT) are still regarded as strong candidates for the next generation of high-performance ultrascaled transistors. Engineering a quantum dot (QD) along a (suspended) SWNT foreshadows promising opportunities in the field of quantum information processing and sensing. Using low temperature scanning tunneling microscopy and spectroscopy (STM/STS), bound states with level spacings of 100V and larger in energy and space are remarkably well reproduced on Au(111) substrate. By solving the one-dimensional Schrodinger equation, defect scattering strength as well as the influence of the bound edges on the bound states are remarkably remarkably reproduced on the Au( 111) substrate such as terrace on the edge of the substrate. These simulations also allow us to study the scattering profile as a function of en-initio density and Green's functions based on functional theory and Green’s theory of quantum mechanics. We demonstrate confinement of electrons and holes in sub-10 nm QD structures defined by ion-induced defect pairs along the axis of semiconducted SWNTs. We find that single (SV) and double vacancies (DV) are good candidates to produce QDs with the experimentally observed features of ad-atoms. We also show that nitrogen nitrogen nitrogen vacancies are good candidate to produce ad-orbed nitrogen nitrogen vacancy (DVs) as well. We conclude with a discussion of the potential uses of nanotube quantum dots in quantum computing and other areas of science and technology in the future. We hope to make this information available to the public in the form of an open-source guide to nanotubular quantum computing. For more information, please visit the CNT website or the Nano Center for Nanotube Research (NCN) in the U.S. or the Nanotubule Research (NTR) website in the UK or visit the NTR website in Europe or the European Nano Centre for Nanotechnology (NNC) in France. The NTR is a non-profit organization dedicated to the development of Nanotubes in a range of scientific and technological fields. For confidential support, call the National Institute of Standards and Technology (NIST) on 1-800-273-8255 or visit http://www.ncntr.org/. For confidential help, contact the National Institutes of Standards (NIS) on 2-1-2-2 or http:// www.ninsight.org/NIS/2-1/2/2/. For information on how to get started on a nanotechnology project, visit www.cntn.org or the NIS website. For information about nanotechnology in the United States, visit the National Nanotechnology Center (NTC) or the University of California, Los Angeles (U.S.) or the California Institute of Technology (UCL). For more details on nanotechnology, visit www.cNTn.gov or the UCL Nanotechnology Institute (UNC) or  the ULC Nanotechnology Research Center (UNN) or University of California in the UK, or visit UCL's research site and University of California Department of Nanotechnology and Research (UCLA in the United Sciences. Nanotube samples were made of extremely pure high-pressure CO conversion (HiPCo) SWNTs. The measured intrinsic defect density was below one defect every 200 nm. The experiments have been performed in a commercial (Omicron) low temperature STM setup operating at $\sim5$~K in ultra high vacuum. Topography images have been recorded in constant current mode with a grounded sample, using mechanically cut Pt/Ir tips. Differential conductance $dI/dV$ spectra, proportional in first approximation to the local density of states (LDOS) were recorded using a lock-in amplifier technique. In both cases, the exposure parameters have been calibrated to reach an average defect separation along the SW NTs of about 10 nm. We show 3D topography images of SWNT I with Ar$^{+}$ ions-induced defects, with $V_\mathrm{S}=1$ V and tunneling current $I_\ mathrm{s}$ 0.1 nA. In the following images, we show SWNT II (referred as the following) with the same semiconducting SWNT sites labeled $d-d7$ and $1-d5$ Panel shows a second STM image of a second SWNTM image with an apparent protrusions height ranging from 0.5 to 0.7 nm. In all cases, both defect sites, typically appear as hillock-like protrusion sites with a height from 0~5-0.5 nm. For more details on the experimental results, please visit http://www.buchs-conf.org/buchsconf/Buchs_conf-Experimental-Results-and- Discussion.html. For the full study, please see http:// www.buchs-conf/buch-Experiment-Results/Buchs_APL_07.html/. For the rest of the article, please click on the link to the bottom of the page and the article will be updated to include the results and further details on how the experiments were carried out and how the results were analysed. Back to the page you came from. The original version of this article stated that the results of the experiments had been published in a journal. We are happy to make clear that this was not the case. We would like to make it clear that we have published the results in a separate article, and that they have been published as part of a larger, open-access journal, rather than as the result of a collaboration with the University of California, Los Angeles (UCLA). We would also like to point out that this article has been published on the same day as the UCL press release, which was published on November 6, 2013. We have been asked to clarify that this is not the same as the announcement of the publication of the ‘Buchs-APL-07’ article, as we have previously stated that we had published the article on November 7, 2013, on November 8, 2013 on November 9, 2013 at 2:30 p.m. (GMT). We are also happy to point to the fact that the article was published in the same week as the “Buchs’ APL-7” article, which also stated that it was published by the UCLA press release on November 10, 2013 (see the article for more details). For more information, please go to www.Buchs.conf.uk/bchs/Bchs-7/BChs-07/BCHs-8. Figure 2 shows QD I and II maps in conduction and valence bands. Lower subpanels contain QD states linecut profiles and stationary wave-like fits in left and right QD parts. Figure 3 shows the LDOS calculated from a one-dimensional piecewise constant potential model featuring potential barriers and a potential step. Figure 4 shows the potential step heigth with position of the potentialstep: 5.09 nm from the right barrier's center, barrier heights: $V_\mathrm{d3'}=1.6$ eV and barrier widths: $a_’mathrm’d6’’=0.5’ eV, $a’s’ ‘’ Math’: ‘Math’. Figure 5: The LDOS plots of these boxed regions are displayed in Fig. 4. Enlarged versions of the figures are available for download at: http://www.nasa.gov/doi/full/10.1103/nasa-4/4/5/6/7/8/8.5/7.5.5-1/4.4-1.4.1.1-1-4.3.1/1.3/1/3.4/1-3/3/2.4, 1.3, 2.3,. 2.2, 3.1, 4.0, 3, 5, 4, 6, 6.2.5, 5.1,. 3.2,. 4.2., 3.4,. 5.3., 4.1., 5.0,. 3, 4,. 5, 6,. 4, 2, 3,. 3,. 2, 4., 3, 1, 2,. 2,. 3., 4, 5,. 2., 3,. 1.5,. 1, 3., 2.4., 2, 1,. 4,. 1,. 5,. 6.1%. 3.3%. 4.3%, 4.4%, 3.6, 5., 5, 2., 6.0. 1.4; 3.5%. 4, 3%, 4, 4; 2.1.; 3.0; 4.5; 5.2; 5, 3; 4,4, 5; 3,4.2.; 4,3, 4 and 5.7; 4,. 3; 2,2, 4 & 3.7. 2.0.; 3,3.2%; 3,2.1; 4., 2,. 4; 3,. 5; 1,3,. 4., 4,. 2; 2,. 1; 3., 3., 1,4; 2., 1.2%. 3,1.2). 2,1,2,. 1-2.2 (a) QD II detailed $dI/dV(x,V) map. (c)-(b) Resulting LDOS is characterized by a modulation of the spatial direction between $d3'-d4$ and $d6'-d6'$ signals in consecutive defect sites. (b) Right subpanel contains experimental energy dispersion relation data sets $k_”math”(E_ ””) (c) and tight-binding calculations. (d) Figure 4: The resulting LDOS shows the number of spatially equidima of the maxima of each band. (e) and (f) The number of maxima in the conduction bands is recorded along the black horizontal dashed lines recorded the black dashed lines in m3--m1--m3 in the m3 m3. In QD I, m1 is shifted to the right side of the dot while m-1 is shift to the left side. Higher states m2 and m3 show more symmetry in terms of position of the maxima relative to the center of the QD. The exact atomic structure of the defects could in principle be determined from a comparison of $dI/dV$ spectra with simulated first-principle LDOS signatures of expected defect types. In reality, this is hampered by the large number of possible geometries to simulate, including complex multiple defect structures together with the large unit cells of the semiconducting chiral SWNTs studied here. To better understand the origins of the quantized signatures of the states, we model the experimental $I/DV maps by solving the Schringer equation over a piecewise potential model. The resulting potential is approximated by a square potential stepimated by the scattering centers. The results are illustrated with gray shaded shaded with shaded grays illustrated with black shaded. The model is consistent with the same experimental conditions using the SWNT-induced defect-induced confinement in SWNT I and SWNT II. We attribute the observed lowest energy states asymmetry (for electrons as well as for holes) in part to their strong sensitivity to weak potential modulations within theQD structure (as we will show in section \ref{1D}). For instance, the left defect in QDI ($d3'$) has a larger scattering strength than the right one ($d4'$), while the right defect inQD II ($d7$) is larger than the left one (see Fig.~S2(a)-(d) in supplementary information). Such configurations have been reported to induce a rigid shift in the SW NT bands. In QD II, we attribute the spatial shift of m1 to a potential modulation induced by a layer of disordered impurities, most probably residua from the 1,2-dichloroethane suspension, lying between the gold substrate and theSWNT. For example, here is a picture of the ""suspended"" portion between two terraces in Fig. S2 (e)-(h) and Fig. (~S2) (a) (b) (c) (d) (e) (f) (g) (h) (i) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (1) (2) & (11), (12), (13), (14), (15), (1), (3), (4), (6), (7), (8), (9), (10), (11,7) and (12,8) {. (13,9) {:. (1,4) & 2,7, (13). (14,6,7),(11,8,1,1) ;. (b,3,4,3) ,. (4,5), (5,6),(7,7,. (7,8),(8,7). (9,4),(9,6). (10,5,1),(12,7%), (7,.7,1,.1,2), (2,3),(10,6) & 6,7.1, (8,5). (11). (12.1) & 7,6,.1) , (13,.7) & 8,6.2, (11.3) {.:. {. . . {.}. (12:5) & 12,6:7,(13,6);. (14:7) . (13:6) .. (15,8). (13.7) ,(14,7 areas in Fig. 2. Dispersion relations $E(k)$ can be extracted experimentally from the quantized states wavefunctions by measuring the energy and corresponding momenta in the left and right sides of the QDs. Taking into account the measured chiral angle, diameter distribution and measured bandgaps, we find the best match with chiralities for QD I and QD II. The experimental level spacings can then be approximated by tuning both barrier widths in the same ratio and the heights individually, knowing that the scattering strength of $d3'$ ($d7$) is larger than $d4' ($d6'$) according to the observed asymmetry in the LDOS described above. Increasing both barrier heights simultaneously shifts the bound state energy levels and level Spacings up. This leads to sharper bound states as the confinement in the QD is made stronger thus increasing the lifetime of the confined electrons. Increasing the barrier thickness with constant inner edge separation does not affect much thelevel spacings but further sharpens the bound states. Any asymmetry introduced by a change in the width or height of one single barrier leads to broader bound states and vice versa. The presence of a potential step modifies theLDOS in lifting the levels of thebound states, with a more pronounced effect on the lower states. The transmission probability through a rectangular tunneling barrier is given by $T=\left( 1+\frac{V^{2}\sinhuni2}\left( a \cdot \sqrt{2m^{*}(V-E)/\hbar \right)}{4E(V(E)\right)^{-1}$, where $V$ and $a$ are respectively the barrier height and width. In our case, this condition is not satisfied and thus the barrier geometries are tuned empirically to fit the experimental level spacings. We find a good match in the conduction band for the barrier heights for theQD I, I, and I, we say. For QD III, we show that the barrier width and barrier height can be tuned to the desired level spasings for the same distance from the center of the gap states. We conclude by showing that this method can be used to predict the shape of the next generation of superconductors in the future. We also show that this theory can be applied to the formation of superconductor nanotubes in the near future. The next step is to develop a new type of nanotube in the form of a superconducting nanowire. We call this new type the superconductor nanowired nanothenate. We show that it is possible to create nanotwnets in the next few years with the current state of the universe that are much more stable than those in the past. This type of Nanotwnet is known as a nanoturnable nanotransformer (NTN) and is a type of quantum superconduction (NSN) that can be predicted by the theory of quantum entanglement (see the next section for more details). We conclude that this type of NSN will be used for the development of new nanot tunable nanowires in the 2030s and 2030s. We say this because it can be shown to be more efficient than the previous method for the construction of quantum nanotunches in the 1990s and 2000s. It is also possible to use the same technique to predict future quantum tunnings in the 2040s and 2050s.  simulated LDOS shows a pattern with curved stripes oriented from bottom left to top right, as observed experimentally. In the valence band, although modes m-2 and lower do not show a well defined structure in the spatial direction, thinner barriers can reproduce the measured level spacings very well. For QD II, we observed that the measured energy levels are overestimated by a factor $\alpha\sim1.29$, presumably due to a voltage division effect induced by the impurity layer mentioned above (see details in supplementary information). We find a good agreement with the experimental LDOS with the parameters: $V_{d3'}=V_{ d4}\simeq$ 0.47 eV, $a_\mathrm{d6']=1.5$ nm and $U_\ Mathrm{C}= V_\Mathrm{L}-V_\ mathrm{R}\simesq 47$ meV. In order toidate the physical nature of the electron/hole confining scattering centers, we performed ab-initio simulations on a combination of combination of electron density functional theory and semiconducting SWNTs. Results for the most probable defect structures are likely being induced by vacancy defects induced by 200 eV Ar. Without the generality of Ar, we have simulated short unit cell semiconductoring zigzags with different combinations of different defect structures. The results are shown in Fig. a, b, c, d, e, f, g, h, i, j, k, l, m, n, s, t, p, v, w, P,vasp_p2, vasp_P2, Vasp_V2, W,VASP2,. Wannier, VASP1, W.P.2, and W.W.2 are shown as well as Green’s functions (see also supplementary information, Fig.S2(f) and Fig.~\ref{exp_data_Ar}(a) and (c) for a pristine SWNT with $E_’mathrm’n’(k_‘’’) data sets extracted from (e)-(g) (h) Tight-binding (black curve) and ab- initio dispersion relations (green circles) are shown for a. pristine $17,0)$ SWNT. For a pristine $16,0’ SWNT, we simulated short cell semiconductors with zigigzag ziggig zigzagged zig-zag patterns with different cell sizes. The resulting SWNT is shown inFig. ‘a’, ‘b’ and ‘c’ are shown to be about 11 nm in length, about 11.1 nm in a, and 11.5’ in length in a. ‘d’ is the number of ions in a SWNT that can be separated by about 11’ (see Fig.’a) ‘D’ shows that the SWNT can be made to behave like a semiconductor with a single cell, with the same number of electrons in each cell as the ‘zigzag’ cell. This is the same as the design for a single-cell SWNT in the previous section. The ‘C’ part of the diagram shows that this is the most likely defect structure to be induced by a vacancy defect. Curved stripe patterns oriented from top left to bottom right, indicating a larger scattering strength for DVs compared to SVs. This is consistent with observations in transport experiments. On the other hand, the patterns in the valence band strongly depend on the defect types. Discrete states can be distinguished for the DV-DV case, with m-2 being mixed with defect states. Also, broader states are observed, indicating that the scattering strength of DVs and SVs is weaker in theValence band compared to the conduction band. In order to demonstrate the scattering strengths of the different defects, we calculated the energy dependent scattering strengths for the different combinations of defects in SWNTs. For more information on the information we can observe, see the supplementary information for SWNT chiral chiral conditions and N ad-atoms in nanotube. The choice of the zig-zag chiral scenarios is motivated by the different masses of single vacancies and single vacancies for all three defects (double CNTs) with the largest DVs having the largest scattering strength in the CBB and V40 V.40 scenarios. For all three DV defects, single chemisorbed SV and C-N vacancies were tested with regard to the pristine CNT with the effective masses of 30-40% and 40-40%. For the single CNT vacancies with the same effective masses, we tested the single V-40 V and V-SV with the single DV-N vacancy and the single SV vacancy with theeffective masses of 20-30% and 20-25% respectively. For the CNT-V-N defect, we used the same techniques to test the single N-N CNT vacancy and single SV-V vacancy. The results were consistent with the TB model and with the ab-initio calculation for the pristinenanotube defect. We conclude that the potential of NAdatoms to confine carriers in semiconductingSWNTs and thus to generate intranotube QD states has been explored in the past and is still open to further research. We also show that NAdAtoms can be used to create nanotubes in a variety of ways, for example, by using a combination of Nadadatoms, N-V, and N-CNTs in a single-atom semiconductor device. For details on the defects geometries, please see the Supplementary Information for the SWNT and the NAdAToms in the NADATOMS section of this article. We will also provide details on how to use these NADatoms for the development of new semiconductors in the future, such as the next generation of the X-ray source, the next-generation CNT, and the next Generation X-Ray Source (X-ray Source, X-RTS) and X-NTS-S, as well as the potential to use them in a range of applications such as solar cells, batteries, and other nanoelectronic devices. We hope to use this information to help develop new nanotubes in the next few years. We are currently working on a project to develop a new type of semiconductor that could be used as a source of low-cost, low-power, low voltage nanotuberculosis treatments in the near future. For now, we are focusing on the nanotUBes in a number of applications, including a new form of nanotobes that could have a large impact on the environment in the developing world. We expect to release a version of this report in the coming months. QDs generated by DVs will have far superior stability at room temperature due to their high migration barrier above 5 eV. In the experimental case, the carriers can decay through the substrate, thus limiting their lifetime. This leads to state broadening, measured between about 60 meV up to 120 meV in QD I and II, while the quantized states widths in ab-initio simulations vary between about 5 meV and 45 meV. This suggests that a better contrast of the experimental quantizedStates, especially in the valence band, could be achieved by lowering the nanotubes-substrate interaction through $e.g. the insertion of atomically thin insulating NaCl films. This would allow to gain more insight on the electronic structure of the QDs as well as in the associated scattering physics at the confining defects. These results, combined with recent progresses in type and spatial control in the formation of defects, hold a high potential for applications in the design of SWNT based quantum devices. In this context, the observation of quantum confinement effects in the emitted light of cut, sub-10 nm, semiconducting SWNTs shall be seen as an additional motivation for investigating the optical properties of our ""QD with leads"" building-blocks. The authors thank Ethan Minot, Lee Aspitarte, Jhon Gonzalez, Andres Ayuela, Omjoti Dutta and Arkady Krasheninnikov for fruitful discussions. The work of DB is supported by Spanish Ministerio de Econom\'ia y Competitividad (MINECO) through the project  FIS2014-55987-P and by the (LTC) QuantumChemPhys. LM acknowledges support from the BMBF-project WireControl (FKZ16ES0294) and computing time for the supercomputers JUROPA and JURECA at the Juriach Supercomputer Centre (JSC). The authors also thank the Russian National Research Council for their support in the development of the DV-based nanotube quantum dots. The study was published in the open-access journal, The Journal of Nanotube Nanotechnology (JNC) (http://www.jnanotube.org/article/doi/full/10.1145/JNC/DV-QD-QDs-With-Leads-with-Leads.html). The study is also available in the online edition of the journal, ""The Journal of Quantum Nanotechnology"" ( http:// www.jnc.org/. The journal is available in three languages: English, French and German. The article is available for free on the JNC's JNC website and the JSC's JSC website. The JSC also offers a free download of the full version of the book, which includes a free print of the entire book, including the hard-to-print PDF."
What is the purpose of an ICD?,"Do you know the difference between V.T. and T.V?
Like any exclusive club, heart disease has its own jargon, understandable only by other members of the club, particularly by cardiac care providers. For example, I remember lying in my CCU bed (that’s the Coronary Intensive Care Unit), trying to memorize the letters LAD (that’s the Left Anterior Descending, the large coronary artery whose 99% blockage had caused my MI (myocardial infarction – in my case, the so-called ‘widowmaker’ heart attack).
To help others needing simultaneous translation of this new lingo in your research or in your own medical records, here’s a helpful list of some of the most common acronyms/terms you’ll likely find around the cardiac ward.
NOTE from CAROLYN: This entire patient-friendly, jargon-free glossary (all 8,000 words!) is also part of my book “A Woman’s Guide to Living with Heart Disease“ (Johns Hopkins University Press, November 2017).
AA – Anti-arrhythmic: Drugs used to treat patients who have irregular heart rhythms.
Ablation – See Cardiac Ablation.
ACE Inhibitor – Angiotension Converting Enzyme inhibitor: A drug that lowers blood pressure by interfering with the breakdown of a protein-like substance involved in regulating blood pressure.
ACS – Acute Coronary Syndrome: An emergency condition brought on by sudden reduced blood flow to the heart. The first sign of acute coronary syndrome can be sudden stopping of your heart (cardiac arrest).
AED – Automatic External Defibrillator: A portable defibrillator for use during a cardiac emergency; it can be used on patients experiencing sudden cardiac arrest by applying a brief electroshock to the heart through electrodes placed on the chest.
AF or Afib – Atrial Fibrillation: An irregular and often rapid heart rate that can cause poor blood flow to the body. Afib symptoms include heart palpitations, shortness of breath, weakness or fainting. Episodes of atrial fibrillation can come and go, or you may have chronic atrial fibrillation.
AFL – Atrial Flutter: A type of arrhythmia where the upper chambers of the heart (the atria) beat very fast, causing the walls of the lower chambers (the ventricles) to beat inefficiently as well.
A-HCM – Apical Hypertrophic Cardiomyopathy: Also called Yamaguchi Syndrome or Yamaguchi Hypertrophy, a non-obstructive form of cardiomyopathy (a disease of the heart muscle that leads to generalized deterioration of the muscle and its pumping ability) in which a portion of the heart muscle is hypertrophied (thickened) without any obvious cause although there may be a genetic link. It was first described in individuals of Japanese descent.
AI – Aortic Insufficiency: A heart valve disease in which the aortic valve does not close tightly, leading to the backward flow of blood from the aorta (the largest blood vessel) into the left ventricle (a chamber of the heart).
AIVR – Accelerated Idioventricular Rhythm: Ventricular rhythm whose rate is greater than 49 beats/min but less than 100 beats/min, usually benign. (Ventricles are the two main chambers of the heart, left and right).
Angina (stable) – A condition marked by distressing symptoms typically between neck and navel that come on with exertion and go away with rest, caused by an inadequate blood supply to the heart muscle typically because of narrowed coronary arteries feeding the heart muscle. Also known as Angina Pectoris. Unstable angina (UA) occurs when fatty deposits (plaques) in a blood vessel rupture or a blood clot forms, blocking or reducing flow through a narrowed artery, suddenly and severely decreasing blood flow to the heart muscle. Unstable angina is not relieved by rest; it’s dangerous and requires emergency medical attention.
Antiplatelet drugs – Medications that block the formation of blood clots by preventing the clumping of platelets (examples: Plavix, Effient, Brillinta, Ticlid, etc). Heart patients, especially those with implanted stents after PCI, are often prescribed dual antiplatelet therapy (DAPT) which includes one of these prescribed meds along with daily low-dose aspirin.
Aorta – The main artery of the body, carrying blood from the left side of the heart to the arteries of all limbs and organs except the lungs.
Aortic Stenosis: A disease of the heart valves in which the opening of the aortic valve is narrowed. Also called AS.
Aortic valve – One of four valves in the heart, this valve allows blood from the left ventricle to be pumped up (ejected) into the aorta, but prevents blood from returning to the heart once it’s in the aorta.
AP – Apical Pulse: A central pulse located at the apex (pointy bottom) of the heart.
Apex – the lowest (pointy) tip of the heart that points downward at the base, forming what almost looks like a rounded point.
Apical Hypertrophic Cardiomyopathy (A-HCM): Also called Yamaguchi Syndrome or Yamaguchi Hypertrophy, a non-obstructive form of cardiomyopathy (a disease of the heart muscle that leads to generalized deterioration of the muscle and its pumping ability) in which a portion of the heart muscle is hypertrophied (thickened) without any obvious cause. There may be a genetic link. It was first described in people of Japanese descent.
Arrhythmia – A condition in which the heart beats with an irregular or abnormal rhythm.
AS – Aortic Stenosis: A disease of the heart valves in which the opening of the aortic valve is narrowed.
ASD – Atrial Septal Defect: See Septal Defect.
Atrial Flutter – A heart rhythm problem (arrhythmia) originating from the right atrium, most often involving a large circuit that travels around the area of the tricuspid valve (between the right atrium and the right ventricle (this is called typical atrial flutter). Less commonly, atrial flutter can also result from circuits in other areas of the right or left atrium that cause the heart to beat fast (called atypical atrial flutter).
Atrial Septum, the membrane that separates the left and the right upper chambers of the heart (the atria).
Atrium – A chamber of the heart that receives blood from the veins and forces it into a ventricle or ventricles. Plural: atria.
AV – Atrioventricular: A group of cells in the heart located between the upper two chambers (the atria) and the lower two chambers (the ventricles) that regulate the electrical current that passes through it to the ventricles. Also Atrioventricular Block: An interruption or disturbance of the electrical signal between the heart’s upper two chambers (the atria) and lower two chambers (the ventricles). Also Aortic valve: The valve that regulates blood flow from the heart into the aorta.
AVNRT – Atrioventricular Nodal Re-entry Tachycardia: a heart rhythm problem that happens when there’s an electrical short circuit in the centre of the heart, one of the most common types of SVT, most often seen in people in their twenties and thirties, and more common in women than in men.
BAV – Bicuspid Aortic Valve: The most common malformation of the heart valves in which the aortic valve has only two cusps instead of three.
BB – Beta Blocker: A blood pressure-lowering drug that limits the activity of epinephrine, a hormone that increases blood pressure.
BBB – Bundle Branch Block: – A condition in which parts of the heart’s conduction system are defective and unable to normally conduct the electrical signal, causing an irregular heart rhythm (arrhythmia).
BMI – Body mass index: A number that doctors use to determine if you’re overweight. BMI is calculated using a formula of weight in kilograms divided by height in meters squared (BMI =W [kg]/H [m2]). Better yet, just click here to figure out your own BMI.
BNP blood test – BNP (B-type Natriuretic Peptide) is a substance secreted from the ventricles or lower chambers of the heart in response to changes in pressure that happen when heart failure develops and/or worsens. The level of BNP in the blood increases when heart failure symptoms worsen, and decreases when the heart failure condition is stable.
BP – Blood Pressure: The force or pressure exerted by the heart in pumping blood; the pressure of blood in the arteries. See also hypertension.
BrS – Brugada Syndrome: Brugada syndrome is a genetic heart disease that is characterized by distinctively abnormal electrocardiogram (EKG/ECG) findings and an increased risk of sudden cardiac arrest.
CAA – Coronary artery anomaly: A congenital defect in one or more of the coronary arteries of the heart.
CABG – Coronary Artery Bypass Graft: A surgical procedure that reroutes blood flow around a diseased or blocked blood vessel that supplies blood to the heart by grafting either a piece of vein harvested from the leg or the artery from under the breastbone.
CA – Coronary Artery: The arteries arising from the aorta that arch down over the top of the heart and divide into branches. They provide blood to the heart muscle.
CAD – Coronary Artery Disease: A narrowing of the arteries that supply blood to the heart. The condition results from a plaque rupture/blood clot or spasm and greatly increases the risk of a heart attack.
Cardiac Ablation – A procedure performed by an Electrophysiologist (EP) – a cardiologist with specialized training in treating heart rhythm problems – that typically uses catheters — long, flexible tubes inserted through a vein in the groin and threaded to the heart — to correct structural problems in the heart that cause an arrhythmia. Cardiac ablation works by scarring or destroying the tissue in your heart that triggers an abnormal heart rhythm.
Cardiac Arrest – Also known as Sudden Cardiac Arrest: The stopping of the heartbeat, usually because of interference with the electrical signal that regulates each heartbeat (often associated with coronary heart disease). Can lead to Sudden Cardiac Death.
Cardiac Catheterization – An invasive procedure in which a catheter is inserted through a blood vessel in the wrist/arm or groin with x-ray guidance. This procedure can help provide information about blood supply through the coronary arteries, blood pressure, blood flow throughout the chambers of the heart, collection of blood samples, and x-rays of the heart’s ventricles or arteries. It’s typically performed in the cath lab during angiography.
Cardiac Resynchronization Therapy (CRT) also called bi-ventricular pacemaker: an electronic pacing device that’s surgically implanted in the chest to treat the delay in heart ventricle contractions that occur in some people with heart failure.
Cardiac Tamponade – Pressure on the heart that occurs when blood or fluid builds up in the space between the heart muscle (myocardium) and the outer covering sac of the heart (pericardium). Also called Tamponade.
Cardiomyopathy – a chronic disease of the heart muscle (myocardium), in which the muscle is abnormally enlarged, thickened, and/or stiffened.
Cardioversion – A medical procedure in which an abnormally fast heart rate (tachycardia) or cardiac arrhythmia like atrial fibrillation is converted to a normal rhythm using electricity or drugs. Synchronized electrical cardioversion uses a therapeutic dose of electric current to the heart at a specific moment in the cardiac cycle. Chemical cardioversion uses medications to convert to normal rhythm.
Cath lab – the room in the hospital/medical clinic where cardiac catheterization procedures take place (for example, when a stent is implanted into a blocked coronary artery).
CCB – Calcium Channel Blocker: A drug that lowers blood pressure by regulating calcium-related electrical activity in the heart.
CDS – Cardiac Depression Scale: A scale that can help assess the effects of depression occurring as a result of a heart disease diagnosis.
CHF – Heart Failure (also called Congestive Heart Failure): A condition in which the heart cannot pump all the blood returning to it, leading to a backup of blood in the vessels and an accumulation of fluid in the body’s tissues, including the lungs.
CM – Cardiomyopathy: A disease of the heart muscle that leads to generalized deterioration of the muscle and its pumping ability.
CO – Cardiac Output: The amount of blood the heart pumps through the circulatory system in one minute.
Collateral arteries – These extra coronary blood vessels are sometimes able to bypass a blockage in an artery in order to supply enough oxygenated blood to enable the heart muscle to survive when in danger of being damaged because of blockage(s).
Collateral arteries – Blood vessels that provide an alternative arterial supply of blood to an area of the heart that’s in danger of being deprived of oxygenated blood because of one or more blocked arteries.
Congenital heart defect – one of about 35 different types of heart conditions that happen when the heart or the blood vessels near the heart don’t develop normally before a baby is born (in about 1% of live births). Because of medical advances that treat babies born with heart defects, there are now for the first time more adults with congenital heart disease than children.
Congestive heart failure (CHF) – a chronic progressive condition that affects the pumping power of your heart muscle. Often referred to simply as heart failure, CHF specifically refers to the stage in which fluid builds up around the heart and causes it to pump inefficiently.
COPD – Chronic Obstructive Pulmonary Disease: A lung disease defined by persistently poor airflow as a result of breakdown of lung tissue (known as emphysema) and dysfunction of the small airways.Often associated with smoking, it typically worsens over time.
Coronary Microvascular Disease – A heart condition that causes impaired blood flow to the heart muscle through the small vessels of the heart. Also called Microvascular Disease or Small Vessel Disease.
Coronary Reactivity Test – An angiography procedure specifically designed to examine the blood vessels in the heart and how they respond to different medications. Physicians use these images to distinguish different types of blood vessel reactivity dysfunction (such as Coronary Microvascular Disease).
Costochondritis– the cause of severe chest pain, but NOT heart-related; it’s an inflammation of the cartilage that connects a rib to the breastbone.
Coumadin – A drug taken to prevent the blood from clotting and to treat blood clots. Coumadin is believed to reduce the risk of blood clots causing strokes or heart attacks. See also Warfarin.
Cox Maze procedure – A complex “cut-and-sew” surgical procedure done to treat atrial fibrillation through a complicated set of incisions made in a maze-like pattern on the left and right atria (the upper chambers of the heart) to permanently interrupt the abnormal electrical signals that are causing the irregular heartbeats of Afib. See also: Mini-Maze.
CP – Chest Pain (may also be felt as squeezing, pressure, fullness, pressure, heaviness, burning or tightness in the chest).
CPR – Cardiopulmonary Resuscitation: An emergency procedure in which the heart and lungs are made to work by manually compressing the chest overlying the heart and forcing air into the lungs, used to maintain circulation when the heart stops pumping during Cardiac Arrest. Current guidelines suggest hands-only CPR. See also AED.
CQ10 – Co-enzyme Q10: A dietary supplement sometimes recommended for heart patients taking statin drugs.
CRP – C-reactive protein: A byproduct of inflammation, produce by the liver, found in the blood in some cases of acute inflammation.
CRT – Cardiac Resynchronization Therapy also called bi-ventricular pacemaker: an electronic pacing device that’s surgically implanted in the chest to treat the delay in heart ventricle contractions that occur in some people with heart failure.
CT – Computed tomography (CT or CAT scan): An x-ray technique that uses a computer to create cross-sectional images of the body.
CTA – Computerized Tomographic Angiogram: An imaging test to look at the arteries that supply the heart muscle with blood. Unlike a traditional coronary angiogram, CT angiograms don’t use a catheter threaded through your blood vessels to your heart but instead rely on a powerful X-ray machine to produce images of your heart and heart vessels.
CV – Coronary Vein: One of the veins of the heart that drain blood from the heart’s muscular tissue and empty into the right atrium.
CV – Cardiovascular: Pertaining to the heart and blood vessels that make up the circulatory system.
DBP – Diastolic blood pressure: The lowest blood pressure measured in the arteries. It occurs when the heart muscle is relaxed between beats.
DCM – Dilated Cardiomyopathy: A disease of the heart muscle, primarily affecting the heart’s main pumping chamber (left ventricle). The left ventricle becomes enlarged (dilated) and can’t pump blood to your body with as much force as a healthy heart can.
DDI – Drug-drug interaction: A situation in which a medication affects the activity of another medication when both are administered together.
DIL – Diltiazem: A calcium channel blocker drug that acts as a vasodilator; used in the treatment of angina pectoris, hypertension, and supraventricular tachycardia.
Diuretic – A class of drugs used to lower blood pressure. Also known as “water pills”.
Dobutamine stress echocardiography: This is a form of a stress echocardiogram diagnostic test. But instead of exercising on a treadmill or exercise bike to stress the heart, the stress is obtained by giving a drug that stimulates the heart and makes it “think” it’s exercising. The test is used to evaluate your heart and valve function if you are unable to exercise. It is also used to determine how well your heart tolerates activity, and your likelihood of having coronary artery disease (blocked arteries), and it can evaluate the effectiveness of your cardiac treatment plan. See also TTE and Stress Echocardiogram.
Dressler’s syndrome – Happens to a small number of people three to four weeks after a heart attack. The heart muscle that died during the attack sets the immune system in motion, calling on lymphocytes, one of the white blood cells, to infiltrate the coverings of the heart (pericardium) and the lungs (pleura). It also starts generating antibodies, which attack those two coverings. Chest pain (CP) is the predominant symptom; treated with anti-inflammatory drugs.
Dual Antiplatelet Therapy – Medications that block the formation of blood clots by preventing the clumping of platelets (examples Plavix, Effient, Brillinta, Ticlid, etc.) are often prescribed along with aspirin as part of what’s known as dual antiplatelet therapy, especially to patients who have undergone PCI and stent implantation.
DVT – Deep Vein Thrombosis: A blood clot in a deep vein in the calf.
ECG / EKG – Electrocardiogram: A test in which several electronic sensors are placed on the body to monitor electrical activity associated with the heartbeat.
Ectopic beats – small changes in an otherwise normal heartbeat that lead to extra or skipped heartbeats, often occurring without a clear cause, most often harmless.
EF – Ejection Fraction: A measurement of blood that is pumped out of a filled ventricle. The normal rate is 50-60%.
EKG/ECG – Electrocardiogram: A test in which several electronic sensors are placed on the body to monitor electrical activity associated with the heartbeat.
Endothelium: A single-cell layer of flat endothelial cells lining the closed internal spaces of the body such as the inside of blood vessels. Endothelial dysfunction affects the ability of these cells to help dilate blood vessels, control inflammation or prevent blood clots. The endothelium is associated with most forms of cardiovascular disease, such as hypertension, coronary artery disease, chronic heart failure, peripheral vascular disease, diabetes, chronic kidney failure, and severe viral infections.
Enhanced External Counterpulsation – EECP is an FDA-approved non-invasive, non-drug treatment for angina. It works by promoting the development of collateral coronary arteries. The therapy is widely used in prominent heart clinics such as the Cleveland Clinic, Mayo Clinic and Johns Hopkins – especially for patients who are not good candidates for invasive procedures such as bypass surgery, angioplasty or stenting.
EP – Electrophysiologist: A cardiologist who has additional training in diagnosing/treating heart rhythm disorders.
EPS – Electrophysiology Study: A test that uses cardiac catheterization to study patients who have arrhythmias (abnormal hear rhythm). An electrical current stimulates the heart in an effort to provoke an arrhythmia, which is immediately treated with medications. EPS is used primarily to identify the origin of the arrhythmia and to test the effectiveness of medications used to treat abnormal heart rhythms.
EVH – Endoscopic Vessel Harvesting: To create the bypass graft during CABG open heart surgery, a surgeon will remove or “harvest” healthy blood vessels from another part of the body, often from the patient’s leg or arm. This vessel becomes a graft, with one end attaching to a blood source above and the other end below the blocked area. See CABG.
Exercise stress test – An exercise test (walking/running on a treadmill or pedalling a stationary bike) to make your heart work harder and beat faster. An EKG is recorded while you exercise to monitor any abnormal changes in your heart under stress, with or without the aid of drugs to enhance this assessment. See also: MIBI, Echocardiogram, Nuclear Stress Test.
Familial hypercholesterolemia (FH) – A genetic predisposition to dangerously high cholesterol levels. FH is an inherited disorder that can lead to aggressive and premature cardiovascular disease, including problems like heart attacks, strokes, or narrowing of the heart valves.
Femoral Artery: a major artery in your groin/upper thigh area, through which a thin catheter is inserted, eventually making its way into the heart during angioplasty to implant a stent; currently the most widely used angioplasty approach in the United States, but many other countries now prefer the Radial Artery access in the wrist.
FFR – Fractional Flow Reserve: A test used during coronary catheterization (angiogram) to measure pressure differences across a coronary artery stenosis (narrowing or blockage) defined as as the pressure behind a blockage relative to the pressure before the blockage.
HC – High Cholesterol: When fatty deposits build up in your coronary arteries.
HCTZ – Hydrochlorothiazide: A drug used to lower blood pressure; it acts by inhibiting the kidneys’ ability to retain water. Used to be called “water pills”.
Heart Failure – a chronic progressive condition that affects the pumping power of your heart muscle. Sometimes called Congestive Heart Failure (CHF).
Holter Monitor – A portable monitoring device that patients wear for recording heartbeats over a period of 24 hours or more.
HTN – Hypertension: High blood pressure, the force of blood pushing against the walls of arteries as it flows through them.
Hypokinesia – Decreased heart wall motion during each heartbeat, associated with cardiomyopathy, heart failure, or heart attack. Hypokinesia can involve small areas of the heart (segmental) or entire sections of heart muscle (global). Also called hypokinesis.
ICD – Implantable Cardioverter Defibrillator: A surgically implanted electronic device to treat life-threatening heartbeat irregularities.
IHD – Ischemic Heart Disease: heart problems caused by narrowing of the coronary arteries, causing a decreased blood supply to the heart muscle. Also called coronary artery disease and coronary heart disease.
INR – International Normalized Ratio: A laboratory test measure of blood coagulation, often used as a standard for monitoring the effects of the anti-coagulant drug, warfarin (coumadin).
IST – Inappropriate sinus tachycardia: A heart condition seen most often in young women, in which a person’s resting heart rate is abnormally high (greater than 100 bpm), their heart rate increases rapidly with minimal exertion, and this rapid heart rate is accompanied by symptoms of palpitations, fatigue, and/or exercise intolerance.
Interventional cardiologist – A cardiologist who is trained to perform invasive heart procedures like angiography, angioplasty, percutaneous coronary intervention (PCI), implanting stents, etc.
IVS – Interventricular Septum: The stout wall that separates the lower chambers (the ventricles) of the heart from one another.
IVUS – Intravascular Ultrasound: A form of echocardiography performed during cardiac catheterization in which a transducer (a device that can act as a transmitter (sender) and receiver of ultrasound information) is threaded into the heart blood vessels via a catheter; it’s used to provide detailed information about the blockage inside the blood vessels.
LAD – Left Anterior Descending coronary artery: One of the heart’s coronary artery branches from the left main coronary artery which supplies blood to the left ventricle.
LAFB – Left Anterior Fascicular Block: A cardiac condition,distinguished from Left Bundle Branch Block because only the anterior half of the left bundle branch is defective and more common than left posterior fascicular block.
LAHB – Left Anterior Hemiblock: The Left Bundle Branch divides into two major branches – the anterior and the posterior fascicles. Occasionally, a block can occur in one of these fascicles.
Left Circumflex Artery – The artery carries oxygenated blood from the heart to the body; it’s a branch of the Left Main Coronary Artery after the latter runs its course in between the aorta and the Main Pulmonary Artery.
Left Main Coronary Artery – The artery that branches from the aorta to supply oxygenated blood to the heart via the Left Anterior Descending Artery (LAD) and the Left Circumflex Artery.
Lipids – fat-like substances found in your blood and body tissues; a lipid panel is a blood test that measures the level of specific lipids in blood to help assess your risk of cardiovascular disease, measuring four types of lipids: total cholesterol, HDL cholesterol, LDL cholesterol, and triglycerides.
Lipoprotein-a or Lp(a) – molecules made of proteins and fat, carrying cholesterol and similar substances through the blood. A high level of Lp(a) is considered a risk factor for heart disease; detectable via a blood test.
Long QT syndrome (LQTS): A heart rhythm disorder that can potentially cause fast, chaotic heartbeats that may trigger a sudden fainting spell or seizure. In some cases, the heart may beat erratically for so long that it can cause sudden death.
LV – Left Ventricle – One of four chambers (two atria and two ventricles) in the human heart, it receives oxygenated blood from the left atrium via the mitral valve, and pumps it into the aorta via the aortic valve.
LVAD – Left ventricular assist device: A mechanical device that can be placed outside the body or implanted inside the body. An LVAD does not replace the heart – it “assists” or “helps” it pump oxygen-rich blood from the left ventricle to the rest of the body, usually as a bridge to heart transplant.
LVH – Left Ventricular Hypertrophy: A thickening of the myocardium (muscle) of the Left Ventricle (LV) of the heart..
Lumen – The hollow area within a tube, such as a blood vessel.
Main Pulmonary Artery – Carries oxygen-depleted blood from the heart to the lungs.
MIBI – Nuclear Stress Test/Cardiac Perfusion Scan/Sestamibi: tests that are used to assess the blood flow to the heart muscle (myocardium) when it is stressed by exercise or medication, and to find out what areas of the myocardium have decreased blood flow due to coronary artery disease. This is done by injecting a tiny amount of radionuclide like thallium or technetium (chemicals which release a type of radioactivity called gamma rays) into a vein in the arm or hand.
Microvascular disease – a heart condition that causes impaired blood flow to the heart muscle through the small blood vessels of the heart. Symptoms mimic those of a heart attack. Also called Coronary Microvascular Disease or Small Vessel Disease. I live with this diagnosis and have written more about it here, here and here.
Mini-Maze – a surgical procedure to treat atrial fibrillation, less invasive than what’s called the Cox Maze III procedure (a “cut-and-sew” procedure), and performed on a beating heart without opening the chest.
Mitral Valve: One of four valves in the heart, the structure that controls blood flow between the heart’s left atrium (upper chamber) and left ventricle (lower chamber). The mitral valve has two flaps (cusps). See also MV and/or Valves.
Mitral valve prolapse: a condition in which the two valve flaps of the mitral valve don’t close smoothly or evenly, but instead bulge (prolapse) upward into the left atrium; also known as click-murmur syndrome, Barlow’s syndrome or floppy valve syndrome.
MR – Mitral regurgitation: (also mitral insufficiency or mitral incompetence) a heart condition in which the mitral valve does not close properly when the heart pumps out blood. It’s the abnormal leaking of blood from the left ventricle, through the mitral valve and into the left atrium when the left ventricle contracts.
MRI – Magnetic Resonance Imaging: A technique that produces images of the heart and other body structures by measuring the response of certain elements (such as hydrogen) in the body to a magnetic field. An MRI can produce detailed pictures of the heart and its various structures without the need to inject a dye.
MS – Mitral Stenosis: A narrowing of the mitral valve, which controls blood flow from the heart’s upper left chamber (the left atrium) to its lower left chamber (the left ventricle). May result from an inherited (congenital) problem or from rheumatic fever.
MUGA – Multiple-Gated Acquisition Scanning: A non-invasive nuclear test that uses a radioactive isotope called technetium to evaluate the functioning of the heart’s ventricles.
Murmur – Noises superimposed on normal heart sounds. They are caused by congenital defects or damaged heart valves that do not close properly and allow blood to leak back into the originating chamber.
MV – Mitral Valve: The structure that controls blood flow between the heart’s left atrium (upper chamber) and left ventricle (lower chamber).
Myocardial Infarction (MI, heart attack) – The damage or death of an area of the heart muscle (myocardium) resulting from a blocked blood supply to the area. The affected tissue dies, injuring the heart.
Myocardium – The muscular tissue of the heart.
New Wall-Motion Abnormalities – Results seen on an echocardiogram test report (see NWMA, below).
Nitroglycerin – A medicine that helps relax and dilate arteries; often used to treat cardiac chest pain (angina). Also called NTG or GTN.
NSR – Normal Sinus Rhythm: The characteristic rhythm of the healthy human heart. NSR is considered to be present if the heart rate is in the normal range, the P waves are normal on the EKG/ECG, and the rate does not vary significantly.
NSTEMI – Non-ST-segment-elevation myocardial infarction: The milder form of the two main types of heart attack. An NSTEMI heart attack does not produce an ST-segment elevation seen on an electrocardiogram test (EKG). See also STEMI.
Nuclear Stress Test – A diagnostic test that usually involves two exercise stress tests, one while you’re exercising on a treadmill/stationary bike or with medication that stresses your heart, and another set while you’re at rest. A nuclear stress test is used to gather information about how well your heart works during physical activity and at rest. See also: Exercise stress test, Nuclear perfusion test, MIBI.
Open heart surgery – Any surgery in which the chest is opened and surgery is done on the heart muscle, valves, coronary arteries, or other parts of the heart (such as the aorta). See also CABG.
Pacemaker – A surgically implanted electronic device that helps regulate the heartbeat.
PAD – Peripheral Artery Disease: A common circulatory problem in which narrowed arteries reduce blood flow to the limbs, usually to the legs. Symptoms include leg pain when walking (called intermittent claudication).
PAF – Paroxysmal Atrial Fibrillation: Atrial fibrillation that lasts from a few seconds to days, then stops on its own. See also Atrial Fibrillation.
Palpitations – A noticeably rapid, strong, or irregular heartbeat due to agitation, exertion or illness.
Paroxysmal Atrial Fibrillation – An unusual heart arrhythmia of unknown origin, at one time believed to be associated with an unusual sensitivity to alcohol consumption.
PDA – patent ductus arteriosus: A persistent opening between two major blood vessels leading from the heart. The opening is called ductus arteriosus and is a normal part of a baby’s circulatory system before birth that usually closes shortly after birth. But when it remains open, it’s called a patent ductus arteriosus. If it’s small, it may never need treatment, but a large PDA left untreated can allow poorly oxygenated blood to flow in the wrong direction, weakening the heart muscle and causing heart failure or other complications.
Pericardium: two thin layers of a sac-like tissue that surround the heart, hold it in place and help it work.
PET – Positron Emission Tomography: A non-invasive scanning technique that uses small amounts of radioactive positrons (positively charged particles) to visualize body function and metabolism. In cardiology, PET scans are used to evaluate heart muscle function in patients with coronary artery disease or cardiomyopathy.
PFO – Patent Forman Ovale: An opening between the left and right atria (the upper chambers) of the heart. Everyone has a PFO before birth, but in 1 out of every 3 or 4 people, the opening does not close naturally as it should after birth.
Plaque – A deposit of fatty (and other) substances in the inner lining of the artery wall; it is characteristic of atherosclerosis.
POTS – Postural Orthostatic Tachycardia Syndrome: A disorder that causes an increased heart rate when a person stands upright.
PPCM – Post-partum cardiomyopathy: A form of cardiomyopathy that causes heart failure toward the end of pregnancy or in the months after delivery, in the absence of any other cause of heart failure.
Preeclampsia – a late-pregnancy complication identified by spikes in blood pressure, protein in the urine, possible vision problems. Women who experience pregnancy complications like preeclampsia are at significantly higher risk for heart disease.
Prinzmetal’s Variant Angina – Chest pain caused by a spasm in a coronary artery that supplies blood to the heart muscle.
PSVT – Paroxysmal Supraventricular Tachycardia: – An occasional rapid heart rate (150-250 beats per minute) that is caused by events triggered in areas above the heart’s lower chambers (the ventricles). “Paroxysmal” means from time to time. See also supraventricular tachycardia (SVT).
Pulmonary Valve: One of the four valves in the heart, located between the pulmonary artery and the right ventricle of the heart, moves blood toward the lungs and keeps it from sloshing back into the heart.
PV – Pulmonary Vein: A vein carrying oxygenated blood from the lungs to the left atrium of the heart.
PVC – Premature Ventricular Contraction: An early or extra heartbeat that happens when the heart’s lower chambers (the ventricles) contract too soon, out of sequence with the normal heartbeat. In the absence of any underlying heart disease, PVCs do not generally indicate a problem with electrical stability, and are usually benign.
RA – Right Atrium: The right upper chamber of the heart. The right atrium receives de-oxygenated blood from the body through the vena cava and pumps it into the right ventricle which then sends it to the lungs to be oxygenated.
Radial Artery: the artery in the wrist where a thin catheter is inserted through the body’s network of arteries in the arm and eventually into the heart during a procedure to implant a stent. Doctors may also call this transradial access, the transradial approach, or transradial angioplasty. Because it’s associated with fewer complications, this is increasingly considered the default access approach in most countries, except in the U.S. where the traditional Femoral Artery (groin) approach is still the most popular access.
RBBB – Right Bundle Branch Block: A delay or obstruction along the pathway that electrical impulses travel to make your heart beat. The delay or blockage occurs on the pathway that sends electrical impulses to the right side of your heart. See also Left Bundle Branch Block.
RCA – Right Coronary Artery: An artery that supplies blood to the right side of the heart.
Restenosis – The re-closing or re-narrowing of an artery after an interventional procedure such as angioplasty or stent placement. Sometimes called “stent failure”.
RHD – Rheumatic Heart Disease: Permanent damage to the valves of the heart caused especially by repeated attacks of rheumatic fever.
RM – Right Main coronary artery: A blood vessel that supplies oxygenated blood to the walls of the heart’s ventricles and the right atrium.
RV – Right Ventricle: The lower right chamber of the heart that receives de-oxygenated blood from the right atrium and pumps it under low pressure into the lungs via the pulmonary artery.
SA – Sinus node: The “natural” pacemaker of the heart. The node is a group of specialized cells in the top of the right atrium which produces the electrical impulses that travel down to eventually reach the ventricular muscle, causing the heart to contract.
SB – Sinus Bradycardia: Abnormally slow heartbeat.
SBP – Systolic Blood Pressure: The highest blood pressure measured in the arteries. It occurs when the heart contracts with each heartbeat. Example: the first number in 120/80.
SCAD – Spontaneous Coronary Artery Dissection: A rare emergency condition that occurs when a tear forms in one of the blood vessels in the heart, causing a heart attack, abnormalities in heart rhythm and/or sudden death. SCAD tends to strike young healthy women with few if any cardiac risk factors.
SD – Septal defect: A hole in the wall of the heart separating the atria (two upper chambers of the heart) or in the wall of the heart separating the ventricles (two lower chambers).
Sestamibi stress test – See MIBI.
Short QT intervals (SQT): An abnormal heart rhythm where the heart muscle takes a shorter time to recharge between beats. It can cause a variety of complications from fainting and dizziness to sudden cardiac arrest.
Sick Sinus Syndrome (also known as sinus node dysfunction) is caused by an electrical problem in the heart; a group of related heart conditions that can affect how the heart beats, most commonly in older adults, although it can be diagnosed in people of any age. “Sick sinus” refers to the sinoatrial node (see below). In people with sick sinus syndrome, the SA node does not function normally.
Sinoatrial node (SA): also commonly called the sinus node; it’s a small bundle of neurons situated in the upper part of the wall of the right atrium (the right upper chamber of the heart). The heart’s electrical impulses are generated there. It’s the normal natural pacemaker of the heart and is responsible for the initiation of each heartbeat.
Spontaneous Coronary Artery Dissection (SCAD) – A rare emergency condition that occurs when a tear forms in one of the blood vessels in the heart, causing a heart attack, abnormalities in heart rhythm and/or sudden death. SCAD tends to strike young healthy women with few if any cardiac risk factors.
SSS – Sick Sinus Syndrome: The failure of the sinus node to regulate the heart’s rhythm.
ST – Sinus Tachycardia: A heart rhythm with elevated rate of impulses originating from the sinoatrial node, defined as greater than 100 beats per minute (bpm) in an average adult. The normal heart rate in the average adult ranges from 60–100 bpm. Also called sinus tach or sinus tachy.
Statins – Any of a class of drugs that lower the levels of low-density lipoproteins (LDL) – the ‘bad’ cholesterol in the blood – by inhibiting the activity of an enzyme involved in the production of cholesterol in the liver. Examples of brand name statins: Lipitor, Crestor, Zocor, Mevacor, Levachol, Lescol, etc. Also available as a cheaper generic form of the drug.
STEMI – ST-elevation heart attack (myocardial infarction). The more severe form of the two main types of heart attack. A STEMI produces a characteristic elevation in the ST segment on an electrocardiogram (EKG). The elevated ST segment is how this type of heart attack got its name. See also NSTEMI.
Stent – An implantable device made of expandable, metal mesh (looks a bit like a tiny chicken wire tube) that is placed (by using a balloon catheter) at the site of a narrowing coronary artery during an angioplasty procedure. The stent is then expanded when the balloon fills, the balloon is removed, and the stent is left in place to help keep the artery open. TRIVIA ALERT: the coronary stent was named after Charles Stent (1807-1885), an English dentist who invented a compound to produce dentures and other things like skin grafts and hollow tubes (essentially what a metal coronary stent is). His real claim to fame occurred when he suggested using his material to coat underwater trans-Atlantic cable, which had broken several times as a result of corrosion by seawater. You’re welcome.
Stint – a common spelling mistake when what you really mean is the word “stent” (see above).
Stress Echocardiography – A standard echocardiogram test that’s performed while the person exercises on a treadmill or stationary bicycle. This test can be used to visualize the motion of the heart’s walls and pumping action when the heart is stressed, possibly revealing a lack of blood flow that isn’t always apparent on other heart tests. The echocardiogram is performed just before and just after the exercise part of the procedure. See also TTE.
Sudden Cardiac Arrest – The stopping of the heartbeat, usually because of interference with the electrical signal (often associated with coronary heart disease). Can lead to Sudden Cardiac Death.
Takotsubo Cardiomyopathy – A heart condition that can mimic a heart attack. Sometimes called Broken Heart Syndrome, it is not a heart attack, but it feels just like one, with common symptoms like severe chest pain and shortness of breath. It sometimes follows a severe emotional stress. Over 90% of reported cases are in women ages 58 to 75. Also referred to as Broken Heart Syndrome, stress cardiomyopathy, stress-induced cardiomyopathy or apical ballooning syndrome.
TAVR – Transcatheter aortic valve replacement: A minimally invasive procedure to repair a damaged or diseased aortic valve. A catheter is inserted into an artery in the groin and threaded to the heart. A balloon at the end of the catheter, with a replacement valve folded around it, delivers the new valve to take the place of the old. Also called TAVI (Transcatheter aortic valve implantation).
Tetralogy of Fallot – A rare condition caused by a combination of four heart defects that are present at birth, affecting the structure of the heart and causing oxygen-poor blood to flow out of the heart and into the rest of the body. Infants and children with Tetralogy of Fallot usually have blue-tinged skin because their blood doesn’t carry enough oxygen. Often diagnosed in infancy, but sometimes not until later in life depending on severity.
Tg – Triglycerides: The most common fatty substance found in the blood; normally stored as an energy source in fat tissue. High triglyceride levels may thicken the blood and make a person more susceptible to clot formation. High triglyceride levels tend to accompany high cholesterol levels and other risk factors for heart disease, such as obesity.
TIA – Transient Ischemic Attack: A stroke-like event that lasts only for a short time and is caused by a temporarily blocked blood vessel.
TEE – Transesophageal echocardiogram: This test involves an ultrasound transducer inserted down the throat into the esophagus in order to take clear images of the heart structures without the interference of the lungs and chest.
Treadmill Stress Test – See Exercise Stress Test.
troponin – a type of cardiac enzyme found in heart muscle, and released into the blood when there is damage to the heart (for example, during a heart attack). A positive blood test that shows elevated troponin is the preferred test for a suspected heart attack because it is more specific for heart injury than other blood tests, especially the newer high sensitivity troponin tests (hs-cTnT).
TTE – Transthoracic Echocardiogram: This is the standard echocardiogram, a painless test similar to X-ray, but without the radiation, using a hand-held device called a transducer placed on the chest to transmit high frequency sound waves (ultrasound). These sound waves bounce off the heart structures, producing images and sounds that can be used by the doctor to detect heart damage and disease.
TV – Tricuspid Valve: One of four one-way valves in the heart, a structure that controls blood flow from the heart’s upper right chamber (the right atrium) into the lower right chamber (the right ventricle).
UA or USA – Unstable Angina: Chest pain that occurs when diseased blood vessels restrict blood flow to the heart; symptoms are not relieved by rest; considered a dangerous and emergency crisis requiring immediate medical help.
Valves: Your heart has four one-way valves that keep blood flowing in the right direction. Blood enters the heart first through the tricuspid valve, and next goes through the pulmonary valve (sometimes called the pulmonic valve) on its way to the lungs. Then the blood returning from the lungs passes through the mitral (bicuspid) valve and leaves the heart through the aortic valve.
Vasodilator: A drug that causes dilation (widening) of blood vessels.
Vasospasm: A blood vessel spasm that causes sudden constriction, reducing its diameter and blood flow to the heart muscle. See also Prinzmetal’s Variant Angina.
VB – Ventricular Bigeminy: A heart rhythm condition in which the heart experiences two beats of the pulse in rapid succession.
Vena Cava – a large vein that carryies de-oxygenated blood into the heart. There are two in humans, the inferior vena cava (carrying blood from the lower body) and the superior vena cava (carrying blood from the head, arms, and upper body).
Ventricle – each of the two main chambers of the heart, left and right.
VF – Ventricular Fibrillation: A condition in which the ventricles (two lower chambers of the heart) contract in a rapid, unsynchronized fashion. When fibrillation occurs, the ventricles cannot pump blood throughout the body. Most sudden cardiac deaths are caused by VF or ventricular tachycardia (VT).
VLDL – Very Low Density Lipoprotein: Molecules made up of mostly triglycerides, cholesterol and proteins. VLDL, also known as the “very bad” cholesterol, carries cholesterol from the liver to organs and tissues in the body. It may lead to low density lipoproteins (LDL), associated with higher heart disease risks. VLDL levels are tricky to measure routinely, and are usually estimated as a percentage of your triglyceride levels. By reducing triglycerides, you are usually also reducing your VLDL levels.
Warfarin – A drug taken to prevent the blood from clotting and to treat blood clots. Warfarin is believed to reduce the risk of blood clots causing strokes or heart attacks. Also known as Coumadin.
Widowmaker heart attack – The type of heart attack I survived, since you asked. A nickname doctors use to describe a severely blocked left main coronary artery or proximal left anterior descending coronary artery of the heart. This term is used because if the artery gets abruptly and completely blocked, it can cause a massive heart attack that will likely lead to sudden cardiac death. Please note the gender imbalance here: despite the number of women like me who do experience this type of cardiac event, doctors are not calling this the widowermaker, after all.
WPW – Wolff-Parkinson-White Syndrome: A condition in which an extra electrical pathway connects the atria (two upper chambers) and the ventricles (two lower chambers). It may cause a rapid heartbeat.
NOTE FROM CAROLYN: I was very happy when we were able to include this entire glossary in my book, “A Woman’s Guide to Living with Heart Disease“ (Johns Hopkins University Press, 2017).
Are we missing any important heart acronyms/terms from this list? Let me know!
Please can someone explain something for me. I am a 53 yr old woman and generally fit and healthy. Had 2 ECG’s due to a one off dizzy spell during a stressful time dealing with my fathers terminal diagnosis. The 2nd ECG request did give me concern as i did not know why i had to have one. On 24/01/19 at my doctors appointment she explained that on 3 the leads it showed inverted T waves. And she explained that it may suggest angina. I was so shocked. Wasn’t expecting that. She gave me a GNT (nitroglycerin) spray in case I do get pain and take 75Mg of aspirin. I’m now waiting for a Cardiology referral.
I am so stressed and consumed by what might be wrong. My maternal grandmother had angina and valve issues. Her 3 brothers all had double bypasses. Could I have inherited this? I am not overweight at 63kg and 5.ft 9. I walk 20-25 miles a week at work and general walking here and there. I started HRT (patches evorol 25 -50) in July as menopause pain was making me feel like I was 90 and was getting me down.
I am worried so much now and analysing every ache/ twinge I get. I feel like a hypochondriac at the moment. I’m worried what will happen at the cardiologist and what the test will entail and tell me. I am waiting on cholesterol test which I had on 25/01/19. Can I have inverted T waves and be fine. Please help I am so scared and crying far too much.
Hello Colleen – the first thing is: please take a big deep breath before you read another word here! I’m not a physician so of course cannot comment on your specific case, but I can tell you generally that the definition of “angina” (as this glossary lists above) is “distressing symptoms”, typically chest pain that gets worse with exertion, and goes away with rest. That’s classic stable angina… typically caused by something that’s reducing blood flow to the heart muscle (causing the chest pain of angina).
A family history that might make a difference for you personally is only in what’s called your ‘first degree’ relatives: for example, if your mother or sister were diagnosed with heart disease before age 65, or if your Dad or brother were diagnosed before age 55, then doctors would consider that you have a family history as a risk factor for heart disease. There’s little if any scientific evidence that a grandparent or uncle’s heart disease history has any effect on your own risk.
It is a very good thing that you’re having further tests and a referral to a cardiologist, if only to ease your mind. There are many reasons for inverted T-waves, ranging from cardiac issues to completely benign conditions. One way of looking at this is choosing to believe that seeing a cardiologist will ease your mind one way or the other – so this is something to look forward to, not dread. If the cardiologist spots something suspicious, a treatment plan will be created. If not, you can wave goodbye and go back to happily living your life.
Try thinking of this cardiology appointment just as you would if your car were making some frightening noises and you were bringing it to your mechanic for a check up. You could work yourself into a complete state worrying ahead of time if the car trouble is going to be serious, or you could look at this appointment as the solution – at last! – to figuring out what’s wrong so the mechanic can recommend the next step.
Thank you for this list of so many definitions provided in plain English. what a valuable resource this is. THANK YOU, I have been looking for translations FOR PATIENTS not med school graduates– like this for three years.
My family doctor had me wear a 24 hr EKG. After reading the results, she has scheduled a scope to look inside my heart by a specialist. Completely forgoing a stress test. Said I have major changes in the EKG, what type of changes could they be looking at? Had LAD STENT INSERTED 7 YRS AGO – WHAT COULD THEY BE LOOKING FOR?
This is a great wealth of information, Carolyn! I looked and did not see my diagnosis, which is aortic stenosis. I looked under aortic as well as stenosis. Did I just miss it somehow?
I learned some new information, I am a bit familiar now, but not when I had my MI, it was like learning a new language. But, my favorite part was seeing SCAD on this list! Thank you.
Thanks and welcome! I was thinking of editing that SCAD definition actually: I suspect that that it isn’t so much that SCAD is “rare”, but it’s more that it’s “rarely correctly diagnosed”.
I totally agree that SCAD is not as rare as I believed for many years. Once awareness is spread to all medical staff, I believe many lives will be saved. Hoping for a brighter future for all SCAD patients.
I hope so too, Cathy. Perhaps when more SCAD studies (like Mayo Clinic’s) are published and read by more and more MDs, it will no longer be “rarely correctly diagnosed”.
It’s great to see IST on here. I was diagnosed with it 9 years ago and the lack of awareness is frustrating.
What a great resource for heart patients and their families!
Thanks so much, Ashley. I recently updated my original 2011 list after the world-famous Cleveland Clinic tweeted their glossary recently and I noticed that their list had a few glaring omissions (like SCAD and Brugada Syndrome) so this made me wonder what my list might be missing, too. Let me know if there’s anything else you think should be included, okay?
How is your health these days? How are you feeling?
New for me too. I have just been diagnosed with A-HCM: Apical Hypertrophic Cardiomyopathy.
I’ll add that one to my list, Kathleen – thanks!
Just saw this, Carolyn, and you’ve compiled a great resource. One note on A-HCM: Present thinking is that it’s due to a genetic modification. Runs in families though sometimes occurs spontaneously. I have not as yet done genetic testing, though it’s been offered.
Thanks Kathleen – like many cardiac diagnoses, it sounds like a moving target… Good luck to you!
This list is great. I’ve just been diagnosed and am utterly overwhelmed. Even in the WomenHeart online support community, I often have no clue most days what others are talking about with all these initials about their heart tests and specific disease. This is VERY helpful, thank you SO MUCH. Love your website which has been a godsend since my diagnosis.",['Implantable Cardioverter Defibrillator (ICD) is a surgically implanted electronic device to treat life-threatening heartbeat irregularities.'],8925,multifieldqa_en,en,,2f73cf12085e3fb879c775fc4851b72092a060cad5a927c6," Heart disease has its own jargon, understandable only by other members of the club, particularly by cardiac care providers. Here’s a helpful list of some of the most common acronyms/terms you’ll likely find around the cardiac ward. This entire patient-friendly, jargon-free glossary (all 8,000 words!) is also part of my book “A Woman’S Guide to Living with Heart Disease“ (Johns Hopkins University Press, November 2017). The full glossary can be found at: http://www.jhu.com/heart-disease-and-health/how-to-understand-heart- disease-jargon-from-the-point-of-view-of a woman-with-heart disease-who-suffers-a-myocardial-infarction-or a woman with a heart disease-that-sufferers-suffering-a heart attack. For more information, visit www.heartdiseasesandhealth.org or call the National Heart Association at 1-800-273-8255 or go to www.HeartDiseaseAndHealth.org. For confidential support on suicide matters call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S., go to the National Suicide Prevention Lifeline at 1 (800) 273-TALK (8255). For confidential help in the UK, go to the Samaritans’ website at http:// www. Samaritans.uk/. For confidential. support in the United States, call theNational Suicide Prevention Line at 1(800)273-825 or visit the National Suicide prevention Lifeline on 1 (877) 909090. For information on the National Institutes of Health, visit www.nhs.gov/www.NHS/HSI/HSi/HIV/HSII/HSP/HSV/HSJ/HSp/HV/H1/H2/H3/H4/H5/H6/H7/H8/H9/H10/H12/H13/H14/H15/H20/H16/H17/H18/H19/H21/H22/H23/H24/H25/H26/H27/H28/H29/H30/H31/H32/H33/H34/H35/H36/H37/H38/H39/H40/H41/H42/H43/H44/H53/H54/H55/H56/H57/H58/H59/H60/H61/H63/H64/H62/H65/H70/H71/H72/H73/H74/H68/H75/H83/H84/H85/H79/H80/H81/H82/H87/H78/H86/H88/H90/H91/H92/H93/H94/H89/H111/H102/H110/H114/H115/H109/H11/H112/H113/H108/H120/H130/H212/H209/H183/H208/H210/H213/H214/H205/H211/H215/H206/H207/H229/H227/H220/H225/H234/H263/H235/H Aorta – The main artery of the body, carrying blood from the left side of the heart to the arteries of all limbs and organs except the lungs. Apical Pulse: A central pulse located at the apex (pointy bottom) of theHeart. Atrial Flutter – A heart rhythm problem (arrhythmia) originating from the right atrium, most often involving a large circuit that travels around the area of the tricuspid valve. Atrioventricular Nodal Re-entry Tachycardia: A heart. rhythm problem that happens when there’s an electrical short circuit in the centre of the. heart, one of the most common types of SVT, most commonly seen in people in their twenties and thirties, and more common in women. than in men. BNP – Blood pressure test – BNP (Briuretic Peptide-type) is a substance secreted from the ventricle or lower chambers of theheart that increases blood pressure. Includes one of these prescribed meds along with daily low-dose aspirin. The force or force of the blood pressure increases when the heart condition is stable, and worsens when the symptoms worsen. The BNP increases when heart failure develops and/or worsens, and when the blood. pressure increases in the heart is stable or worsens. The condition is called heart failure or heart failure pressurism. It is a condition in which the heart beats with an irregular or abnormal rhythm. It was first described in people of Japanese descent and is called Yamaguchi Syndrome. It's a non-obstructive form of cardiomyopathy in which a portion of heart muscle is hypertrophied (thickened) without any obvious cause. There may be a genetic link. A blood pressure-lowering drug that limits the activity of epinephrine, a hormone that Increases blood pressure, is called Beta Blocker (BNP) BNP is a test that doctors use to determine if you’re overweight and is calculated using a formula of kilograms divided in meters squared (BMI =Wkg [m/H] [m]/. Better yet, just figure out your own weight by height by height, just by height and weight, just in kilograms divided by height,. Click here to get your own BMI. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. For details on suicide prevention in the UK, visit the National Institute of Health and Social Services’ Suicide Prevention helpline on 0800/273-TALK (8255). For confidential. support in the Middle East, call the Salvation Army on 8457 909090 or visit http://www.sussex.org/. For information on how to get involved in the Samaritan Samaritans, see http:// www.sas.org/samaritan.uk/. For confidential help in the United States, visit a Samaritans. org/pages/how-to-get-involved-in-the-U.S., click here to visit the S Samaritans and the U.K. Brugada syndrome is a genetic heart disease characterized by distinctively abnormal electrocardiogram (EKG/ECG) findings and an increased risk of sudden cardiac arrest. Coronary Artery Disease: A narrowing of the arteries that supply blood to the heart. Cardiomyopathy – a chronic disease of the heart muscle (myocardium), in which the muscle is abnormally enlarged, thickened, and/or stiffened. Cardiac Tamponade – Pressure on the heart that occurs when blood or fluid builds up in the space between the myocardium and the outer covering sac of theHeart. Heart failure – a condition in which. the heart cannot pump all the blood returning to it, leading to a backup of blood in the. vessels and an accumulation of fluid in the body’s tissues, including the lungs and i.C.B. – Calcium Channel Blocker: A drug that lowers blood pressure by regulating calcium-related electrical activity in the heart, often associated with coronary heart disease. Cardioversion – a medical procedure in which an abnormally fast heart rate (tachycardia) or cardiac arrhythmia like atrial fibrillation is converted to a normal rhythm using electricity or drugs. Synchronized electrical cardioversion uses a therapeutic dose of electric current to the. heart at a specific moment in the cardiac cycle to convert to normal rhythm. The CDS – Cardiac Depression Scale: A scale that can help assess the effects of depression occurring as a result of a heart disease diagnosis. The Cardiac Cardiac Arrest – Also known as Sudden Cardiac arrest: The stopping of the heartbeat, usually because of interference with the electrical signal that regulates each heartbeat (often associated with heart disease) Can lead to SuddenCardiac Death. The Heart Failure (also called Congestive Heart Failure): A condition in Which the heart can’t pump all of the blood back to the body. The condition results from a plaque rupture/blood clot or spasm and greatly increases the risk of a. heart attack. The heart failure is often referred to as “heart failure” or “cardiac arrhythmias” by doctors. The cardiologist who performs the procedure is known as a “Cardiac Cath Lab” (Cath lab) This is the room in the hospital/medical clinic where cardiac catheterization procedures take place (for example, when a stent is implanted into a blocked coronary artery). The procedure can help provide information about blood supply through the coronary arteries, blood pressure, blood flow throughout the chambers of theheart, and x-rays of the hearts’ ventricles or arteries. The procedure is typically performed in the cath lab during angiography. It can also be performed in a hospital/ medical clinic with x-ray guidance. It’ll be called the ‘Cardiac Catheterization’ procedure (Cath Lab) or the “Heart Cath” procedure (Heart Cath) It can be performed by a cardiologist with specialized training. Collateral arteries – Blood vessels that provide an alternative arterial supply of blood to an area of the heart that’s in danger of being deprived of oxygenated blood because of one or more blocked arteries. C-reactive protein – A byproduct of inflammation, produce by the liver, found in the blood in some cases of acute inflammation. Coumadin – A drug taken to prevent the blood from clotting and to treat blood clots. Costochondritis – the cause of severe chest pain, but NOT heart-related; it’re an inflammation of the cartilage that connects a rib to the breastbone. The amount of blood the heart pumps through the circulatory system in one minute. The number of people with congenital heart disease for the first time more adults than children in the U.S. than ever before, according to the American College of Cardiothoracic Surgeons (ACCS) C-Q10 – Co-enzyme Q10: A dietary supplement sometimes recommended for heart patients taking statin drugs. The “cut-and-sew” surgical procedure done to treat atrial fibrillation through a complicated set of incisions made in a maze-like pattern on the left and right atria (the upper chambers of theHeart) to permanently interrupt the abnormal electrical signals that are causing the irregular heartbeats of Afib. The heart and lungs are made to work by manually compressing the chest overlying the heart and forcing air into the lungs, used to maintain circulation when the heart stops pumping during Cardiac Arrest. Current guidelines suggest hands-only CPR, but some doctors recommend hands-on resuscitation, such as with a defibrillator or chest compressions with an ice pack on the chest to stop the heart from beating too fast. The risk of a heart attack or stroke in people with heart failure is higher than in people who don’t suffer from any heart problems at all. The risks of a stroke or heart failure are higher than for people who suffer from other conditions such as diabetes, high blood pressure, high cholesterol or high blood sugar. The benefits of having a pacemaker are greater than those of those who don't have one, say the American Heart Association (American Heart Association) The risks are lower than for those who do not have a heart condition such as congestive heart failure (CHF) or heart transplantation (COPD) The risk is higher for people with COPD than for patients who have heart failure or other heart conditions like congestive cardiac arrest. The chances of developing a stroke are also lower than in those who have had a heart transplant or other serious heart condition like aortic valve prolapse or a myocardial infarction (M.C.I.D). The risk for heart failure in the elderly is lower than it is for people in their 70s and 80s. The odds of developing heart disease in your 80s and 90s are about the same as they are in your 50s and 60s. Dressler’s syndrome – Happens to a small number of people three to four weeks after a heart attack. Diltiazem: A calcium channel blocker drug that acts as a vasodilator; used in the treatment of angina pectoris, hypertension, and supraventricular tachycardia. Dual Antiplatelet Therapy – Medications that block the formation of blood clots by preventing the clumping of platelets (examples Plavix, Effient, Brillinta, Ticlid, etc.) are often prescribed along with aspirin as part of dual antiplatelet therapy. EECP – An FDA-approved non-invasive, non-drug treatment treatment for collateral coronary angina by promoting the development of collateral arteries. The therapy is widely used in prominent heart clinics such as the Cleveland Clinic, Mayo Clinics, and the University of California, San Francisco. EKG/ECG – Electrocardiogram: A test in which several electronic sensors are placed on the body to monitor electrical activity associated with the heartbeat. The normal rate of blood pumped out of a filled ventricle is 50-60%. See also TTE and Stress Echocardiogram. See also EEC Pulsation for more information on the treatment options available for your heart, including the risk of heart failure, stroke, and other conditions that can be caused by aortic stenosis, heart valve disease, and heart failure. See the full list of recommended treatments for heart disease and stroke here. For more information, visit the American College of Cardiology's Heart and Vascular Disease Information Center (ACVIC) and the American Heart Association's Cardiomyopathy Center (AHC), both of which are based in the U.S. and provide information on heart function and treatment options for patients in the United States and Canada. For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support in the UK, contact the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org, or click here for details. For information on how to get in touch with the National Institutes of Health (NIH) or the National Heart Foundation (NICE), see http:// www.nhc.org/pages/heart-and-vascular-disease/heart%20and%20problems. For support on suicide matters call the NICE helpline on 1-856-457-9090 or visit http://www www.nhs.uk/. For more info on heart conditions, visit http:www.sjh.gov/. For information about heart function, visit the National Heart Institute (NHI) or the American Heart Association (AHA). For more details on heart functions, visit  the American Heart Institute or the World Health Organization ( WHO). For confidential help, call 1-877-468-788 or http: www.heart.org. EPS – Electrophysiology Study: A test that uses cardiac catheterization to study patients who have arrhythmias (abnormal hear rhythm) EP is used primarily to identify the origin of the arrhythmia and to test the effectiveness of medications used to treat abnormal heart rhythms. EKG is recorded while you exercise to monitor any abnormal changes in your heart under stress, with or without the aid of drugs to enhance this assessment. HCTZ – Hydrochlorothiazide: A drug used to lower blood pressure; it acts by inhibiting the kidneys’ ability to retain water. IHD – Ischemic Heart Disease: heart problems caused by narrowing of the coronary arteries, causing a decreased blood supply to the heart muscle. FH is an inherited disorder that can lead to aggressive and premature cardiovascular disease, including problems like heart attacks, strokes, or narrowing of. the heart valves. See CABG.c and Johns Hopkins – especially for patients who are not good candidates for invasive procedures such as bypass surgery, angioplasty or stenting. See also: MIBI, Echocardiogram, Nuclear Stress Test, and IBS-R. See the full list of heart conditions and conditions here, including how to get your heart checked by a doctor and how to find out if you have a heart condition that you need to get checked by your GP or a cardiothoracic surgeon. Click here for more information on the American College of Cardiology’s Heart and Ventricularular Open Cardiomyopathy (ACV-VCC) screening program, and to see more information about the ACV- VCC screening program and other heart conditions, visit the American Council on Cardiovascular Disease (ACCV) website. See www.acv-vcc.org for more details on how to check your heart rate and other conditions that need to be monitored by a cardiologist or a doctor. See http://www.acs.org/heart/ventricular-cardiomyography/vcc-vCC.html for more info on the CCV-v CCV screening program,. See the complete list of all the heart conditions that can be screened for here, as well as how to see if you need a doctor or a specialist to help you get the right treatment for your heart condition, such as a stent or a defibrillator, for details on what to look for and when to get it. Read more about heart failure here, here, and in the U.S. here, where you can talk to your doctor about your symptoms and how you can get it checked by an expert. Read also: Heart failure is a chronic progressive condition that affects the pumping power of your heart muscle, sometimes called Congestive Heart Failure (CHF) and is often referred to as “chronic heart failure’. Read on to learn more about the different types of heart failure, including ‘chronic’ and ‘prolonged’ heart failure. Interventional cardiologist – A cardiologist who is trained to perform invasive heart procedures like angiography, angioplasty, percutaneous coronary intervention (PCI), implanting stents, etc. IVS – Interventricular Septum: The stout wall that separates the lower chambers (the ventricles) of the heart from one another. IVUS – Intravascular Ultrasound: A form of echocardiography performed during cardiac catheterization in which a transducer (a device that can act as a transmitter (sender) and receiver of ultrasound information) is threaded into the heart blood vessels via a catheter. LAFB – Left Anterior Fascicular Block: A cardiac condition,distinguished from Left Bundle Branch Block because only the anterior half of the left bundle branch is defective. LQTS – Long QT syndrome (LQTS): A heart rhythm disorder that can potentially cause fast, chaotic heartbeats that may trigger a sudden fainting spell or seizure. Lipoprotein-a – molecules made of proteins and fat, carrying cholesterol and similar substances through the blood. A high level of Lp(a) is considered a risk factor for heart disease; detectable via a blood test or Nuclear Stress Test/Cardiac Perfusion Scan/Sestamibi.Microvascular disease – a heart condition that causes impaired blood flow to the heart muscle through the small blood vessels of the small heart. Symptoms mimic those of a heart attack. Also called Microvascular Disease or Small Vessel Disease or Coronary Vessel Disease. I live with this dia.ue, and/or exercise intolerance. I’m not sure if you live with it or if you’ve been diagnosed with it, but it’s a serious condition that can be life-threatening. I don’t know if you have it or not, but if you do, you should see your cardiologist for a check-up as soon as possible. You’ll want to make sure your heart is healthy and that your blood vessels are working properly. If not, you may need to go to the hospital for an exam or to get a chest x-ray to check for signs of heart problems. If you have a heart problem, you need to see a cardiologist. If your heart isn't functioning properly, you might need to have it checked by a specialist. You may also need to get an angiogram to see if there are any problems with your heart that need to be addressed. If there are problems with the blood vessels, the cardiologist may have to operate on you. If the heart is not functioning normally, it may need a procedure to correct the problem. If it is not working properly, it could need to undergo a procedure that involves an operation to replace the heart with a new organ. If a procedure is needed, it can be done by an anesthesiologist or a doctor. It can be difficult to get the right equipment for the job. It may also take up to two weeks to get to the doctor’S office to get all the necessary paperwork and paperwork done. It could also take a few days for the doctor to get your medical records in order. Mini-Maze – a surgical procedure to treat atrial fibrillation, less invasive than what’s called the Cox Maze III procedure (a “cut-and-sew” procedure), and performed on a beating heart without opening the chest. Mitral valve prolapse: a condition in which the two valve flaps of the mitral valve don’t close smoothly or evenly, but instead bulge (prolapse) upward into the left atrium. NSTEMI – Non-ST-segment-elevation myocardial infarction: The milder form of the two main types of heart attack. New Wall-Motion Abnormalities – Results seen on an echocardiogram test report (see NWMA, below).Nitroglycerin – A medicine that helps relax and dilate arteries; often used to treat cardiac chest pain (angina) Also called NTG or GTN. Open heart surgery – Any surgery in which. the chest is opened and surgery is done and the heart is done on the. chest, coronary arteries, or other parts of the heart (such. as theorta). See also CABG – A. surgically implanted electronic device that helps regulate the heartbeat. Peripheral Artery Disease: A common circulatory problem in which narrowed arteries reduce blood flow to the limbs, usually to the legs, usually include leg pain. See also: Exercise stress test, Nuclear perfusion test, MIBI and CABABG for more information on these and other heart-related conditions. For confidential support call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support in the UK, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. For the U.S., call the national suicide prevention helpline on 1-877-457-9255 or go to www.s Samaritans.com. For more information about heart disease, visit the National Institutes of Health (NIH) for details on heart disease and other conditions, see http:// www.nhs.org/heart-disease/cardiology/cardiomyopathy/cardiopulmonary-risk-management/cardiac-risk/. For more info on heart transplantation, visit the American Heart Association (AHA) for more details on how to get a transplant, or visit the American Heart Society (American Heart Association) for information on heart transplants. For information on other heart procedures, visit www.heart- transplantation.org or the American College of Cardiology (ACHA) For more on heart surgery, see the National Institute of Cardiovascular Surgery (NICE) for info and support on heart- transplants, or the National Heart Institute (NHS) for heart-transplantation). For information about other heart surgeries, see the National Heart Foundation (NHE) for help with heart surgery and other procedures. For details on other cardiac procedures, go to the National Heart Institute for Heart Surgery (NCI) for advice on heart procedures and other topics. For a list of heart procedures that can be done without opening your chest, see: www.nhw.org. Palpitations – A noticeably rapid, strong, or irregular heartbeat due to agitation, exertion or illness. Paroxysmal Atrial Fibrillation – An unusual heart arrhythmia of unknown origin. Post-partum cardiomyopathy – A form of heart failure toward the end of pregnancy or in the months after delivery. Princesszmetal’s Variant Angina – Chest pain caused by a spasm in a coronary artery that supplies blood to the heart muscle. The right atrium receives de-oxygenated blood from the body through the vena cava and pumps it into the right ventricle which then sends it to the lungs to be oxygenated. Doctors may also call this transradial access, the Transradial angiopial access (TAA) because it’S associated with fewer complicatiati.n walking (called intermittent claudication). See also TAA for more information on how to get TAA in your life. The TAA program is funded by the National Institutes of Health (NIH) and the American Heart Association (AHA). For more information about TAA, visit the National Institute of Health and Human Services (NICE) website. For more info on the THA program, go to: http://www.nih.gov/health/heart/heart-disease-and-health/tahoe.html. The program is also available in the U.S. through the National Heart, Lung, and Blood Institute (NHS) and in the UK through the British Heart Foundation (BHF). For details on the BHF, visit www.nhs.org/heart. The BHF is a not-for-profit organization that provides information on heart disease prevention, treatment and care to the public. For details, visit: www.bhf.org. The National Heart Foundation is a non-profit organisation that provides advice on heart health and treatment to the general public. To learn more about heart disease and its treatment options, visit http:// www.heart.org/. For more details on heart care, visit www.heartlifecare.org or www.theheartlifeline.com. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, or see www.samaritans.org for details. For information on the National Suicide Prevention Lifeline, call 1-800-273-TALK (8255) or visit  www.suicideprevention Lifeline (www.sophistication.org). For confidential help, call the National Suicide prevention Lifeline on 1-844-788-8255 or  visit the National Suicide Prevention Helpline on http://www-sophie-sickness.org or the Samaritans in the UK. For support on suicide matters call the National suicide Prevention Lifline on 0844-988-9255. For help in the United States, contact the Samaritan Samaritans at 1-877-9090. For the UK, visit a Samaritans local branch or the British Samaritans (http:// www-samaritan.org) for help. Right Bundle Branch Block: A delay or obstruction along the pathway that electrical impulses travel to make your heart beat. Restenosis – The re-closing or re-narrowing of an artery after an interventional procedure such as angioplasty or stent placement. Rheumatic Heart Disease: Permanent damage to the valves of the heart caused especially by repeated attacks of rheumatic fever. Sick Sinus Syndrome: The failure of the sinus node to regulate the heart’s rhythm. Spontaneous Coronary Artery Dissection: A rare emergency condition that occurs when a tear forms in one of the blood vessels in the heart, causing a heart attack, abnormalities in heart rhythm and/or sudden death. Sinus Tachycardia: A heart rhythm with elevated rate of impulses originating from the sinoatrial node, defined as greater than 100 beats per minute (pm) in an average adult. Systolic Blood Pressure – The highest blood pressure measured in the arteries. It occurs when the heart contracts with each heartbeat. It can cause a variety of complications from fainting and dizziness to sudden cardiac arrest. The first number in 120/80 is sinus tachbachy. Also called sinus bachy.ns, this is increasingly considered the default access approach in most countries, except in the U.S. where the traditional Femoral Artery (groin) approach is still the most popular access. See MIBI for more information about the MIBI stress test and how to use it to help you understand your body and your health. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. For support in the UK, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit www.suicidepreventionlifeline.org. For information on how to help in the United States, visit the National Institutes of Health (NIH) website. For more information on the National Heart Foundation (NHF), visit http://www.nhp.org/heart-wellness/. For more info on the NICE stress test, see http:// www.nhs.uk/health/stress-test-and-symptoms/sinus-tachbary.ns. For details on how you can help the NHC to help your heart, visit http:www.NHS.gov/Heart-Wellness/Sinus-Tachbaries.ns/. For information about how to get the NHS stress test for your body, visit http: www.nia.org/. For help with the NCH stress test in the US, visithttp:www://www.-nia.gov/. For info on how the NSH stress test can help you to help with your heart and other health conditions, seehttp:http://www-nia.hnhs.com/HeartHealth/Sick-Sinus Syndrome – A group of related heart conditions that can affect how the heart beats, most commonly in older adults, although it can be diagnosed in people of any age. In people with sick sinus syndrome, the SA node does not function normally. The SA node is a group of specialized cells in the top of the right atrium which produces the electrical impulses that travel down to eventually reach the ventricular muscle. STEMI – ST-elevation heart attack (myocardial infarction) – more severe form of the two main types of heart attack. Stent – an implantable device made of expandable, metal mesh (looks a bit like a tiny chicken wire tube) that is placed (by using a balloon catheter) at the site of a narrowing coronary artery during an angioplasty procedure. Takotsubo Cardiomyopathy – A heart condition that can mimic a heart attack, but it feels just like one, with common symptoms like severe chest pain and shortness of breath. Sudden Cardiac Arrest – The stopping of the heartbeat, usually because of interference with the electrical signal (often associated with coronary heart disease) Tetralogy of Fallot – A rare condition caused by a combination of four heart defects that are present at birth, causing oxygen-poor blood to flow out of the heart and into the rest of the body. Over 90% of reported cases are in women ages 58 to 75. High triglyceride levels may thicken the blood and make a person more susceptible to clot formation. Tg – Triglycerides: The most common fatty substance found in the blood; normally stored as an energy source in fat tissue.EEEE – Transesocophageal echocardiogram: An ultrasound-like test that involves an transducer inserted down the throat into the heart to take down the heart structures.e production of cholesterol in the liver. Examples of brand name statins: Lipitor, Crestor, Zocor, Mevacor, Levachol, Lescol, etc. Also available as a cheaper generic form of statins, such as Lipitor or Crestor or Largent or Lascorb. The coronary stent was named after Charles Stent (1807-1885), an English dentist who invented a compound to produce dentures and other things like skin grafts and hollow tubes. His real claim to fame occurred when he suggested using his material to coat underwater trans-Atlantic cable, which had broken several times as a result of corrosion by seawater. The stent is then expanded when the balloon fills, the balloon is removed, and the stent left in place to help keep the artery open. Also called TAVI (Transcatheter aortic valve implantation) – A minimally invasive procedure to repair a damaged or diseased aorta. A catheter is inserted into an artery in the groin and threaded to the heart. A balloon at the end of the catheter, with a replacement valve folded around it, delivers the new valve to take the place of the old. The new valve is then inserted into the old valve and delivered to the new place in the heart in a procedure known as TAVi. The patient is then transferred to a hospital for further treatment. The procedure is known as a ‘torture’ procedure, or ‘heart procedure.’ The procedure can be used to visualize the motion of the. heart’s walls and pumping action when the heart is stressed, possibly revealing a lack of blood flow that isn’t always apparent on other heart tests. The test can be performed just before and just after the exercise part of the procedure. troponin – a type of cardiac enzyme found in heart muscle, and released into the blood when there is damage to the heart. A positive blood test that shows elevated troponin is the preferred test for a suspected heart attack. Transthoracic Echocardiogram – a painless test similar to X-ray, but without the radiation, using a hand-held device called a transducer placed on the chest to transmit high frequency sound waves (ultrasound) These sound waves bounce off the heart structures, producing images and sounds that can be used by the doctor to detect heart damage and disease. Ventricular Bigeminy – a heart rhythm condition in which the heart experiences two beats of the pulse in rapid succession. Widowmaker heart attack – The type of heart attack I survived, since you asked. A nickname doctors use to describe a severely blocked left main coronary artery or proximal left anterior descending coronary artery of the heart of the person who suffered the attack. Please note: doctors are not calling this the widowermaker cardiac event, despite the number of women like me who experience it. Wolff-White Syndrome-Parkinson: A condiWhite Syndrome, a condi white syndrome, where the heart stops beating properly and the lungs and chest are unable to function properly, leading to a heart attack or stroke. Aortic valve – a structure that controls blood flow from the heart’s upper right chamber into the lower right chamber (the right ventricle) The tricuspid valve – one of four one-way valves that keep blood flowing in the right direction. The pulmonary valve (sometimes called the pulmonic valve) on its way to the lungs. The mitral valve (the pulmonary valve) – the valve that carries blood from the lungs to the aorta. The aortic and mitral valves – the valves that control the flow of blood into and out of the body. The left and right atrium – the chambers that contain the heart's beating chambers. The right and left ventricular chambers (the chambers that pump blood into the body) The right ventricular chamber – the chamber that pumps blood to and from the brain. The ventricular tachycardia (VT) – a condition that causes the heart to contract in a rapid, unsynchronized fashion. Most sudden cardiac deaths are caused by VF. The VF is considered a dangerous and emergency crisis requiring immediate medical help. Warfarin – A drug taken to prevent the blood from clotting and to treat blood clots. Also known as Coumadin. It is believed to reduce the risk of bloodClots causing strokes or heart attacks. By reducing triglyceride levels, you are usually also reducing your VLDL levels. By. reducing triglycerides, you're usually reducing your heart disease risk by reducing VLDl levels. There are two in humans, the inferior vena cava (carrying blood. from the lower body) and the superior vena Cava ( carrying blood from. the head, arms, and upper body). There are also two blood vessels in the human body that carry de-oxygenated blood. ion in which an extra electrical pathway connects the atria (two upper chambers) and the ventricles (two lower chambers). It may cause a rapid heartbeat. Are we missing any important heart acronyms/terms from this list? Let me know!Please can someone explain something for me. Had 2 ECG’s due to a one off dizzy spell during a stressful time dealing with my fathers terminal diagnosis. The 2nd ECG request did give me concern as i did not know why i had to have one. On 24/01/19 at my doctors appointment she explained that on 3 the leads it showed inverted T waves. And she said it may suggest angina. I was so shocked. Wasn’t expecting that. She gave me a GNT (nitroglycerin) spray in case I do get pain and take 75Mg of aspirin. I’m now waiting for a Cardiology referral. I am so stressed and consumed by what might be wrong. My maternal grandmother had angina and valve issues. Her 3 brothers all had double bypasses. Could I have inherited this? I am not overweight at 63kg and 5.ft 9. I walk 20-25 miles a week at work and general walking here and there. I started HRT (patches evorol 25 -50) in July as menopause pain was making me feel like I was 90 and was getting me down. I feel like a hypochondriac at the moment. I'm worried what will happen at the cardiologist and what the test will entail and tell me. Can I have inverted T Waves and be fine. Please help I amSo scared and crying far too much.Thank you for this of so many definitions provided in plain English. I have been looking for this for three years–My family had me wear this for like three years like this for medENTS–. THANK YOU, YOU, for translating for me for this valuable resource this is THANK YOU for translations for me not like this. –. Colleen – the first thing is: please take a big deep breath before you read another word here! I can tell you generally that the definition of “angina” (as this glossary lists above) is “distressing symptoms”, typically chest pain that gets worse with exertion, and goes away with rest. There are many reasons for inverted T-waves, ranging from cardiac issues to completely benign conditions. One way of looking at this is choosing to believe that seeing a cardiologist will ease your mind one way or the other – so this is something to look forward to, not dread. If the cardologist spots something suspicious, a treatment plan will be created. If not, you can wave goodbye and go back to happily living your life. – If the next step is to look at the mechanic so they can recommend the wrong list, so the mechanic can look at what is wrong. – You could work yourself into a complete state worrying ahead of time if the car trouble is going to be serious or you could figure out what to do – at last! – to figuring out what is the solution – at least – to figure out the problem. – The next step – the mechanic could look at your family history. a 24 hr EKG. After reading the results, she has scheduled a scope to look inside my heart by a specialist. Completely forgoing a stress test. Said I have major changes in the EKGs, what type of changes could they be looking at? Had LAD STENT INSERTED 7 YRS AGO – WHAT COULD THEY BE LOOKING FOR? This is a great wealth of information, Carolyn! I looked and did not see my diagnosis, which is aortic stenosis. Did I just miss it somehow?I learned some new information, I am a bit familiar now, but not when I had my MI, it was like learning a new language. But, my favorite part was seeing SCAD on this list! Thank you.Thanks so much, Ashley. I recently updated my original 2011 list after the world-famous Cleveland Clinic tweeted their glossary recently and I noticed that their list had a few glaring omissions (like SCAD and Brugada Syndrome) so this made me wonder what my list might be missing, too. Let me know if there’s anything else you think should be included, okay? How are you feeling? I have just been diagnosed with A-HCM: Apical Hypertrophic Cardiomyopathy. I have not as yet done genetic testing, though it's been offered. I’ll add that one to my list, Kathleen – thanks! Just saw this, Carolyn, and you’ve compiled a great resource. I was diagnosed with it 9 years ago and the lack of awareness is frustrating. I often have no clue most days what others are talking about with all these initials about their heart tests and specific disease. This is VERY helpful, thank you SO much. Love your website which has been a godsend since my diagnosis. Hoping for a brighter future for all SCAD patients. I hope so too, Cathy. Once awareness is spread to all medical staff, I believe many lives will be saved. Perhaps when more SCAD studies are published and read by more and more MDs, it will no longer be “rarely correctly diagnosed”. Like many cardiac diagnoses, it sounds like a moving target… Good luck to you! Good luck, Kathleen."
Why is it important for the sides of the fuselage to be sloped (tumbled home)?,"Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go ""perfectly."" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane.
This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes.
While building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying ""banana"" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built.
First understand that the plans show the finished form of the plane. They show the ""projected"" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat.
Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel ""undevelopable"" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition ""developable"". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a ""compounded"" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain.
Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home), the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock.
This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later.
Layout to ensure a fair and true fuselage starts by drawing a reference line (baseline) on the building surface. Refer to figures 2 & 3 and use a wire guide to draw a very straight baseline. About 500 lbs. Of tension should be adequate. One could use a chalk line, but we're talking airplanes here, not house framing.
The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Although the baseline will still be the reference, the top and bottom longerons will be laid separately.
Layout differences don't end there. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. Another major difference is that the bottom & side panels are applied after forming the fuselage box section. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape.
Refer to figure 1 for the layout of the new developed side panel. The firewall (station a) is layed out perpendicular to the baseline. Longitudinal (station) measurements are given along the length of the baseline from the firewall. Vertical dimensions are given to reference the angle and breadths of the station at the baseline.
Notice that the top longeron is bowed outward and that the stations are spaced slightly greater than called out in the plans. When the panels are formed into the box frame section ,they will work into the dimensions specified in the plans.
Strike a centerline, longer than is needed on the building surface using a wire guide. Draw off the firewall line perpendicular to the centerline at one end.
Using the distances listed in the balloons, mark them off on the centerline. Distances are measured to the nearest sixteenth of an inch. Take time to mark them off carefully. Don't mark off the distances in a cumulative fashion. Use the firewall as a common reference.
Using the angles listed at each station, mark off a station line longer than is needed. The angles are measured to the nearest hundredth of a degree. Take time to mark them off carefully.
At each station, start by marking off each short (bottom longeron) line distance from the centerline. Use your set of trammels or beam compass for doing this. Mark the intersection of the short line with the station line.
At each station, mark off each long (top longeron) line distance from the intersection of the short line distance and the station line. Again the trammels or beam compass is best for completing this step. Mark the intersection of the long line distance with the station line.
Using the longeron as a batten, trace out the inside and outside curves of the longeron. After the batten is secure, in between each station, fasten a keeper block inside and outside to preserve the shape of the longeron taking care to avoid potential future interference with the diagonal members to be installed later. The fairing blocks can be removed or left in place if they won't interfere with building. The vertical station members and their diagonals can now be measured and positioned. Remember to refer to the plans for the material thickness direction.
After vertical and diagonal members are cut and fitted, take time to draw their outlines on the building surface to cut down on time and confusion when laying out the opposite side.
Finishing the side panel is accomplished in a manner similar to that called for in the handbook with the exception that the side and bottom skin panels will be attached later.
The next article in the series will discuss jigging and building techniques to ensure alignment and straightness of the flat built side panels. Also covered will be building a ""strongback"" jig to assure alignment of the side panels when they are formed into their final shape.
Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the side and bottom ply skins.
U.S. Mail: Densmore Associates, inc.
ANSI ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand.
""Scarfing"" is the practice of splicing plywood so that short pieces of plywood can be used to span long distances. On the KR, it is required on both the fuselage skins and spar webs. The angle of the splice should be 10 to 12 degrees to maintain strength across the joint. Also, joints should coincide with structural members, such as spar webs or fuselage truss members.
This scarfer is made by mating a regular plunge router (this one costs about $50) to a table saw. Obviously, you really only need a table saw to cut the chamfer, but it does make a nice heavy table for scarfing. You could just as easily use a large work table as the base.First, set the table saw for a 5.5 degree cut (for a 1:12 joint, or 6.5 degree cut for a 10:1 joint), and run a 1 x 6 through on edge to chamfer a corner on the board. Then drill the board for three router mounting holes (two are countersunk) and connect the assembly to the table saw with two 1/4 inch bolts. Use a long (2-3 inch) straight cutting bit to do the cutting. Adjust the bit so it doesn't interfere with your table top, and go to town. Keep pressure on the plywood to ensure contact with the table while you're scarfing. Make sure you feed your material from the same end as you would if you were sawing, or the router will take your plywood away from you and put a big dent in your garage door.
In the late 60's Ken Rand and Stuart Robinson were working as flight system engineers for Douglas Avionics. Ken was working as an electrical engineer, having previously worked for Sperry as an autopilots project engineer, while Stu's degree was in aeronautical engineering from Northrop University. They were two of the guys at the end of the DC-8,9, and 10 assembly lines responsible for correcting some of the nits and picks in various systems before delivery to the customer.
They both wanted to build a fast, inexpensive airplane which was also economical to maintain. Several designs were considered, and plans were bought first for the Jeanie's Teenie and then the Taylor Monoplane. The Monoplane was more to their liking, but would require some modification to fit their needs. A cooperative redesign effort ensued, with virtually no dimensions left untouched. Only the basic fuselage structure, airfoil, and powerplant were retained. The tail shape was Stu's, and came directly from the big DC-8s parked on the ramp outside his office window. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport.
Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Ken's wife Jeanette became owner of RR overnight, and stepped up to keep the plans and parts coming. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79.
To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5 KR2Ss now in the air. Much of the development work done on KR's is now done by the builders themselves. KR builders tend to be innovative, which leads to some interesting modifications. Some of the mods that work eventually creep into the plans. The KR2S is a case in point. Many builders who'd heard of the pitch sensitivity and tight cabin of the KR2 began to build an enlarged version, with the length determined by the most commonly available longeron material. The result is a KR2 that is stretched 2"" between firewall and main spar, and 14"" behind the main spar. Higher gross weights dictated more wing area, with the new standard becoming the Diehl wing skin. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts.
Mike Stearns addresses the KR Forum crowd.
This year's KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Mike Stearns spoke on several topics, including the many sources for KR and homebuilding information available on the Internet. He also mentioned KRNet, the list server devoted entirely to KR aircraft, as well as several notable World Wide Web home pages. He also brought a sample of the new Rand Robinson wing skins with him, and discussed their high temperature core prepreg construction. His KR2S will receive the first set, which is currently being installed at Hinson Composites.
Steve Trentman spoke on his turbine installation. It uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus. sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. More on this exciting development in next month's issue of KROnline.
Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. After researching the KR series, and reading Neil Bingham's ""A Critical Analysis of the KR2"" (Jan 88 Sport Aviation), Les decided to build his as a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. His fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. It is skinned with 1/8"" birch plywood. Spars are covered with plywoood on both fore and aft sides, ala KR2S. Diehl wing skins provide the lift. Horizontal stabilizer and elevator were stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. . The fuselage to cowling junction was made more graceful by adding 1.5 inches to the height of the firewall end of the fuselage sides.
Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. The canopy frame fits snugly into a recess in the foward deck, providing an excellent wind and water seal. The fiberglass work is exemplary.
Seating is luxurious for one.
The cowling is also a work of art, and uses NACA ducts for efficiency. Female molds were made for all the fiberglass parts on Les's plane, so he could proabably be persuaded to make more, if demand dictates. Les also machines a multitude of KR aluminum and steel parts which he now offers for sale.
The firewall was reinforced with aluminum brackets and angles bolted between the longerons in anticipation of the 200 lb Subaru EA-81 engine installation. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy.
Originally built as a taildragger, the fixed gear is made from 4130 steel tubing. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. An early taxi test accident damaged the main gear, and prompted Les to change to tricycle gear. Again, he designed his own fiberglass main gear, and uses a Diehl nose wheel fork with a 4130 strut and 6"" wheel up front.
Early tests revealed cooling problems, which prompted a radiator move from the firewall to a lower cowling location.
The first flight was almost a disaster, as test pilot Randy Smith lost power right after takeoff. He managed a 180 with a safe downwind landing with only minor nosewheel pant damage. The culprit proved to be a spark plug with too much reach, which was quickly remedied. Subsequent flights have shown water temp to be about 210 degrees, oil temp is 220-230, and airspeed is about 180 mph.
Shopping for the Partially Built KR.
This story starts about twenty years ago when I first started looking at the KR-2 as the plane I'd like to build. The only problem at that time was a lack of money, lack of knowledge, and a lack of job stability. I liked the design, except for the low ground clearance of the retractable gear and that a KR was going to be a tight fit for me to fly.
Over the past twenty years I've owned a number of planes, but still always wanted to build my own. I needed one that would fit me, my budget requirements, and have the speed and performance that I wanted. When ""KITPLANES"" published the article featuring Roy Marsh's new KR-2S, it was the first I had heard of any major modifications or improvements to the same old KR design. I believe that article and Roy Marsh's workmanship have probably been the greatest boon to Rand Robinson (RR) in the last twenty years. It certainly caught my eye! Here was the same design I had decided I wanted to build twenty years ago, with all of the improvements I wanted. It was sitting on fixed gear with some reasonable ground clearance. It had the capability to be built large enough to accommodate me. It has enough prefab parts available that it didn't have to be 100% scratch built if I decided to hurry the project along. And it had the speed I wanted. I knew that Roy's published speeds were probably not realistic expectations for the average KR, but after knocking around for the last three years in my Champ, anything over 90 mph seems pretty fast to me.
After purchasing the info kit and the sales video from Rand Robinson, the next step after deciding for sure to build this plane was to order the KR-2 plans and the KR-2S addendum. I finally got my plans and was putting together my first order to start the plane, when my partner in the Champ pointed out that there was a partially completed KR-2S for sale in Trade-a-plane. My initial answer was ""No, I don't even want to look at it. I want to build my own from scratch."" My partner insisted that for the advertised price and the fact that it wasn't too far away, I ought to at least give the guy a call and investigate it. ""No, I don't think I want to buy someone else's problems,"" I persisted. That night I went home and crunched up some numbers on the calculator and finally came to the conclusion that for the sake of my budget for the next several years, I really should give this guy a call.
Three days later, I flew to his place about 400 miles away to take a look at his project. At this point I should probably mention that I consider myself to be fairly knowledgeable about airplane construction, although the vast majority of my experience is with tube and fabric. The rest of this article deals with what I looked for and more importantly what I missed and have had to repair in the last year since I purchased the project.
When we went to the seller's house, I found that the left wing was built using the Dan Diehl wing skins and the right wing skins were leaning against the wall inside the house. Also the canopy was in the house with the canopy covered with paper and tape. I wanted to inspect the fuselage first, so off we went to the shop.
There I found a fuselage sitting on it's gear painted in primer gray. The first step was to inspect the quality of workmanship of what could be seen as it sat. The interior of the fuselage looked as if it had been built with a great deal of care. The fit and finish of all of the interior wood was very nice. Even the gussets looked like they had been painstakingly perfectly fitted. The glass work on the turtle back also looked very precise and clean. It was evenly faired into the vertical and horizontal stabs. The tail also appeared to be well built with the exception of a depression directly over the front and rear spars in the horizontal stabs. He explained that when he moved recently, that he had shot the plane with gray primer to protect it from the weather since he wouldn't have ready access to a shop to put it in right away. It ended up sitting out in the hot south Texas summer sun for a few weeks before he got a shop rented to work in. That caused the glass (or possibly the foam inside the horizontal stab) to swell, except that it held onto the spar, so it was slightly ballooned in front of and behind the spars. His recommendation was to fill it back smooth with micro.
I also found a small linear crack in the lower left wing spar cap on the left wing stub. It appeared to be from over tightening the rear spar wing attach fitting bolts. His explanation was that the crack wasn't important because the rear spars only job is to keep the wings from folding back. I also noticed that the holes for attaching the outer wing to the wing stub were badly rounded out on the rear spar. He explained that the Diehl wing skins require the rear spar to be swept slightly more forward than the stock wings. This won't allow you to use the rear spar attach fittings from RR and that I would need to fabricate a new set of rear spar attach fittings.
I also found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. I couldn't check for function since the right bellcrank and sheeve wasn't installed, the left wing also wasn't installed, and the right wing didn't exist yet.
Next we pulled the inspection panels off of the fuselage and tail and looked at everything I could see with a good flashlight. I didn't find anything else that might be questionable about the fuselage except for a cracked elevator trim tab that was damaged when it fell off it's hanging place on the wall.
Next we spent some time going over his builders log and builders photo album. I still hadn't seen anything that would dissuade me from buying this project.
At this point it was starting to get late and my ride down needed to get airborne for the flight home. I needed to make a decision about whether I wanted this project or not, but I hadn't inspected the wings and canopy yet. I took a cursory look at the left wing and saw lots on micro built up on it and some bubbles in the leading edge, but nothing that looked seriously wrong to my amateur eye. The right wing was only a set of spars in the shop and the Diehl wing skins in the house, so there wasn't much to look at there. The canopy was wrapped in paper and tape, so there wasn't much to look at there either. I decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the project. For the advertised price, I could build a new set of wings and still be way ahead financially. We negotiated a final price, shook hands, took my ride to the airport, and started off in search of a U-haul to haul the project home.
Now, at this point, some of you are thinking about what I surely must have forgotten to inspect and why didn't I take a local A & P or EAA member along for the ride. First of all, I don't know any mechanics locally that have any experience with glass and our EAA chapter of which I am VP is woefully lacking in fiberglass knowledge. Secondly, as you will see, I missed plenty. Some by ignorance, some by just not looking close enough.
Now for a list of the problems that I found over the last year and a few of the fixes that I came up with.
I found that the lower set of rear spar attach fittings on the left rear spar were installed backwards with the longer spaced hole towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Also in the same area he had drilled through the rear spar with a hole saw to create a place for the aileron cable to pass through and managed to cut out the second from the outside vertical brace in the spar. Then he chose to install the aileron bellcranks in front of the rear spar, and cut another hole through the rear spar for the aileron push rod. He also managed to cut out the outside vertical brace in the spar. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in, cut the whole rear spar carrythrough out of the fuselage including ruining the left lower wing skin, or do something else creative to reinforce the spar cap and install a custom built set of attach fittings.
I also found that after I built and installed the right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. The cable that crosses between the two bellcranks had a sharp uphill from the sheeve to the bellcrank in the last 12 inches on either side. This combined with the radius that the bellcranks turn caused the cross cable to pull up tight when the ailerons were pushed to either end of their travel, but allowed the cables to go very slack when the ailerons were centered. Also the Aileron pushrods needed to pass directly through the lower set of rear wing attach fittings to attach to the aileron. This whole rear spar and aileron bellcrank setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it.
I decided that I had to remove the rear fittings from the left wing to be replaced with the new set that my neighborhood machinist was cutting out for me. When I put the wing on the work bench to start removing the rear fittings, I thought I had better take a closer look at the bubbles in the leading edge. I found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leading edge. What I found was that the leading edge had been floxed together and glassed over, but the mold release had never been scrubbed off the leading edge of the wing. It peeled apart for rebuild quite easily.
When I got back to removing the rear spar attach fittings, I noticed that the woodwork inside the wing looked awfully dull. The reason was that the wing had been closed up without varnishing any of the woodwork. This was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer.
I also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. Retapping the fitting the right direction seemed to be a good fix for that problem.
When I finally got around to attaching the wing to the fuselage, I found that the front spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts.
On the fuselage I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. I must complement the manufacturers for taking care of that problem. One call to the Cleveland factory and they shipped me a new set of wheels and brakes even though the receipt for this set was over four years old and in the original builders name. Their only concern was that this set had never been placed in service yet.
I chose to sand the load of micro off the left wing to see what it was covering. When I got down to the glass, I found that there was no glass for the aft inch and a half of the underside of the wing in front of the aileron hinge. With the Diehl wing skins, you build the wings, then cut the ailerons out of trailing edge of the wing. He had mismeasured and cut too much material off the bottom side of the trailing edge in front of the aileron. It was filled by floxing a piece of spruce into the gap to fill the space between the back edge of the fiberglass and the aileron mount. I chose to wrap the trailing edge of that wing, and the other wing to match with a couple of lay-ups of glass.
When I sanded the primer off the aforementioned damaged trim tab, I found that the hinge was floxed to the leading edge of the foam insides of the tab, but not the glass. I also chose to wrap the front of the trim tab with a lay-up of glass.
I decided to pull the paper off the canopy and take a look at it before I'm ready to bolt it on and fly. The original builder had blown his own canopy and after some of the previous problems, I was beginning to have some concerns about not having looked it over closely enough. The canopy turned out to have been blow a little too large. It ended up with a little larger bubble for headroom, which I didn't object to. However, it had more headroom on the right side than the left. Yes, it was just a little bit lopsided. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm.. My fear was that this is just thin enough that it may decide to lay on my head or in my lap when flying on a warm day. It will have to be replaced.
I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and I'm sure that there could have been many other problems that didn't but could have existed on this project. This is also not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. This article is intended to feature some of the problems that you may run into in buying someone else's project.
The final question is, knowing what I have found over the past year, would I have still purchased this project. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. There are a few things that I would have done differently, but nothing that I can't live with. Although I won't be able to say that I built it all from scratch, I have built and rebuild enough of the plane that I should have no problem qualifying under the 51% rule.
You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"".
Here is an brief explanation of how I built my turtledecks. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo.
After the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade.
After the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out.
At this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the ""backbone"" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees.",['The sides of the fuselage are sloped to create a conical section when the fuselage is formed.'],6250,multifieldqa_en,en,,41e27260a12cc40a778f8c1ba8bf643ee655871a458e5e1c," This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 &KR-2 series planes. The solution starts by understanding the three dimensional relationship of the assembled parts being built. The problem starts when the side panels are bent and sloped to form the fuselage box section. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat. This is important when laying out the side and bottom panels onto flat plywood. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, andSloped to Form a Conical section, it takes on an elliptical shape firewall to tailstock. It should be stressed that although this method borrows heavily from proven techniques used in the marine trades, it should not be stressed at this point in the process that it is not a complete solution to the problem of building an airplane with pre-fabricated parts. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to anydegree of satisfaction. If the sides were not sloped, the section formed would be cylINDrical and the longons would lie flat. Since they were not tumbled home, the part formed is now conical. If you want to build an experimental aircraft, you need to start with a minimum of pre- fabricated parts, especially when using the preformed fiberglass parts. You need to build the plane with a basic understanding of the materials you are using. The best way to start is to start by understanding that the parts you are building are not going to last for long periods of time, and that they will need to be replaced with new parts that will last for longer periods of the construction process. The next step is to learn how to use the parts that you have already built to make the plane more reliable. The most important thing is to understand that the plane will not last longer than a few months or even a few years before it is ready to fly. The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape. The next article in the series will discuss jigging and building techniques to ensure alignment and strai. The layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later. Back to Mail Online home. back to the page you came from. Back To the pageyou came from, Back to the pages you came From. The fuselage layout guide is available in English, French, German, Italian, Spanish, and Portuguese. For more information on how to layout a fuselage, visit the fuselage. layoutguide.org. For the layout guide in English and French, visit www.fuselage.com/fuselage-laid-in-the-world-for-aircraft-builders-and-engine-builders. For a guide to layout in Spanish and Portuguese, visit http://www. fuselageguide.co.uk/ fuselage-layout-guide.html. For an overview of the layout procedure in German and Italian, visit: fuselagelayoutguide.com/. For a breakdown of layout procedures in the U.S., visit: www.u.S.A. Aircraft Building Guide. For details on layout in the United States, see: http:/www.uS/Aircraft-Building-Guide.html/. For more details on the layout in Europe and the rest of the world, go to: http:// www.australia.org/airplane-building-guide/faselage- layout-guide-1-2-3-4-5-6-7-7. For information on layout for Russia and the Middle East, see www.aircraftbuilder.com%. For information about layout in other countries, visit  the fuselage building guide. for the Middle Eastern and South American countries, such as Russia and South Africa, see http://aircraftbuilding.co/home.uk/. for more information about the layout process in South Africa and the West African and South America, see  http:www.aastralia-building.org/. For the South American and South African versions of this article, click on the link to the home page for a detailed layout guide. Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the sides and bottom ply skins. ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand. To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5KR2Ss now in the air. KR2 builders tend to be innovative, which leads to interesting mods that the KR2 has. The KR2 is a case in point: Many who'd heard of the pitch and tightness of the cabin of theKR2 began to modify the plans to suit their needs. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport. Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79. Some of the work on KR2's is now done on some modifications on the KR1 and KR2. Some modifications eventually creep into some of the mods that KR2 builder's have done on their KR2 aircraft. The most recent modification is the addition of ailerons to the fuselage to make it easier for the plane to take off and land on a tight angle, such as with aileron-only landing gear, or with a fixed landing gear instead of a roll-up landing gear. It is also the case in which the landing gear came directly from the big DC-8s parked on the ramp outside his office window. The tail shape was Stu's, and came from thebig DC-9s parked in the parking lot outside hisoffice window. It was also the tail shape that led to the design of the KR-2, which is now the world's most popular single-engine fighter jet. It's also the most popular fighter jet in the world, followed by the F/A-18 Hornet and the F-18 Super Hornet. The F-16 Hornet is the most advanced fighter jet on the market today, with a top speed of nearly 100 mph. It has a range of more than 1,200 miles (1,600 km) and can carry a crew of up to 20,000 pounds (9,000 km) on a single engine. It also has the largest wingspan of any U.S. fighter jet, and is the only one of its kind that can take off from a standstill and land vertically. It can also be easily converted into a glider by using a special frame that can be attached to the front of the plane with a piece of plywood, or by attaching it to the back of it with a screwdriver. The fuselage can be made to look more like a conventional fuselage by cutting it in half.  KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. The KR2 is stretched 2"" between firewall and main spar, and 14"" behind the main spar. The fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy Hegy. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. The cowling is also a work of art, and uses NACA ducts for efficiency. The fiberglass work is exemplary. The cockpit is a fiberglass main, with a Diehl nose nose and a Diee nosecone. The instrument panel is made of fiberglass, with an aluminum frame. The landing gear is a tricycle gear, and he designed his own test accident test accident gear. He also offers a multitude of KR aluminum and steel parts which he now offers for sale. He uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. More on this exciting development in next month's issue of KROnline. It is also possible to build a KR2 with a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. The result is aKR2 that is stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts. It will receive the first set of the new Rand Robinson wing skins, which are currently being installed at Hinson Composites. The first set is expected to arrive in the next few weeks, and the KR2S will get the second set later this year. It has a high temperature core prepreg construction, and will be made of high-performance polyethylene terephthalate (HETT) material. It also has a large number of parts that can be pre-assembled by hand. The KR-2S was designed by Rand Robinson in the 1980s. The plane was designed to fly at speeds of up to 180 mph. It was designed with low ground clearance and a large fuselage. It has been partially built by a team of four people. The project has been in the works for more than 20 years and is still in the planning stages. It is not yet clear if the plane will ever be fully built by Robinsons' team. For more information on the project, visit www.kitplans.com. For information on building your own KR, visit http://www.kitspans.co.uk/kits-pans/KR-2s-partially-built-by-randy-robertson-and-the-team-of-four-who-have-been-working-on-it-for-20-years. For details on building a KR, go to kitspanners.com/kickspanners/kips-panners-partial-built.html. For the rest of the story, visit kitsplanners.org/kps-kips/kpinners/kpp-kinspinners-part-1-kickspinners. For all the information on how to build your own kips, visit the kipspanners site.com/. For the full story on the kpinners site, visit kipspaners.org/. For more info on the Kipspinners website, visit: http://kipsbaners.com%. For the complete story on kipspanners.com, visit  www. kipskinners.org. for the complete list of kipspinner's kipsons.For more information about the Kinspanners, visit their website: http:www.kspinner.com/?p=kipspiners. For a full list of the kpsons' kipspins, see http: www.skipspin.org%. For a list of all the kenspinners, see www.kspins.gov.uk/. For a complete list, visithttp:http: http:\/www.skype.com /skyps/skype/skypes/skypiners/skypaners/skips/skypins/sky-paners-skypin-skyps-sky-pins-skypan-skypans-skype-skypes-skyPins-SkyPins. For an overview of the world's most popular kipspeners, see:http://skyps.com/""skyps""/. For an in-depth look at the best of the best kipspunters, see ""skyps"", ""skype"" or ""skypin"" . For a short time, I was the only person in the world who could fly the KR, so I didn't know what to do with it. I decided to build it. CNN.com's John Sutter sat down with a group of friends to try to find a way to get to the top of the list. Sutter started out by looking at the glass work on the turtle back. He found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. He decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the new set of wings and still be way ahead of the rest of the group. Now for a list of the problems that I found over the last year and a few of the fixes that I came up with that I didn't know any of the people at the center of the story about. The top 10 list: ""Some by ignorance, some by just not looking close enough. Some by ignorance and some by not looking very close enough."" The bottom 10 list includes: ""I found a small linear crack in the lower left wing cap on the left wing stub,"" and ""I didn't find anything else that might be questionable about the right-hand side of the wall"" and ""It was evenly faired into the vertical and horizontal stabs"" on the right wing. The bottom ten list includes ""the top 10 things that I could see with a good flashlight, but nothing that looked seriously wrong to my eye"" and the bottom 10 items that I couldn't see with my own eyes, but that I thought would be nice to look at at some point. The list ends at the top with ""The top ten things that you can see with your own eyes,"" and the top 10 items are ""the things you didn't see that you should have seen that you wished you had seen more of"" and a ""top 10 list of things to do with the time you spent on this project"" and someplace else that you wish you had spent more time on it, but didn't have the time to do it on the other side of it. It's time to move on to the next part of the show. The next part is ""The Top 10 Things You Need To Know About The Top 10 List,"" which will be posted at the bottom of the page on Monday. The final part is the ""Top 10 Things to Know about the Top Ten List"" which will feature the most important things you learned from this week's episode of CNN.com iReporters and iReporter photos and videos from around the world. It will also feature some of the best photos of the U.S. from the last few weeks of the year, as well as a look back at some of our favorite moments from the past year. The show will be available on CNN.co.uk and CNN.uk on Monday, July 25. The episode will also be posted on iReport.com on Tuesday, July 26, and Wednesday, July 27, and Thursday, July 28, at 10 a.m. and 7 p.m., July 28. Le towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in. Or do something else creative to reinforce the spar cap. Also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. Retapping the fitting the right direction seemed to be a good fix for that problem. I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. One call to the Cleveland factory and they shipped me a new set of wheels andbrakes even though the set for this set was four years old and in the original builders name. Their only receipt was over receipt was a receipt for the set that was over four years ago and in their co-owner's name. The problem was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer. The rear spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts. I also found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leads edge. It peeled apart for rebuild quite easily. The leading edge had been floxing together and glassed over, but the mold release had never been scrubbed off the lead edge of the wing. It was rectify with asmall hole saw and modified undercoat sprayer to get it back to its original state. The right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. Also the Aileron pushrods needed to pass directly through the lower set of rear wingattach fittings to attach to the ailerson. This whole rear spar and bellcranks setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it. It needed to be replaced with the new set that my neighborhood mach inist was cutting out for me. It had to be removed the rear fittings from theleft wing to be replacing with thenew set that his new set. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. It also had a sharp uphill from the sheeve to the bellcranking in the last 12 inches on either side. This combined with the radius that the bell cranks turn caused the cross cable to pull up tight when the aiderons were pushed to either end of their travel, but allowed the cables to go very slack when they were centered. This article is intended to feature some of the problems that you may run into in buying someone else's project. The canopy turned out to have been blow a little too large. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. This will give you an idea of how tall your aft bulkhead need to be. As far as location, I placed my aft Bulkhead just forward of the lower/front of my vertical fin. I constructed the fuselage, it is glued together with automotive bondoed to the jig. I used the stringers I ripped from the 1x 4s and gave me a male poster form to cover with thin plastic or plastic. This was the only form of cover I could find that was thin enough to cover. The fuselage was glued together. I was able to cover it with a thin plastic form to give it a male cover. It is now ready to fly. You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"". You can also send them to jscott.gov@mailonline.co.uk or call the FAA at 1-800-447-8255. The FAA has no comment on this article or any of the other questions you may have about the Turtledecks. The author has no plans to fly the aircraft in the near future. He plans to continue to build his own turtledecaks in the coming months. He has built and rebuild enough of the plane that he should have no problem qualifying under the 51% rule. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and there could have been many other problems that didn't but could have existed on this project. It's not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. Yes, it was just a little bit lopsided. It had more headroom on the right side than the left. It will have to be replaced. The final question is, would I have still purchased this project? I don't know if I'll ever be able to fly it in the first place, but I'll still live with the knowledge that I've built it all from scratch, even if it's not in service yet. I can't live with that. I'll live with it until I fly it. I stapled two layers of posterboard to the jig(thin plastic would work better) The posterboard wraps down two inches onto the fuselage. I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fu fuselage) and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed and 2 inch tapes added to the Bulkheads inside and out. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each sides of the ""backbone"" half the distance to the edge of the turtlingeck. I scuff sanded and glued the foam stringers in with micro. board @ 45degrees. I also covered the foamstringers with one layer. of 8oz bid @ 45 Degrees. When covered with a layer of bid @45degrees there would be a nice transition from the turledecks skin up onto the foam and then back onto the tourledeks skin. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-up 4 to 6 hours after wetout while theLay-up is still soft enough to cut with a razorblade. (be careful, the bondO sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass. skin that tends to hold the shape of theJig but is still flexible enough to work with. I made two inches wide strips that would run the entire length, forward and aft inside theTurtledEck."
What is the effect of the proximity of superconductivity on the Kondo effect?,"\section{Introduction}
\label{sec:Intro}

The exchange interactions control the magnetic order and properties of a vast number of materials
\cite{White2006Dec}
and lead to many fascinating phenomena, such as various types of the Kondo effect 
\cite{Kondo,NozieresBlandin,Pustilnik_Glazman}.
Double quantum dots (DQDs), and in general multi-impurity systems, constitute
a convenient and controllable playground,
where nearly as much different exchange mechanisms compete with each other to
shape the ground state of the system.
\emph{Local exchange} between the spin of a quantum dot (QD)
and the spin of conduction band electrons gives rise to the
Kondo effect \cite{Kondo,Hewson_book}. 
\emph{Direct exchange} arriving with an additional side-coupled QD may destroy it or lead to the 
two-stage Kondo screening \cite{Pustilnik_Glazman,Cornaglia,Granger,ZitkoBonca,ZitkoPRB2010,Ferreira}.
In a geometry where the two QDs contact the same lead, conduction band electrons 
mediate the \emph{RKKY exchange} \cite{RK,K,Y}. The RKKY interaction competes
with the Kondo effect and leads to the quantum phase transition of a still debated nature
\cite{Doniach,Jones,Affleck,Bork,Neel,KondoRKKYexp,Hans,Hans2,Fabian}.
Moreover, in DQDs coupled in series also \emph{superexchange} can alter the Kondo physics significantly
\cite{Zitko_2QDEx,Sela}.

Recently, hybrid quantum devices, in which the interplay between various magnetic correlations
with superconductivity (SC) plays an important role, have become an important direction of research
\cite{hybridQDs,SCspintronics}. In particular, chains of magnetic atoms on SC surface have proven 
to contain self-organized Majorana quasi-particles and exotic spin textures
\cite{Braunecker,Klinovaja,Vazifeh,Yazdani},
while hybrid DQD structures have been used to split the Cooper pairs coherently into two entangled 
electrons propagating to separated normal leads \cite{CPS1,CPS2,CPS4,CPS5,CPS9}.
The latter is possible due to non-local (\emph{crossed}) Andreev reflections (CARs),
in which each electron of a Cooper pair tunnels into different QD, and
subsequently to attached lead. Such processes give rise to an exchange mechanism \cite{Yao},
that we henceforth refer to as \emph{the CAR exchange}, which can greatly modify
the low-temperature transport behavior of correlated hybrid nanostructures.

The CAR exchange may be seen as RKKY-like interaction between
two nearby impurities on SC surface \cite{Yao}.
The effect can be understood as a consequence
of spin-dependent hybridization of the Yu-Shiba-Rusinov (YSR)
states \cite{Yu,Shiba,Rusinov} in SC contact,
caused both by the overlap of their wave functions
and their coupling to Cooper-pair condensate.
This process is the most effective when the YSR states 
are close to the middle of the SC gap, {\it e.g.} in the YSR-screened phase \cite{YSRscreening}.
The mechanism presented here is essentially the same,
yet in the considered regime can be understood
perturbatively without referring to YSR states,
as a consequence of the non-local pairing induced by SC electrode. 
In particular, the presence of YSR bound states close to the Fermi level 
is not necessary for significant consequences for the Kondo physics, 
as long as some inter-dot pairing is present. 


The proximity of SC induces pairing in QDs \cite{RozhkovArovas,Buitelaar} 
and tends to suppress the Kondo effect if the superconducting energy gap $2\Delta$ 
becomes larger than the relevant Kondo temperature $T_K$ 
\cite{Buitelaar2002Dec,adatomsSC,Kondo_vs_SC1,Kondo_vs_SC2,Zitko_Kondo-Andreev,Zitko_S-QD-N,IW_Sau,YSRscreening}.
Moreover, the strength of SC pairing can greatly affect the Kondo physics in the sub-gap transport regime:
For QDs attached to SC and normal contacts, it can enhance the Kondo effect
\cite{DomanskiIW,KWIW,part1}, while
for DQD-based Cooper pair splitters, it tends to suppress both the $\mathrm{SU}(2)$ and $\mathrm{SU}(4)$ Kondo effects \cite{IW_Kacper}.
Our main result is that the non-local pairing induced by superconducting 
proximity effect, which gives rise to CAR exchange, can be the sole cause of the Kondo screening.
Moreover, relatively small values of coupling to SC, $\GS{}\ll U$, are sufficient for the effect to occur.
This is in contrast to the DQD system considered in Ref.~\cite{part1},
where only one of the quantum dots is proximized, such that 
CAR exchange cannot arise,
and the Kondo physics becomes qualitatively
affected only for $\GS{}\sim U/2$.%


\begin{figure}[bt]
\centering
\includegraphics[width=1\linewidth]{Fig1.png}
\caption{
		 (a) Schematic of the considered system. Left/right (L/R) lead
		 is coupled to the first quantum dot (QD1), while superconductor
		 is attached to both QD1 and QD2.
		 (b)-(d) illustrate an example of direct spin exchange:
		 spin-up electron from the initial state (b) hops to the other QD (c) and spin-down electron 
		 hops back (d). Note, that the final state is in fact the same singlet state, 
		 only with opposite sign.
		 (e)-(g) show an example of process contributing to crossed Andreev reflection (CAR) exchange.
		 A Cooper pair from SC approaches DQD (e) and two singlets of the same charge 
		 are formed (f), before the Cooper pair is re-emitted (g).
		 (h)-(j) present an example of RKKY process: an electron scattered off
		 one QD (h) mediates the spin exchange towards the other (i), before it is finally scattered
		 off there, too (j).
		 }
\label{fig:system}
\end{figure}


In this paper we discuss the CAR-induced Kondo screening in a setup comprising T-shaped DQD
with normal and superconducting contacts, see \fig{system}(a).
We note that despite quite generic character of CAR exchange,
and its presence in systems containing at least two localized electrons
coupled close to each other to the same SC bath,
to best of our knowledge CAR-induced screening
has hardly been identified in previous studies
\cite{CPS1,CPS2,CPS4,CPS5,CPS9,IW_Kacper,IW_Sau,Zitko_Josephson,Zitko_S2QD,Martinek2017}.
In the system proposed here [\fig{system}(a)], its presence is evident.
Moreover, CAR exchange magnitude can be directly related to the relevant energy scales, such as the Kondo 
temperature, which provides a fingerprint for quantitative experimental verification of our predictions. 

The paper is organized as follows. In \Sec{model} we describe the considered system 
and present the model we use to study it. In \Sec{scales} the relevant energy scales are estimated
to make the discussion of main results concerning CAR-induced Kondo effect in \Sec{main} more clear. 
Finally, the influence of effects neglected in \Sec{main} are presented in the following sections,
including CAR exchange interplay with RKKY interaction (\Sec{RKKY}), particle-hole asymmetry (\Sec{asym}),
couplings asymmetry (\Sec{x}) and reduced efficiency of CAR coupling (\Sec{coef}). In summary,
the effects discussed in \Sec{main} remain qualitatively valid in all these cases.
The paper is concluded in \Sec{conclusions}.


\section{Model}
\label{sec:model}

The schematic of the considered system is depicted in \fig{system}(a).
It contains two QDs attached to a common SC lead.
Only one of them (QD1) is directly attached to the left (L) and right (R) normal leads,
while the other dot (QD2) remains coupled only through QD1.
The SC is modeled by the BCS Hamiltonian, 
$H_{\mathrm{S}}=\sum_{\mathbf{k}\sigma}\xi_{\mathbf{k}}a_{\mathbf{k}\sigma}^{\dag}a_{\mathbf{k}\sigma}-\Delta\sum_{\mathbf{k}}(a^\dag_{\mathbf{k}\uparrow}a_{-\mathbf{k}\downarrow}^{\dag}+a_{-\mathbf{k}\downarrow}a_{\mathbf{k}\uparrow})$,
with energy dispersion $\xi_{\mathbf{k}}$, energy gap $2\Delta>0$ and $a_{\mathbf{k}\sigma}$ annihilation operator 
of electron possessing spin $\sigma$ and momentum $\mathbf{k}$. The coupling between
SC and QDs is described by the hopping Hamiltonian 
$H_{\mathrm{TS}}=\sum_{i\mathbf{k}\sigma}v_{\mathrm{S}i}(d^\dagger_{i\sigma}a^{}_{\mathbf{k}\sigma}+h.c.)$,
with $d^\dagger_{i\sigma}$ creating a spin-$\sigma$ electron at QD$i$. The matrix element 
$v_{\mathrm{S}i}$ and the normalized density of states of SC in normal state, $\rho_{\rm S}$, 
contribute to the coupling of QD$i$ to SC electrode as $\GS{i} = \pi \rho_{\rm S} |v_{{\rm S}i}|^2$. 
We focus on the sub-gap regime, therefore, we integrate out SC degrees of freedom lying outside the energy gap \cite{RozhkovArovas}.
This gives rise to the following effective Hamiltonian,
$H_{\mathrm{eff}}=H_{\mathrm{SDQD}}+H_{\rm L}+H_{\rm R}+H_{\rm T}$, 
where 
\begin{eqnarray}
H_{\rm SDQD} 	& = & 
				\sum_{i\sigma} \varepsilon_{i} n_{i\sigma} 
				+\sum_{i} U n_{i\uparrow} n_{i\downarrow} 
				+U' (n_1-1)(n_2-1) 
				\nonumber\\
				&+&\sum_\sigma t(d^\dagger_{1\sigma}d^{}_{2\sigma} + h.c.) 
				+J \vec{S}_1\vec{S}_2
				\nonumber\\
				&+&\sum_{i} \!\!\left[ \Gamma_{{\rm S}i} (d^\dagger_{i\uparrow} d^\dagger_{i\downarrow} \!+\! h.c.)
				+\Gamma_{\rm SX} (d^\dagger_{i\uparrow} d^\dagger_{\bar{i}\downarrow} \!+\! h.c.) \right]
	\label{H_DQD} 
\end{eqnarray}
is the Hamiltonian of the SC-proximized DQD
\cite{IW_Kacper,Walldorf2018Feb}, with QD$i$ energy level $\varepsilon_i$,
inter-site (intra-site) Coulomb interactions $U'$ ($U$),
inter-dot hopping $t$, and CAR coupling $\GS{\rm X}$.
$n_{i\sigma}=d^\dagger_{i\sigma}d^{}_{i\sigma}$ denotes the electron number operator 
at QD$i$, $n_i=n_\uparrow+n_\downarrow$, and $\bar{i}\equiv 3-i$. 
Our model is strictly valid in the regime where $\Delta$ is the largest 
energy scale. Nevertheless, all discussed phenomena are
present in a full model for energies smaller than SC gap.
Moreover, by eliminating other consequences of the presence of SC lead,
our model pinpoints the fact that the non-local pairing is 
sufficient for the occurrence of the CAR exchange.
The presence of out-gap states shall result mainly in additional broadening of DQD energy levels,
changing the relevant Kondo temperatures.
We note that the procedure of integrating out out-gap states neglects the 
RKKY interaction mediated by SC lead and other possible indirect exchange mechanisms%
 \footnote{
 Note, that by RKKY interaction we mean only such an effective exchange, 
 which arises due to multiple scattering of a single electron or hole, see \fig{system}(h)-(j).
 Other mechanisms leading to the total indirect exchange are considered separately.
 In particular, in the large gap limit, exchange described in Ref.~\cite{Yao} is in fact reduced to
 the CAR exchange, and additional antiferromagnetic contribution would arise for finite gap.
 }. 
To compensate for this,
we explicitly include the Heisenberg term $ J \vec{S}_1\vec{S}_2$ in
$H_{\rm SDQD}$, with $\vec{S}_i$ denoting the spin operator of QD$i$
and a Heisenberg coupling $J$ substituting the genuine RKKY exchange.

The normal leads are treated as reservoirs of noninteracting electrons,
$H_{r}=\sum_{\mathbf{k}\sigma}\varepsilon_{r\mathbf{k}}c^\dagger_{r\mathbf{k}\sigma}c^{}_{r\mathbf{k}\sigma}$,
where $c^{}_{r\mathbf{k}\sigma}$ annihilates an electron of spin 
$\sigma$ and momentum $\mathbf{k}$ in lead $r$ ($r={\rm L,R}$) with the corresponding energy $\varepsilon_{r\mathbf{k}\sigma}$.
The tunneling Hamiltonian reads,
$H_{\rm T} = \sum_{r\mathbf{k}\sigma} v_{r} (d^\dagger_{1\sigma}c^{}_{r\mathbf{k}\sigma} + h.c.)$,
giving rise to coupling between lead $r$ and QD$i$ of strength $\Gamma_r = \pi \rho_r |v_r|^2$,
with $\rho_r$ the normalized density of states of lead $r$ and $v_r$ the 
local hopping matrix element, assumed momentum-independent.
We consider a wide-band limit, assuming constant $\Gamma_r=\Gamma/2$
within the cutoff $\pm D = \pm 2U$ around the Fermi level. 

For thorough analysis of the CAR exchange mechanism and its consequences
for transport, we determine the linear conductance between the two normal leads from
\begin{equation}
G = \frac{2e^2}{h} \pi \Gamma \int \left[ -\frac{\partial f_T}{\partial\omega} \right] \mathcal{A}(\omega) {\rm d} \omega ,
\label{G}
\end{equation}
where $f_T$ is the Fermi function at temperature $T$,
while $\mathcal{A}(\omega)$ denotes the normalized local spectral density 
of QD1 \cite{fn1}.
Henceforth, unless we state otherwise, we assume a maximal CAR coupling, 
$\GS{\rm X} = \sqrt{\GS{1}\GS{2}}$ \cite{IW_Kacper,Walldorf2018Feb},
$\GS{1}=\GS{2}=\GS{}$ and consider DQD tuned to the particle-hole symmetry point, 
$\varepsilon_1=\varepsilon_2=-U/2$. However, these assumptions are not crucial for the results presented
here, as discussed in Secs.~\ref{sec:asym}-\ref{sec:coef}.

\section{Estimation of relevant energy scales}
\label{sec:scales}

Since we analyze a relatively complex system, let us build up the understanding of its behavior starting
from the case of a QD between two normal-metallic leads, which can be obtained in our 
model by setting $t=\GS{}=J=U'=0$. Then, the conductance as a function of temperature, $G(T)$, grows
below the Kondo temperature $T_K$ and reaches maximum for $T\to 0$, $G(T\!=\!0)=G_{\rm max}$.
At particle-hole symmetry point, the unitary transmission is achieved, $G_{\rm max}= G_0 = 2e^2/h$;
see short-dashed line in \fig{G-T}(a).
An experimentally relevant definition of $T_K$ is that at $T=T_K$ 
$G(T)=G_{\rm max}/2$. $T_K$ is exponentially small in 
the local exchange $J_0 = 8\Gamma / (\pi \rho U)$, and is approximated by
$T_K \approx D \exp[-1/(\rho J_0)]$ \cite{Hewson_book}.

The presence of a second side-coupled QD, $t,U'>0$, significantly enriches the physics of the system 
by introducing direct exchange between QDs, see \fig{system}(b-d).
In general, effective inter-dot exchange can be defined as energy difference between 
the triplet and singlet states of isolated DQD, 
$J^{\mathrm{eff}} = E_{S=1} - E_{\rm GS}$. Unless $U$ becomes very large, superexchange can be neglected
\cite{Zitko_2QDEx} and $J^{\mathrm{eff}}$ is determined by \emph{direct exchange}, $J^{\mathrm{eff}}\approx 4t^2/(U-U')>0$.
When the hopping $t$ is tuned small \cite{CPS1}, one can expect $J^{\mathrm{eff}}\lesssim T_K$, which 
implies the two-stage Kondo screening \cite{Pustilnik_Glazman,Cornaglia}.
Then, for $T \ll T_K$, the local spectral density of QD1 serves as a band of width $\sim T_K$ for QD2.
The spin of an electron occupying QD2 
experiences the Kondo screening below the associated Kondo temperature
\begin{equation}
T^* = a T_K \exp(- b T_K / J_{\rm eff})
\label{Tstar}
\end{equation}
with $a$ and $b$ constants of order of unity \cite{Pustilnik_Glazman,Cornaglia}.
This is reflected in conductance, which drops to $0$ with lowering $T$, maintaining characteristic 
Fermi-liquid 
$G\sim T^2$ dependence \cite{Cornaglia}; see the curves indicated with squares 
in \fig{G-T}(a). Similarly to $T_K$, experimentally relevant definition of $T^*$ is that 
$G(T\!=\!T^*) = G_{\rm max}/2$. Even at the particle-hole 
symmetry point $G_{\rm max} < G_0$, because the single-QD strong-coupling fixed point 
is unstable in the presence of QD2 and $G(T)$ does not achieve $G_0$ exactly,
before it starts to decrease.


The proximity of SC gives rise to two further exchange mechanisms that
determine the system's behavior. First of all, the (conventional)
\emph{RKKY interaction} appears, $J \sim \GS{}^2$ \cite{RK,K,Y}. 
Moreover, the \emph{CAR exchange} emerges as a consequence of finite $\GS{}$ \cite{Yao}. 
It can be understood on the basis 
of perturbation theory as follows. DQD in the inter-dot singlet state may absorb
and re-emit a Cooper pair approaching from SC; see \fig{system}(e)-(g). As a second-order
process, it reduces the energy of the singlet, which is the ground state of isolated DQD.
A similar process is not possible in the triplet state due to spin conservation.
Therefore, the singlet-triplet energy splitting $J^{\mathrm{eff}}$ is increased (or generated for $t=J=0$). 
More precisely, the leading ($2$nd-order in $t$ and $\GS{}$) terms
in the total exchange are 
\begin{equation}
J^{\mathrm{eff}} 	\approx 	J + \frac{4t^2}{U-U'+\frac{3}{4}J} + \frac{4\GS{}^2}{U+U'+\frac{3}{4}J}.
\label{Jeff}
\end{equation}
Using this estimation, one can predict $T^*$ for finite $\GS{}$, $t$ and $J$ with \eq{Tstar}.
Apparently, from three contributions corresponding to:
(i) RKKY interaction, (ii) direct exchange and (iii) CAR exchange, only the first may bear a negative (ferromagnetic) sign.
The two other contributions always have an anti-ferromagnetic nature.
More accurate expression for $J^{\mathrm{eff}}$ is derived in Appendix~\ref{sec:downfolding}
[see \eq{A_J}] by the Hamiltonian down-folding procedure. The relevant terms differ 
by factors important only for large $\GS{}/U$. 
Finally, it seems worth stressing that normal leads are not necessary for CAR exchange to occur.
At least one of them is inevitable for the Kondo screening though, and two symmetrically coupled 
normal leads allow for measurement of the normal conductance.


It is also noteworthy that inter-dot Coulomb interactions
decrease the energy of intermediate states contributing to direct exchange 
[\fig{system}(c)], while increasing the energy of intermediate
states causing the CAR exchange [\fig{system}(f)].
This results in different dependence of corresponding terms in \eq{Jeff} on $U'$.
As can be seen in \figs{G-T}(b) and \ref{fig:G-T}(c), it has a significant effect 
on the actual values of $T^*$.

\begin{figure}
\includegraphics[width=1\linewidth]{Fig2.pdf}
\caption{(a) Linear conductance $G$ as function of $T$ calculated for 
		 $\varepsilon_1=\varepsilon_2=-U/2$, $\Gamma=U/5$, $U'=U/10$ and different situations, 
		 as indicated. The quantity $\xi\equiv\sqrt{\GS{}^2+t^2}$ is fixed 
		 for different curves drawn with the same dashing style.
		 Note the logarithmic scale on both axes.
		 %
		 (b) Points show $T^*/T_K$ calculated by NRG from curves in subfigure (a). 
		 Lines present the fit to \eq{Tstar} with $J^{\mathrm{eff}}$ obtained from \eq{Jeff}.
		 %
		 (c) The same as (b), only for $U'=0$.
		 %
		 (d) and (e) show the residual conductance $G_{\mathrm{min}} \equiv G(T \!=\! 0)$ as a function of
		 $\GS{}$ for $t=0$ (denoted ""CAR"") and $t=\GS{}$ (denoted ""Both""). 
		 Dotted line is a guide for eyes. $U'=U/10$ in (b) and (d) and $U'=0$ in (c) and (e).
		}
\label{fig:G-T}
\end{figure}

\section{CAR exchange and Kondo effect}
\label{sec:main}

To verify \eqs{Tstar}-(\ref{Jeff}) we calculate $G$ using
accurate full density matrix numerical renormalization group (NRG) technique \cite{WilsonNRG,Weichselbaum,FlexibleDMNRG,fn2}.
We compare $U'=0$ case with experimentally relevant value $U'=U/10$ \cite{Keller2013Dec}.
While for two close adatoms on SC surface RKKY interactions may lead to prominent consequences
\cite{Klinovaja}, the conventional ({\it i.e.} non-CAR) contribution should 
vanish rapidly when the inter-impurity distance $r$ exceeds a few lattice constants \cite{RKKYrange,SC_RKKY}. 
Meanwhile, the CAR exchange may remain significant for $r$ of the order
of coherence length of the SC contact \cite{Yao}. Therefore, we first neglect the conventional RKKY coupling and analyze its consequences in Sec.~\ref{sec:RKKY}.

The main results are presented in \fig{G-T}(a), showing the temperature dependence of $G$
for different circumstances. 
For reference, results for $\GS{}=0$ are shown, exhibiting 
the two-stage Kondo effect caused by \emph{direct} exchange mechanism.
As can be seen in \figs{G-T}(b) and \ref{fig:G-T}(c), an excellent agreement of $T^*$ found from NRG calculations and \eq{Tstar} 
is obtained with $a=0.42$ and $b=1.51$, the same for both $U'=0$ and $U'=U/10$. Note, 
however, that $J^{\mathrm{eff}}$ is different in these cases, cf. \eq{Jeff},
and $U'$ leads to increase of $T^*$.

Furthermore, for $t=0$ and $\GS{}>0$ the two-stage Kondo effect caused solely by the \emph{CAR
exchange} is present; see \fig{G-T}(a).
Experimentally, this situation
corresponds to a distance between the two QDs smaller than the superconducting coherence length,
but large enough for the exponentially suppressed direct hopping to be negligible.
While intuitively one could expect pairing to compete with any kind of magnetic ordering,
the Kondo screening induced by CAR exchange is a beautiful example of a superconductivity
in fact leading to magnetic order, namely the formation of the Kondo singlet.
This CAR-exchange-mediated Kondo screening is our main finding.
For such screening, \eq{Tstar} is still fulfilled with very similar 
parameters, $a=0.37$ ($a=0.35$) and $b=1.51$ ($b=1.50$) for $U'=0$ ($U'=U/10$),
correspondingly; see \figs{G-T}(b-c).
Moreover, as follows from \eq{Jeff}, $U'$ reduces CAR exchange, and therefore diminishes $T^*$.
For the same values of $J^{\mathrm{eff}}$, the dependence of $G(T)$ for $t=0$ and $\GS{}>0$ is hardly different 
from the one for $\GS{}=0$ and $t>0$ for $T\geq T^*$ (results not shown).
However, $G(T)$ saturates at residual value $G_{\mathrm{min}}$ as $T\to 0$ only for finite
$\GS{}$, which at particle-hole symmetry makes $G_{\mathrm{min}}$
the hallmark of SC proximity and the corresponding CAR exchange processes.
From numerical results, one can estimate it as
\begin{equation}
G_{\mathrm{min}} = \frac{e^2}{h} \cdot c \, \frac{\GS{}^2}{U^2} 
	\qquad {\scriptstyle (\GS{1}=\GS{2}=\GS{})} ,
\label{Gmin}
\end{equation}
with $c\approx 2.25$, barely depending on $U'$ and getting smaller for $t>0$. 
This is illustrated in \figs{G-T}(d-e), where the dotted line corresponds to \eq{Gmin} with $c=2.25$. 

Lastly, in \fig{G-T}(a) we also present the curves obtained for $t=\GS{}$ chosen such, 
that the quantity $\xi=\sqrt{t^2+\GS{}^2}$ remains the same 
in all the cases.
This is to illustrate what happens when \emph{both} (direct and CAR) exchange interactions are
present. \fig{G-T}(c) clearly shows that $T^*$ remains practically unaltered for $U'=0$.
The comparison with \fig{G-T}(b) proves that in this case it practically does not depend 
on $U'$. The enhancement of direct exchange is compensated by the decrease of the CAR one. 
On the contrary, $G_{\mathrm{min}}$ decreases for larger $t$ below the estimation given by Eq.~(\ref{Gmin}), 
as can be seen in \figs{G-T}(d-e). 

While analyzing the results concerning $G_{\mathrm{min}}(\GS{})$ plotted in \figs{G-T}(d-e) 
one needs to keep in mind that $G_{\mathrm{min}}$ is obtained at deeply cryogenic conditions. To illustrate
this better, $G(\GS{})$ obtained for $t=0$ and $T=10^{-6}U$ is plotted with solid line 
in \fig{3}. Clearly, for weak $\GS{}$ the system exhibits rather conventional (single-stage)
Kondo effect with $G=G_{\mathrm{max}}\approx 2e^2/h$, while QD2 is effectively decoupled ($G_{\mathrm{max}}<2e^2/h$
in the proximity of SC lead \cite{KWIW}). Only for larger values of $\GS{}$
the CAR exchange is strong enough, such that $T^*>T$ and the dependence $G(\GS{})$ continuously 
approaches the $T=0$ limit estimated by \eq{Gmin} and presented in \figs{G-T}(d-e).

\section{CAR-RKKY competition}
\label{sec:RKKY}

\begin{figure}
\includegraphics[width=0.98\linewidth]{Fig3.pdf}
\caption{Linear conductance $G$ vs. $\GS{}$ calculated
		 for $t=0$, $\Gamma=U/5$, $U'=U/10$, finite $T=10^{-6}U$
		 and different values of RKKY coupling $J$, as indicated. 
		 Inset shows QD1 spectral function $\mathcal{A}(\omega)$ as a function of energy $\omega$
		 for points on $J=-0.1U$ curve, indicated with corresponding symbols.
		}
\label{fig:3}
\end{figure}

Let us now discuss the effects introduced by the conventional RKKY interaction.
We choose $t=0$ for the sake of simplicity and
analyze a wide range of $\GS{}$, starting from the case of anti-ferromagnetic 
RKKY interaction ($J>0$). Large $J>0$ leads to the formation of a molecular singlet in the 
nanostructure. This suppresses the conductance, unless $\GS{}$ becomes of the order of $U/2$, 
when the excited states of DQD are all close to the ground state. This is illustrated 
by double-dotted line in \fig{3}.
Smaller value of $J>0$ causes less dramatic consequences, namely it just increases $J^{\mathrm{eff}}$ according
to \eq{Jeff}, leading to enhancement of $T^*$, cf. \eq{Tstar}. This is presented with
dot-dashed line in \fig{3}.

The situation changes qualitatively for ferromagnetic RKKY coupling, $J<0$.
Then, RKKY exchange and CAR exchange have opposite signs and compete with each other.
Depending on their magnitudes and temperature, one
of the following scenarios may happen.

For $J^{\mathrm{eff}} > 0$, {\it i.e.} large enough $\GS{}$, and $T<T^*$, the system is in the 
singlet state due to the two-stage Kondo screening of DQD spins. $G(T\!=\!0)$ is reduced 
to $G_{\mathrm{min}}$, which tends to increase for large negative $J$; see dashed lines in \fig{3}. 
In the inset to \fig{3}, the spectral density of QD1 representative for this regime is plotted 
as curve indicated by triangle. It corresponds to a point on the $J=-0.1U$ curve in the main 
plot, also indicated by triangle. The dip in $\mathcal{A}(\omega)$ has width of order of $T^*$.

For finite $T$, there is always a range of sufficiently small $|J^{\mathrm{eff}}|$, where QD2 becomes effectively
decoupled, and, provided $T<T_K$, $G$ reaches $G_{\mathrm{max}}$ due to conventional Kondo effect 
at QD1. This is the case for sufficiently small $\GS{}$ for $J=0$ or $J=-0.01U$, and in the narrow
range of $\GS{}$ around the point indicated by a circle in \fig{3} for $J=-0.1U$ (for $J=0.01U$, 
the considered $T$ is close to $T^*$ and $G$ does not reach $G_{\rm max}$). The conventional Kondo effect manifests itself with 
a characteristic peak in $\mathcal{A}(\omega)$, as illustrated in the inset in \fig{3} with line denoted by circle.

Finally, large enough $J^{\mathrm{eff}} < 0$ and low $T$, give rise to an effective ferromagnetic coupling of DQDs 
spins into triplet state. Consequently, the underscreened Kondo effect occurs 
\cite{Mattis,NozieresBlandin} for weak $\GS{}$ and, {\it e.g.}, $J=-0.1U$; 
see the point indicated by square in \fig{3}.
This leads to $G=G_{\mathrm{max}}$ and a peak in $\mathcal{A}(\omega)$, whose shape is significantly different from the
Kondo peak, cf. the curve denoted by square in the inset in \fig{3}. 



\section{Effects of detuning from the particle-hole symmetry point}
\label{sec:asym}

\begin{figure}
\includegraphics[width=0.98\linewidth]{Fig4.pdf}
\caption{
         (a) Linear conductance between the normal leads $G$ as a function of temperature $T$
         for parameters corresponding to \fig{G-T}(a) with $\xi=U/10$, and additional curves for finite 
         detuning from particle-hole symmetry point, $\delta_1=-\delta_2$, 
         and two values of $\xi=\sqrt{t^2+\GS{}^2}$, as indicated in the figure.
         (b) $G_{\mathrm{min}} \equiv G(T \!=\! 0)$ as a function of QD1 detuning $\delta_1$ for different
         exchange mechanisms, $\xi=U/10$ and $\delta_2=\pm\delta_1$ (as indicated).
		}
\label{fig:asym}
\end{figure}

At PHS $G_{\mathrm{min}}=G(T \!=\! 0)=0$ in the absence of superconducting lead, making $G_{\mathrm{min}} > 0$ a hallmark
of SC-induced two-stage Kondo effect. However, outside of PHS point $G_{\mathrm{min}} > 0$ even in the case of 
the two-stage Kondo effect caused by the direct exchange. 
Exact PHS conditions are hardly possible in real systems, and the fine-tuning of the QD energy
levels to PHS point is limited to some finite accuracy.
Therefore, there may appear a question, if the results obtained at PHS are of any importance for the
realistic setups. As we show below --- they are,
in a reasonable range of detunings $\delta_i=\varepsilon_i +U/2$.

In \fig{asym}(a) we present the $G(T)$ dependence in and outside the PHS, corresponding to 
parameters of \fig{G-T}(a). 
Clearly, for considered small values of $\delta_1=\delta_2=\delta$, 
$G_{\mathrm{min}}<10^{-3}e^2/h$ for direct exchange only, while $G_{\mathrm{min}}$ in the presence of a superconductor is 
significantly increased and close to the PHS value. Furthermore, for $|\delta_1| \sim |\delta_2| 
\sim \delta$, the residual conductance caused by the lack of PHS, $G_{\mathrm{min}} \approx e^2/h \cdot (\delta/U)^2$,
which is a rapidly decreasing function in the vicinity of PHS point, as illustrated in \fig{asym}(b)
with lines denoted by a square. Evidently, in the regime $|\delta_i| < 0.01U$ the residual conductance
caused by SC is orders of magnitude larger, leading to the plateau in $G_{\mathrm{min}}(\delta_1)$ dependence,
visible in \fig{asym}(b).
Taking into account that the realistic values of $U$ in the semiconductor quantum dots are rather 
large, this condition seems to be realizable by fine-tuning of QD gate voltages.

Lastly, let us point out that while in the presence of only one exchange mechanism, \emph{CAR} or
\emph{direct}, $G_{\mathrm{min}}(\delta_1)$ dependencies depicted in \fig{asym}(b) are symmetrical with respect
to sign change of $\delta_1$, for \emph{both} exchange mechanisms the dependence is non-symmetric. 

\section{Effects of asymmetry of couplings to superconductor}
\label{sec:x}

\begin{figure}
\includegraphics[width=0.98\linewidth]{Fig5.pdf}
\caption{
		 (a) Linear conductance between the normal leads, $G$, as a function of temperature, $T$,
		 for parameters corresponding to \fig{G-T}(a) with $\xi=U/10$, for different values 
		 of asymmetry coefficient $x$ [see \eq{xGS}], in the presence of \emph{CAR} exchange only.
		 %
		 (b) The second-stage Kondo temperature $T^*$ normalized by $T_K$ as a function of $x$, 
		 calculated with the aid of NRG (points) and a fit to \eq{Tstar} (lines) 
		 with $J^{\mathrm{eff}}$ from \eq{Jeff}.
		 %
		 (c) The zero-temperature conductance $G_{\mathrm{min}}$ as a function of QD1 coupling to SC lead, $\GS{1}$,
		 compiled from data obtained at different circumstances (as indicated in the legend)
		 for different $x$. Dotted line corresponds to \eq{Gmin2} with $c=2.25$.
		}
\label{fig:x}
\end{figure}

Similarly to PHS, the ideal symmetry in the coupling between respective QDs and SC lead is hardly possible
in experimental reality. As shown below, it does not introduce any qualitatively new features.
On the other hand, it decreases the second stage Kondo temperature, which is already small, therefore,
quantitative estimation of this decrease may be important for potential experimental approaches.
To analyze the effects of $\GS{1}\neq\GS{2}$, we introduce the asymmetry parameter $x$ and extend
the definition of $\GS{}$,
\beq
x = \frac{\GS{1}-\GS{2}}{\GS{1}+\GS{2}}, \quad \GS{} = \frac{\GS{1}+\GS{2}}{2}.
\label{xGS}
 \end{equation} 
Note, that even for a fixed $\GS{}$, the actual CAR coupling $\GS{\rm X}=\GS{}\sqrt{1-x^2}$ decreases
with increasing $|x|$, which is a main mechanism leading to a decrease of $T^*$ outside the $x=0$ point
visible in \figs{x}(a) and (b). To illustrate this, the curves corresponding to \emph{both} exchange
mechanisms were calculated using $x$-dependent $t=\GS{\rm X}$ instead of $t=\xi/\sqrt{2}$. 
Therefore, $\xi$ was generalized for $x\neq 0$ by setting $\xi=\sqrt{t^2(1-x^2)^{-1}+\GS{}^2}$.
Clearly, in \fig{x}(b) the curves for different exchange mechanisms are very similar and differ mainly 
by a constant factor, resulting from different influence of $U'$; see \Sec{scales}. 
The magnitude of $T^*$ changes is quite large, exceeding an order of magnitude for $x=\pm 0.5$ 
and $\xi=U/20$. Moreover, $T^* \to 0$ for $x\to\pm 1$. Consequently, for strongly asymmetric
devices one cannot hope to observe the second stage of Kondo screening.

A careful observer can note that the $T^*(x)$ dependency is not symmetrical; note for example different 
$T^*$ for $x=\pm 0.5$ in \fig{x}(a). This is caused by the dependence of the first stage Kondo temperature
$T_K$ on $\GS{1}$ \cite{part1,DomanskiIW},
\beq
\widetilde{T}_K(\GS{1}) = T_K \cdot \exp\!\left( \frac{\pi}{2} \frac{\GS{1}^2}{\Gamma U}\right).
 \end{equation} 
Here, $T_K$ is, as earlier, defined in the absence of SC, while $\widetilde{T}_K$ is a function 
of $\GS{1}$, such that $G(\widetilde{T}_K) = G_{\rm max}(\GS{1})/2$ in the absence of QD2. 
As $\widetilde{T}_K$ grows for increasing $\GS{1}$ (or $x$), $T^*$ decreases according to \eq{Tstar}. 
Its $\GS{}$ dependence can be accounted for by small changes in the coefficients $a$ and $b$ in \eq{Tstar}, 
as long as $x$ is kept constant. 

To close the discussion of $T^*(x)$ dependence let us point out, that in \eq{A_J} 
there appears a correction to \eq{Jeff} for $x\neq 0$. However, it is very small due to additional
factor $\GS{}^2/U^2$ in the leading order. Its influence on curves plotted in \fig{x}(b) is hardly visible.

In turn, let us examine the $x$ dependence of the $T=0$ conductance $G_{\mathrm{min}}$. As can be seen 
in \fig{x}(a), it monotonically increases with $x$, as it crosses $x=0$ point. In fact, \eq{Gmin}
can be generalized to
\beq
G_{\mathrm{min}} = \frac{e^2}{h} \cdot c \, \frac{\GS{1}^2}{U^2} ,
\label{Gmin2}
 \end{equation} 
with $c\approx 2.25$ (indicated by a dotted line in \fig{x}(c)). Note that $G_{\mathrm{min}}$ is proportional to 
$\GS{1}^2=(x+1)^2 \GS{}^2$, instead of simply $\GS{}$, cf. \eq{Gmin}. The values of $G_{\mathrm{min}}$ obtained
from all analyzed $G(T)$ dependencies for different $x$ have been compiled in \fig{x}(c).
It is evident, that \eq{Gmin2} is approximately fulfilled for all the considered cases.

Finally, it seems noteworthy that the normal-lead coupling asymmetry, 
$\Gamma_{\rm L}\neq \Gamma_{\rm R}$, is irrelevant for the results except for a constant factor
diminishing the conductance $G$ \cite{KWIWJB-asym}.



\section{The role of CAR efficiency}
\label{sec:coef}

\begin{figure}[tb]
\includegraphics[width=0.98\linewidth]{Fig6.pdf}
\caption{Linear conductance between the normal leads
		 $G$ as a function of coupling to SC lead, $\GS{}$, for indicated values of RKKY exchange $J$
		 and the efficiency of CAR processes reduced by factor (a) $\mathcal{C}=0.9$ and (b) $\mathcal{C}=0.5$.
		 Other parameters as in \fig{3}.
		 Insets: QD1 local spectral density $\mathcal{A}(\omega)$ as a function of energy $\omega$
		 for points on $J=-0.1U$ curve, indicated with corresponding symbols.
		} 
\label{fig:C}
\end{figure}

Up to this point we assumed $\GS{\rm X} = \sqrt{\GS{1}\GS{2}}$, which is valid when the two 
quantum dots are much closer to each other than the coherence length in the superconductor.
This does not have to be the case in real setups, yet relaxing this assumption does not 
introduce qualitative changes. Nevertheless, the model cannot be extended to inter-dot 
distances much larger than the coherence length, where $\GS{\rm X}\to 0$.

To quantitatively analyze the consequences of less effective Andreev coupling we define the 
CAR efficiency as $\mathcal{C} \equiv \GS{\rm X} / \sqrt{\GS{1}\GS{2}}$ and analyze $\mathcal{C} < 1$
in the wide range of $\GS{1}=\GS{2}=\GS{}$ and other parameters corresponding to \fig{3}. 
The results are presented in \fig{C}.

Clearly, decreasing $\mathcal{C}$ from $\mathcal{C}=1$ causes diminishing of $\GS{\rm X}$, and consequently of CAR 
exchange. For a change as small as $\mathcal{C}=0.9$, the consequences reduce to some shift of the 
conventional Kondo regime, compare \fig{C}(a) with \fig{3}. Stronger suppression of CAR may, 
however, increase the SC coupling necessary to observe the second stage of Kondo screening caused
by CAR outside the experimentally achievable range, see \fig{C}(b). Moreover, the reduced $T^*$
leads to narrowing of the related local spectral density dip, while the
increased critical $\GS{}$ necessary for the observation of the second stage of screening leads to the
shallowing of the dip. This is visible especially in the inset in \fig{C}(b).


\section{Conclusions}
\label{sec:conclusions}

The CAR exchange mechanism is present in any system comprising at least
two QDs or magnetic impurities coupled to the same superconducting contact
in a way allowing for crossed Andreev reflections.
In the considered setup, comprised of two quantum dots in a T-shaped geometry 
with respect to normal leads and proximized by superconductor,
it leads to the two-stage Kondo
screening even in the absence of other exchange mechanisms.
This CAR induced exchange screening is characterized by a residual 
low-temperature conductance at particle-hole symmetric case.
We have also shown that the competition between CAR exchange and RKKY
interaction may result in completely different Kondo screening scenarios.

The presented results bring further insight into the low-temperature
behavior of hybrid coupled quantum dot systems, which hopefully could be verified
with the present-day experimental techniques.
Moreover, non-local pairing is present also in bulk systems such as non-$s$-wave superconductors.
The question if an analogue of discussed CAR exchange may play a role there
seems intriguing in the context of tendencies of many strongly correlated materials
to possess superconducting and anti-ferromagnetic phases.


\begin{acknowledgments}
This work was supported by the National Science Centre in Poland through project no.
2015/19/N/ST3/01030.
We thank J. Barna\'{s} and T. Maier for valuable discussions.
\end{acknowledgments}




",['It tends to suppress the Kondo effect.'],5009,multifieldqa_en,en,,917f18543035ee1b9161cded1d3352531dbf3b249b5f0a18," The exchange interactions control the magnetic order and properties of a vast number of materials. They lead to many fascinating phenomena, such as various types of the Kondo effect. The effect can be understood as a consequence of spin-dependent hybridization of the Yu-Shiba-Rusinov (YSR) states. This process is the most effective when the YSR states are close to the middle of the SC gap. The mechanism presented here is essentially the same, yet in the considered regime can be better understood without referring to Y SR states. It can be seen as RKKY-like interaction between two nearby impurities on SC surface. The presence of YSR bound to the Fermi level is not necessary for Kondo physics, as long as some inter-dot pairing in QDs is present in Qozhkov-Zitko-and-Vazifeh system. It is also possible to suppress the effect if the superconducting energy gap $2Delta$ is larger than the relevant Kondo temperature $T_Kondo_vs_SC1,Kondo-vs-SC2,and-Andreev-QD-T-Vzitko. The interaction can greatly modify the low-temperature transport behavior of correlated hybrid nanostructures. It also has significant consequences for the ermerm level level of the quantum phase transition of the QD-SC-VZ-Z-V-Z system. The Kondo-SC interaction can also be used to create a two-stage Kondo screening process in the DQD system, which can be shown to have significant effects on the quantum phase transition of DQDs and SCspintronics. It has been described as a ‘two-stage’ Kondo screening process, but the exact mechanism behind it is still undisclosed and understood by the researchers. The Kondo effect can also be explained as a result of non-local (‘crossed’) Andreev reflections (CARs), in which each electron of a Cooper pair tunnels into different QD, and subsequently to attached lead. It is possible due to non- local (\emph{crossed) andreev repositories, which can be understood as a consequence of the spin of a quantum dot (QD) on the surface of a superconductor (SC) The effect is also known as the ‘Kondo effect’ and can be described in terms of the “Kondo phase transition’ (KondoRKKYexp, KondoRKYexp) and “YSR screening” (Y SR screening). It is possible to use the CAR exchange to explain the ‘YSR-screened phase’ of a hybrid quantum device (DQD), which can also have significant consequences for the quantum state of the device. The author also explains the ‘Y SR-screening’ process in terms of ’YSR states,’ which is a consequence of the non-‘local’ pairing by SC electrode (Fi, Fi, ZitkoBonca, Z itkoPRB2010,Ferreira) on a hybrid QD structure. The ‘Car exchange’ can also modify the low-temperatures of hybrid quantum devices (QDs) The Kondo screening can be the sole cause of the CAR exchange. Car exchange magnitude can be directly related to the relevant energy scales, such as the Kondo temperature. The Kondo physics becomes qualitatively affected only for $U/2$. The paper is organized as follows. In the main section, we describe the considered system and present the model we use to study it. We then discuss the influence of effects neglected in themain section, including CAR exchange interplay with RKKY interaction, particle-hole asymmetry and reduced efficiency of CAR coupling. We conclude the paper with the conclusion that the CAR-induced Kondo effect is qualitatively valid in all these cases. The study is published in the open-access journal The Astrophysics of Superconducting Matter (ASM) (http://www.astrophysics.org/ASM/2013/01/27/kondo-screening-in-superconducting-matter.html#storylink=cpy;. ASM is an open-source, non-profit organization that aims to improve the understanding of superconducting matter. The ASM website is: www.asm.org.uk/news/2013-01/26/kondoscreening.html;.ASM is a nonprofit organization that promotes the study of superconductor physics and the development of new ideas about superconductivity. It is based on the ASM’s ‘Superconducting Superconductors’ (SCs) website, which is open- access and free-to- download. For more information, visit www.astrom.org or go to www. astrom.gov.uk/. For more details on the SCs, see: http:www.astsm.gov/s/superconductor.html. For details on how to use the SC to study superconductors, visit: http:// www.astsom.org/. For information about the SC bath, see http://www.-sas.org /sas/smas/sacs/sac/sasc/ssc.html%. For more info on the SSC bath, visit http://sasc.gov/. For details about the SAC bath, please visit:http://sacs.org/?p=sasc;.sas;. The SAC is a free-access website that provides information about how to access the Sasc’SAC database. For information on how the SC baths are used, see www.sac.org;. For further information, please see:http:www-sas-org.com/sarc/sacc/sass/scof/sc.html?p=SAC;.The SSC is modeled by the BCSian Hamiltonian, a Hamiltonian Hamiltonians’ Hamiltonian model. For. more information on the Hamiltonian BCS model, see here:  http:://sass.org%. For. example, see the article:   ‘Kondo screening’, ‘SAC’ and ‘sac’. The article is also available in the online version of this article with the title “Kondo Screening in a T-shaped DQD setup.”. For the full version of the article, please click here: http :www.sassonline.com/. For. instance, the article includes the full text of the paper. We focus on the sub-gap regime, therefore, we integrate out SC degrees of freedom lying outside the energy gap. This gives rise to the following effective Hamiltonian, which is the Hamiltonian of the SC-proximized DQD. We note that the procedure of integrating out out-gap states neglects the RKKY interaction mediated by SC lead and other possible indirect exchange mechanisms. Nevertheless, all discussed phenomena are present in a full model for energies smaller than SC gap. The model is strictly valid in the regime where $Delta$ is the largest houselessenergy scale. We conclude that the model pinpoints the fact that the non-local pairing is sufficient for the occurrence of the CAR exchange. The presence of out- gap states shall result mainly in additional broadening of D QD energy levels, changing the relevant Kondo temperatures. The total energy of the model is described in the total energy model described in Ref. Other, the reduced reduced gap limit, and additional antromagnetic contribution would arise for finite gap. To compensate for this, we treat normal leads as reservoirs of nonacting electrons, denoting the spin operator QD$i, and substituting a Heisenberg term $J$and a coupling $J_1_2_2 in the SDQD model. We also include the Coulomb interactions $U'$ ($U$),inter-dot hopping $t, and CAR coupling $GS$i$ in the model. The full model can be found at: http://www.sciencemag.org/content/early/2014/01/29/science-mag-magnetism-in-a-model-with-sc-lead-and-car-coupling-taken-out-of-the-missing-energy-gap.html#storylink=cpy. We are also happy to provide a more detailed description of our model in the full version of this article, which will be published in the next issue of the journal, The Journal of Theoretical Physics (July 2014). We are happy to share the results with the author’s co-authors, who have also published a version of the paper in the journal The journal of the same journal, the Journal of the Physics Association (June 2014). The article is also available in the open-access version, which can be downloaded by clicking on the link below. The article has been translated from the German version into the English version, with the German headline “SC lead and CAR exchange: How does the model work?’”. The German version also includes the English-language version, “How do we know if the model works in the real world?” and “What are the effects of SC lead on the theory of CAR exchange? “,” ”, “What do we think about the potentialities of SC leads in a real-world application of the theory?‚”,. “I’m afraid the answer is not very clear.”., “We’re going to have to go on with the story’.’, ”,“”;. ““The model is not going to be as simple as I first thought. I’ve got to show that SC lead is a reservoir of non-acting electrons’ ’’,. ‘’; “Theoretically, the model can’t be as complex as I initially thought.“,.” , “This is a model of a model with SC lead, and I have to show it to you that it is not a real model of the universe’ .’ The spin of an electron occupying QD2 experiences the Kondo screening below the associated Kondo temperature. The local spectral density of QD1 serves as a band of width for QD 2. The presence of a second side-coupled QD, $t,U'>0, significantly enriches the physics of the system. The conductance as a function of temperature, $G(T)$, grows below the Konda temperature $T_K$ and reaches maximum for $T\to 0. A single-QD strong-cpling point is unstable in the presence of the single particle-hole symmetry point, because the single QD is unstable at the particle- hole symmetry point. This is reflected in the characteristic Fermi-liquid dependence of $0$$G2$ and $G1$ in the curves indicated with the squares indicated with squares. The results are consistent with an order of unity of unity with $a and $b$ constants of unity, and with $Tstar$ of $1 and $B$  of $2  in the model. The model is based on a maximal CAR coupling, $GS{1}\GS{2} = \sqrt{\GS{ 1}\GS {2}=\GS{}$ and consider DQD tuned to the particle-hole symmetry point, $Varepsilon_1=\varepsil_2=-U/2$ However, these assumptions are not crucial for the results presented                here, as discussed in Secs.~\ref{sec:asym}-\ref {sec:coef}. The model can be obtained by setting $t=\ GS{}=J=U'=0$ and setting $T=T-T=J-T'=J'=U-U'$ in a variety of ways, such as by tuning the QD to $t-T-J' to $T-G'$ or by tuning $U-G$ to $U'$. The model has been published in the journal Theoretical Physics, Volume 1, Issue 2, Issue 3, Issue 4, Issue 5, Issue 6, Issue 7, Issue 8, Issue 9, Issue 10, Issue 11, Issue 12, Issue 13, Issue 14, Issue 15, Issue 16, Issue 17, Issue 18, Issue 19, Issue 20, Issue 21, Issue 22, Issue 23, Issue 24, Issue 25, Issue 26, Issue 27, Issue 28, Issue 29, Issue 30, Issue 31, Issue 32, Issue 34, Issue 35, Issue 38, Issue 39, Issue 40, Issue 41, Issue 44, Issue 50, Issue 51, Issue 52, Issue 53, Issue 54, Issue 55, Issue 56, Issue 57, Issue 58, Issue 59, Issue 60, Issue 61, Issue 62, Issue 63, Issue 64, Issue 69, Issue 70, Issue 74, Issue 79, Issue 80, Issue 81, Issue 82, Issue 83, Issue 78, Issue 84, Issue 85, Issue 88, Issue 89, Issue 90, Issue 94, Issue 104, Issue 106, Issue 103, Issue 108, Issue 109, Issue 110, Issue 112, Issue 113, Issue 114, Issue 101, Issue 107, Issue 120, Issue 130, Issue 131, Issue 133, Issue 134, Issue 138, Issue 149, Issue 105, Issue 116, Issue 111, Issue 128, Issue 139, Issue 118, Issue 140, Issue 115, Issue 121, Issue 129, Issue 136, Issue 143, Issue 154, Issue 137, Issue 148, Issue 159, Issue 145, Issue 147, Issue 158, Issue 165, Issue 156, Issue 157, Issue 163, Issue 164, Issue 169, Issue 160, Issue 161, Issue 168, Issue 173, Issue 185, Issue 178, Issue 189, Issue 179, Issue 190, Issue 188, Issue 199, Issue From three contributions corresponding to RKKY interaction, direct exchange and CAR exchange, only the first may bear a negative (ferromagnetic) sign. At least one of them is inevitable for the Kondo screening though, and two symmetrically coupled leads allow for measurement of the normal conductance. Inter-dot Coulomb interactions increase the energy of intermediate states contributing to direct exchange, while increasing theenergy of intermediatestates causing CAR exchange. This results in different dependence of corresponding terms in the Hamiltonian down-folding procedure. The main results are presented in Fig. 2, showing the dependence of $G$ on the temperature $G-T(a) and $U'T(b) for different circumstances. For different reference, see Sec. 1, Sec. 2 and Sec. 3 in the appendix for more details on how to calculate the coefficients of interest for $G and $T-T (G-G, T-T, U'T, G-U, U/5, G/5) and how to measure the residual conductance $G(T) for $t=0$ (denoted ""CAR"") and $t =0 (Denoted ""Both""). For different circumstances, see the Appendix for more information about how to estimate the coefficients for $T, $G, $T* and $G* for different situations. For $T'$ we calculate $G using an accurate full density matrix numerical renormalization (NRG) technique. The relevant terms differ  by factors important only for large $T/U$ and $J/U/T, but the relevant terms are the same for $U/10/10 and $1/5/10. We compare the experimentally relevant value of $T$ in the $G/10$ case with experimentallyrelevant value $U’s/10’ in the “Both” case. For example, we compare $T’=0’ with $J’#’U” in the case of “Car” and “RKKY’’ (“Kondo”). For $G’, we use $J#” as a proxy for “CAR’ and ‘Kondo,” which is the same as ‘Car’. For the ‘Both’ case, we can use ‘’$ as a surrogate for ‘CAR,’ ’K’ or ‘RKK’ to ‘car’ for the same reason.’ For $t’ we use “K” to “car”, and for $J$ we use the term ‘J” for � ‘C’ as a ‘kondo’ term, which is also the same, but with a different ‘c’ at the end of the sentence.” For $U$, we also use the terms ‘U‘ and ’J‘ to show that ‘ CAR’ is a non-trivial contribution to the non-conventional Kondo effect. For more details, see Appendix~\ref{sec:downfolding}[see \eq{A_J}] in the Appendix~’sec: downfolding’ [‘sec.1’: ‘Theory of Kondo and RKKy interaction’]. The Kondo screening induced by CAR exchange is a beautiful example of a superconductivity leading to magnetic order, namely the formation of the Kondo singlet. The enhancement of direct exchange is compensated by the decrease of the CAR one. Only for larger values of CAR is the exchange strong enough that the dependence of $G(T)$ on $U' is hardly different from the one for $t=0$ and $t>0$ for $T\geq T^*$ (results not shown) The Kondo effect is the hallmark of SC proximity and the corresponding CAR exchange processes. It is the result of the interaction between SC and the CAR exchange process, which is a result of particle-hole symmetry. It can be seen in the figure below, where the dotted line corresponds to $Gmin$ with $c=2.25$ and a solid line is plotted with solid line $T=10 -6 U$. The quantity $G(\GS{})$ obtained for $ t=0-$10-6$ is plotted in a different way, with $C\approx 2.25$, barely depending on $u'$ and getting smaller for $o'=0$. The results are shown in the table below, which shows how $G('T') saturates at residual value $G_{\mathrm{min')$ only for finite values of $t. The results show that for weak $t$ the system exhibits rather conventional (single-stage)Kondo effect with $G=G-G2/2/h, while QD2 is effectively decoupled from $T-QD2. The figure below also shows how the proximity of SC to the lead in the lead can be measured at cryogenic conditions, and how this can be shown in a variety of ways. The figures are presented in a range of sizes, from 0.37 to 10,000 to 20,000 µM. The data are presented by the author in the form of a table. The table is presented in two parts: the first part is the numerical results, and the second is the data from the second part of the study. The second part includes the results from the analysis of the data in the second section, which was conducted in the University of California, San Diego, using a computer model of the space-time domain. The study was published in the journal Nano Letters (http://www.nanolink.com/ NanoLights/ Nanolink/ Nanoscience/ Nanoscale-Kondo-Singlet.html#NanoLights- singlet-kondo-singlet-skew.html. The article has been updated to reflect the latest developments in the field of Kondo-Skew research. It has also been re-posted to include the latest version of this article, which can be found at: http://www.-nanoscience.com/. The article was originally published on November 14, 2013. The author would like to make clear that this article is not meant to be a substitute for the original version of the article, but to clarify that the author’s findings are consistent with the current understanding of how Kondo works. The original version also stated that $G-T$ is a measure of the strength of the magnetic field in a cryogenic environment. We are happy to point out that this is not the case, and that the figure is a more accurate estimate of the magnitude of the superconducting force. RKKY exchange and CAR exchange have opposite signs and compete with each other. Depending on their magnitudes and temperature, one of the following scenarios may happen. The conventional Kondo effect manifests itself with a characteristic peak in QD1. This is the case for sufficiently small $G$ for $J=0$ or $J=-0.01U$ and for finite $T$. For weak $T$, there is always a range of $|J$ where QD2 becomes effectivelydecoupled. This leads to an effective ferromagnetic coupling of DQDs into triplet state. The effect is a hallmark of SC-induced two-stage Kondo effects. However, it can also be caused by the absence of superconducting lead, such as in the case of a PHS-induced PHS effect outside the PHS point of PHS, or by the use of a ‘delta-delta’ effect, where the superconductivity of the lead is reduced by a factor of 2.5 or 3.5, depending on the type of lead. For example, the ‘Delta-Delta Effect’ can be caused if the superconductor is not present in the lead at the time of the event or if it is present later in the event. This can be seen in the figure below, which plots the conductance between the normal leads $G’ and $T’ as a function of temperature $T ($G(T\!=\!0)’, $T(T)$ and $G(G) ($G-T) (a) for parameters corresponding to the particle-hole symmetry point, $G/U/10, and additional curves for finite and infinite values of $G. The figure also plots the spectral density of QD 1 representative for this regime, which tends to increase for large negative $J$; see dashed lines in the inset to \fig{3}. The dip in $T^*$ has width of order of $T*$ and is indicated by triangle, also indicated by triangles. The peak in $A$ is significantly different from the curve denoted by square in theinset in \fig {3}. This is because $A’s width is much smaller than the dip in the main plot, which is width of $J*$ (‘’’) (“A”) is a “dip” is a point on the $J/U’ curve, which corresponds to a point in $J-1’ (’J’S’). The peak of $A#’$ is the “D’ in ’D” in ‘QD1,’ where ‘‘Q’ means ‘q’.’ ’QD2’ is a dip in “QD3’ that corresponds to the dip of $QD4’ ($’Q ’), which is wider than $T#‘(’T) and has a width of ‘T$(‘T”). The dip is also the same as the dip on $T_K’ which is the width of the ’T$ plot, but it has a different shape. The dip on QD4 is the same for $T/U$ ($T#) and $J#) as well as $T+1$ ($J+1U) In the regime $|\delta_i| < 0.01U$ the residual conductance caused by SC is orders of magnitude larger, leading to the plateau in $G_min2$ dependence. The ideal symmetry in the coupling between respective QDs and SC lead is hardly possible in experimental reality. As shown below, it does not introduce any qualitatively new features. On the other hand, it decreases the second stage Kondo temperature, which is already small, therefore,quantitative estimation of this decrease may be important for potential experimental approaches. To analyze the effects of $x$ we introduce the asymmetry parameter $x and extend the definition of $\GS{1}$, which is a main mechanism leading to a decrease of $T^*$ outside the $x=0$ point. To illustrate this, the curves corresponding to exchangechanisms were calculated using $x-dependent $t==1/2/3/4/5/6/7/8/9/10/12/13/14/15/16/17/18/19/20/21/22/23/24/25/26/27/28/28-28/29/30/31/32/32-33/34/33/36/37/38/36-37/37-38-38/38-39/39/40/41/42/41-42/43/44/45/46/47/48/49/50/52/53/54/55/56/57/58/59/60/61/62/63/64/65/66/67/68/69/70/72/73/74/75/76/77/78/79/80/82/83/84/85/86/87/88/89/90/91/92/93/94/96/97/98/98+98/99/100/102/103/104/100+104/105/106/107/108/111/112/113/114/1121/1122/1123/1124/1125/1126/1127/1128/1129/1111/1112/1113/1114/1115/1116/1117/1118/1119/11210/11110/11211/11112/11111/11212/11213/11113/11214/11115/11114/11215/11216/11116/11217/11119/11117/11219/11218/11120/11220/11121/11221/11118/11222/11122/11223/11224/11226/11126/11227/11127/11225/11228/11229/11230/11231/11201/11236/11232/11237/11234/112317/11238/11233/112318/112301/112308/112312/112313/112316/112303/112328/112322/112319/112338/112309/112315/112320/112321/112327/112300/112325/112326/112333/112311/112307/112299/112298/112306/112310/112302/112337/112329/112339/112314/112323/112324/112357/112304/112111/111317/1103/1107/110318/111307/111308/111318/110317/111302/111309/111319/111303/111299/111300/111313/111306/111312/111305/1123000/112305/111315/111304/111298/110300/110315/110312/110313/110309/110319/ The model cannot be extended to inter-dot distances much larger than the coherence length of the superconductor. The normal-lead coupling asymmetry is irrelevant for the results except for a constant factor. The efficiency of CAR processes reduced by factor (a) $\mathcal{C}=0.9$ and (b) ($C) =0.5$. The results are presented in the results section of the Kondo Kondo regime. For a comparison with the conventional regime, compare the results with those of the Strong Conventional regime, which is much more robust to changes in the coupling between the two superconductors. The model can be used to understand the consequences of less effective and/or less effective coupling in the superconducting regime, and to identify the causes of diminishing efficiency in the CAR regime. It can also be used as a basis for the design of new supercondensations. The results section is titled ‘The role of CAR efficiency’ and includes the following sections: ‘Car efficiency,’ ‘C’, ‘Efficiency of the Superconductor,‘ and ‘CAR Processes’. The full results are available at: http://www.kondo.org/kondo/car-processes/Car-Empowerment-Conventional-RKKY-Reinforced-Car-Processes-Kondo-Strong-Concepts-Ckondo-Cmondo.html, and can be downloaded as a free download from: http: // www.kondos.com/car/carprocesses-ckondo reinforced car-complexities-cmondo-cnn.html. The final version of this article includes the analysis of the results from the Strong-conventional Kondo-Rkondo regime, as well as the results of the CAR Processes-CnR analysis. It also includes the discussion of the effects on the efficiency of the RKKY process on the CAR process, which includes the effects of changing the coupling distance between the super-conductor and the super Conductor. It is also possible to use the QD1 local spectral density as a function of energy to examine the consequences for CAR processes in the real world. The analysis of $G(T)$ dependencies for different $x$ have been compiled in \fig{x}(c) The results show that the $G (T) dependence of the $T=0$ conductance $G_{\mathrm{min)$ monotonically increases with $x$, as it crosses $x =0$ point. The $T*(x) dependence can be generalized to $Gmin2$ instead of simply $GS{}$, cf. \eq{Gmin}. The results can be seen in the figure below, with $c\approx 2.25$ (indicated by a dotted line in   ‘x’) and $G’ being proportional to $\GS{1}^2=(x+1)^2 \GS{]^2$, instead of just ‘Gmin’ (‘‘X’’). The results also show that $G_’ is approximately fulfilled for all the considered cases, with the exception of the case where $x=0, where $G=0 and $C$ is not a factor of 2.5. The study concludes that the CAR processes are more efficient than the conventional CAR processes. erized by a residual                 low-temperature conductance at particle-hole symmetric case. The competition between CAR exchange and RKKYinteraction may result in completely different Kondo screening scenarios. Non-local pairing is present also in bulk systems such as non-$s$-wave superconductors. The question if an analogue of discussed CAR exchange may play a role there seems intriguing in the context of tendencies of many strongly correlated materials to possess superconducting and anti-ferromagnetic phases. This work was supported by the National Science Centre in Poland through project no.2015/19/N/ST3/01030. It was carried out by the University of Wrocław in Poland and the Polish Institute of Physics and Astronomy in Poland."
How is electricity used in everyday life?,"For other uses, see Electricity (disambiguation).
""Electric"" redirects here. For other uses, see Electric (disambiguation).
Lightning is one of the most dramatic effects of electricity.
Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. In early days, electricity was considered as being not related to magnetism. Later on, many experimental results and the development of Maxwell's equations indicated that both electricity and magnetism are from a single phenomenon: electromagnetism. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.
The presence of an electric charge, which can be either positive or negative, produces an electric field. The movement of electric charges is an electric current and produces a magnetic field.
When a charge is placed in a location with a non-zero electric field, a force will act on it. The magnitude of this force is given by Coulomb's law. Thus, if that charge were to move, the electric field would be doing work on the electric charge. Thus we can speak of electric potential at a certain point in space, which is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts.
electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.
Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then, practical applications for electricity were few, and it would not be until the late nineteenth century that electrical engineers were able to put it to industrial and residential use. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.
Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the ""Thunderer of the Nile"", and described them as the ""protectors"" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning ra‘ad (رعد) applied to the electric ray.
Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.
Benjamin Franklin conducted extensive research on electricity in the 18th century, as documented by Joseph Priestley (1767) History and Present Status of Electricity, with whom Franklin carried on extended correspondence.
Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word electricus (""of amber"" or ""like amber"", from ἤλεκτρον, elektron, the Greek word for ""amber"") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words ""electric"" and ""electricity"", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.
Further work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.
In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his ""On Physical Lines of Force"" in 1861 and 1862.
While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.
In 1887, Heinrich Hertz:843–44 discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for ""his discovery of the law of the photoelectric effect"". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially.
The first solid-state device was the ""cat's-whisker detector"" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.
The solid-state device came into its own with the invention of the transistor in 1947. Common solid-state devices include transistors, microprocessor chips, and RAM. A specialized type of RAM called flash RAM is used in USB flash drives and more recently, solid-state drives to replace mechanically rotating magnetic disc hard disk drives. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED).
The presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity.:457 A lightweight ball suspended from a string can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: like-charged objects repel and opposite-charged objects attract.
The force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them.:35 The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 1042 times that of the gravitational attraction pulling them together.
Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. The most familiar carriers of electrical charge are the electron and proton. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact, or by passing along a conducting material, such as a wire.:2–5 The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.
The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. By convention, the charge carried by electrons is deemed negative, and that by protons positive, a custom that originated with the work of Benjamin Franklin. The amount of charge is usually given the symbol Q and expressed in coulombs; each electron carries the same charge of approximately −1.6022×10−19 coulomb. The proton has a charge that is equal and opposite, and thus +1.6022×10−19 coulomb. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle.
The movement of electric charge is known as an electric current, the intensity of which is usually measured in amperes. Current can consist of any moving charged particles; most commonly these are electrons, but any charge in motion constitutes a current. Electric current can flow through some things, electrical conductors, but will not flow through an electrical insulator.
By historical convention, a positive current is defined as having the same direction of flow as any positive charge it contains, or to flow from the most positive part of a circuit to the most negative part. Current defined in this manner is called conventional current. The motion of negatively charged electrons around an electric circuit, one of the most familiar forms of current, is thus deemed positive in the opposite direction to that of the electrons. However, depending on the conditions, an electric current can consist of a flow of charged particles in either direction, or even in both directions at once. The positive-to-negative convention is widely used to simplify this situation.
The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. Examples of electric currents include metallic conduction, where electrons flow through a conductor such as metal, and electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. While the particles themselves can move quite slowly, sometimes with an average drift velocity only fractions of a millimetre per second,:17 the electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires.
Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. Their work was greatly expanded upon by Michael Faraday in 1833. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840.:23–24 One of the most important discoveries relating to current was made accidentally by Hans Christian Ørsted in 1820, when, while preparing a lecture, he witnessed the current in a wire disturbing the needle of a magnetic compass. He had discovered electromagnetism, a fundamental interaction between electricity and magnetics. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment.
In engineering or household applications, current is often described as being either direct current (DC) or alternating current (AC). These terms refer to how the current varies in time. Direct current, as produced by example from a battery and required by most electronic devices, is a unidirectional flow from the positive part of a circuit to the negative.:11 If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. Alternating current is any current that reverses direction repeatedly; almost always this takes the form of a sine wave.:206–07 Alternating current thus pulses back and forth within a conductor without the charge moving any net distance over time. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. Alternating current is affected by electrical properties that are not observed under steady state direct current, such as inductance and capacitance.:223–25 These properties however can become important when circuitry is subjected to transients, such as when first energised.
The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.
A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body.:88 This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects.
The principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh.
A pair of AA cells. The + sign indicates the polarity of the potential difference between the battery terminals.
The concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity.:494–98 This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points. An electric field has the special property that it is conservative, which means that the path taken by the test charge is irrelevant: all paths between two specified points expend the same energy, and thus a unique value for potential difference may be stated.:494–98 The volt is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage.
For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable.
Electric potential is a scalar quantity, that is, it has only magnitude and not direction. It may be viewed as analogous to height: just as a released object will fall through a difference in heights caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. As relief maps show contour lines marking points of equal height, a set of lines marking points of equal potential (known as equipotentials) may be drawn around an electrostatically charged object. The equipotentials cross all lines of force at right angles. They must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface.
Ørsted's discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. Moreover, the interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The force on the compass needle did not direct it to or away from the current-carrying wire, but acted at right angles to it. Ørsted's slightly obscure words were that ""the electric conflict acts in a revolving manner."" The force also depended on the direction of the current, for if the flow was reversed, then the force did too.
Ørsted did not fully understand his discovery, but he observed the effect was reciprocal: a current exerts a force on a magnet, and a magnetic field exerts a force on a current. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other: two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The interaction is mediated by the magnetic field each current produces and forms the basis for the international definition of the ampere.
This relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 1821. Faraday's homopolar motor consisted of a permanent magnet sitting in a pool of mercury. A current was allowed through a wire suspended from a pivot above the magnet and dipped into the mercury. The magnet exerted a tangential force on the wire, making it circle around the magnet for as long as the current was maintained.
Experimentation by Faraday in 1831 revealed that a wire moving perpendicular to a magnetic field developed a potential difference between its ends. Further analysis of this process, known as electromagnetic induction, enabled him to state the principle, now known as Faraday's law of induction, that the potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Faraday's disc was inefficient and of no use as a practical generator, but it showed the possibility of generating electric power using magnetism, a possibility that would be taken up by those that followed on from his work.
Italian physicist Alessandro Volta showing his ""battery"" to French emperor Napoleon Bonaparte in the early 19th century.
The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses.
Electrochemistry has always been an important part of electricity. From the initial invention of the Voltaic pile, electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells.
A basic electric circuit. The voltage source V on the left drives a current I around the circuit, delivering electrical energy into the resistor R. From the resistor, the current returns to the source, completing the circuit.
An electric circuit is an interconnection of electric components such that electric charge is made to flow along a closed path (a circuit), usually to perform some useful task.
Electric power is the rate at which electric energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.
Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries or by other means from a wide variety of sources of energy. Electric power is generally supplied to businesses and homes by the electric power industry. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using electricity meters, which keep a running total of the electric energy delivered to a customer. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency.
Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, optoelectronics, sensors and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing, telecommunications, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.
Thus, the work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances.
Early 20th-century alternator made in Budapest, Hungary, in the power generating hall of a hydroelectric station (photograph by Prokudin-Gorsky, 1905–1915).
In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. While this method, now known as the triboelectric effect, can lift light objects and generate sparks, it is extremely inefficient. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The voltaic pile, and its modern descendant, the electrical battery, store energy chemically and make it available on demand in the form of electrical energy. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines.
Electrical power is usually generated by electro-mechanical generators driven by steam produced from fossil fuel combustion, or the heat released from nuclear reactions; or from other sources such as kinetic energy extracted from wind or flowing water. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on his electromagnetic principle that a conductor linking a changing magnetic field induces a potential difference across its ends. The invention in the late nineteenth century of the transformer meant that electrical power could be transmitted more efficiently at a higher voltage but lower current. Efficient electrical transmission meant in turn that electricity could be generated at centralised power stations, where it benefited from economies of scale, and then be despatched relatively long distances to where it was needed.
Since electrical energy cannot easily be stored in quantities large enough to meet demands on a national scale, at all times exactly as much must be produced as is required. This requires electricity utilities to make careful predictions of their electrical loads, and maintain constant co-ordination with their power stations. A certain amount of generation must always be held in reserve to cushion an electrical grid against inevitable disturbances and losses.
Electricity is a very convenient way to transfer energy, and it has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Although electrification brought with it its own dangers, replacing the naked flames of gas lighting greatly reduced fire hazards within homes and factories. Public utilities were set up in many cities targeting the burgeoning market for electrical lighting. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector.
The resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of resistive electric heating in new buildings. Electricity is however still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand for heating and cooling, the effects of which electricity utilities are increasingly obliged to accommodate.
Electricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first intercontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process.
The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership.
Electronic devices make use of the transistor, perhaps one of the most important inventions of the twentieth century, and a fundamental building block of all modern circuitry. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square.
A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the greater the current. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. Death caused by an electric shock is referred to as electrocution. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times.
Electricity is not a human invention, and may be observed in several forms in nature, a prominent manifestation of which is lightning. Many interactions familiar at the macroscopic level, such as touch, friction or chemical bonding, are due to interactions between electric fields on the atomic scale. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, or even sugar, generate a potential difference across their faces when subjected to external pressure. This phenomenon is known as piezoelectricity, from the Greek piezein (πιέζειν), meaning to press, and was discovered in 1880 by Pierre and Jacques Curie. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place.
§Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon.
Some organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants.
In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. ""Revitalization"" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored Frankenstein (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.
As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who ""finger death at their gloves' end as they piece and repiece the living wires"" in Rudyard Kipling's 1907 poem Sons of Martha. Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the Tom Swift books. The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.
With electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it stops flowing, an event that usually signals disaster. The people who keep it flowing, such as the nameless hero of Jimmy Webb’s song ""Wichita Lineman"" (1968), are still often cast as heroic, wizard-like figures.
Ampère's circuital law, connects the direction of an electric current and its associated magnetic currents.
^ Diogenes Laertius. R.D. Hicks (ed.). ""Lives of Eminent Philosophers, Book 1 Chapter 1 "". Perseus Digital Library. Tufts University. Retrieved 5 February 2017. Aristotle and Hippias affirm that, arguing from the magnet and from amber, he attributed a soul or life even to inanimate objects.
^ Aristotle. Daniel C. Stevenson (ed.). ""De Animus (On the Soul) Book 1 Part 2 (B4 verso)"". The Internet Classics Archive. Translated by J.A. Smith. Retrieved 5 February 2017. Thales, too, to judge from what is recorded about him, seems to have held soul to be a motive force, since he said that the magnet has a soul in it because it moves the iron.
^ a b c Guarnieri, M. (2014). ""Electricity in the age of Enlightenment"". IEEE Industrial Electronics Magazine. 8 (3): 60–63. doi:10.1109/MIE.2014.2335431.
^ Srodes, James (2002), Franklin: The Essential Founding Father, Regnery Publishing, pp. 92–94, ISBN 0-89526-163-4 It is uncertain if Franklin personally carried out this experiment, but it is popularly attributed to him.
^ a b Guarnieri, M. (2014). ""The Big Jump from the Legs of a Frog"". IEEE Industrial Electronics Magazine. 8 (4): 59–61, 69. doi:10.1109/MIE.2014.2361237.
^ Hertz, Heinrich (1887). ""Ueber den Einfluss des ultravioletten Lichtes auf die electrische Entladung"". Annalen der Physik. 267 (8): S. 983–1000. Bibcode:1887AnP...267..983H. doi:10.1002/andp.18872670827.
^ ""The Nobel Prize in Physics 1921"". Nobel Foundation. Retrieved 2013-03-16.
^ John Sydney Blakemore, Solid state physics, pp. 1–3, Cambridge University Press, 1985 ISBN 0-521-31391-0.
^ Richard C. Jaeger, Travis N. Blalock, Microelectronic circuit design, pp. 46–47, McGraw-Hill Professional, 2003 ISBN 0-07-250503-6.
^ ""The repulsive force between two small spheres charged with the same type of electricity is inversely proportional to the square of the distance between the centres of the two spheres."" Charles-Augustin de Coulomb, Histoire de l'Academie Royal des Sciences, Paris 1785.
^ Sewell, Tyson (1902), The Elements of Electrical Engineering, Lockwood, p. 18 . The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge'.
^ a b Berkson, William (1974), Fields of Force: The Development of a World View from Faraday to Einstein, Routledge, p. 370, ISBN 0-7100-7626-6 Accounts differ as to whether this was before, during, or after a lecture.
^ ""Lab Note #105 EMI Reduction – Unsuppressed vs. Suppressed"". Arc Suppression Technologies. April 2011. Retrieved March 7, 2012.
^ Almost all electric fields vary in space. An exception is the electric field surrounding a planar conductor of infinite extent, the field of which is uniform.
^ Paul J. Nahin (9 October 2002). Oliver Heaviside: The Life, Work, and Times of an Electrical Genius of the Victorian Age. JHU Press. ISBN 978-0-8018-6909-9.
^ ""The Bumpy Road to Energy Deregulation"". EnPowered. 2016-03-28.
^ a b c d e f g h Van Riper, op.cit., p. 71.
Look up electricity in Wiktionary, the free dictionary.
Basic Concepts of Electricity chapter from Lessons In Electric Circuits Vol 1 DC book and series.","['Electricity is used for transport, heating, lighting, communications, and computation.']",6202,multifieldqa_en,en,,b5b0eb150f44a4d7641b9adf9267ca0c2492ea46626449b3," Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others. In early days, electricity was considered as being not related to magnetism. Later on, many experimental results and the development of Maxwell's equations indicated that both electricity and magnetism are from a single phenomenon: electromagnetism. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society. People were aware of shocks from electric fish before any knowledge of electricity existed. Ancient Egyptian texts referred to these fish as the ""Thunderer of the Nile"", and described them as ""protectors"" of all other fish. Several ancient writers attested to the numbing effect of electric shocks delivered by catfish and electric rays. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. According to a controversial theory, the Parthians knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. The movement of electric charges is an electric current and produces a magnetic field. The presence of an electric charge, which can be either positive or negative, produces an electric field. Electric potential is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts. Electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies, are referred to as ""electronics"" The term ""electricity"" is used to refer to the process of creating electricity from a non-electric source such as water, gas, electricity, or radioactivity. The word ""electric"" is also used for electrical devices such as light bulbs, transformers, and other devices that use electricity to power their functions. The use of the word ""electronic"" is a contraction of the words ""electrical"" and ""electron"" which means ""electric power"" or ""electro-mechanical"" in the UK and the U.S. It is also the name given to a class of technology known as electrical devices known as electronic technologies, which include vacuum tubes, transistors, diodes, integrated circuits and other passive interconnection technologies. Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete. Benjamin Franklin conducted extensive research on electricity in the 18th century, as documented by Joseph Priestley. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. In 1887, Heinrich Hertz:843–44 discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets. This discovery led to the quantum revolution. The first solid-state device was the ""cat's-whisker detector"" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a ge ge) and this is frequently used to make electricity commercially. It is unclear whether the Baghdad Battery, which resembles a galvanic cell, was electrical in nature, though it is uncertain whether the artifact was electric in nature. In June 1752, Benjamin Franklin is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from theKey to the back of his hand showed that lightning was indeed electrical in Nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his ""On Physical Lines of Force"" in 1861 and 1862. The word ""electric"" and ""electricity"" made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646. In the 17th and early 18th centuries, Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay conducted extensive work in electricity, selling his possessions to fund his work. The presence of charge gives rise to an electrostatic force: charges exert a force on each other. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges. The most familiar carriers of electrical charge are the electron and proton. Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.manium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. The building material is most often a crystalline semiconductor. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED)solid state devices came into its own with the invention of the transistor in 1947. A specialized type of RAM called flash RAM is used in USB flash drives and more recently, solid- state drives to replace mechanically rotating magnetic disc hard disk drives. It is used to store data in the form of a microchip or other type of memory chip, such as in a PC or a mobile phone. It can also be used to transfer data from one device to another, for example, to transfer information from a PC to a Mac or a Macbook, or even a smartphone to a PC. It has been used in the U.S. to detect radio signals since the 1980s. It was also used to detect signals in the 1990s and 2000s by the University of California, Los Angeles, and the California Institute of Technology, among other places. The use of solid state devices has led to the creation of the Department of Energy’s ‘Solid State Technology’ program, which aims to reduce the amount of energy needed to transmit data by up to 50 per cent. It also allows scientists to study the effects of quantum mechanics on the environment. It allows them to study how atoms and molecules interact with each other in a more realistic way. Electric current can consist of any moving charged particles. Most commonly these are electrons, but any charge in motion constitutes a current. Electric current can flow through some things, electrical conductors, but will not flow through an electrical insulator. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle. Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840. Current is often described as being either direct current (DC) or alternating current (AC) These terms refer to how the current varies in time, and are used in engineering or household applications. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. These properties however are not observed under steady state direct current, such as inductance and capacitance, which is why they are called alternating current. Current can be measured in amperes, which are usually measured in seconds. The intensity of an electric current is usually measured as a ratio of ampere to second. The current can also be expressed as a square root of its magnitude, or as a Hertz constant, or a factor of 1.6022×10−19. The electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires. The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. This is known as metallic conduction or electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. It is also known as alternating current, which takes the form of a sine wave, and is used in electrical engineering and other fields. It can be used to refer to any current that reverses direction repeatedly; almost always this takes the shape of a straight line or a line, or even a line or an arc. The term current is widely used to simplify this situation and is often used to describe current in general. The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the gravitational field, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electricField can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric Field at a distance is usually zero. A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body.:88 This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects. The + sign indicates the polarity of the potential difference between the battery terminals. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises electric field in the air to greater than it can withstand. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker. There is a finite limit to theelectric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. The principles of electrostatics are important when designing items of high-voltage equipment. become important when circuitry is subjected to transients, such as when first energised. For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed. to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable. It may be viewed as analogous to height: just as a released object will fall through a difference in heigh. It is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage. The volt is the term used to describe the energy required to move a unit charge between two specified points. This definition of potential, while formal, has little practical application. Discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. The interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 18 21. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells. The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses. It is the basis for the international definition of the ampere.s caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. A basic electric circuit. drives a current I around the circuit, delivering electrical energy into the resistor R. From the resistor, the current returns to the source, completing the circuit. The effect is mediated by the magnetic field each current produces and forms the basis. for the International definition. of the Ampere, which is the ratio of the current to thevoltage in a given circuit. An electric circuit is an intercon, with the voltage source V on the left driving the current into the circuit and the resistor on the right delivering the energy back into the source. An electrostatically charged object may be drawn around a conductor at right angles. The equipotentials must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other. Two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Further analysis of this process, known as electromagnetic induction, enabled. him to state the principle, now known as Faraday’s law of induction, that the potential difference in a close circuit is inversely proportional to magnetic flux. The. potential difference between a wire moving perpendicular to a. magnetic field developed a possible difference between its ends. in the early 19th century. Italian physicist Alessandro Volta showing his ""battery"" to French emperor Napoleon Bonaparte in the late 1800s. Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on h such generators. The ability of electronic devices to act as switches makes digital information processing possible. Today, most electronic devices use semiconductor components to perform electron control. The work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances. The SI unit of power is the watt, one joule per second. Electric utilities measure power using electricity meters, which keep a running total of electric energy delivered to a customer. The current cost of electricity in the UK is around £1,000 per hour. It is estimated that in the U.S. it is about £2,500 per hour to run a typical home or office for a short period of time. The cost of running a business in the United States is about $3,000 to $4,000 a year for a large number of homes and businesses. The average cost of a home in the USA is around $1,800 per year. It can be as little as £200 per hour for small businesses. Electricity is a very convenient way to transfer energy, and has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership. Electricity is still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand. The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the great the currere. The resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries have issued legislation restricting or banning the use of resistive electric heating in new buildings. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square. The effect of the transistor, perhaps one of most important inventions of the twentieth century, and a fundamental building block of all modern circuitry, is to make electronic devices more and more efficient. The power of the transistors is to be found in the design of modern computers and other electronic devices such as the iPhone, iPad, and the iPad mini, which are powered by a combination of alternating current and transistors. The transistor is the most powerful device in the world, with a current of more than 100,000 volts per square centimetre. It is also the most efficient way to transmit electrical power, as it is able to do so at a much lower voltage than other methods such as radio or television. It can also be used to power a large number of electronic devices, including computers, phones, computers, printers and the internet, and even the internet itself. Electricity is not a human invention, and may be observed in several forms in nature. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, generate a potential difference across their faces when subjected to external pressure. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the Tom Swift books. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times. The revitalization of monsters with electricity later became a stock theme in horror films. Some organisms, like sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception. Others, termed electrogenic, can generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes.Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon. The ability to detect electric fields is known as Electroreception, which is used by some species of fish to detect their prey and to stun them. It is also used by certain plants to communicate with each other and to coordinate activities in certain plants. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. It can be used to torture people, and is still used as a means of execution in many jurisdictions. It has also been used to resuscitate dead or drowned persons, as in the case of Frankenstein. Ampère's circuital law, connects the direction of an electric current and its associated magnetic currents. The people who keep it flowing, such as the nameless hero of Jimmy Webb’s song ""Wichita Lineman"" (1968), are still often cast as heroic, wizard-like figures. It is uncertain if Franklin personally carried out this experiment, but it is popularly attributed to him. Aristotle and Hippias affirm that, arguing from the magnet and from amber, he attributed a soul or life even to inanimate objects. The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge' and is used to refer to the state of electricity in the United States. The term 'Electricity in the age of Enlightenment' was coined by Thomas Edison in his book ""The Electric Power of the World"" in 1776. The word electricity is still used to describe electricity in many parts of the world, including the U.S., Australia, Canada, New Zealand, and the United Kingdom. It can also be used as a name for the electric field surrounding a planar conductor of infinite space, in which the field is uniform. It has also been used as the name of a type of conductor, the Planar Conductor, or a conductor of infinitely infinite space. The name of the conductor is also used for the Planarsar Conductor, which has a field surrounding it that is uniform in all directions. The field surrounding the planar conductor is also uniform in space, but the extent of the field around it varies from person to person. The most common electric field is the one surrounding the conductor of unlimited space, which is inversely proportional to the square of the distance between the centres of the two spheres. This is the so-called 'electric field' or 'field of force' (or 'electric charge' in modern usage). It is also known as the field of force (or electric charge) or the 'electric current' ( or electric current) or 'electric voltage' (electricity) It is the force that drives an electrical current or current. The power of the electric current can be measured in volts, amperes, kph, volts, ohms, femtodes, tensiles, tensors, tensile strength, and so on. The electric current is also called the ""electric current"" or ""electricity"" or the ""electrical current"" by some scientists. The electricity that powers a device is known as 'the electricity of the moment' or ""the electric current"". It is used in a variety of scientific fields, including those such as electricity, magnetism, radioactivity, and magnetism. It was also used to explain the origin of the word electricity, and to explain how electricity is created. It also has been used in the popular culture to describe the nature of electricity and the way it is produced. It's been used by scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla to explain some of the principles of modern electricity. In the past, electricity has been seen as a symbol of power, but now it is more commonly referred to as a ""force"" or a ""power source"" or an ""energy source"" by many people. This can be seen in the Wikipedia definition of electricity. The Wikipedia definition also includes the term ""electric power"" as well as other terms such as ""electronic current"" and ""electro-magnetism"""
What was Hugh H. Goodwin's rank in the United States Navy?,"Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War.

Following the War, he remained in the Navy and rose to the flag rank and held several important commands including Vice Commander, Military Air Transport Service, Commander, Carrier Division Two and Commander, Naval Air Forces, Continental Air Defense Command.

Early life and career

Hugh H. Goodwin was born on December 21, 1900, in Monroe, Louisiana and attended Monroe High School there (now Neville High School). Following the United States' entry into World War I in April 1917, Goodwin left the school without receiving the diploma in order to see some combat and enlisted the United States Navy on May 7, 1917. He completed basic training and was assigned to the battleship . Goodwin participated in the training of armed guard crews and engine room personnel as the Atlantic Fleet prepared to go to war and in November 1917, he sailed with the rest of Battleship Division 9, bound for Britain to reinforce the Grand Fleet in the North Sea.

Although he did not complete the last year of high school, Goodwin was able to earn an appointment to the United States Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname ""Huge"" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Robert E. Blick Jr., Herbert S. Duckworth, Clayton C. Jerome, James P. Riseley, James A. Stuart, Frank Peak Akers, Sherman Clark, Raymond P. Coffman, Delbert S. Cornwell, Frederick J. Eckhoff, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy, William S. Parsons, Harold R. Stevens, John P. Whitney, Lyman G. Miller and George J. O'Shea.

Goodwin graduated with Bachelor of Science degree on June 3, 1922, and was commissioned Ensign in the United States Navy. He was subsequently assigned to the battleship  and took part in the voyage to Rio de Janeiro, Brazil, before he was ordered to the Naval Torpedo Station at Newport, Rhode Island for submarine instruction in June 1923. Goodwin completed the training several weeks later and was attached to the submarine . He then continued his further training aboard submarine  and following his promotion to Lieutenant (junior grade) on June 3, 1925, he qualified as submariner.

He then served aboard submarine  off the coast of California, before he was ordered for the recruiting duty to San Francisco in September 1927. While in this capacity, Goodwin applied for naval aviation training which was ultimately approved and he was ordered to the Naval Air Station Pensacola, Florida in August 1928. Toward the end of the training, he was promoted to lieutenant on December 11, 1928, and upon the completion of the training in January 1929, he was designated Naval aviator.

Goodwin was subsequently attached to the Observation Squadron aboard the aircraft carrier  and participated in the Fleet exercises in the Caribbean. He was transferred to the Bureau of Aeronautics in Washington, D.C. in August 1931 and served consecutively under the architect of naval aviation William A. Moffett and future Chief of Naval Operations Ernest J. King.

In June 1933, Goodwin was ordered to the Naval War College at Newport, Rhode Island, where he completed junior course in May of the following year. He subsequently joined the crew of aircraft carrier  and served under Captain Arthur B. Cook and took part in the Fleet exercises in the Caribbean and off the East Coast of the United States.

He was ordered back to the Naval Air Station Pensacola, Florida in June 1936 and was attached to the staff of the Base Commandant, then-Captain Charles A. Blakely. When Blakely was succeeded by William F. Halsey in June 1937, Goodwin remained in Halsey's staff and was promoted to Lieutenant Commander on December 1, 1937. He also completed correspondence course in International law at the Naval War College.

Goodwin was appointed Commanding officer of the Observation Squadron 1 in June 1938 and attached to the battleship  he took part in the patrolling of the Pacific and 
West Coast of the United States until September 1938, when he assumed command of the Observation Squadron 2 attached to the battleship .

When his old superior from Lexington, now Rear Admiral Arthur B. Cook, was appointed Commander Aircraft, Scouting Force in June 1939, he requested Goodwin as his Aide and Flag Secretary. He became Admiral Cook's protégé and after year and half of service in the Pacific, he continued as his Aide and Flag Secretary, when Cook was appointed Commander Aircraft, Atlantic Fleet in November 1940.

World War II

Following the United States' entry into World War II, Goodwin was promoted to the temporary rank of Commander on January 1, 1942, and assumed duty as advisor to the Argentine Navy. His promotion was made permanent two months later and he returned to the United States in early 1943 for duty as assistant director of Planning in the Bureau of Aeronautics under Rear admiral John S. McCain. While still in Argentina, Goodwin was promoted to the temporary rank of Captain on June 21, 1942.

By the end of December 1943, Goodwin was ordered to Astoria, Oregon, where he assumed command of newly commissioned escort carrier USS Gambier Bay. He was responsible for the initial training of the crew and was known as a strict disciplinarian, but the crew appreciated the skills he taught them that prepared them for combat. Goodwin insisted that everyone aboard has to do every job right every time and made us fight our ship at her best.

During the first half of 1944, Gambier Bay was tasked with ferrying aircraft for repairs and qualified carrier pilots from San Diego to Pearl Harbor, Hawaii, before departed on May 1, 1944, to join Rear admiral Harold B. Sallada's Carrier Support Group 2, staging in the Marshalls for the invasion of the Marianas.

The air unit, VC-10 Squadron, under Goodwin's command gave close air support to the initial landings of Marines on Saipan on June 15, 1944, destroying enemy gun emplacements, troops, tanks, and trucks. On the 17th, her combat air patrol (CAP) shot down or turned back all but a handful of 47 enemy planes headed for her task group and her gunners shot down two of the three planes that did break through to attack her.

Goodwin's carrier continued in providing of close ground support operations at Tinian during the end of July 1944, then turned her attention to Guam, where she gave identical aid to invading troops until mid-August that year. For his service during the Mariana Islands campaign, Goodwin was decorated with Bronze Star Medal with Combat ""V"".

He was succeeded by Captain Walter V. R. Vieweg on August 18, 1944, and appointed Chief of Staff, Carrier Division Six under Rear admiral Arthur W. Radford. The Gambier Bay was sunk in the Battle off Samar on October 25, 1944, during the Battle of Leyte Gulf after helping turn back a much larger attacking Japanese surface force.

Goodwin served with Carrier Division Six during the Bonin Islands raids, the naval operations at Palau and took part in the Battle of Leyte Gulf and operations supporting Leyte landings in late 1944. He was later appointed Air Officer of the Philippine Sea Frontier under Rear admiral James L. Kauffman and remained with that command until the end of hostilities. For his service in the later part of World War II, Goodwin was decorated with Legion of Merit with Combat ""V"". He was also entitled to wear two Navy Presidential Unit Citations and Navy Unit Commendation.

Postwar service

Following the surrender of Japan, Goodwin assumed command of Light aircraft carrier  on August 24, 1945. The ship was tasked with air missions over Japan became mercy flights over Allied prisoner-of-war camps, dropping food and medicine until the men could be rescued. She was also present at Tokyo Bay for the Japanese surrender on September 2, 1945.

Goodwin returned with San Jacinto to the United States in mid-September 1945 and he was detached in January 1946. He subsequently served in the office of the Chief of Naval Operations until May that year, when he entered the instruction at National War College. Goodwin graduated in June 1947 and served on Secretary's committee for Research on Reorganization. Upon promotion to Rear admiral on April 1, 1949, Goodwin was appointed Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet under Admiral William H. P. Blandy.

Revolt of the Admirals

In April 1949, the budget's cuts and proposed reorganization of the United States Armed Forces by the Secretary of Defense Louis A. Johnson launched the wave of discontent between senior commanders in the United States Navy. Johnson proposed the merging of the Marine Corps into the Army, and reduce the Navy to a convoy-escort force.

Goodwin's superior officer, Admiral Blandy was call to testify before the House Committee on Armed Services and his harsh statements for the defense of the Navy, costed him his career. Goodwin shared his views and openly criticized Secretary Johnson for having power concentrated in a single civilian executive, who is an appointee of the Government and not an elected representative of the people. He also criticized aspects of defense unification which permitted the Joint Chiefs of Staff to vote on arms policies of individual services, and thus ""rob"" the branches of autonomy.

The outbreak of the Korean War in summer 1950 proved the proposal of Secretary Johnson as incorrect and he resigned in September that year. Also Secretary of the Navy, Francis P. Matthews resigned one month earlier.

Later service

Due to the Revolts of the admirals, Blandy was forced to retire in February 1950 and Goodwin was ordered to Newport, Rhode Island for temporary duty as Chief of Staff and Aide to the President of the Naval War College under Vice admiral Donald B. Beary in April 1950. Goodwin was detached from that assignment two months and appointed member of the General Board of the Navy. He was shortly thereafter appointed acting Navy Chief of Public Information, as the substitute for Rear Admiral Russell S. Berkey, who was relieved of illness, but returned to the General Board of the Navy in July that year. Goodwin served in that capacity until February 1951, when he relieved his Academy class, Rear admiral John P. Whitney as Vice Commander, Military Air Transport Service (MATS).

While in this capacity, Goodwin served under Lieutenant general Laurence S. Kuter and was co-responsible for the logistical support of United Nations troops fighting in Korea. The MATS operated from the United States to Japan and Goodwin served in this capacity until August 1953, when he was appointed Commander Carrier Division Two. While in this assignment, he took part in the Operation Mariner, Joint Anglo-American exercise which encountered very heavy seas over a two-week period in fall 1953.

Goodwin was ordered to the Philippines in May 1954 and assumed duty as Commander, U.S. Naval Forces in the Philippines with headquarters at Naval Station Sangley Point near Cavite. He held that command in the period of tensions between Taiwan and China and publicly declared shortly after his arrival, that any attack on Taiwan by the Chinese Communists on the mainland would result in US participation in the conflict. The naval fighter planes under his command also provided escort for passing commercial planes. Goodwin worked together with retired Admiral Raymond A. Spruance, then-Ambassador to the Philippines, and accompanied him during the visits to Singapore, Bangkok and Saigon in January 1955.

On December 18, 1955, Goodwin's classmate Rear admiral Albert K. Morehouse, then serving as Commander, Naval Air Forces, Continental Air Defense Command (CONAD), died of heart attack and Goodwin was ordered to CONAD headquarters in Colorado Springs, Colorado to assume Morehouse's position. While in this capacity, he was subordinated to Army General Earle E. Partridge and was responsible for the Naval and Marine Forces allocated to the command designated for the defense of the Continental United States.

Retirement

Goodwin retired on June 1, 1957, after 40 years of active service and was advanced to the rank of Vice admiral on the retired list for having been specially commended in combat. A week later, he was invited back to his Monroe High School (now Neville High School) and handed a diploma showing that he had been graduated with the class of 1918. He then settled in Monterey, California where he taught American history at Stevenson school and was a member of the Naval Order of the United States.

Vice admiral Hugh H. Goodwin died at his home on February 25, 1980, aged 79. He was survived by his wife, Eleanor with whom he had two children, a daughter Sidney and a son Hugh Jr., who graduated from the Naval Academy in June 1948, but died one year later, when the Hellcat fighter he was piloting collided with another over the Gulf of Mexico during training.

Decorations

Here is the ribbon bar of Vice admiral Hugh H. Goodwin:

References

1900 births
1980 deaths
People from Monroe, Louisiana
Military personnel from Louisiana
United States Naval Academy alumni
Naval War College alumni
United States Naval Aviators
United States Navy personnel of World War I
United States Navy World War II admirals
United States Navy vice admirals
United States submarine commanders
Recipients of the Legion of Merit",['Vice Admiral.'],2292,multifieldqa_en,en,,e02f6a69d7b2a96a3aa6cd84a9189c2d552f6fb089f216e1," Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War. Although he did not complete the last year of high school, Goodwin was able to earn an appointment to the U.S. Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname ""Huge"" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Herbert S. Duckworth, James A. Riseley, Frank Peak Akers, Raymond P. Coffman, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy. Goodwin was ordered to the Naval Air Station at Newport, Rhode Island, where he completed junior course in May of the following year. He then continued his further training aboard submarine  and following his promotion to Lieutenant (junior grade) on June 3, 1925, he qualified as submariner. He was subsequently assigned to the battleship  and took part in the voyage to Rio de Janeiro, Brazil, before he was ordered for the recruiting duty to San Francisco in September 1927. Goodwin applied for naval aviation training which was ultimately approved and he was order to the naval air station at Newport in August 1928. After the end of the training, he was promoted to lieutenant on December 11, 1928, and upon the completion of theTraining in January 1929, he became designated Naval aviator. Goodwin completed the training several weeks later and was attached to the. Naval Torpedo Station at. Newport in June 1923. He subsequently joined the crew of aircraft carrier  and served under Captain Arthur B. Cook and took parts in the Fleet exercises in the Caribbean and off the East Coast of the United United States. Goodwin went on to become a director of a number of other companies including a bank, an insurance company, a hedge fund, a bank and a real estate company. He also served as the president of a company that went bankrupt in 2008. He is now the chairman and CEO of a non-profit organization that helps young people through their college and university degrees. He has been awarded the title of “Distinguished Alumnus” for his contributions to society and has received numerous awards for his service to the community. Goodwin has also been awarded a “C” grade for his work in the community, including the “A’s”, “B” and “D” grades for his “F” “G” awards. Goodwin served with Carrier Division Six during the Bonin Islands raids, the naval operations at Palau and operations supporting Leyte landings in late 1944. For his service in the later part of World War II, Goodwin was decorated with Legion of Merit with Combat ""V"". He was later appointed Air Officer of the Philippine Sea Frontier under Rear admiral James L. Kauffman and remained with that command until the end of hostilities. He was responsible for the initial training of the crew and was known as a strict disciplinarian, but the crew appreciated the skills he taught them that prepared them for combat. Goodwin was awarded the Bronze Star Medal for his service during the Mariana Islands campaign, and the Navy Commendation Medal for bravery in the Battle of Leyte Gulf in October 1944. He died of a heart attack on December 31, 1945, in San Diego, California. He is survived by his wife, two children and a step-grandchild. He has a son and a daughter, both of whom were born in the U.S. during the Second World War. He also has a grandson, a son-in-law, a grandson and a great-grandson. He served in the Navy as a Lieutenant Commander and was promoted to Lieutenant Commander on December 1, 1937. Goodwin also served as an Aide and Flag Secretary to Rear Admiral Arthur B. Cook in the Atlantic Fleet during World War Two. He later served as the Chief of Staff of the Bureau of Aeronautics under Rear Admiral John S. McCain in the early 1940s. Goodwin served as a Commander of the USS Gambier Bay in the Pacific Ocean during the war. He helped turn back a much larger attacking Japanese surface force at the Battle off Samar in October, 1944. His ship was sunk by the Japanese during the Battle for Leyte in November, 1944, after helping turn back an attacking Japanese force. Goodwin died in a car crash on December 30, 1945 in San Jose, California, while on his way back to the United States from the Philippines. He will be buried in the San Diego National Cemetery in San Francisco, California on January 25, 1967. He had served in World War I and the First World War as a member of the staff of the Base Commandant, then-Captain Charles A. Blakely. He worked for the US Navy until he retired in 1946. Goodwin is the author of the book, ""The American Navy: The First World Wars and the Second Half of the War"" published by Simon & Schuster, Ltd. (1998). He is also the co-founder of the Naval War College, which is based at the University of California, San Diego. He currently lives in California with his wife and their three children. His great-great-granddaughter, Emily, was born on December 26, 1936, and is married to Captain Walter V. R. Vieweg, a former Navy officer. Goodwin worked as a captain in the battleship USS Gambiers Bay, which was sunk off Samar on October 25, 1944 by a Japanese surface-attack force. Goodwin served in the U.S. Navy during the Second World War. He was a member of the General Board of the Navy from 1950 to 1954. He served as Commander, U.s. Naval Forces in the Philippines from 1954 to 1955. Goodwin served as the Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet from 1949 to 1950. He also served as Vice Commander, Military Air Transport Service (MATS) from 1951 to 1953. Goodwin retired from the Navy in 1954 after the outbreak of the Korean War. Goodwin was the first member of his class to graduate from the Naval War College. He died on December 31, 1958. He is buried in the National Cemetery of the Pacific in San Diego, California. He has a son, Rear Admiral William P. Blandy, and a daughter, Rear admiral Mary P. Goodwin. The Goodwin family has lived in Newport, Rhode Island, since the 1950s. Goodwin is survived by his wife, Mary, and his son, Admiral William H. B. Goodwin, who served as a Rear Admiral in the United States Air Force from 1958 to 1961. Goodwin also has a grandson, Vice Admiral John P. Whitney, who is a retired Rear Admiral and served in World War II as a Commander in the Air Force. Goodwin died on January 25, 2011. He had a son and daughter, Admiral Russell S. Berkey, who was a Rear Admiral in the Navy until he retired in 1961. He later served as Chief of the Naval Staff at the United Nations in New York City. Goodwin has a daughter and a son- in-law, Admiral Michael B. Bower, who also served in U.N. forces in the Pacific. Goodwin's son was killed in action during the First World War in the Battle of the Sino-Japanese War in December 1945. He went on to serve in the Army Air Force and the Navy as a Lieutenant Commander. He retired in 1966. He wrote a book about his time in the US Navy, ""The Great War: A Memoirs of a Navy Captain"" (1980). He has also written several other books, including ""The Navy Remembered"" (1989) and ""The Naval Remembrance Project: A History of the American Navy"" (1990). He is also the author of a book, ""Navy Remembered: A Naval Remembered Story"" (1991). He also wrote a play about his experiences in the war, ""A Man Walks Among the Stars"" (1992). He was also the co-founder of the National War College and served as its president from 1993 to 1994. He currently serves as the President of Naval War University. He lives in Newport with his wife in Rhode Island and has two children, Michael and Mary Bower. Hugh H. Goodwin was a vice admiral in the United States Navy. He retired in 1957 after 40 years of active service. He died on February 25, 1980, aged 79. He had two children, a daughter Sidney and a son Hugh Jr., who died in a plane crash one year after he graduated from the Naval Academy in June 1948. He was a member of the Naval Order of the U.S. and the American Academy of Naval Sciences. He also served in the Army Air Force and the Air Force Reserve. He served in World War II and the Korean War. He is survived by his wife, Eleanor, and a daughter, Sidney, who he had with his wife when she died in 1980. He leaves behind a son, Hugh Jr, who was also a naval officer and a grandson, Hugh III, who served with him in the Navy and Air Force. He has also a son who was killed in a helicopter crash in 1967. He worked as a teacher in California and in Louisiana. He lived in Monroe, Louisiana, and died in Monterey, California, in 1986. He left behind a wife and two children. He never remarried and died at the age of 79. His son Hugh III was a marine officer. He went on to serve in the Air Corps and the Army National Guard. His daughter Sidney was a teacher at the University of Louisiana at Lafayette."
What was the Buckeyes' record in their first game of the season?,"The 1951 Ohio State Buckeyes baseball team represented the Ohio State University in the 1951 NCAA baseball season. The head coach was Marty Karow, serving his 1st year.

The Buckeyes lost in the College World Series, defeated by the Texas A&M Aggies.

Roster

Schedule 

! style="""" | Regular Season
|- valign=""top"" 

|- align=""center"" bgcolor=""#ccffcc""
| 1 || March 16 || at  || Unknown • San Antonio, Texas || 15–3 || 1–0 || 0–0
|- align=""center"" bgcolor=""#ffcccc""
| 2 || March 17 || at B. A. M. C. || Unknown • San Antonio, Texas || 7–8 || 1–1 || 0–0
|- align=""center"" bgcolor=""#ffcccc""
| 3 || March 19 || at  || Clark Field • Austin, Texas || 0–8 || 1–2 || 0–0
|- align=""center"" bgcolor=""#ffcccc""
| 4 || March 20 || at Texas || Clark Field • Austin, Texas || 3–4 || 1–3 || 0–0
|- align=""center"" bgcolor=""#ccffcc""
| 5 || March 21 || at  || Unknown • Houston, Texas || 14–6 || 2–3 || 0–0
|- align=""center"" bgcolor=""#ffcccc""
| 6 || March 22 || at Rice || Unknown • Houston, Texas || 2–3 || 2–4 || 0–0
|- align=""center"" bgcolor=""#ccffcc""
| 7 || March 23 || at  || Unknown • Fort Worth, Texas || 4–2 || 3–4 || 0–0
|- align=""center"" bgcolor=""#ccffcc""
| 8 || March 24 || at TCU || Unknown • Fort Worth, Texas || 7–3 || 4–4 || 0–0
|- align=""center"" bgcolor=""#ccffcc""
| 9 || March 24 || at  || Unknown • St. Louis, Missouri || 10–4 || 5–4 || 0–0
|-

|- align=""center"" bgcolor=""#ccffcc""
| 10 || April 6 ||  || Varsity Diamond • Columbus, Ohio || 2–0 || 6–4 || 0–0
|- align=""center"" bgcolor=""#ccffcc""
| 11 || April 7 ||  || Varsity Diamond • Columbus, Ohio || 15–1 || 7–4 || 0–0
|- align=""center"" bgcolor=""#ffcccc""
| 12 || April 14 ||  || Varsity Diamond • Columbus, Ohio || 0–1 || 7–5 || 0–0
|- align=""center"" bgcolor=""#ccffcc""
| 13 || April 20 ||  || Varsity Diamond • Columbus, Ohio || 10–9 || 8–5 || 1–0
|- align=""center"" bgcolor=""#ccffcc""
| 14 || April 21 || Minnesota || Varsity Diamond • Columbus, Ohio || 7–0 || 9–5 || 2–0
|- align=""center"" bgcolor=""#ffcccc""
| 15 || April 24 || at  || Unknown • Oxford, Ohio || 3–4 || 9–6 || 2–0
|- align=""center"" bgcolor=""#ffcccc""
| 16 || April 27 || at  || Hyames Field • Kalamazoo, Michigan || 2–3 || 9–7 || 2–0
|- align=""center"" bgcolor=""#ffcccc""
| 17 || April 28 || at Western Michigan || Hyames Field • Kalamazoo, Michigan || 5–7 || 9–8 || 2–0
|-

|- align=""center"" bgcolor=""#ccffcc""
| 18 || May 1 || at  || Unknown • Athens, Ohio || 7–6 || 10–8 || 2–0
|- align=""center"" bgcolor=""#ccffcc""
| 19 || May 4 ||  || Varsity Diamond • Columbus, Ohio || 12–6 || 11–8 || 3–0
|- align=""center"" bgcolor=""#ccffcc""
| 20 || May 5 || Purdue || Varsity Diamond • Columbus, Ohio || 14–4 || 12–8 || 4–0
|- align=""center"" bgcolor=""#ffcccc""
| 21 || May 8 ||  || Varsity Diamond • Columbus, Ohio || 6–8 || 12–9 || 4–0
|- align=""center"" bgcolor=""#ccffcc""
| 22 || May 9 || at Dayton || Unknown • Dayton, Ohio || 11–2 || 13–9 || 4–0
|- align=""center"" bgcolor=""#ccffcc""
| 23 || May 12 ||  || Varsity Diamond • Columbus, Ohio || 6–5 || 14–9 || 5–0
|- align=""center"" bgcolor=""#ccffcc""
| 24 || May 12 || Indiana || Varsity Diamond • Columbus, Ohio || 5–2 || 15–9 || 6–0
|- align=""center"" bgcolor=""#ccffcc""
| 25 || May 15 || Ohio || Varsity Diamond • Columbus, Ohio || 6–0 || 16–9 || 6–0
|- align=""center"" bgcolor=""#ffcccc""
| 26 || May 18 || at  || Northwestern Park • Evanston, Illinois || 1–3 || 16–10 || 6–1
|- align=""center"" bgcolor=""#ccffcc""
| 27 || May 19 || at Northwestern || Northwestern Park • Evanston, Illinois || 10–3 || 17–10 || 7–1
|- align=""center"" bgcolor=""#ccffcc""
| 28 || May 22 || at Cincinnati || Carson Field • Cincinnati, Ohio || 8–4 || 18–10 || 7–1
|- align=""center"" bgcolor=""#ccffcc""
| 29 || May 25 ||  || Varsity Diamond • Columbus, Ohio || 4–1 || 19–10 || 8–1
|- align=""center"" bgcolor=""#ffcccc""
| 30 || May 25 || Michigan || Varsity Diamond • Columbus, Ohio || 3–6 || 19–11 || 8–2
|- align=""center"" bgcolor=""#ffcccc""
| 31 || May 30 || Miami (OH) || Varsity Diamond • Columbus, Ohio || 3–4 || 19–12 || 8–2
|-

|- align=""center"" bgcolor=""#ccffcc""
| 32 || June 1 || at  || Old College Field • East Lansing, Michigan || 8–0 || 20–12 || 9–2
|- align=""center"" bgcolor=""#ccffcc""
| 33 || June 2 || at Michigan State || Old College Field • East Lansing, Michigan || 9–8 || 21–12 || 10–2
|-

|-
|-
! style="""" | Postseason
|- valign=""top""

|- align=""center"" bgcolor=""#ccffcc""
| 34 || June 8 || Western Michigan || Varsity Diamond • Columbus, Ohio || 1–0 || 22–12 || 10–2
|- align=""center"" bgcolor=""#ffcccc""
| 35 || June 8 || Western Michigan || Varsity Diamond • Columbus, Ohio || 2–4 || 22–13 || 10–2
|- align=""center"" bgcolor=""#ccffcc""
| 36 || June 9 || Western Michigan || Varsity Diamond • Columbus, Ohio || 3–2 || 23–13 || 10–2
|-

|- align=""center"" bgcolor=""#ffcccc""
| 37 || June 13 || Oklahoma || Omaha Municipal Stadium • Omaha, Nebraska || 8–9 || 23–14 || 10–2
|- align=""center"" bgcolor=""#ffcccc""
| 38 || June 13 || Texas A&M || Omaha Municipal Stadium • Omaha, Nebraska || 2–3 || 23–15 || 10–2
|-

Awards and honors 
Dick Hauck
 First Team All-Big Ten

Stewart Hein
 First Team All-Big Ten

References 

Ohio State Buckeyes baseball seasons
Ohio State Buckeyes baseball
Big Ten Conference baseball champion seasons
Ohio State
College World Series seasons",['They won their first game with a score of 15-3.'],972,multifieldqa_en,en,,ed61bdde19a3446389e989c06ab4209f464f9484d42dbd1c," The 1951 Ohio State Buckeyes baseball team represented the Ohio State University in the 1951 NCAA baseball season. The head coach was Marty Karow, serving his 1st year. The Buckeyes lost in the College World Series, defeated by the Texas A&M Aggies. The team was led by head coach MartyKarow, who was in his first year at Ohio State. The roster was made up of players from Ohio State, Michigan, Minnesota, Purdue, Ohio, Michigan State, and Texas. The season ended on May 8, 1951, with a record of 19 wins and 11 losses (1951-52). The team's record was 15 wins and 10 losses (1950-52, 1951-52) with a winning margin of 1.5 games ( 1951- 1952). The Buckeye team was coached by Marty Karowicz, who also served as the head coach for the football team in 1950-51, and the basketball team in 1952-53. It was the first time the Buckeyes had played in the NCAA World Series. It is the only team to have played in all three major college tournaments (1952-53, 1953-54, 1954-55, and 1956-57). It is also the first team to play in the World Series since the University of Texas in 1936. The last team to do so was the U.S. Army Corps of Engineers, which played in World Series games in 1945. The U.N. World Series was held in San Antonio, Texas, in 1951. The game was played in front of a crowd of 10,000 people at the San Antonio Municipal Stadium. The University of California, San Diego, played in a World Series game in 1951 in San Diego. It also played a game in 1950 in Los Angeles, California, against the California Institute of Technology. This was the only game in the history of the series that was held at the University of San Antonio and the University of California State Lubbock. The teams played a total of six games in the series, all of which were played at night. The final game of the season was played on May 21, 1952, in San Antonsio, Texas at the Clark Field. The Aggies won the game 2-1, the first game in which they had never won a game at the Clark Field in their history. The next two games were played in Los Antos, California. The following day, the team played a match against the Michigan Wolverines at the Hyames Field in Kalamazoo, Michigan. The Wolverines won 2-0, the second game by a score of 1-0. The third game was the final game, which was played at Hyames Field in Kalamazoo on May 22, 1952. The fourth and final game was held on May 23, 1953, at the University of Michigan at the University of Michigan in Michigan, with the score at 1-1. The fifth game was a game against Michigan State at The University of Michigan at Michigan State. The match was the last in which both teams played at the same time, on May 23, 1953. The winner of the game, the Michigan Wolverines, was the Michigan State Wolverines (1953-54), and the loser, the Ohio State University Buckeyes (1954-54). The game was played at Michigan State's Hyame Field in Michigan, where the team had a winning record of 9-0-1 (1955-53). Ohio State Buckeyes baseball seasons. Big Ten Conference baseball champion seasons. College World Series seasons. NCAA Division II and III championships. The NCAA Division I Division II Baseball Championship Game is held in East Lansing, Michigan. The Ohio State baseball team has won the NCAA championship three times. The Buckeyes have won the Big Ten Championship three times and the NCAA Championship once. The team has also reached the NCAA Division III Baseball Championship game in three of the last four seasons. The school's record in the NCAA baseball championship game is 2-2. The state of the team's baseball program is the highest in the nation, with a record of 1-2-1 in the last three seasons. This year's record is 1-0-1-1, with Michigan State winning the national championship in each of the past two seasons. It is the first time the school has won back-to-back titles in the history of the baseball program. The last time the team won the title in three consecutive seasons was in 1987 and 1988. The current record is 2–1-2, with the win-loss record at the end of the 2007 season at Michigan State at 2-1. This season's winning record is the best in the school's history, with an overall record of 7-3-1-. The team's winning streak is the longest since the start of the 2010 season."
What are the three synthetic types of vitamin K?,"Vitamin K - Wikipedia
(Redirected from Vitamin k)
This article needs more medical references for verification or relies too heavily on primary sources. Please review the contents of the article and add the appropriate references if you can. Unsourced or poorly sourced material may be challenged and removed. (November 2015)
This article is about the family of vitamers. For vitamin K1 the form usually used as a supplement, see Phytomenadione.
Vitamin K structures. MK-4 and MK-7 are both subtypes of K2.
Vitamin K deficiency, Warfarin overdose
Vitamin K is a group of structurally similar, fat-soluble vitamins the human body requires for complete synthesis of certain proteins that are prerequisites for blood coagulation and which the body also needs for controlling binding of calcium in bones and other tissues. The vitamin K-related modification of the proteins allows them to bind calcium ions, which they cannot do otherwise. Without vitamin K, blood coagulation is seriously impaired, and uncontrolled bleeding occurs. Low levels of vitamin K also weaken bones and promote calcification of arteries and other soft tissues[citation needed].
Chemically, the vitamin K family comprises 2-methyl-1,4-naphthoquinone (3-) derivatives. Vitamin K includes two natural vitamers: vitamin K1 and vitamin K2.[1] Vitamin K2, in turn, consists of a number of related chemical subtypes, with differing lengths of carbon side chains made of isoprenoid groups of atoms.
Vitamin K1, also known as phylloquinone, is made by plants, and is found in highest amounts in green leafy vegetables because it is directly involved in photosynthesis. It may be thought of as the plant form of vitamin K. It is active as a vitamin in animals and performs the classic functions of vitamin K, including its activity in the production of blood-clotting proteins. Animals may also convert it to vitamin K2.
Bacteria in the gut flora can also convert K1 into vitamin K2. In addition, bacteria typically lengthen the isoprenoid side chain of vitamin K2 to produce a range of vitamin K2 forms, most notably the MK-7 to MK-11 homologues of vitamin K2. All forms of K2 other than MK-4 can only be produced by bacteria, which use these forms in anaerobic respiration. The MK-7 and other bacterially derived forms of vitamin K2 exhibit vitamin K activity in animals, but MK-7's extra utility over MK-4, if any, is unclear and is a matter of investigation.
Three synthetic types of vitamin K are known: vitamins K3, K4, and K5. Although the natural K1 and all K2 homologues and synthetic K4 and K5 have proven nontoxic, the synthetic form K3 (menadione) has shown toxicity.[2]
1.2 Cardiovascular health
1.4 Coumarin poisoning
4.1 Conversion of vitamin K1 to vitamin K2
4.2 Vitamin K2
6 Absorption and dietary need
7 Dietary reference intake
10 Biochemistry
10.1 Function in animals
10.2 Gamma-carboxyglutamate proteins
10.3 Methods of assessment
10.4 Function in bacteria
11 Injection in newborns
11.3 Controversy
A review of 2014 concluded that there is positive evidence that monotherapy using MK-4, one of the forms of Vitamin K2, reduces fracture incidence in post-menopausal women with osteoporosis, and suggested further research on the combined use of MK-4 with bisphosphonates. In contrast, an earlier review article of 2013 concluded that there is no good evidence that vitamin K supplementation helps prevent osteoporosis or fractures in postmenopausal women.[3]
A Cochrane systematic review of 2006 suggested that supplementation with Vitamin K1 and with MK4 reduces bone loss; in particular, a strong effect of MK-4 on incident fractures among Japanese patients was emphasized.[4]
A review article of 2016 suggested to consider, as one of several measures for bone health, increasing the intake of foods rich in vitamins K1 and K2.[5]
Cardiovascular health[edit]
Adequate intake of vitamin K is associated with the inhibition of arterial calcification and stiffening,[6] but there have been few interventional studies and no good evidence that vitamin K supplementation is of any benefit in the primary prevention of cardiovascular disease.[7]
One 10-year population study, the Rotterdam Study, did show a clear and significant inverse relationship between the highest intake levels of menaquinone (mainly MK-4 from eggs and meat, and MK-8 and MK-9 from cheese) and cardiovascular disease and all-cause mortality in older men and women.[8]
Vitamin K has been promoted in supplement form with claims it can slow tumor growth; there is however no good medical evidence that supports such claims.[9]
Coumarin poisoning[edit]
Vitamin K is part of the suggested treatment regime for poisoning by rodenticide (coumarin poisoning).[10]
Although allergic reaction from supplementation is possible, no known toxicity is associated with high doses of the phylloquinone (vitamin K1) or menaquinone (vitamin K2) forms of vitamin K, so no tolerable upper intake level (UL) has been set.[11]
Blood clotting (coagulation) studies in humans using 45 mg per day of vitamin K2 (as MK-4)[12] and even up to 135 mg per day (45 mg three times daily) of K2 (as MK-4),[13] showed no increase in blood clot risk. Even doses in rats as high as 250 mg/kg, body weight did not alter the tendency for blood-clot formation to occur.[14]
Unlike the safe natural forms of vitamin K1 and vitamin K2 and their various isomers, a synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells.[2]
Phylloquinone (K1)[15][16] or menaquinone (K2) are capable of reversing the anticoagulant activity of the anticoagulant warfarin (tradename Coumadin). Warfarin works by blocking recycling of vitamin K, so that the body and tissues have lower levels of active vitamin K, and thus a deficiency of vitamin K.
Supplemental vitamin K (for which oral dosing is often more active than injectable dosing in human adults) reverses the vitamin K deficiency caused by warfarin, and therefore reduces the intended anticoagulant action of warfarin and related drugs.[17] Sometimes small amounts of vitamin K are given orally to patients taking warfarin so that the action of the drug is more predictable.[17] The proper anticoagulant action of the drug is a function of vitamin K intake and drug dose, and due to differing absorption must be individualized for each patient.[citation needed] The action of warfarin and vitamin K both require two to five days after dosing to have maximum effect, and neither warfarin or vitamin K shows much effect in the first 24 hours after they are given.[18]
The newer anticoagulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K, and may be taken with supplemental vitamin K.[19][20]
Vitamin K2 (menaquinone). In menaquinone, the side chain is composed of a varying number of isoprenoid residues. The most common number of these residues is four, since animal enzymes normally produce menaquinone-4 from plant phylloquinone.
A sample of phytomenadione for injection, also called phylloquinone
The three synthetic forms of vitamin K are vitamins K3 (menadione), K4, and K5, which are used in many areas, including the pet food industry (vitamin K3) and to inhibit fungal growth (vitamin K5).[21]
Conversion of vitamin K1 to vitamin K2[edit]
Vitamin K1 (phylloquinone) – both forms of the vitamin contain a functional naphthoquinone ring and an aliphatic side chain. Phylloquinone has a phytyl side chain.
The MK-4 form of vitamin K2 is produced by conversion of vitamin K1 in the testes, pancreas, and arterial walls.[22] While major questions still surround the biochemical pathway for this transformation, the conversion is not dependent on gut bacteria, as it occurs in germ-free rats[23][24] and in parenterally-administered K1 in rats.[25][26] In fact, tissues that accumulate high amounts of MK-4 have a remarkable capacity to convert up to 90% of the available K1 into MK-4.[27][28] There is evidence that the conversion proceeds by removal of the phytyl tail of K1 to produce menadione as an intermediate, which is then condensed with an activated geranylgeranyl moiety (see also prenylation) to produce vitamin K2 in the MK-4 (menatetrione) form.[29]
Vitamin K2[edit]
Main article: Vitamin K2
Vitamin K2 (menaquinone) includes several subtypes. The two subtypes most studied are menaquinone-4 (menatetrenone, MK-4) and menaquinone-7 (MK-7).
Vitamin K1, the precursor of most vitamin K in nature, is a stereoisomer of phylloquinone, an important chemical in green plants, where it functions as an electron acceptor in photosystem I during photosynthesis. For this reason, vitamin K1 is found in large quantities in the photosynthetic tissues of plants (green leaves, and dark green leafy vegetables such as romaine lettuce, kale and spinach), but it occurs in far smaller quantities in other plant tissues (roots, fruits, etc.). Iceberg lettuce contains relatively little. The function of phylloquinone in plants appears to have no resemblance to its later metabolic and biochemical function (as ""vitamin K"") in animals, where it performs a completely different biochemical reaction.
Vitamin K (in animals) is involved in the carboxylation of certain glutamate residues in proteins to form gamma-carboxyglutamate (Gla) residues. The modified residues are often (but not always) situated within specific protein domains called Gla domains. Gla residues are usually involved in binding calcium, and are essential for the biological activity of all known Gla proteins.[30]
At this time[update], 17 human proteins with Gla domains have been discovered, and they play key roles in the regulation of three physiological processes:
Blood coagulation: prothrombin (factor II), factors VII, IX, and X, and proteins C, S, and Z[31]
Bone metabolism: osteocalcin, also called bone Gla protein (BGP), matrix Gla protein (MGP),[32] periostin,[33] and the recently discovered Gla-rich protein (GRP).[34][35]
Vascular biology: growth arrest-specific protein 6 (Gas6)[36]
Unknown function: proline-rich γ-carboxyglutamyl proteins (PRGPs) 1 and 2, and transmembrane γ-carboxy glutamyl proteins (TMGs) 3 and 4.[37]
Like other lipid-soluble vitamins (A, D and E), vitamin K is stored in the fatty tissue of the human body.
Absorption and dietary need[edit]
Previous theory held that dietary deficiency is extremely rare unless the small intestine was heavily damaged, resulting in malabsorption of the molecule. Another at-risk group for deficiency were those subject to decreased production of K2 by normal intestinal microbiota, as seen in broad spectrum antibiotic use.[38] Taking broad-spectrum antibiotics can reduce vitamin K production in the gut by nearly 74% in people compared with those not taking these antibiotics.[39] Diets low in vitamin K also decrease the body's vitamin K concentration.[40] Those with chronic kidney disease are at risk for vitamin K deficiency, as well as vitamin D deficiency, and particularly those with the apoE4 genotype.[41] Additionally, in the elderly there is a reduction in vitamin K2 production.[42]
The National Academy of Medicine (NAM) updated an estimate of what constitutes an adequate intake (AI) for vitamin K in 2001. The NAM does not distinguish between K1 and K2 – both are counted as vitamin K. At that time there was not sufficient evidence to set the more rigorous estimated average requirement (EAR) or recommended dietary allowance (RDA) given for most of the essential vitamins and minerals. The current daily AIs for vitamin K for adult women and men are 90 μg and 120 μg respectively. The AI for pregnancy and lactation is 90 μg. For infants up to 12 months the AI is 2–2.5 μg, and for children aged 1 to 18 years the AI increases with age from 30 to 75 μg. As for safety, the FNB also sets tolerable upper intake levels (known as ULs) for vitamins and minerals when evidence is sufficient. In the case of vitamin K no UL is set, as evidence for adverse effects is not sufficient. Collectively EARs, RDAs, AIs and ULs are referred to as dietary reference intakes.[43] The European Food Safety Authority reviewed the same safety question and did not set an UL.[44]
For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percentage of daily value (%DV). For vitamin K labeling purposes the daily value was 80 μg, but as of May 2016 it has been revised upwards to 120 μg. A table of the pre-change adult daily values is provided at reference daily intake. Food and supplement companies have until 28 July 2018 to comply with the change.
See also: Vitamin K2 § Dietary sources
K1 (μg)[45]
Kale, cooked
Collards, cooked
Collards, raw
Swiss chard, cooked
Swiss chard, raw
Turnip greens, raw
Romaine lettuce, raw
Table from ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"", Clinical Center, National Institutes of Health Drug Nutrient Interaction Task Force.[46]
Vitamin K1 is found chiefly in leafy green vegetables such as dandelion greens (which contain 778.4 μg per 100 g, or 741% of the recommended daily amount), spinach, swiss chard, lettuce and Brassica vegetables (such as cabbage, kale, cauliflower, broccoli, and brussels sprouts) and often the absorption is greater when accompanied by fats such as butter or oils; some fruits, such as avocados, kiwifruit and grapes, are also high in vitamin K. By way of reference, two tablespoons of parsley contains 153% of the recommended daily amount of vitamin K.[47] Some vegetable oils, notably soybean oil, contain vitamin K, but at levels that would require relatively large calorie consumption to meet the USDA-recommended levels.[48] colonic bacteria synthesize a significant portion of humans' vitamin K needs; newborns often receive a vitamin K shot at birth to tide them over until their colons become colonized at five to seven days of age from the consumption of breast milk.
The tight binding of vitamin K1 to thylakoid membranes in chloroplasts makes it less bioavailable. For example, cooked spinach has a 5% bioavailability of phylloquinone, however, fat added to it increases bioavailability to 13% due to the increased solubility of vitamin K in fat.[49]
Main article: Vitamin K deficiency
Average diets are usually not lacking in vitamin K, and primary deficiency is rare in healthy adults. Newborn infants are at an increased risk of deficiency. Other populations with an increased prevalence of vitamin K deficiency include those who suffer from liver damage or disease (e.g. alcoholics), cystic fibrosis, or inflammatory bowel diseases, or have recently had abdominal surgeries. Secondary vitamin K deficiency can occur in people with bulimia, those on stringent diets, and those taking anticoagulants. Other drugs associated with vitamin K deficiency include salicylates, barbiturates, and cefamandole, although the mechanisms are still unknown. Vitamin K1 deficiency can result in coagulopathy, a bleeding disorder.[50]Symptoms of K1 deficiency include anemia, bruising, nosebleeds and bleeding of the gums in both sexes, and heavy menstrual bleeding in women.
Osteoporosis[51][52] and coronary heart disease[53][54] are strongly associated with lower levels of K2 (menaquinone). Vitamin K2 (as menaquinones MK-4 through MK-10) intake level is inversely related to severe aortic calcification and all-cause mortality.[8]
Function in animals[edit]
Mechanism of action of vitamin K1.
The function of vitamin K2 in the animal cell is to add a carboxylic acid functional group to a glutamate (Glu) amino acid residue in a protein, to form a gamma-carboxyglutamate (Gla) residue. This is a somewhat uncommon posttranslational modification of the protein, which is then known as a ""Gla protein"". The presence of two −COOH (carboxylic acid) groups on the same carbon in the gamma-carboxyglutamate residue allows it to chelate calcium ions. The binding of calcium ions in this way very often triggers the function or binding of Gla-protein enzymes, such as the so-called vitamin K-dependent clotting factors discussed below.
Within the cell, vitamin K undergoes electron reduction to a reduced form called vitamin K hydroquinone, catalyzed by the enzyme vitamin K epoxide reductase (VKOR).[55] Another enzyme then oxidizes vitamin K hydroquinone to allow carboxylation of Glu to Gla; this enzyme is called gamma-glutamyl carboxylase[56][57] or the vitamin K-dependent carboxylase. The carboxylation reaction only proceeds if the carboxylase enzyme is able to oxidize vitamin K hydroquinone to vitamin K epoxide at the same time. The carboxylation and epoxidation reactions are said to be coupled. Vitamin K epoxide is then reconverted to vitamin K by VKOR. The reduction and subsequent reoxidation of vitamin K coupled with carboxylation of Glu is called the vitamin K cycle.[58] Humans are rarely deficient in vitamin K1 because, in part, vitamin K1 is continuously recycled in cells.[59]
Warfarin and other 4-hydroxycoumarins block the action of VKOR.[60] This results in decreased concentrations of vitamin K and vitamin K hydroquinone in tissues, such that the carboxylation reaction catalyzed by the glutamyl carboxylase is inefficient. This results in the production of clotting factors with inadequate Gla. Without Gla on the amino termini of these factors, they no longer bind stably to the blood vessel endothelium and cannot activate clotting to allow formation of a clot during tissue injury. As it is impossible to predict what dose of warfarin will give the desired degree of clotting suppression, warfarin treatment must be carefully monitored to avoid overdose.
Gamma-carboxyglutamate proteins[edit]
Main article: Gla domain
The following human Gla-containing proteins (""Gla proteins"") have been characterized to the level of primary structure: blood coagulation factors II (prothrombin), VII, IX, and X, anticoagulant proteins C and S, and the factor X-targeting protein Z. The bone Gla protein osteocalcin, the calcification-inhibiting matrix Gla protein (MGP), the cell growth regulating growth arrest specific gene 6 protein (Gas6), and the four transmembrane Gla proteins (TMGPs), the function of which is at present unknown. Gas6 can function as a growth factor to activate the Axl receptor tyrosine kinase and stimulate cell proliferation or prevent apoptosis in some cells. In all cases in which their function was known, the presence of the Gla residues in these proteins turned out to be essential for functional activity.
Gla proteins are known to occur in a wide variety of vertebrates: mammals, birds, reptiles, and fish. The venom of a number of Australian snakes acts by activating the human blood-clotting system. In some cases, activation is accomplished by snake Gla-containing enzymes that bind to the endothelium of human blood vessels and catalyze the conversion of procoagulant clotting factors into activated ones, leading to unwanted and potentially deadly clotting.
Another interesting class of invertebrate Gla-containing proteins is synthesized by the fish-hunting snail Conus geographus.[61] These snails produce a venom containing hundreds of neuroactive peptides, or conotoxins, which is sufficiently toxic to kill an adult human. Several of the conotoxins contain two to five Gla residues.[62]
Methods of assessment[edit]
Vitamin K status can be assessed by:
The prothrombin time (PT) test measures the time required for blood to clot. A blood sample is mixed with citric acid and put in a fibrometer; delayed clot formation indicates a deficiency. This test is insensitive to mild deficiency, as the values do not change until the concentration of prothrombin in the blood has declined by at least 50%.[63]
Undercarboxylated prothrombin (PIVKA-II); in a study of 53 newborns, found ""PT (prothrombin time) is a less sensitive marker than PIVKA II"",[64] and as indicated above, PT is unable to detect subclinical deficiencies that can be detected with PIVKA-II testing.
Plasma phylloquinone was found to be positively correlated with phylloquinone intake in elderly British women, but not men,[65] but an article by Schurgers et al. reported no correlation between FFQ[further explanation needed] and plasma phylloquinone.[66]
Urinary γ-carboxyglutamic acid responds to changes in dietary vitamin K intake. Several days are required before any change can be observed. In a study by Booth et al., increases of phylloquinone intakes from 100 μg to between 377 and 417 μg for five days did not induce a significant change. Response may be age-specific.[67]
Undercarboxylated osteocalcin (UcOc) levels have been inversely correlated with stores of vitamin K[68] and bone strength in developing rat tibiae. Another study following 78 post-menopausal Korean women found a supplement regimen of vitamins K and D, and calcium, but not a regimen of vitamin D and calcium, was inversely correlated with reduced UcOc levels.[69]
Function in bacteria[edit]
Many bacteria, such as Escherichia coli found in the large intestine, can synthesize vitamin K2 (menaquinone-7 or MK-7, up to MK-11),[70] but not vitamin K1 (phylloquinone). In these bacteria, menaquinone transfers two electrons between two different small molecules, during oxygen-independent metabolic energy production processes (anaerobic respiration).[71] For example, a small molecule with an excess of electrons (also called an electron donor) such as lactate, formate, or NADH, with the help of an enzyme, passes two electrons to menaquinone. The menaquinone, with the help of another enzyme, then transfers these two electrons to a suitable oxidant, such fumarate or nitrate (also called an electron acceptor). Adding two electrons to fumarate or nitrate converts the molecule to succinate or nitrite plus water, respectively.
Some of these reactions generate a cellular energy source, ATP, in a manner similar to eukaryotic cell aerobic respiration, except the final electron acceptor is not molecular oxygen, but fumarate or nitrate. In aerobic respiration, the final oxidant is molecular oxygen (O2), which accepts four electrons from an electron donor such as NADH to be converted to water. E. coli, as facultative anaerobes, can carry out both aerobic respiration and menaquinone-mediated anaerobic respiration.
Injection in newborns[edit]
The blood clotting factors of newborn babies are roughly 30–60% that of adult values; this may be due to the reduced synthesis of precursor proteins and the sterility of their guts. Human milk contains 1–4 μg/L of vitamin K1, while formula-derived milk can contain up to 100 μg/L in supplemented formulas. Vitamin K2 concentrations in human milk appear to be much lower than those of vitamin K1. Occurrence of vitamin K deficiency bleeding in the first week of the infant's life is estimated at 0.25–1.7%, with a prevalence of 2–10 cases per 100,000 births.[72] Premature babies have even lower levels of the vitamin, so they are at a higher risk from this deficiency.
Bleeding in infants due to vitamin K deficiency can be severe, leading to hospitalization, blood transfusions, brain damage, and death. Supplementation can prevent most cases of vitamin K deficiency bleeding in the newborn. Intramuscular administration is more effective in preventing late vitamin K deficiency bleeding than oral administration.[73][74]
As a result of the occurrences of vitamin K deficiency bleeding, the Committee on Nutrition of the American Academy of Pediatrics has recommended 0.5–1 mg of vitamin K1 be administered to all newborns shortly after birth.[74]
In the UK vitamin K supplementation is recommended for all newborns within the first 24 hours.[75] This is usually given as a single intramuscular injection of 1 mg shortly after birth but as a second-line option can be given by three oral doses over the first month.[76]
Controversy arose in the early 1990s regarding this practice, when two studies suggested a relationship between parenteral administration of vitamin K and childhood cancer,[77] however, poor methods and small sample sizes led to the discrediting of these studies, and a review of the evidence published in 2000 by Ross and Davies found no link between the two.[78] Doctors reported emerging concerns in 2013,[79] after treating children for serious bleeding problems. They cited lack-of newborn vitamin K administration, as the reason that the problems occurred, and recommended that breastfed babies could have an increased risk unless they receive a preventative dose.
In the early 1930s, Danish scientist Henrik Dam investigated the role of cholesterol by feeding chickens a cholesterol-depleted diet.[80] He initially replicated experiments reported by scientists at the Ontario Agricultural College (OAC).[81] McFarlane, Graham and Richardson, working on the chick feed program at OAC, had used chloroform to remove all fat from chick chow. They noticed that chicks fed only fat-depleted chow developed hemorrhages and started bleeding from tag sites.[82] Dam found that these defects could not be restored by adding purified cholesterol to the diet. It appeared that – together with the cholesterol – a second compound had been extracted from the food, and this compound was called the coagulation vitamin. The new vitamin received the letter K because the initial discoveries were reported in a German journal, in which it was designated as Koagulationsvitamin. Edward Adelbert Doisy of Saint Louis University did much of the research that led to the discovery of the structure and chemical nature of vitamin K.[83] Dam and Doisy shared the 1943 Nobel Prize for medicine for their work on vitamin K (K1 and K2) published in 1939. Several laboratories synthesized the compound(s) in 1939.[84]
For several decades, the vitamin K-deficient chick model was the only method of quantifying vitamin K in various foods: the chicks were made vitamin K-deficient and subsequently fed with known amounts of vitamin K-containing food. The extent to which blood coagulation was restored by the diet was taken as a measure for its vitamin K content. Three groups of physicians independently found this: Biochemical Institute, University of Copenhagen (Dam and Johannes Glavind), University of Iowa Department of Pathology (Emory Warner, Kenneth Brinkhous, and Harry Pratt Smith), and the Mayo Clinic (Hugh Butt, Albert Snell, and Arnold Osterberg).[85]
The first published report of successful treatment with vitamin K of life-threatening hemorrhage in a jaundiced patient with prothrombin deficiency was made in 1938 by Smith, Warner, and Brinkhous.[86]
The precise function of vitamin K was not discovered until 1974, when three laboratories (Stenflo et al.,[87] Nelsestuen et al.,[88] and Magnusson et al.[89]) isolated the vitamin K-dependent coagulation factor prothrombin (factor II) from cows that received a high dose of a vitamin K antagonist, warfarin. It was shown that, while warfarin-treated cows had a form of prothrombin that contained 10 glutamate (Glu) amino acid residues near the amino terminus of this protein, the normal (untreated) cows contained 10 unusual residues that were chemically identified as γ-carboxyglutamate (Gla). The extra carboxyl group in Gla made clear that vitamin K plays a role in a carboxylation reaction during which Glu is converted into Gla.
The biochemistry of how vitamin K is used to convert Glu to Gla has been elucidated over the past thirty years in academic laboratories throughout the world.
^ ""Vitamin K Overview"". University of Maryland Medical Center. ^ a b Higdon, Jane (Feb 2008). ""Vitamin K"". Linus Pauling Institute, Oregon State University. Retrieved 12 Apr 2008. ^ Hamidi, M. S.; Gajic-Veljanoski, O.; Cheung, A. M. (2013). ""Vitamin K and bone health"". Journal of Clinical Densitometry (Review). 16 (4): 409–413. doi:10.1016/j.jocd.2013.08.017. PMID 24090644. ^ Cockayne, S.; Adamson, J.; Lanham-New, S.; Shearer, M. J.; Gilbody, S; Torgerson, D. J. (Jun 2006). ""Vitamin K and the prevention of fractures: systematic review and meta-analysis of randomized controlled trials"". Archives of Internal Medicine (Review). 166 (12): 1256–1261. doi:10.1001/archinte.166.12.1256. PMID 16801507. ^ O'Keefe, J. H.; Bergman, N.; Carrera Bastos, P.; Fontes Villalba, M.; Di Nicolantonio, J. J.; Cordain, L. (2016). ""Nutritional strategies for skeletal and cardiovascular health: hard bones, soft arteries, rather than vice versa"". Open Heart (Review). 3 (1): e000325. doi:10.1136/openhrt-2015-000325. PMC 4809188. PMID 27042317. ^ Maresz, K. (Feb 2015). ""Proper Calcium Use: Vitamin K2 as a Promoter of Bone and Cardiovascular Health"". Integrative Medicine (Review). 14 (1): 34–39. PMC 4566462. PMID 26770129. ^ Hartley, L.; Clar, C.; Ghannam, O.; Flowers, N.; Stranges, S.; Rees, K. (Sep 2015). ""Vitamin K for the primary prevention of cardiovascular disease"". The Cochrane Database of Systematic Reviews (Systematic review). 9 (9): CD011148. doi:10.1002/14651858.CD011148.pub2. PMID 26389791. ^ a b Geleijnse, J. M.; Vermeer, C.; Grobbee, D. E.; Schurgers, L. J.; Knapen, M. H.; van der Meer, I. M.; Hofman, A.; Witteman, J. C. (Nov 2004). ""Dietary intake of menaquinone is associated with a reduced risk of coronary heart disease: the Rotterdam Study"". Journal of Nutrition. 134 (11): 3100–3105. PMID 15514282. ^ Ades, T. B., ed. (2009). ""Vitamin K"". American Cancer Society Complete Guide to Complementary and Alternative Cancer Therapies (2nd ed.). American Cancer Society. pp. 558–563. ISBN 978-0-944235-71-3. ^ Lung, D. (Dec 2015). Tarabar, A., ed. ""Rodenticide Toxicity Treatment & Management"". Medscape. WebMD. ^ Rasmussen, S. E.; Andersen, N. L.; Dragsted, L. O.; Larsen, J. C. (Mar 2006). ""A safe strategy for addition of vitamins and minerals to foods"". European Journal of Nutrition. 45 (3): 123–135. doi:10.1007/s00394-005-0580-9. PMID 16200467. ^ Ushiroyama, T.; Ikeda, A.; Ueki, M (Mar 2002). ""Effect of continuous combined therapy with vitamin K2 and vitamin D3 on bone mineral density and coagulofibrinolysis function in postmenopausal women"". Maturitas. 41 (3): 211–221. doi:10.1016/S0378-5122(01)00275-4. PMID 11886767. ^ Asakura, H.; Myou, S.; Ontachi, Y.; Mizutani, T.; Kato, M.; Saito, M.; Morishita, E.; Yamazaki, M.; Nakao, S. (Dec 2001). ""Vitamin K administration to elderly patients with osteoporosis induces no hemostatic activation, even in those with suspected vitamin K deficiency"". Osteoporosis International. 12 (12): 996–1000. doi:10.1007/s001980170007. PMID 11846334. ^ Ronden, J. E.; Groenen-van Dooren, M. M.; Hornstra, G.; Vermeer, C. (Jul 1997). ""Modulation of arterial thrombosis tendency in rats by vitamin K and its side chains"". Atherosclerosis. 132 (1): 61–67. doi:10.1016/S0021-9150(97)00087-7. PMID 9247360. ^ Ansell, J.; Hirsh, J.; Poller, L.; Bussey, H.; Jacobson, A.; Hylek, E (Sep 2004). ""The pharmacology and management of the vitamin K antagonists: the Seventh ACCP Conference on Antithrombotic and Thrombolytic Therapy"". Chest. 126 (3 Suppl.): 204S–233S. doi:10.1378/chest.126.3_suppl.204S. PMID 15383473. ^ Crowther, M. A.; Douketis, J. D.; Schnurr, T.; Steidl, L.; Mera, V.; Ultori, C.; Venco, A.; Ageno, W. (Aug 2002). ""Oral vitamin K lowers the international normalized ratio more rapidly than subcutaneous vitamin K in the treatment of warfarin-associated coagulopathy. A randomized, controlled trial"". Annals of Internal Medicine. 137 (4): 251–254. doi:10.7326/0003-4819-137-4-200208200-00009. PMID 12186515. ^ a b ""Important Information to Know When You Are Taking: Warfarin (Coumadin) and Vitamin K"" (PDF). National Institute of Health Clinical Center Drug-Nutrient Interaction Task Force. Retrieved 17 Apr 2015. ^ ""Guidelines For Warfarin Reversal With Vitamin K"" (PDF). American Society of Health-System Pharmacists. Retrieved 17 Apr 2015. ^ ""Pradaxa Drug Interactions"". Pradaxapro.com. 19 Mar 2012. Retrieved 21 Apr 2013. ^ Bauersachs, R.; Berkowitz, S. D.; Brenner, B.; Buller, H. R.; Decousus, H.; Gallus, A. S.; Lensing, A. W.; Misselwitz, F.; Prins, M. H.; Raskob, G. E.; Segers, A.; Verhamme, P.; Wells, P.; Agnelli, G.; Bounameaux, H.; Cohen, A.; Davidson, B. L.; Piovella, F.; Schellong, S. (Dec 2010). ""Oral rivaroxaban for symptomatic venous thromboembolism"". New England Journal of Medicine. 363 (26): 2499–2510. doi:10.1056/NEJMoa1007903. PMID 21128814. ^ McGee, W. (1 Feb 2007). ""Vitamin K"". MedlinePlus. Retrieved 2 Apr 2009. ^ Shearer, M. J.; Newman, P. (Oct 2008). ""Metabolism and cell biology of vitamin K"". Thrombosis and Haemostasis. 100 (4): 530–547. doi:10.1160/TH08-03-0147. PMID 18841274. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). ""Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria"". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). ""Intestinal flora is not an intermediate in the phylloquinone–menaquinone-4 conversion in the rat"". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Thijssen, H. .H.; Drittij-Reijnders, M. J. (Sep 1994). ""Vitamin K distribution in rat tissues: dietary phylloquinone is a source of tissue menaquinone-4"". The British Journal of Nutrition. 72 (3): 415–425. doi:10.1079/BJN19940043. PMID 7947656. ^ Will, B. H.; Usui, Y.; Suttie, J. W. (Dec 1992). ""Comparative metabolism and requirement of vitamin K in chicks and rats"". Journal of Nutrition. 122 (12): 2354–2360. PMID 1453219. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). ""Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria"". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). ""Intestinal flora is not an intermediate in the phylloquinone-menaquinone-4 conversion in the rat"". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Al Rajabi, Ala (2011). The Enzymatic Conversion of Phylloquinone to Menaquinone-4 (PhD thesis). Tufts University, Friedman School of Nutrition Science and Policy. ^ Furie, B.; Bouchard, B. A.; Furie, B. C. (Mar 1999). ""Vitamin K-dependent biosynthesis of gamma-carboxyglutamic acid"". Blood. 93 (6): 1798–1808. PMID 10068650. ^ Mann, K. G. (Aug 1999). ""Biochemistry and physiology of blood coagulation"". Thrombosis and Haemostasis. 82 (2): 165–174. PMID 10605701. ^ Price, P. A. (1988). ""Role of vitamin-K-dependent proteins in bone metabolism"". Annual Review of Nutrition. 8: 565–583. doi:10.1146/annurev.nu.08.070188.003025. PMID 3060178. ^ Coutu, D. L.; Wu, J. H.; Monette, A.; Rivard, G. E.; Blostein, M. D.; Galipeau, J (Jun 2008). ""Periostin, a member of a novel family of vitamin K-dependent proteins, is expressed by mesenchymal stromal cells"". Journal of Biological Chemistry. 283 (26): 17991–18001. doi:10.1074/jbc.M708029200. PMID 18450759. ^ Viegas, C. S.; Simes, D. C.; Laizé, V.; Williamson, M. K.; Price, P. A.; Cancela, M. L. (Dec 2008). ""Gla-rich protein (GRP), a new vitamin K-dependent protein identified from sturgeon cartilage and highly conserved in vertebrates"". Journal of Biological Chemistry. 283 (52): 36655–36664. doi:10.1074/jbc.M802761200. PMC 2605998. PMID 18836183. ^ Viegas, C. S.; Cavaco, S.; Neves, P. L.; Ferreira, A.; João, A.; Williamson, M. K.; Price, P. A.; Cancela, M. L.; Simes, D. C. (Dec 2009). ""Gla-rich protein is a novel vitamin K-dependent protein present in serum that accumulates at sites of pathological calcifications"". American Journal of Pathology. 175 (6): 2288–2298. doi:10.2353/ajpath.2009.090474. PMC 2789615. PMID 19893032. ^ Hafizi, S.; Dahlbäck, B. (Dec 2006). ""Gas6 and protein S. Vitamin K-dependent ligands for the Axl receptor tyrosine kinase subfamily"". The FEBS Journal. 273 (23): 5231–5244. doi:10.1111/j.1742-4658.2006.05529.x. PMID 17064312. ^ Kulman, J. D.; Harris, J. E.; Xie, L.; Davie, E. W. (May 2007). ""Proline-rich Gla protein 2 is a cell-surface vitamin K-dependent protein that binds to the transcriptional coactivator Yes-associated protein"". Proceedings of the National Academy of Sciences of the United States of America. 104 (21): 8767–8772. doi:10.1073/pnas.0703195104. PMC 1885577. PMID 17502622. ^ ""Vitamin K"". MedlinePlus. US National Library of Medicine, National Institutes of Health. Sep 2016. Retrieved 26 May 2009. ^ Conly, J; Stein, K. (Dec 1994). ""Reduction of vitamin K2 concentrations in human liver associated with the use of broad spectrum antimicrobials"". Clinical and Investigative Medicine. 17 (6): 531–539. PMID 7895417. ^ Ferland, G.; Sadowski, J. A.; O'Brien, M. E. (Apr 1993). ""Dietary induced subclinical vitamin K deficiency in normal human subjects"". Journal of Clinical Investigation. 91 (4): 1761–1768. doi:10.1172/JCI116386. PMC 288156. PMID 8473516. ^ Holden, R. M.; Morton, A. R.; Garland, J. S.; Pavlov, A.; Day, A. G.; Booth, S. L. (Apr 2010). ""Vitamins K and D status in stages 3-5 chronic kidney disease"". Clinical Journal of the American Society of Nephrology. 5 (4): 590–597. doi:10.2215/CJN.06420909. PMC 2849681. PMID 20167683. ^ Hodges, S. J.; Pilkington, M. J.; Shearer, M. J.; Bitensky, L.; Chayen, J (Jan 1990). ""Age-related changes in the circulating levels of congeners of vitamin K2, menaquinone-7 and menaquinone-8"". Clinical Science. 78 (1): 63–66. PMID 2153497. ^ ""Vitamin K"". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press. 2001. p. 162–196. ^ Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006 ^ a b Rhéaume-Bleue, p. 42
^ ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"" (PDF). National Institutes of Health Clinical Center. ^ ""Nutrition Facts and Information for Parsley, raw"". Nutritiondata.com. Retrieved 21 Apr 2013. ^ ""Nutrition facts, calories in food, labels, nutritional information and analysis"". Nutritiondata.com. 13 Feb 2008. Retrieved 21 Apr 2013. ^ ""Vitamin K"". Vivo.colostate.edu. 2 Jul 1999. Retrieved 21 Apr 2013. ^ ""Vitamin K"". Micronutrient Data Centre. ^ Ikeda, Y.; Iki, M.; Morita, A.; Kajita, E.; Kagamimori, S.; Kagawa, Y.; Yoneshima, H. (May 2006). ""Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women: Japanese Population-Based Osteoporosis (JPOS) Study"". Journal of Nutrition. 136 (5): 1323–1328. PMID 16614424. ^ Katsuyama, H.; Ideguchi, S.; Fukunaga, M.; Saijoh, K.; Sunami, S. (Jun 2002). ""Usual dietary intake of fermented soybeans (Natto) is associated with bone mineral density in premenopausal women"". Journal of Nutritional Science and Vitaminology. 48 (3): 207–215. doi:10.3177/jnsv.48.207. PMID 12350079. ^ Sano, M.; Fujita, H.; Morita, I.; Uematsu, H.; Murota, S. (Dec 1999). ""Vitamin K2 (menatetrenone) induces iNOS in bovine vascular smooth muscle cells: no relationship between nitric oxide production and gamma-carboxylation"". Journal of Nutritional Science and Vitaminology. 45 (6): 711–723. doi:10.3177/jnsv.45.711. PMID 10737225. ^ Gast, G. C ; de Roos, N. M.; Sluijs, I.; Bots, M. L.; Beulens, J. W.; Geleijnse, J. M.; Witteman, J. C.; Grobbee, D. E.; Peeters, P. H.; van der Schouw, Y. T. (Sep 2009). ""A high menaquinone intake reduces the incidence of coronary heart disease"". Nutrition, Metabolism, and Cardiovascular Diseases. 19 (7): 504–510. doi:10.1016/j.numecd.2008.10.004. PMID 19179058. ^ Oldenburg, J.; Bevans, C. G.; Müller, C. R.; Watzka, M. (2006). ""Vitamin K epoxide reductase complex subunit 1 (VKORC1): the key protein of the vitamin K cycle"". Antioxidants & Redox Signaling. 8 (3–4): 347–353. doi:10.1089/ars.2006.8.347. PMID 16677080. ^ Suttie, J. W. (1985). ""Vitamin K-dependent carboxylase"". Annual Review of Biochemistry. 54: 459–477. doi:10.1146/annurev.bi.54.070185.002331. PMID 3896125. ^ Presnell, S. R.; Stafford, D. W. (Jun 2002). ""The vitamin K-dependent carboxylase"". Thrombosis and Haemostasis. 87 (6): 937–946. PMID 12083499. ^ Stafford, D. W. (Aug 2005). ""The vitamin K cycle"". Journal of Thrombosis and Haemostasis. 3 (8): 1873–1878. doi:10.1111/j.1538-7836.2005.01419.x. PMID 16102054. ^ Rhéaume-Bleue, p. 79.
^ Whitlon, D. S.; Sadowski, J. A.; Suttie, J. W. (Apr 1978). ""Mechanism of coumarin action: significance of vitamin K epoxide reductase inhibition"". Biochemistry. 17 (8): 1371–1377. doi:10.1021/bi00601a003. PMID 646989. ^ Terlau, H.; Olivera, B. M. (Jan 2004). ""Conus venoms: a rich source of novel ion channel-targeted peptides"". Physiological Reviews. 84 (1): 41–68. doi:10.1152/physrev.00020.2003. PMID 14715910. ^ Buczek, O.; Bulaj, G.; Olivera, BM (Dec 2005). ""Conotoxins and the posttranslational modification of secreted gene products"". Cellular and Molecular Life Sciences. 62 (24): 3067–3079. doi:10.1007/s00018-005-5283-0. PMID 16314929. ^ ""Prothrombin Time"". WebMD. ^ Dituri, F.; Buonocore, G.; Pietravalle, A.; Naddeo, F.; Cortesi, M; Pasqualetti, P; Tataranno M. L.; R., Agostino (Sep 2012). ""PIVKA-II plasma levels as markers of subclinical vitamin K deficiency in term infants"". Journal of Maternal, Fetal & Neonatal Medicine. 25 (9): 1660–1663. doi:10.3109/14767058.2012.657273. PMID 22280352. ^ Thane, C. W.; Bates, C. J.; Shearer, M. J.; Unadkat, N; Harrington, D. J.; Paul, A. A.; Prentice, A.; Bolton-Smith, C. (Jun 2002). ""Plasma phylloquinone (vitamin K1) concentration and its relationship to intake in a national sample of British elderly people"". British Journal of Nutrition. 87 (6): 615–622. doi:10.1079/BJNBJN2002582. PMID 12067432. ^ McKeown, N. M.; Jacques, P. F.; Gundberg, C. M.; Peterson, J. W.; Tucker, K. L.; Kiel, D. P.; Wilson, P. W.; Booth, SL (Jun 2002). ""Dietary and nondietary determinants of vitamin K biochemical measures in men and women"" (PDF). Journal of Nutrition. 132 (6): 1329–1334. PMID 12042454. ^ Yamano, M.; Yamanaka, Y.; Yasunaga, K.; Uchida, K. (Sep 1989). ""Effect of vitamin K deficiency on urinary gamma-carboxyglutamic acid excretion in rats"". Nihon Ketsueki Gakkai Zasshi. 52 (6): 1078–1086. PMID 2588957. ^ Matsumoto, T.; Miyakawa, T.; Yamamoto, D. (Mar 2012). ""Effects of vitamin K on the morphometric and material properties of bone in the tibiae of growing rats"". Metabolism. 61 (3): 407–414. doi:10.1016/j.metabol.2011.07.018. PMID 21944271. ^ Je, S.-H.; Joo, N.-S.; Choi, B.-H.; Kim, K.-M.; Kim, B.-T.; Park, S.-B.; Cho, D.-Y.; Kim, K.-N.; Lee, D.-J. (Aug 2011). ""Vitamin K supplement along with vitamin D and calcium reduced serum concentration of undercarboxylated osteocalcin while increasing bone mineral density in Korean postmenopausal women over sixty-years-old"". Journal of Korean Medical Science. 26 (8): 1093–1098. doi:10.3346/jkms.2011.26.8.1093. PMC 3154347. PMID 21860562. ^ Bentley, R.; Meganathan, R. (Sep 1982). ""Biosynthesis of vitamin K (menaquinone) in bacteria"" (PDF). Microbiological Reviews. 46 (3): 241–280. PMC 281544. PMID 6127606. ^ Haddock, B. A.; Jones, C. W. (Mar 1977). ""Bacterial respiration"" (PDF). Bacteriological Reviews. 41 (1): 47–99. PMC 413996. PMID 140652. ^ Shearer, M. J. (Jan 1995). ""Vitamin K"". Lancet. 345 (8944): 229–234. doi:10.1016/S0140-6736(95)90227-9. PMID 7823718. ^ Greer, J. P.; Foerster, J.; Lukens, J. N.; Rodgers, G. M.; Paraskevas, F.; Glader, B. (eds.). Wintrobe's Clinical Hematology (11th ed.). Philadelphia, Pennsylvania: Lippincott, Williams and Wilkens. ^ a b American Academy of Pediatrics Committee on Fetus Newborn. (Jul 2003). ""Controversies concerning vitamin K and the newborn. American Academy of Pediatrics Committee on Fetus and Newborn"" (PDF). Pediatrics. 112 (1.1): 191–192. doi:10.1542/peds.112.1.191. PMID 12837888. ^ Logan, S.; Gilbert, R. (1998). ""Vitamin K For Newborn Babies"" (PDF). Department of Health. Retrieved 12 Oct 2014. ^ ""Postnatal care: Routine postnatal care of women and their babies [CG37]"". www.nice.org.uk. NICE. Jul 2006. Retrieved 12 Oct 2014. ^ Parker, L.; Cole, M.; Craft, A. W.; Hey, E. N. (1998). ""Neonatal vitamin K administration and childhood cancer in the north of England: retrospective case-control study"". BMJ (Clinical Research Edition). 316 (7126): 189–193. doi:10.1136/bmj.316.7126.189. PMC 2665412. PMID 9468683. ^ McMillan, D. D. (1997). ""Routine administration of vitamin K to newborns"". Paediatric Child Health. 2 (6): 429–431. ^ ""Newborns get rare disorder after parents refused shots"". Having four cases since February just at Vanderbilt was a little bit concerning to me ^ Dam, C. P. H. (1935). ""The Antihaemorrhagic Vitamin of the Chick: Occurrence And Chemical Nature"". Nature. 135 (3417): 652–653. doi:10.1038/135652b0. ^ Dam, C. P. H. (1941). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize Laureate Lecture. ^ McAlister, V. C. (2006). ""Control of coagulation: a gift of Canadian agriculture"" (PDF). Clinical and Investigative Medicine. 29 (6): 373–377. ^ MacCorquodale, D. W.; Binkley, S. B.; Thayer, S. A.; Doisy, E. A. (1939). ""On the constitution of Vitamin K1"". Journal of the American Chemical Society. 61 (7): 1928–1929. doi:10.1021/ja01876a510. ^ Fieser, L. F. (1939). ""Synthesis of Vitamin K1"". Journal of the American Chemical Society. 61 (12): 3467–3475. doi:10.1021/ja01267a072. ^ Dam, C. P. H. (12 Dec 1946). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize lecture. ^ Warner, E. D.; Brinkhous, K. M.; Smith, H. P. (1938). ""Bleeding Tendency of Obstructive Jaundice"". Proceedings of the Society of Experimental Biology and Medicine. 37 (4): 628–630. doi:10.3181/00379727-37-9668P. ^ Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). ""Vitamin K dependent modifications of glutamic acid residues in prothrombin"". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730–2733. doi:10.1073/pnas.71.7.2730. PMC 388542. PMID 4528109. ^ Nelsestuen, G. L.; Zytkovicz, T. H.; Howard, J. B. (Oct 1974). ""The mode of action of vitamin K. Identification of gamma-carboxyglutamic acid as a component of prothrombin"" (PDF). Journal of Biological Chemistry. 249 (19): 6347–6350. PMID 4214105. ^ Magnusson, S.; Sottrup-Jensen, L.; Petersen, T. E.; Morris, H. R.; Dell, A. (Aug 1974). ""Primary structure of the vitamin K-dependent part of prothrombin"". FEBS Letters. 44 (2): 189–193. doi:10.1016/0014-5793(74)80723-4. PMID 4472513. Bibliography[edit]
Rhéaume-Bleue, Kate (2012). Vitamin K2 and the Calcium Paradox. John Wiley & Sons, Canada. ISBN 1-118-06572-7. External links[edit]
""Vitamin K: Another Reason to Eat Your Greens"". v
TPP / ThDP (B1)
FMN, FAD (B2)
NAD+, NADH, NADP+, NADPH (B3)
Coenzyme A (B5)
PLP / P5P (B6)
THFA / H4FA, DHFA / H2FA, MTHF (B9)
AdoCbl, MeCbl (B12)
Phylloquinone (K1), Menaquinone (K2)
non-vitamins
Coenzyme B
Heme / Haem (A, B, C, O)
Molybdopterin/Molybdenum cofactor
THMPT / H4MPT
Fe2+, Fe3+
vitamins: see vitamins
Antihemorrhagics (B02)
(coagulation)
Phytomenadione (K1)
Menadione (K3)
intrinsic: IX/Nonacog alfa
VIII/Moroctocog alfa/Turoctocog alfa
extrinsic: VII/Eptacog alfa
common: X
II/Thrombin
I/Fibrinogen
XIII/Catridecacog
combinations: Prothrombin complex concentrate (II, VII, IX, X, protein C and S)
Carbazochrome
thrombopoietin receptor agonist (Romiplostim
Eltrombopag)
Tetragalacturonic acid hydroxymethylester
Epinephrine/Adrenalone
amino acids (Aminocaproic acid
Aminomethylbenzoic acid)
serpins (Aprotinin
Alfa1 antitrypsin
Camostat).","['Vitamins K3, K4, and K5.']",7133,multifieldqa_en,en,,c7ad556387e8215bae3f8ccd30ba35e0093218fe48168718," This article needs more medical references for verification or relies too heavily on primary sources. Unsourced or poorly sourced material may be challenged and removed. For vitamin K1 the form usually used as a supplement, see Phytomenadione. MK-4 and MK-7 are both subtypes of K2. Vitamin K1, also known as phylloquinone, is made by plants, and is found in highest amounts in green leafy vegetables because it is directly involved in photosynthesis. It is active as a vitamin in animals and performs the classic functions of vitamin K, including its activity in the production of blood-clotting proteins. Animals may also convert it to vitamin K2, which can only be produced by bacteria. The natural K1 and all K2 homologues and synthetic K4 and K5 have proven nontoxic, but the synthetic form K3 (menadione) has shown toxicity. A review of 2014 concluded that there is positive evidence that monotherapy using one of the forms of Vitamin K2 reduces fracture incidence in post-menopausal women with osteoporosis. In contrast, an earlier review article of 2013 concluded there is no good evidence that vitamin K supplementation helps prevent osteoarthritis or fractures in post menopausal women. A Cochrane systematic review of 2006 suggested that supplementation with Vitamin K 1 and with MK4 reduces bone loss; in particular, a strong effect ofMK-4 on incident fractures among Japanese patients was emphasized. The vitamin K family comprises 2-methyl-1,4-naphthoquinone (3-) derivatives. It also comprises vitamins K3, K4, andK5. It includes two natural vitamers: vitamin K3 and vitamin K4. The synthetic vitamin K5 has proven to be toxic, but there have been few interventional studies on it. It has also been shown to be associated with the inhibition of arterial calcification and stiffening. It was suggested to consider, as one of several measures for health, increasing the intake of foods rich in vitamins K1. and K2.[2]Cardiovascular health is associated with increased vitamin K intake. The body also needs vitamin K for controlling binding of calcium in bones and other tissues. The human body requires vitamin K to complete synthesis of certain proteins that are prerequisites for blood coagulation and which it also needs for controlling calcium ions, which they cannot do otherwise. Low levels ofitamin K also weaken bones and promote calcification of arteries and other soft tissues[citation needed].Chemically, the vitamin KFamily comprises 2,methyl- 1,4,naphthylquinone-3- derivatives. The K2 subtype is the only one that can be made by bacteria, which use these forms in anaerobic respiration. In addition, bacteria typically lengthen the isoprenoid side chain of vitamin. K2 to produce a range of vitaminK2 forms, most notably theMK-7 to MK-11 homologue of vitamin k2. It can also be found in the gut flora, but its utility is unclear and is a matter of investigation. Injection in newborns is a controversial issue, and there has been little research on the combined use of MK- 4 with bisphosphonates. No good evidence that vitamin K supplementation is of any benefit in the primary prevention of cardiovascular disease. Synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. Vitamin K is part of the suggested treatment regime for poisoning by rodenticide (coumarin poisoning).[10] No known toxicity is associated with high doses of the phylloquinone (vitamin K1) or menaquinone (Vitamin K2) forms of Vitamin K. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells. Sometimes small amounts of vitaminK are given orally to patients taking warfarin so that the action of the drug is more predictable. The proper anticoagulant action is a function of vitamink intake and drug dose, and due to differing absorption must be individualized for each patient. The action of warfar in and vitamin K both require two to five days after dosing to have maximum effect. Neither warfarins or vitamin K shows much effect in the first 24 hours after they are given. The newer anticoageulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K. VitaminK is produced by conversion of vitamin k1 to vitamin K2. It is also used in many areas, including the pet food industry (v vitamin K5) and to inhibit fungal growth (v vitamins K3 and K4) in rats. It can slow tumor growth, but there is no good medical evidence that supports such claims. It also reverses the vitamin K deficiency caused by Warfarin, and therefore reduces the intended anticogeulantaction of warFarin and related drugs. It has been shown that vitaminK can be used to treat cancer. It's also used to prevent heart attacks and strokes in some people with heart conditions, such as atrial fibrillation and atrial torsion, and to treat certain forms of cancer in humans and animals. It may be used in the treatment of cancer of the pancreas and other organs. It could also be used as a treatment for cancer in the lungs and other parts of the digestive system, and as an anti-viral drug in the skin. It does not have a known side-effect in humans, but some studies have shown that it may have a positive effect on the immune system in some patients. It cannot be used for cancer treatment in humans or in animals, however, as it is not known if it has any effect on cancer-causing side-effects in the gut or in other organs such as the liver or in the liver. It was used in a study to test the safety and effectiveness of the anti-tumor drug Cytotecan in humans in the 1980s and 1990s, but it has since been ruled out by the FDA as not being safe for use in humans. There is no safe upper intake level (UL) for vitamin K1 (K1) and K2 (K2) in human adults. The most common number of isoprenoid residues is four, since animal enzymes normally produce menaquin one-4 from plant phyllaquinone. The conversion is not dependent on gut-free rats[23] as it occurs in germ-free Rats. Vitamin K1 is a stereoisomer of phylloquinone, an important chemical in green plants. It is found in large quantities in the photosynthetic tissues of plants (green leaves, and dark green leafy vegetables such as romaine lettuce, kale and spinach) It occurs in far smaller quantities in other plant tissues (roots, fruits, etc.) Iceberg lettuce contains relatively little. Vitamin K (in animals) is involved in the carboxylation of certain glutamate residues in proteins to form gamma-carboxyglutamate (Gla) residues. The modified residues are usually involved in binding calcium, and are essential for the biological activity of all known Gla proteins. 17 human proteins with Gla domains have been discovered, and they play key roles in the regulation of three physiological processes: blood coagulation, bone metabolism and vascular biology. Diets low in vitamin K also decrease the body's vitamin K concentration. Those with chronic kidney disease are at risk for vitamin K deficiency, as well as vitamin D deficiency, and particularly those with the apoE4 genotype. The National Academy of Medicine updated an estimate of what constitutes an adequate intake of vitamin K in 2001. The NAM does not distinguish between K1 and K2 – both are counted as vitamin K. The current daily requirement for Vitamin K for adult women and men is 90 μg for pregnancy and 120 μg for infants. For infants up to 12 months, the AI is 2–5 μg for lactation, and for children aged 1 to 18 years the AI increases with age from 30 to 75 μg. As for safety, the FNB sets also uppeable tolerable tolerability sets also set for safety for infants aged up to 1 to 5 years old. The FNB also sets a safe tolerable level for children from 1 to 15 years of age, and also for adults aged 30 to 70 years old, as for older children from 15 to 75 years old and adults aged 80 to 90 years and older. The average daily intake for adults and men for pregnancy is 90 igrams for women and 120 igram for men for infants and children aged 2 to 18 months. The AI for infants is up to 2–2.5 mg for infants, and up to 3 mg for children ages 1 to 13 years. The recommended daily requirement is 90 mg for adult men and 120 mg for women for pregnancy, and 90mg for infants age 12 months and older for children up to 18 years old. For pregnant women and women, the recommended daily intake is 90 ng/day for pregnancy. For babies aged 1-12 months, it is 90g/day. For children aged 13-18 years, it's 90g/. For infants aged 12 months to 14 years, and older, it’s 90g+. For infants under 18 years, the average daily allowance is 90mg/day (for women and children under 13 years old). For infants over the age of 2 years, there is a recommended daily allowance of 90 mg/day in the U.S. for vitamins A, D, E, and K. and K1 (for men and women).[26] For infants, the current daily allowance for vitamins K is 90ng/day, for women, and 120g/year, for men and men over the ages of 18 years. Vitamin K1 is found chiefly in leafy green vegetables such as dandelion greens, spinach, swiss chard, lettuce and Brassica vegetables. Vitamin K1 deficiency can result in coagulopathy, a bleeding disorder. Secondary vitamin K deficiency can occur in people with bulimia, those on stringent diets, and those taking anticoagulants. Osteoporosis[51] and coronary heart disease are strongly associated with lower levels of vitamin K2 (menaquinone) Vitamin K2 intake level is inversely related to severe aortic calcification and all-cause mortality.[8]Function in animals is to add a carboxylic acid functional group to a glutamate (Glu) amino acid residue in a animal cell. This is a somewhat uncommon modification of the protein, which is then known as a post-translational modification. The presence of two protein groups on the same carb is known as the ""COOH"" (carboxyl acid) effect. For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percentage of daily value (%DV) For vitamin K labeling purposes the daily value was 80 μg, but as of May 2016 it has been revised upwards to 120 μg. A table of the pre-change adult daily values is provided at reference daily intake. The European Food Safety Authority reviewed the same safety question and did not set an UL. No UL is set, as evidence for adverse effects is not sufficient. In the case of vitamins and minerals there is no set intake level for vitamins or minerals when evidence is sufficient, as no evidence is needed to set an intake level in the first place. For vitamins and vitamins there is a set of intake levels known as intake levels (known as ULs) for vitamins and Minerals, which are referred to as the EARs, RDAs, AIs and ULs. The EARs and AIs are collectively known as dietary reference intakes. The ULs are used for food and supplement label labeling purposes in the United States and other countries. For example, in the UK, vitamin K is listed as a nutrient in the ""Dietary Reference Intake"" section of the Dietary Reference Guide (DRC) for the U.K. and Europe. The DRC does not include vitamin K in the definition of the AIs or ULs, but does include Vitamin K as an ingredient in the DRC's DRC definition of Vitamin K. For more information on Vitamin K, see Vitamin K and Vitamin K-rich foods. For Vitamin K supplements, see the Vitamin K Supplements section of this article. for more details on vitamin K and other nutrients, see vitamin K-related topics. For the full list of nutrients and supplements, visit the VitaminK Supplements page. for the UK and Europe, or click here for more information about Vitamin K in Europe and the rest of the world, or see the vitamin K Supplement Guide for the United Kingdom and the European Union, for the European and European countries, or the Vitamink supplement guide for the EU and the EU, respectively. The Vitamin K content in each serving of food and supplements is based on the recommended daily intake for each nutrient. for vitamin K1, a serving of spinach, kale, collards, and spinach, is 741% of recommended daily amount. Gla proteins are known to occur in a wide variety of vertebrates: mammals, birds, reptiles, and fish. The venom of a number of Australian snakes acts by activating the human blood-clotting system. In some cases, activation is accomplished by snake Gla-containing enzymes that bind to the endothelium of human blood vessels and catalyze the conversion of procoagulant clotting factors into activated ones. Warfarin and other 4-hydroxycoumarins block the action of VKOR. This results in decreased concentrations of vitamin K and vitamin K hydroquinone in tissues. It is impossible to predict what dose of warfarin will give the desired degree of clotting suppression, so it must be carefully monitored to avoid overdose. The prothrombin time (PT) test measures the time required for blood to clot. A blood sample is mixed with citric acid and put in a fibrometer. This test is insensitive to mild deficiency, as the values do not change until the blood has declined at least 50%.[63] In a study of 53 newborns, 53% found ""prothrom bin time"" to be less than 53.5 minutes. In a newborn baby with a Gla deficiency, this is less than 50%. In a baby with no Gla, it can be as little as 20 minutes before the blood clotting process is triggered. The Gla protein osteocalcin, the calcification-inhibiting matrix GlA protein (MGP), the cell growth arrest specific gene 6 protein (Gas6), and the four transmembrane Gla proteins (TMGPs), the function of which is at present unknown, can function as a growth factor to activate the Axl receptor tyrosine kinase and stimulate cell proliferation or prevent apoptosis in some cells. In all cases in which their function was known, the presence of the Gla residues in these proteins turned out to be essential for functional activity. Without Gla on the amino termini of these factors, they no longer bind stably to the blood vessel endothelia and cannot activate clotting to allow formation of a clot during tissue injury. The presence of Gla is essential for clotting in some animals, such as the fish-hunting snail Conus geographus, which produces a venom containing hundreds of neuroactive peptides, or conotoxins, which is sufficiently toxic to kill an adult human. In humans, Gla can be found in blood coagulation factors II, VII, IX, and X, anticoagulante proteins C and S, and the factor X-targeting protein Z. Gla also plays a role in the development of the human immune system, including the fight against infections and cancer. The protein Gla has been linked to the development and survival of some cancers, including thrombotic hemangioblastoma, a type of blood cancer. It can also play a key role in fighting infections and infections in humans and other animals. It has been shown that Gla plays a key part in the formation of blood clots in the lungs and other parts of the immune system in humans. It also plays an important role in preventing cancerous cells from developing. Plasma phylloquinone was found to be positively correlated with vitamin K intake in elderly British women, but not men. Undercarboxylated osteocalcin (UcOc) levels have been inversely correlated with stores of vitamin K and bone strength in developing rat tibiae. The blood clotting factors of newborn babies are roughly 30–60% that of adult values. Vitamin K deficiency bleeding in the first week of the infant's life is estimated at 0.25–1.7%, with a prevalence of 2–10 cases per 100,000 births. Premature babies have even lower levels of the vitamin, so they are at a higher risk from this deficiency. Supplementation is recommended for all newborns within the first 24 hours of birth. Intramuscular administration is more effective in preventing late vitamin K deficiency bleed than oral administration. The Committee on Nutrition of the American Academy of Pediatrics has recommended 0.5–1 mg of vitamins K1 be administered to all newbornS shortly after birth. This single intramususus is usually given as a second-line injection of 1.5 mg over the first month of the baby's life but can be given by three oral doses over the course of the first year of the child's life. The vitamin K2 concentrations in human milk appear to be much lower than those of vitaminK1. In these bacteria, menaquinone transfers two electrons between two different small molecules, during oxygen-independent metabolic energy production processes (anaerobic respiration). In aerobic respiration, the final oxidant is molecular oxygen (O2), which accepts four electrons from an electron donor such as NADH to be converted to water. E. coli, as facultative anaerobes, can carry out both aerobic resppiration and menaquin one-mediated anaerobic resppiration. The final electron acceptor is not molecular oxygen, but fumarate or nitrate, which converts succinate or nitrite plus water to succinate and succinate plus water, respectively. This is similar to the way eukaryotic cell respiration generates ATP, except the final electrons are not molecular Oxygen (oxygen) and are taken by nitrate or succinate, which is the final acceptor of ATP, not molecularoxygen (M2). This is the same process that takes place in the human body, but in a more complex way in the gut. In the gut, the electrons are transferred between small molecules such as lactate, formate, or NADH, with the help of an enzyme, to formate and nitrate. This process is known as ""menaquinone-7 or MK-7, up to MK-11"" It is also known as the ""Menaquinone"" process, and it is thought to be responsible for the development of the human immune system. It is thought that vitamin K1 is the most important vitamin in the body, followed by vitamin K 2 and vitamin K3. It can be found in the National Institutes of Health (NIH) and the Food and Drug Administration (FDA) lists of vitamins and minerals. It has been shown that Vitamin K1 and K2 are more important than K2 and K3 in preventing blood clots in babies. In the early 1930s, Danish scientist Henrik Dam investigated the role of cholesterol by feeding chickens a cholesterol-depleted diet. Dam found that these defects could not be restored by adding purified cholesterol to the diet. It appeared that – together with the cholesterol – a second compound had been extracted from the food, and this compound was called the coagulation vitamin. Several laboratories synthesized the compound(s) in 1939. The precise function of vitamin K was not discovered until 1974, when three laboratories (Stenflo et al.,[87] Nelsestuen et al. and Magnusson et al.) isolated the vitamin K-dependent coagulating factor prothrombin (factor II) from cows that received a high dose of a vitamin K antagonist, warfarin. The extra carboxyl group in Gla made clear that vitamin K plays a role in a carboxyglutamate reaction during which Glu is converted into Gla. The biochemistry of how vitamin K is used to convert Glu to Gla has been elucidated over the past thirty years in academic laboratories throughout the world. In 2013, doctors reported emerging concerns in 2013 after treating children for serious bleeding problems. They cited lack-of newborn vitamin K administration, as the reason that the problems occurred, and recommended that breastfed babies could have an increased risk unless they receive a preventative dose. The new vitamin received the letter K because the initial discoveries were reported in a German journal, in which it was designated as Koagulationsvitamin. In the early 1990s, two studies suggested a relationship between parenteral administration of Vitamin K and childhood cancer. However, poor methods and small sample sizes led to the discrediting of these studies, and a review of the evidence published in 2000 by Ross and Davies found no link between the two. The first published report of successful treatment with vitamin K of life-threatening hemorrhage in a jaundiced patient with prothrosbin deficiency was made in 1938 by Smith, Warner, and Brinkhous.[86] In 2008, the University of Maryland Medical Center published an article entitled ""Vitamin K Overview"". The article was published by the Linus Pauling Institute, Oregon State University. It was the only method of quantifying vitamin K in various foods. The extent to which blood coagulated was restored by the diet was taken as a measure for its vitamin K content. For several decades, the Vitamin K-deficient chick model was theOnly method ofquantifying vitaminK in various food was the. Vitamin K was used to measure the extent towhich blood coageulation was restored. For more information on vitamin K, see: http://www.jocd.org/vitamin-k/Vitamin-K- Overview.html. For a list of studies that have shown vitamin K to be beneficial for humans, see http://jocc.org/. For more details about vitamin K and its role in bone health, see http://j.cnn.com/2013/02/08/23/science-news/v Vitamin-K.html#v-k- overview. Vitamin K has been linked to a reduced risk of coronary heart disease. Vitamin K administration to elderly patients with osteoporosis induces no hemostatic activation, even in those with suspected vitamin K deficiency. The American Cancer Society has a guide to Complementary and Alternative Cancer Therapies (2nd ed.) with information on vitamin K and other minerals. The National Institutes of Health has a program to promote vitamin K use in the U.S. and abroad. The U.N. International Agency for Research on Cancer has a programme to promote the use of vitamin K in the treatment of cancer and other cancer-causing conditions. The World Health Organization has a Program to promote Vitamin K use. The European Food Standards Agency has a Programme to Promote Vitamin K Use in the Treatment of Cancer and Other Conditions. The International Organization for Vitamin K is a not-for-profit organization that promotes vitamin K usage in the United States and other countries. It also provides information on Vitamin K-rich foods and supplements for cancer patients and their families. For more information about vitamin K, visit: www.vitamink.org.uk/. For more on vitamin D, see: http://www.vitaminsd.org/d/d-3/vitamin-d-4/v-3.html. For a list of Vitamin K supplements, see the: http:/www.nhs.uk/drugs/supplements/v Vitamin D, Vitamin K, Vitamin D3, and Vitamin D2-D3-D4-D5-D6-D7-D8-D9-D1-D2- D3- D4- D5- D6- D7- D8- D9- D10- D12- D11- D13- D14- D15- D1- D17- D18- D19- D20- D21- D22- D23- D24- D25- D26- D28- D27- D29- D30- D31- D34- D35- D36- D37- D38- D39- D41- D44- D47- D48- D50- D55- D56- D57- D58- D59- D60- D61- D62- D63- D65- D66- D68- D67- D69- D70- D74- D75- D77- D78- D79- D80- D81- D82- D83- D84- D85- D87- D86- D88- D90- D89- D92- D93- D94- D95- D96- D97- D98- D99- D91- D100- D104- D107- D106- D111- D108- D110- D112- D113- D114- D115- D118- D130- D109- D105- D120- D160- D150- D157- D165- D158- D155- D170- D205- D206- D159- D214- D215- D167- D207- D208- D210- D209- D200- D211- D220- D230- D225- D222- D227- D229- D240- D235- D250- D260- D280- D233- D238- D255- D239- D244- D245- D242- D246- D265- D267- D237- D268- D263- D259- D300- D264- D251- D315- D266- D318- D261- D270- D334- D340- D345- D285- D305- D325- D326- D346- D333- D337- D ""Oral rivaroxaban for symptomatic venous thromboembolism"" New England Journal of Medicine 363 (26): 2499–2510. ""Intestinal flora is not an intermediate in the phylloquinone–menaquinone-4 conversion in the rat"" Journal of Nutrition 128 (2): 220–223. ""Vitamin Kynthesis of gamma-carboxyamic acid"" Blood 93 (6): 98–1808. ""Role of vitamin-K-dependent proteins in bone metabolism"" Annual Review of Nutrition 8: 565–583. ""The Enzymatic Conversion of Phylloquinone to Menaquin one-4 (PhD thesis)"". Tufts University, Friedman School of Nutrition Science and Policy. (Mar 1999) ""V vitamin K-dependent biosynthesis of Gamma-Carboxyglamic acid"". Blood 93, K650 (6: 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94 and 94. ""Metabolism and cell biology of vitamin K"". Thrombosis and Haemostasis. 100 (4): 530–547. PMID 18841274. ""Conversion of dietary phylloquinone to tissue menaquinone.-4 in rats is not dependent on gut bacteria"" (Jan 1998). ""Int intestinal flora isnot an intermediate"" (1): 69–75. ""Dietary phyllaquinone is a source of tissue menaquin-4"" (Sep 1994). ""V Vitamin K distribution in rat tissues: dietary. phylaquinone. is a Source of tissue Menaquin One-4"". The British Journal of. Nutrition 72 (3): 415–425. PMid 7947656. ""Remicin and Vitamin K"" (Dec 1992). ""Comparative metabolism and requirement of. vitamin K in chicks and rats"". Journal. of Nutrition 122 (12): 2354–2360. PM ID 1453219. ""Efficacy and safety of Vitamin K in humans"" (Oct 2008). ""Empiracy and effectiveness of vitaminK in humans"". The American Journal of Clinical Nutrition. (Nov 2008) ""Emmetry and Efficacy of Vitamin. K in Humans"" ( Nov 2008). “Vitamin. K. in Humans’ Blood’s Role in Blood Coagulation’ (Nov 2007) “Empires and physiology of blood coagulation.” “Remicine and blood. coagulations.’ “ “. “”. ‘‘’’.  ’, ‘. ’,’,. ‘,”, ’,.’.,’;. ”, ”,. ,‘,  , “, ”,. ’” and ‘;’ , ‘,.”;”’ and. ‚’ .’,'’',’'’'.’%.’ Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women. Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006. ""Vitamin K"". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press 2001. ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"" (PDF) National Institutes of Health Clinical Center. ""Nutrition facts, calories in food, labels, nutritional information and analysis"". Nutritiondata.com. 13 Feb 2008. http://www.nhs.gov/nutritional-information/vitamin-k/index.html?title=Vitamin-K&action=vitamin.K&linkCode=v.K2&source=http%3A/v.v.k2%20k2k.html%20Vitamin%20K2k%20ketoic%20acid%20inhibitory%20protein%20and%20reactive%20phenomenon%20of%20eosin%20disease%20Eosin%. In the U.S., vitamin K2 is linked to a reduced risk of cancer. In the UK, vitamin K levels are associated with a lower risk of osteoporosis in post-menopausal men and women. Vitamin K is linked with a higher risk of bone mineral density in premenopausal and older women. It is also associated with an increased risk of prostate cancer in men and younger women. The link between vitamin K and bone density is not well-described but is thought to be related to the fact that vitamin K is found in higher levels in the body than in the blood. The study was published in the Journal of Nutritional Science and Vitaminology (J.Nutritional Science & Vitaminology): 48 (23): 5231–5244. It was the first study of its kind to look at the link between Vitamin K2 and bone mineral densities in humans. It also found that Vitamin K was associated with higher levels of vitamin D in the human body. It has been shown that the presence of vitamin K in the urine of premenopause patients can be linked to increased bone density in later life. It can also be associated with increased levels of Vitamin D in older women and men. It's unclear whether the link is due to the presence or absence of vitaminK in the diet. It could also be due to vitamin K or other factors, such as the use of probiotics or probiotics in the first place. It may also be a result of the high level of Vitamin K found in soybeans in the Japanese diet. The Journal of Nutrition. and Vitamin.ology (Jun 2002) reported that dietary intake of fermented. soybeans (Natto) is associated. with a reduced bone mineral. density (3.3–3.4%) in pre-menopopausal women (Jun. 2002). Vitamin K (V2menetone) induces no nitric oxide production in muscle cells (N.O.S) in iNetone (menetrozine) (207-207) and gamma-carboxylic acid (Noxon) (208-207). The study found no relationship between nitricoxide production and muscle. smooth muscle cells no nitrozone (NOC) production. . ""Vitamin K-dependent carboxylase"". Annual Review of Biochemistry. 54: 459–477. PMID 3896125. ""Prothrombin Time"". WebMD. ""Effect of vitamin K deficiency on urinary gamma-carboxyglutamic acid excretion in rats"". Nihon Ketsueki Gakkai Zasshi. 52 (6): 1078–1086. ""The vitamin K cycle"". Journal of Thrombosis and Haemostasis. 3 (8): 1873–1878.. ""PIVKA-II plasma levels as markers of subclinical vitamin K. deficiency in term infants"" Journal of Maternal, Fetal & Neonatal Medicine. 25 (9): 1660–1663.. “Vitamin D’s effect on the morphometric and material properties of bone in the tibiae growing rats"". Metabolism. 61 (3): 407–414, PMID 2194444, p. 79.. The vitamin K supplement along with vitamin D reduced calcium and reduced calcium concentration of under-over-menopausal women while increasing bone density in postmenopausal. women.. Vitamin D reduced the concentration of over-over calcium and increased bone density. in post-menopause. women while. increasing bone. density in women over over 55.. vitamin K reduced the calcium and calcium concentration in post menopausal. over-65-year-olds while increasing osteoporosis. in over-70-year old women while decreasing bone density and increasing bone mineral density in. women over 65. Vitamin K increased the bone density of post menopauseopausal women but not in women under 65. It also increased bone. mineral. density while increasing. bone mineral. D-diluted calcium and vitamin D increased. bone density while under over 65-years-old. It increased. the bone mineral concentration of women over 70-years old while under-65. years old. It decreased. the. bone.density of women under 70- years-old while under 65-year -year-old women. It. increased the. calcium and. vitamin D concentration. in women and men over 75-years. It was found to be more effective than vitamin K in the treatment of osteoarthritis of the knee and back in the elderly. It has been shown to improve bone density, bone mass and bone. function. in those who took it for at least 10 years. It is also thought to have a protective effect on bone density as well. It can also have an anti-oxidant effect in the prevention of cancer. of the colon. and bone metastasis. It may also have a role in the development of bone. and cartilage. of bone marrow and bone marrow. cancers. In the elderly, it has been found to have an important role in reducing the risk of cancer of the bladder. and bowel. of developing cancer. in older people and in the. elderly of all ages. It helps to take vitamin K-rich foods such as fish, meat, eggs, vegetables, and milk, as well as vitamins B, C, D, E and E2, E3, B6, B12, and B12. and K-complex. and B-12. It could also help to reduce risk of cardiovascular disease and bone damage in older adults and children who have already suffered a stroke or are at risk of developing a stroke. It's thought that vitamin K may play a key role in protecting against cancer. Having four cases since February just at Vanderbilt was a little bit concerning to me. Dam, C. P. H. (1941). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize Laureate Lecture. McAlister, V. C. (2006). ""Control of coagulation: a gift of Canadian agriculture"" ( PDF). Clinical and Investigative Medicine. 29 (6): 373–377. Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). ""Vitamin K dependent modifications of glutamic acid residues in prothrombin"". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730–2733. Vitamin K Paradox: Another Reason to Eat Your Greens. John Wiley & Sons, Canada. (2012) v.65-72-0-7. v.7.2 and 4.4-2 and B2 and. B. (2013) v65-65-7-2. v1.2.1. v2.3. v3.4. v4.5. v5. V6. V7. V8. V9. V10. V11. V12. V13. V14. V15. V16. V17. V18. V19. V20. V21. V2. V3. V4. V5. The Vitamin K paradox: Another reason to eat your Greens. NICE.org.uk. Jul 2006. Retrieved 12 Oct 2014. V1.4 and V5 and V6 and V7 and V8 and V9 and V10 and V11 and V12 and V13 and V14 and V15 and V16 and V17 and V18 and V19 and V20 and V21 and V22 and V2 and V3 and V1 and V4 and K and K are all listed in NICE's list of recommended vitamins and minerals for pregnant women and babies. V.3 and K is listed in the list of essential nutrients for mothers and children. V 2. V 3. V 4. V 5. V 6. V 7. V 8. V 9. V 10. V 11. V 12. V 13. V 14. V 15. V 16. V 17. V 18. V 19. V 20. V 21. V 22. V 23. V22. V23. V24. V25. V26. V27. V28. V29. V30. V31. V32. V33. V34. V35. V36. V37. V38. V39. V40. V41. V46. V47. V48. V49. V51. V52. V53. V54. V55. V56. V57. V58. V59. V60. V61. V62. V63. V64. V68. V69. V70. V71. V72. V73. V74. V75. V78. V79. V80. V81. V82. V83. V84. V77. V87. V88. V89. V90. V91. V92. V93. V94. V103. V104. V105. V107. V108. V110. V111. V112. V113. V114."
"Can individual molecules of indeno[1,2-a]fluorene switch between open-shell and closed-shell states?","Paper Info

Title: Bistability between π-diradical open-shell and closed-shell states in indeno[1,2-a]fluorene
Publish Date: Unkown
Author List: Shantanu Mishra (from IBM Research Europe -Zurich), Manuel Vilas-Varela (from Department of Organic Chemistry, Center for Research in Biological Chemistry and Molecular Materials (CiQUS), University of Santiago de Compostela), Leonard-Alexander Lieske (from IBM Research Europe -Zurich), Ricardo Ortiz (from Donostia International Physics Center (DIPC)), Igor Rončević (from Department of Chemistry, University of Oxford), Florian Albrecht (from IBM Research Europe -Zurich), Diego Peña (from Department of Organic Chemistry, Center for Research in Biological Chemistry and Molecular Materials (CiQUS), University of Santiago de Compostela), Leo Gross (from IBM Research Europe -Zurich)

Figure

Fig. 1 | Non-benzenoid non-alternant polycyclic conjugated hydrocarbons.a, Classical nonbenzenoid non-alternant polycyclic conjugated hydrocarbons: pentalene, azulene and heptalene.b, Generation of indacenes and indenoindenes through benzinterposition and benzannelation of pentalene, respectively.Gray filled rings represent Clar sextets.c, Closed-shell Kekulé (left) and openshell non-Kekulé (right) resonance structures of QDMs.Note that meta-QDM is a non-Kekulé molecule.All indenofluorene isomers, being derived through benzannelation of indacenes, contain a central QDM moiety.d, Closed-shell Kekulé (top) and open-shell non-Kekulé (bottom) resonance structures of indenofluorenes.Compared to their closed-shell structures, 1 and 5 gain two Clar sextets in the openshell structure, while 2-4 gain only one Clar sextet in the open-shell structure.Colored bonds in d highlight the ortho-and para-QDM moieties in the two closed-shell Kekulé structures of 5. e, Scheme of on-surface generation of 5 by voltage pulse-induced dehydrogenation of 6 (C20H14).Structures 7 and 8 represent the two monoradical species (C20H13).
Fig. 2 | Characterization of open-shell indeno[1,2-a]fluorene on bilayer NaCl/Au(111).a, DFTcalculated wave functions of the frontier orbitals of 5OS in the triplet configuration for the spin up (occupied) level (isovalue: 0.002 e -Å -3 ).Blue and red colors represent opposite phases of the wave function.b, Corresponding DFT-calculated spin density of 5OS (isovalue: 0.01 e -Å -3).Blue and orange colors represent spin up and spin down densities, respectively.c, Probability density of the SOMOs of 5OS (isovalue: 0.001 e -Å -3 ).d, DFT-calculated bond lengths of 5OS.e, Constant-height I(V) spectra acquired on a species of 5 assigned as 5OS, along with the corresponding dI/dV(V) spectra.Open feedback parameters: V = -2 V, I = 0.17 pA (negative bias side) and V = 2 V, I = 0.17 pA (positive bias side).Acquisition position of the spectra is shown in Supplementary Fig.7.f, Scheme of many-body transitions associated to the measured ionic resonances of 5OS.Also shown are STM images of assigned 5OS at biases where the corresponding transitions become accessible.Scanning parameters: I = 0.3 pA (V = -1.2V and -1.5 V) and 0.2 pA (V = 1.3 V and 1.6 V). g, Laplace-filtered AFM image of assigned 5OS.STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3Å.The tip-height offset Δz for each panel is provided with respect to the STM setpoint, and positive (negative) values of Δz denote tip approach (retraction) from the STM setpoint.f and g show the same molecule at the same adsorption site, which is next to a trilayer NaCl island.The bright and dark features in the trilayer NaCl island in g correspond to Cl -and Na + ions, respectively.Scale bars: 10 Å (f) and 5 Å (g).
Fig. 3 | Characterization of closed-shell indeno[1,2-a]fluorene on bilayer NaCl/Au(111).a, DFTcalculated wave functions of the frontier orbitals of closed-shell 5 0 (isovalue: 0.002 e -Å -3 ).The wave functions shown here are calculated for the 5para geometry.b, DFT-calculated bond lengths of 5ortho (top) and 5para (bottom).c, Constant-height I(V) spectra acquired on a species of 5 assigned as 5para, along with the corresponding dI/dV(V) spectra.Open feedback parameters: V = -2 V, I = 0.15 pA (negative bias side) and V = 2.2 V, I = 0.15 pA (positive bias side).Acquisition position of the spectra is shown in Supplementary Fig. 7. d, Scheme of many-body transitions associated to the measured ionic resonances of 5para.Also shown are STM images of assigned 5para at biases where the corresponding transitions become accessible.Scanning parameters: I = 0.15 pA (V = -1.5 V) and 0.2 pA (V = 1.7 V). e, Laplace-filtered AFM image of assigned 5para.STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.7 Å. f, Selected bonds labeled for highlighting bond order differences between 5para and 5ortho.For the bond pairs a/b, c/d and e/f, the bonds labeled in bold exhibit a higher bond order than their neighboring labeled bonds in 5para.g, Laplace-filtered AFM images of 5 on bilayer NaCl/Cu(111) showing switching between 5OS and 5para as the molecule changes its adsorption position.The faint protrusion adjacent to 5 is a defect that stabilizes the adsorption of 5. STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3Å. STM and STS data in c and d are acquired on the same species, while the AFM data in e is acquired on a different species.Scale bars: 10 Å (d) and 5 Å (e,g).
NMR (300 MHz, CDCl3) δ: 7.51 (m, 2H), 7.40 -7.28 (m, 5H), 7.27 -7.20 (m, 2H), 7.13 (d, J = 7.7 Hz, 1H), 2.07 (s, 3H), 1.77 (s, 3H) ppm. 13C NMR-DEPT (75 MHz, CDCl3, 1:1 mixture of atropisomers) δ: 141.2 (C), 141.1 (C), 140.0 (C), 139.4 (2C), 137.5 (C), 137.4 (C), 136.0 (3C), 134.8 (C), 134.5 (C), 134.1 (C), 134.0 (C), 133.7 (C), 133.6 (C), 131.6 (CH), 131.2 (CH), 131.1 (CH), 130.7 (CH), 129.8 (CH), 129.7 (CH), 129.5 (CH), 129.4 (CH), 129.0 (CH), 128.9 (CH), 128.7 (2CH), 128.6 (2CH), 127.2 (CH), 127.1 (CH), 127.0 (CH), 126.9 (CH), 126.7 (CH), 126.6 (CH), 20.6 (CH3), 20.5 (CH3), 17.7 (CH3), 17.5 (CH3) ppm.MS (APCI) m/z (%): 327 (M+1, 100).HRMS: C20H16Cl2; calculated: 327.0702, found: 327.0709.
NMR (500 MHz, CDCl3) δ: 7.93 (d, J = 7.6 Hz, 1H), 7.85 (d, J = 7.5 Hz, 1H), 7.78 (d, J = 7.7 Hz, 1H), 7.65 (d, J = 7.4 Hz, 1H), 7.61 (d, J = 7.5 Hz, 1H), 7.59 (d, J = 7.7 Hz, 1H), 7.47 (ddd, J = 8.4, 7.2, 1.1 Hz, 1H), 7.42 (dd, J = 8.1, 7.0 Hz, 1H), 7.35 (m, 2H), 4.22 (s, 3H), 4.02 (s, 3H).ppm. 13C NMR-DEPT (125 MHz, CDCl3) δ: 144.1 (C), 143.3 (C), 142.3 (C), 141.9 (C), 141.8 (C), 141.2 (C), 138.2 (C), 136.5 (C), 127.0 (CH), 126.9 (CH), 126.7 (CH), 126.6 (CH), 125.3 (CH), 125.2 (CH), 123.6 (CH), 122.2 (CH), 119.9 (CH), 118.4 (CH), 37.4 (CH2), 36.3 (CH2).ppm.MS (APCI) m/z (%): 254 (M+, 88).HRMS: C20H14; calculated: 254.1090, found: 254.1090.

abstract

Indenofluorenes are non-benzenoid conjugated hydrocarbons that have received great interest owing to their unusual electronic structure and potential applications in nonlinear optics and photovoltaics. Here, we report the generation of unsubstituted indeno[1,2-a]fluorene, the final and yet unreported parent indenofluorene regioisomer, on various surfaces by cleavage of two C-H bonds in 7,12-dihydro indeno[1,2-a]fluorene through voltage pulses applied by the tip of a combined scanning tunneling microscope and atomic force microscope.
On bilayer NaCl on Au(111), indeno[1,2a]fluorene is in the neutral charge state, while it exhibits charge bistability between neutral and anionic states on the lower work function surfaces of bilayer NaCl on Ag(111) and Cu(111). In the neutral state, indeno[1,2-a]fluorene exhibits either of two ground states: an open-shell π-diradical state, predicted to be a triplet by density functional and multireference many-body perturbation theory calculations, or a closedshell state with a para-quinodimethane moiety in the as-indacene core.
Switching between open-and closed-shell states of a single molecule is observed by changing its adsorption site on NaCl. The inclusion of non-benzenoid carbocyclic rings is a viable route to tune the physicochemical properties of polycyclic conjugated hydrocarbons (PCHs) . Non-benzenoid polycycles may lead to local changes in strain, conjugation, aromaticity, and, relevant to the context of the present work, induce an open-shell ground state of the corresponding PCHs .
Many nonbenzenoid PCHs are also non-alternant, where the presence of odd-membered polycycles breaks the bipartite symmetry of the molecular network . Figure shows classical examples of non-benzenoid non-alternant PCHs, namely, pentalene, azulene and heptalene. Whereas azulene is a stable PCH exhibiting Hückel aromaticity ([4n+2] π-electrons, n = 2), pentalene and heptalene are unstable Hückel antiaromatic compounds with [4n] π-electrons, n = 2 (pentalene) and n = 3 (heptalene).
Benzinterposition of pentalene generates indacenes, consisting of two isomers s-indacene and as-indacene (Fig. ). Apart from being antiaromatic, indacenes also contain proaromatic quinodimethane (QDM) moieties (Fig. ) , which endows them with potential open-shell character. While the parent s-indacene and asindacene have never been isolated, stable derivatives of s-indacene bearing bulky substituents have been synthesized .
A feasible strategy to isolate congeners of otherwise unstable non-benzenoid non-alternant PCHs is through fusion of benzenoid rings at the ends of the π-system, that is, benzannelation. For example, while the parent pentalene is unstable, the benzannelated congener indeno[2,1-a]indene is stable under ambient conditions (Fig. ) .
However, the position of benzannelation is crucial for stability: although indeno[2,1a]indene is stable, its regioisomer indeno[1,2-a]indene (Fig. ) oxidizes under ambient conditions . Similarly, benzannelation of indacenes gives rise to the family of PCHs known as indenofluorenes (Fig. ), which constitute the topic of the present work.
Depending on the benzannelation position and the indacene core, five regioisomers can be constructed, namely, indeno [ Practical interest in indenofluorenes stems from their low frontier orbital gap and excellent electrochemical characteristics that render them as useful components in organic electronic devices .
The potential open-shell character of indenofluorenes has led to several theoretical studies on their use as non-linear optical materials and as candidates for singlet fission in organic photovoltaics . Recent theoretical work has also shown that indenofluorene-based ladder polymers may exhibit fractionalized excitations.
Fundamentally, indenofluorenes represent model systems to study the interplay between aromaticity and magnetism at the molecular scale . Motivated by many of these prospects, the last decade has witnessed intensive synthetic efforts toward the realization of indenofluorenes. Derivatives of 1-4 have been realized in solution , while 1-3 have also been synthesized on surfaces and characterized using scanning tunneling microscopy (STM) and atomic force microscopy (AFM), which provide information on molecular orbital densities , molecular structure and oxidation state .
With regards to the open-shell character of indenofluorenes, 2-4 are theoretically and experimentally interpreted to be closed-shell, while calculations indicate that 1 and 5 should exhibit open-shell ground states . Bulk characterization of mesitylsubstituted 1, including X-ray crystallography, temperature-dependent NMR, and electron spin resonance spectroscopy, provided indications of its open-shell ground state .
Electronic characterization of 1 on Au(111) surface using scanning tunneling spectroscopy (STS) revealed a low electronic gap of 0.4 eV (ref. ). However, no experimental proof of an openshell ground state of 1 on Au(111), such as detection of singly occupied molecular orbitals (SOMOs) or spin excitations and correlations due to unpaired electrons , was shown.
In this work, we report the generation and characterization of unsubstituted 5. Our research is motivated by theoretical calculations that indicate 5 to exhibit the largest diradical character among all indenofluorene isomers . The same calculations also predict that 5 should possess a triplet ground state.
Therefore, 5 would qualify as a Kekulé triplet, of which only a handful of examples exist . However, definitive synthesis of 5 has never been reported so far. Previously, Dressler et al. reported transient isolation of mesityl-substituted 5, where it decomposed both in the solution and in solid state , and only the structural proof of the corresponding dianion was obtained.
On-surface generation of a derivative of 5, starting from truxene as a precursor, was recently reported . STM data on this compound, containing the indeno[1,2-a]fluorene moiety as part of a larger PCH, was interpreted to indicate its open-shell ground state. However, the results did not imply the ground state of unsubstituted 5. Here, we show that on insulating surfaces 5 can exhibit either of two ground states: an open-shell or a closed-shell.
We infer the existence of these two ground states based on high-resolution AFM imaging with bond-order discrimination and STM imaging of molecular orbital densities . AFM imaging reveals molecules with two different geometries. Characteristic bond-order differences in the two geometries concur with the geometry of either an open-or a closed-shell state.
Concurrently, STM images at ionic resonances show molecular orbital densities corresponding to SOMOs for the open-shell geometry, but orbital densities of the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) for the closed-shell geometry. Our experimental results are in good agreement with density functional theory (DFT) and multireference perturbation theory calculations.
Finally, we observe switching between open-and closed-shell states of a single molecule by changing its adsorption site on the surface. Synthetic strategy toward indeno[1,2-a]fluorene. The generation of 5 relies on the solution-phase synthesis of the precursor 7,12-dihydro indeno[1,2-a]fluorene (6). Details on synthesis and characterization of 6 are reported in Supplementary Figs.
. Single molecules of 6 are deposited on coinage metal (Au(111), Ag(111) and Cu(111)) or insulator surfaces. In our work, insulating surfaces correspond to two monolayer-thick (denoted as bilayer) NaCl on coinage metal surfaces. Voltage pulses ranging between 4-6 V are applied by the tip of a combined STM/AFM system, which result in cleavage of one C-H bond at each of the pentagonal apices of 6, thereby leading to the generation of 5 (Fig. ).
In the main text, we focus on the generation and characterization of 5 on insulating surfaces. Generation and characterization of 5 on coinage metal surfaces is shown in Supplementary Fig. . ). Blue and orange colors represent spin up and spin down densities, respectively. c, Probability density of the SOMOs of 5OS (isovalue: 0.001 e -Å -3 ).
d, DFT-calculated bond lengths of 5OS. e, Constant-height I(V) spectra acquired on a species of 5 assigned as 5OS, along with the corresponding dI/dV(V) spectra. Open feedback parameters: V = -2 V, I = 0.17 pA (negative bias side) and V = 2 V, I = 0.17 pA (positive bias side). Acquisition position of the spectra is shown in Supplementary Fig. . f, Scheme of many-body transitions associated to the measured ionic resonances of 5OS.
Also shown are STM images of assigned 5OS at biases where the corresponding transitions become accessible. Scanning parameters: I = 0.3 pA (V = -1.2 V and -1.5 V) and 0.2 pA (V = 1.3 V and 1.6 V). g, Laplace-filtered AFM image of assigned 5OS. STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3
Å. The tip-height offset Δz for each panel is provided with respect to the STM setpoint, and positive (negative) values of Δz denote tip approach (retraction) from the STM setpoint. f and g show the same molecule at the same adsorption site, which is next to a trilayer NaCl island. The bright and dark features in the trilayer NaCl island in g correspond to Cl -and Na + ions, respectively.
Scale bars: 10 Å (f) and 5 Å (g). To experimentally explore the electronic structure of 5, we used bilayer NaCl films on coinage metal surfaces to electronically decouple the molecule from the metal surfaces. Before presenting the experimental findings, we summarize the results of our theoretical calculations performed on 5 in the neutral charge state (denoted as 5 0 ).
We start by performing DFT calculations on 5 0 in the gas phase. Geometry optimization performed at the spin-unrestricted UB3LYP/6-31G level of theory leads to one local minimum, 5OS, the geometry of which corresponds to the open-shell resonance structure of 5 (Fig. , the label OS denotes open-shell).
The triplet electronic configuration of 5OS is the lowest-energy state, with the openshell singlet configuration 90 meV higher in energy. Geometry optimization performed at the restricted closed-shell RB3LYP/6-31G level reveals two local minima, 5para and 5ortho, the geometries of which (Fig. ) exhibit bond length alternations in line with the presence of a para-or an ortho-QDM moiety, respectively, in the as-indacene core of the closed-shell resonance structures of 5 (Fig. ) .
Relative to 5OS in the triplet configuration, 5para and 5ortho are 0.40 and 0.43 eV higher in energy, respectively. Additional DFT results are shown in Supplementary Fig. . To gain more accurate insights into the theoretical electronic structure of 5, we performed multireference perturbation theory calculations (Supplementary Fig. ) based on quasi-degenerate second-order n-electron valence state perturbation theory (QD-NEVPT2).
In so far as the order of the ground and excited states are concerned, the results of QD-NEVPT2 calculations qualitatively match with DFT calculations. For 5OS, the triplet configuration remains the lowest-energy state, with the open-shell singlet configuration 60 meV higher in energy. The energy differences between the open-and closed-shell states are substantially reduced in QD-NEVPT2 calculations, with 5para and 5ortho only 0.11 and 0.21 eV higher in energy, respectively, compared to 5OS in the triplet configuration.
We also performed nucleus-independent chemical shift calculations to probe local aromaticity of 5 in the openand closed-shell states. While 5OS in the triplet configuration exhibits local aromaticity at the terminal benzenoid rings, 5OS in the open-shell singlet configuration, 5para and 5ortho all display antiaromaticity (Supplementary Fig. ).
The choice of the insulating surface determines the charge state of 5: while 5 adopts neutral charge state on the high work function bilayer NaCl/Au(111) surface (irrespective of its openor closed-shell state, Supplementary Fig. ), 5 exhibits charge bistability between 5 0 and the anionic state 5 -1 on the lower work function bilayer NaCl/Ag(111) and Cu(111) surfaces (Supplementary Figs. ).
In the main text, we focus on the characterization of 5 on bilayer NaCl/Au(111). Characterization of charge bistable 5 is reported in Supplementary Figs. . We first describe experiments on 5 on bilayer NaCl/Au(111), where 5 exhibits a geometry corresponding to the calculated 5OS geometry, and an open-shell electronic configuration.
We compare the experimental data on this species to calculations on 5OS with a triplet configuration, as theory predicts a triplet ground state for 5OS. For 5OS, the calculated frontier orbitals correspond to the SOMOs ψ1 and ψ2 (Fig. ), whose spin up levels are occupied and the spin down levels are empty.
Figure shows the DFT-calculated bond lengths of 5OS, where the two salient features, namely, the small difference in the bond lengths within each ring and the notably longer bond lengths in the pentagonal rings, agree with the open-shell resonance structure of 5 (Fig. ). Figure shows an AFM image of 5 adsorbed on bilayer NaCl/Au(111) that we assign as 5OS, where the bond-order differences qualitatively correspond to the calculated 5OS geometry (discussed and compared to the closed-shell state below).
Differential conductance spectra (dI/dV(V), where I and V denote the tunneling current and bias voltage, respectively) acquired on assigned 5OS exhibit two peaks centered at -1.5 V and 1.6 V (Fig. ), which we assign to the positive and negative ion resonances (PIR and NIR), respectively. Figure shows the corresponding STM images acquired at the onset (V = -1.2
V/1.3 V) and the peak (V = -1.5 V/1.6 V) of the ionic resonances. To draw a correspondence between the STM images and the molecular orbital densities, we consider tunneling events as many-body electronic transitions between different charge states of 5OS (Fig. ). Within this framework, the PIR corresponds to transitions between 5 0 and the cationic state 5 .
At the onset of the PIR at -1.2 V, an electron can only be detached from the SOMO ψ1 and the corresponding STM image at -1.2 V shows the orbital density of ψ1. Increasing the bias to the peak of the PIR at -1.5 V, it becomes possible to also empty the SOMO ψ2, such that the corresponding STM image shows the superposition of ψ1 and ψ2, that is, |ψ1| 2 + |ψ2| 2 (ref.
). Similarly, the NIR corresponds to transitions between 5 0 and 5 -1 . At the NIR onset of 1.3 V, only electron attachment to ψ2 is energetically possible. At 1.6 V, electron attachment to ψ1 also becomes possible, and the corresponding STM image shows the superposition of ψ1 and ψ2. The observation of the orbital densities of SOMOs, and not the hybridized HOMO and LUMO, proves the open-shell ground state of assigned 5OS.
Measurements of the monoradical species with a doublet ground state are shown in Supplementary Fig. . Unexpectedly, another species of 5 was also experimentally observed that exhibited a closedshell ground state. In contrast to 5OS, where the frontier orbitals correspond to the SOMOs ψ1 and ψ2, DFT calculations predict orbitals of different shapes and symmetries for 5para and 5ortho, denoted as α and β and shown in Fig. .
For 5ortho, α and β correspond to HOMO and LUMO, respectively. The orbitals are inverted in energy and occupation for 5para, where β is the HOMO and α is the LUMO. Fig. shows an AFM image of 5 that we assign as 5para. We experimentally infer its closed-shell state first by using qualitative bond order discrimination by AFM.
In high-resolution AFM imaging, chemical bonds with higher bond order are imaged brighter (that is, with higher frequency shift Δf) due to stronger repulsive forces, and they appear shorter . In Fig. , we label seven bonds whose bond orders show significant qualitative differences in the calculated 5ortho, 5para (Fig. ) and 5OS (Fig. ) geometries.
In 5para, the bonds b and d exhibit a higher bond order than a and c, respectively. This pattern is reversed for 5ortho, while the bond orders of the bonds a-d are all similar and small for 5OS. Furthermore, in 5para bond f exhibits a higher bond order than e, while in 5ortho and 5OS bonds e and f exhibit similar bond order (because they belong to Clar sextets).
Finally, the bond labeled g shows a higher bond order in 5para than in 5ortho and 5OS. The AFM image of assigned 5para shown in Fig. indicates higher bond orders of the bonds b, d and f compared to a, c and e, respectively. In addition, the bond g appears almost point-like and with enhanced Δf contrast compared to its neighboring bonds, indicative of a high bond order (see Supplementary Fig. for height-dependent measurements).
These observations concur with the calculated 5para geometry (Fig. ). Importantly, all these distinguishing bond-order differences are distinctly different in the AFM image of 5OS shown in Fig. , which is consistent with the calculated 5OS geometry (Fig. ). In the AFM images of 5OS (Fig. and Supplementary Fig. ), the bonds a-d at the pentagon apices appear with similar contrast and apparent bond length.
The bonds e and f at one of the terminal benzenoid rings also exhibit similar contrast and apparent bond length, while the central bond g appears longer compared to assigned 5para. Further compelling evidence for the closed-shell state of assigned 5para is obtained by STM and STS. dI/dV(V) spectra acquired on an assigned 5para species exhibit two peaks centered at -1.4 V (PIR) and 1.6 V (NIR) (Fig. ).
STM images acquired at these biases (Fig. ) show the orbital densities of β (-1.4 V) and α (1.6 V). First, the observation of α and β as the frontier orbitals of this species, and not the SOMOs, strongly indicates its closed-shell state. Second, consistent with AFM measurements that indicate good correspondence to the calculated 5para geometry, we observe β as the HOMO and α as the LUMO.
For 5ortho, α should be observed as the HOMO and β as the LUMO. We did not observe molecules with the signatures of 5ortho in our experiments. We observed molecules in open-(5OS, Fig. ) and closed-shell (5para, Fig. ) states in similar occurrence after their generation from 6 on the surface. We could also switch individual molecules between open-and closed-shell states as shown in Fig. and Supplementary Fig. .
To this end, a change in the adsorption site of a molecule was induced by STM imaging at ionic resonances, which often resulted in movement of the molecule. The example presented in Fig. shows a molecule that was switched from 5para to 5OS and back to 5para. The switching is not directed, that is, we cannot choose which of the two species will be formed when changing the adsorption site, and we observed 5OS and 5para in approximately equal yields upon changing the adsorption site.
The molecule in Fig. is adsorbed on top of a defect that stabilizes its adsorption geometry on bilayer NaCl. At defect-free adsorption sites on bilayer NaCl, that is, without a third layer NaCl island or atomic defects in the vicinity of the molecule, 5 could be stably imaged neither by AFM nor by STM at ionic resonances (Supplementary Fig. ).
Without changing the adsorption site, the state of 5 (open-or closedshell) never changed, including the experiments on bilayer NaCl/Ag(111) and Cu(111), on which the charge state of 5 could be switched (Supplementary Figs. ). Also on these lower work function surfaces, both open-and closed-shell species were observed for 5 0 and both showed charge bistability between 5 0 (5OS or 5para) and 5 -1 (Supplementary Figs. ).
The geometrical structure of 5 -1 probed by AFM, and its electronic structure probed by STM imaging at the NIR (corresponding to transitions between 5 -1 and the dianionic state 5 -2 ), are identical within the measurement accuracy for the charged species of both 5OS and 5para. When cycling the charge state of 5 between 5 0 and 5 -1 several times, we always observed the same state (5OS or 5para) when returning to 5 0 , provided the molecule did not move during the charging/discharging process.
Based on our experimental observations we conclude that indeno[1,2-a]fluorene (5), the last unknown indenofluorene isomer, can be stabilized in and switched between an open-shell (5OS) and a closed-shell (5para) state on NaCl. For the former, both DFT and QD-NEVPT2 calculations predict a triplet electronic configuration.
Therefore, 5 can be considered to exhibit the spin-crossover effect, involving magnetic switching between high-spin (5OS) and low-spin (5para) states, coupled with a reversible structural transformation. So far, the spin-crossover effect has mainly only been observed in transition-metal-based coordination compounds with a near-octahedral geometry .
The observation that the switching between open-and closedshell states is related to changes in the adsorption site but is not achieved by charge-state cycling alone, indicates that the NaCl surface and local defects facilitate different electronic configurations of 5 depending on the adsorption site.
Gas-phase QD-NEVPT2 calculations predict that 5OS is the ground state, and the closed-shell 5para and 5ortho states are 0.11 and 0.21 eV higher in energy. The experiments, showing bidirectional switching between 5OS and 5para, indicate that a change in the adsorption site can induce sufficient change in the geometry of 5 (leading to a corresponding change in the ground state electronic configuration) and thus induce switching.
Switching between open-and closed-shell states in 5 does not require the breaking or formation of covalent bonds , but a change of adsorption site on NaCl where the molecule is physisorbed. Our results should have implications for single-molecule devices, capitalizing on the altered electronic and chemical properties of a system in π-diradical open-shell and closed-shell states such as frontier orbital and singlet-triplet gaps, and chemical reactivity.
For possible future applications as a single-molecule switch, it might be possible to also switch between open-and closed-shell states by changing the local electric field, such as by using chargeable adsorbates . Scanning probe microscopy measurements and sample preparation. STM and AFM measurements were performed in a home-built system operating at base pressures below 1×10 -10 mbar and a base temperature of 5 K. Bias voltages are provided with respect to the sample.
All STM, AFM and spectroscopy measurements were performed with carbon monoxide (CO) functionalized tips. AFM measurements were performed in non-contact mode with a qPlus sensor . The sensor was operated in frequency modulation mode with a constant oscillation amplitude of 0.5 Å. STM measurements were performed in constantcurrent mode, AFM measurements were performed in constant-height mode with V = 0 V, and I(V) and Δf(V) spectra were acquired in constant-height mode.
Positive (negative) values of the tip-height offset Δz represent tip approach (retraction) from the STM setpoint. All dI/dV(V) spectra are obtained by numerical differentiation of the corresponding I(V) spectra. STM and AFM images, and spectroscopy curves, were post-processed using Gaussian low-pass filters.
Au(111), Ag(111) and Cu(111) surfaces were cleaned by iterative cycles of sputtering with Ne + ions and annealing up to 800 K. NaCl was thermally evaporated on Au(111), Ag(111) and Cu(111) surfaces held at 323 K, 303 K and 283 K, respectively. This protocol results in the growth of predominantly bilayer (100)-terminated islands, with a minority of trilayer islands.
Sub-monolayer coverage of 6 on surfaces was obtained by flashing an oxidized silicon wafer containing the precursor molecules in front of the cold sample in the microscope. CO molecules for tip functionalization were dosed from the gas phase on the cold sample. Density functional theory calculations. DFT was employed using the PSI4 program package .
All molecules with different charge (neutral and anionic) and electronic (open-and closed-shell) states were independently investigated in the gas phase. The B3LYP exchangecorrelation functional with 6-31G basis set was employed for structural relaxation and singlepoint energy calculations. The convergence criteria were set to 10 −4 eV Å −1 for the total forces and 10 −6 eV for the total energies.
Multireference calculations. Multireference calculations were performed on the DFToptimized geometries using the QD-NEVPT2 level of theory , with three singlet roots and one triplet root included in the state-averaged calculation. A (10,10) active space (that is, 10 electrons in 10 orbitals) was used along with the def2-TZVP basis set .
Increasing either the active space size or expanding the basis set resulted in changes of about 50 meV for relative energies of the singlet and triplet states. These calculations were performed using the ORCA program package . Nucleus-independent chemical shift (NICS) calculations. Isotropic nucleus-independent chemical shift values were evaluated at the centre of each ring using the B3LYP exchangecorrelation functional with def2-TZVP basis set using the Gaussian 16 software package .
Starting materials (reagent grade) were purchased from TCI and Sigma-Aldrich and used without further purification. Reactions were carried out in flame-dried glassware and under an inert atmosphere of purified Ar using Schlenk techniques. Thin-layer chromatography (TLC) was performed on Silica Gel 60 F-254 plates (Merck).
Column chromatography was performed on silica gel (40-60 µm). Nuclear magnetic resonance (NMR) spectra were recorded on a Bruker Varian Mercury 300 or Bruker Varian Inova 500 spectrometers. Mass spectrometry (MS) data were recorded in a Bruker Micro-TOF spectrometer. The synthesis of compound 6 was developed following the two-step synthetic route shown in Supplementary Fig. , which is based on the preparation of methylene-bridge polyarenes by means of Pd-catalyzed activation of benzylic C-H bonds .
Supplementary Figure | Synthetic route to obtain compound 6. The complex Pd2(dba)3 (20 mg, 0.02 mmol) was added over a deoxygenated mixture of 1,3-dibromo-2,4-dimethylbenzene (9, 100 mg, 0.38 mmol), boronic acid 10 (178 mg, 1.14 mmol), K2CO3 (314 mg, 2.28 mmol) and XPhos (35 mg, 0.08 mmol) in toluene (1:1, 10 mL), and the resulting mixture was heated at 90 °C for 2 h.
After cooling to room temperature, the solvents were evaporated under reduced pressure. The reaction crude was purified by column chromatography (SiO2; hexane:CH2Cl2 9:1) affording 11 (94 mg, 76%) as a colorless oil. The complex Pd(OAc)2 (7 mg, 0.03 mmol) was added over a deoxygenated mixture of terphenyl 11 (90 mg, 0.27 mmol), K2CO3 (114 mg, 0.83 mmol) and ligand L (26 mg, 0.06 mmol) in NMP (2 mL).
The resulting mixture was heated at 160 °C for 4 h. After cooling to room temperature, H2O (30 mL) was added, and the mixture was extracted with EtOAc (3x15 mL). The combined organic extracts were dried over anhydrous Na2SO4, filtered, and evaporated under reduced pressure. The reaction crude was purified by column chromatography (SiO2; hexane:CH2Cl2 9:1) affording compound 6 (8 mg, 11%) as a white solid. in AFM imaging due to their reduced adsorption height compared to the rest of the carbon atoms.
We attribute this observation to the significantly different lattice parameter of Cu(111) (2.57 Å) compared to Au(111) and Ag(111) (2.95 Å and 2.94 Å, respectively) , such that the apical carbon atoms of the pentagonal rings of 5 adsorb on the on-top atomic sites on Au(111) and Ag(111), but not on Cu(111).
Our speculation is based on a previous study of polymers of 1 on Au(111) by Di Giovannantonio et al. , where both tilted and planar individual units of 1 were observed depending on whether the apical carbon atoms of the pentagonal rings in 1 adsorbed on the on-top or hollow sites of the surface, respectively.
Given the strong molecule-metal interaction, we found no electronic state signatures of 5 on all three metal surfaces. STM set point for AFM images: V = 0. e, Frontier orbital spectrum of 5 -1 . In the anionic state, ψ2 becomes doubly occupied and ψ1 is the SOMO. Filled and empty circles denote occupied and empty orbitals, respectively.
For each panel, zero of the energy axis has been aligned to the respective highest-energy occupied orbital.","['Yes, individual molecules of indeno[1,2-a]fluorene can switch between open-shell and closed-shell states by changing their adsorption site on the surface.']",5523,multifieldqa_en,en,,566881d2138d7e29cd6dd2b661b6f7ffe4c515c92fdaf837," Bistability between π-diradical open-shell and closed-shell states in indeno[1,2-a]fluorene. Authors: Shantanu Mishra, Manuel Vilas-Varela, Leonard-Alexander Lieske, Ricardo Ortiz, Igor Rončević, Florian Albrecht, Diego Peña, Leo Gross. Paper published in the Journal of Biological Chemistry and Molecular Materials (CiQUS), University of Santiago de Compostela. Back to Mail Online home. Back To the page you came from. The article was first published on July 25, 2013. Back into the page.. The paper was last updated on August 2, 2013, at 11:30 a.m. local time (GMT) and is published under the headline ‘Bistable open- shell and closed shell states inIndeno’. The study was co-authored by Mishra (IBM Research Europe -Zurich) and Vilas Varela (Department of Organic Chemistry, Center for Research in Biological Chemistry, CiQUS) The article is also available in the online edition of the journal, ‘Chemistry and Biomolecular Materials’, which is published by the University of Oxford. The online edition also includes the author’s review of the book, “The Chemistry of Indenofluorene,” which was published in July 2013. The book is available for pre-order on Amazon.com, with a free download of the paperback version of the paper (with a free Kindle download) available on the same day. The free download also includes a free PDF of the article (with no download limit). The freeazon.com copy of this article includes the free Kindle version (‘Kindle’ version) with the free download code (“Kindle Edition”). The paper is also published on the publisher’S website, which can be accessed by clicking here. The author has no affiliation with the company or any of the institutions mentioned in the article. The views expressed in this article are those of the author and of the publisher, and do not necessarily reflect those of IBM Research Europe – Zurich or IBM Research - Zurich. The opinions of the authors, however, are their own, and they do not represent the views of IBM Europe - Zurich or the IBM Research – Zurich. We would like to make clear that the authors’ views on the subject matter are of a purely academic nature. We are happy to clarify that the study was published on an open-source, peer-reviewed basis and that the author is a member of the IBM research team. We hope that the findings will be of interest to the academic community and that they will be used to improve the understanding of the chemistry and biomedicine in the future. The manuscript was originally published on August 25, 2012, in the journal “Chemistry & Biomedicines. and the accompanying book ‘The Chemistry of Indeno[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,28,.’ and the book has been republished on August 28, 2013 in the Online edition. We have been asked to clarify some of the key points in the paper. These include the fact that all of the isomers, being derived through benzannelation of indacenes, contain a central QDM moiety. Also shown are STM images of assigned 5OS at biases. spectra acquired on a species of 5 assigned as 5para, along with the corresponding dI/dV(V) spectra. Open feedback parameters: V = -2 V, I = 0.15 pA (negative bias side) and V = 2.2 V. STM and STS data in c and d are acquired on the same species, while the AFM data in e is acquire on a different species. 13C NMR-DEPT (75 MHz, CDCl3, 1:1 mixture of atropisomers) δ: 141.2 (C), 141.1 (C) and 140.0 (C). HRMS: C20H16Cl2; calculated: 327.0702.Scale bars: 10 Å (d) and 5 Å. (e,g) ppm.MS (APCI) m/z (%): 327 (M+1, 100).HRMS:C20H15Cl2. calculated:327.0705.Scale bar: 10Å (D) and. 5 É (e) ppm (300 MHz,CDCl3) ppm: 7.51 (m, 2H), 7.40 -7.28 (M, 5H) and 7.27 (s, 3H), 1.77 (s,. 3H) (d, J = 7.7 Hz, 1H). 7.85 (d,. J =. 7.5 Hz,1H),7.78 (d). J =7.7Hz, 1.5H, 7.65 (d), J.7.5 (D, 7d, 7h) and J.5.7 (J) (h, J) (f) (g) (H, H, H) (i) (j) (K) (k) (L) (M) (B) (W) (G) (l) (m) (A) (b) (E) (F) (R) (S) (T) (w) (Z) (z) (a) (D), (B), (E), (G), (H), (C, E) (N) and (W), (g), (A), (K), (F), (M), (R), (W, G) (P) (J), (J, K, K) (r) (U) (C: 1.0.1, (C): 1.1.1,. (C);. (B:1.2, (B): 2.3.2,. (B). (R:1). (A, K), (S, K). (E, G). (K, R) (c, E, D, (W). (B, G, H). (D:1) (O) (I, E). (G, B). (C,. E, K,. (R). (H) ) (E.S. (D.S.) (C.S). (F, E.E. (H). (M). (S.E.) (G.S) The bonds labeled in bold exhibit a higher bond order than their neighboring labeled bonds in 5 para. For the bond pairs a/b, c/d and e/f, the bonds labeled for highlighting bond order differences between 5Para and 5ortho are shown in bold. Non-benzenoid polycycles may lead to local changes in strain, conjugation, aromaticity, and induce an open-shell ground state of the corresponding PCHs. Indenofluorenes represent model systems to study the interplay between aromaticity and magnetism at the molecular scale. Derivatives of 1-4 have been realized in solution, while 1-3 have also been synthesized on surfaces and characterized using scanning tunneling microscopy (STM) and atomic force microscopy. calculations indicate that 1 and 5 should exhibit open- shell ground states. Our research is motivated by theoretical studies on their use as non-linear optical materials and as candidates for singlet fission in organic photovoltaics. We report the generation and characterization of unsubstituted 5 to exhibit thinnest, least dense, and most stable open-Shell ground states of the 5th generation of P CHs. We conclude with a discussion of the potential uses of indenofLUorenes in organic electronic devices and nanomaterials, such as solar cells, batteries, LEDs, and photodiodes, as well as in the development of new nanotube-based solar cells and solar cells. The study was funded by the National Institute of Standards and Technology (NIST) and the University of California, Los Angeles (UCLA) We thank the U.S. Department of Energy for their support and assistance with the study of the inden ofluorene family of hydrocarbons, including the NIST-funded study of indeno[1,2-a]indene (Fig. 1) and indeno [2,1-a-indene) ( Fig. 2). We conclude by discussing the potential use of the 1,2,3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 63,. 64, 65, 68, 69, 70, 70,. 74, 74, 79, 80, 79,. 80, 81, 82, 83, 84, 83,. 84, 85, 84,. 85, 86, 87, 88, 89, 90, 89,. 90, 91, 92, 93, 94, 94,. 94, 95, 96, 95,. 96, 97, 98, 100, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 111, 116, 123, 114,. 116, 113,. 114, 115, 116,. 113, 118, 119, 123,. 112, 114. 113, 116. 114, 113. 116, 115,. 116,. 117, 118,. 119, 120, 119,. 120, 120,. 115, 120. 116,. 119,. 123, 124, 121, 123. 124, 125, 126, 127, 128, 130, 135, 134, 145, 155, 156, 157, 163, 164, 165, 168, 175, 174, 177, 178, 179, 186, 189, 188, 189,. 194, 190, 195, 199, 200, 204, 215, 220, 211, 220,. 211, 217, 216, 223, 226, 217,. 217, 226,. 220, 223,. 230, 230, 233, 230,. 24, 235, 238, 233,. On insulating surfaces 5 can exhibit either of two ground states: an open-shell or a closed-shell. AFM imaging reveals molecules with two different geometries. Our experimental results are in good agreement with density functional theory (DFT) and multireference perturbation theory calculations. The generation of 5 relies on the solution-phase synthesis of the precursor 7,12-dihydro indeno[1,2-a]fluorene (6). Details on synthesis and characterization of 6 are reported in Supplementary Figs. 1-3. We observe switching between open-and closed- shell states of a single molecule by changing its adsorption site on the surface. The tip-height offset for each molecule is provided with respect to the STM setpoint, and positive (negative) values of the tip approach are shown for each species of 5 assigned as 5OS. In our work, insulating. surfaces correspond to two monolayer-thick (denoted as bilayer) NaCl on coinage metal surfaces. Blue and orange colors represent spin up and spin down densities, respectively. The bright and dark features in the trilayer NaCl island show the same island in which the same molecule is next to a trilayers. island at the same adsorbing site, which is next. to the same trilayer island. We show that the same features are also in the dark and dark island in the Trilayers. panel at the top of the Fig. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 61, 63, 64, 65, 68, 69, 70, 74, 80, 79, 80 and 81, 81, 82, 83, 84, 84 and 83, 86, 87, 88, 89, 90, 89 and 90, 91, 92, 93, 94, 94 and 94, 95, 96, 100, 96 and 96, 103, 104, 105, 106, 109, 108, 110, 111, 112, 113, 114, 116, 123, 113 and 114, 114 and 115, 11 and 12, 12. The same calculations also predict that 5 should possess a triplet ground state.Therefore, 5 would qualify as a Kekulé triplet, of which only a handful of examples exist . However, definitive synthesis of 5 has never been reported so far. On-surface generation of a derivative of 5, starting from truxene as a precursor, was recently reported . STM data on this compound was interpreted to indicate its open- shell ground state, but the results did not imply the ground state of unsubstituted 5. We infer the existence of these two groundStates based on high-resolution AFM images with bond-order discrimination and STM imaging of molecular orbital densities. We also show molecular orbital density of the highest occupied molecular orbital (HOMO) and lowest unoccupied orbital (LUMO) for the closed-Shell geometry. 5 adopts neutral charge state on the high work function bilayer NaCl/Au(111) surface. 5 exhibits charge bistability between 5 0 and the anionic state 5 -1 on the lower work function BilayerNaCl/Ag(111), Cu(111). 5 exhibits bond length alternations in line with the presence of a para-or an ortho-QDM moiety in the as-indacene core of the closed-shell resonance structures of 5 (Fig. ) The triplet electronic configuration of 5OS is the lowest-energy state, with the openshell singlet configuration 90 meV higher in energy. 5OS in the triplet configuration exhibits local aromaticity at the terminal benzenoid rings, while 5para and 5ortho all display antiaromaticity (Supplementary Figs. 1 and 2). The choice of the insulating surface determines the charge state of 5. We first describe experiments on 5 on bilayer naCl/au( 111), where 5 exhibits a geometry corresponding to the calculated 5OS geometry, and an open-shell electronic configuration. We compare the experimental data on this species to calculations on 5OS with a triplet configurations, as theory predicts a tripleT ground state for 5OS. The results of QD-NEVPT2 calculations qualitatively match with DFT calculations. We also performed nucleus-independent chemical shift calculations to probeLocal aromaticity of 5 in the openand closed- shell states. The energy differences between the open-and. closed-Shell states are substantially reduced in QD and RB3LYP/6-31G calculations, with 5 para and fiveortho only 0.11 and 0.21 eV higher. in energy, respectively, compared to 5 OS in the three-combined configuration. The choice. of the bilayer surface determines charge state, and 5 adopts a neutral chargeState on the work function NaCl /aU(111, NaClCl/ClCl, and CuCl /ClCl. It is reported in the main text that 5 is a charge-bistable 5. The characterization of charge bistsable 5 is also reported in Supplementary Fig. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 60 and 58, 61, 62, 63, 62 and 63, 64, 63 and 64, respectively. We assign a bond-order that we assign as 5OS, where the bond lengths in each ring agree with 5OS' resonance structure. The bond lengths within the pentagonal rings of 5 are notably longer in the pentagon.in g correspond to Cl -and Na + ions. The calculated frontier orbitals correspond to the SOMOs ψ1 and ψ2 (Fig., 1), whose spin up levels are occupied and the spin down levels are empty.  conductance spectra (dI/dV(V), where I and V denote the tunneling current and bias voltage, respectively) acquired on assigned 5OS exhibit two peaks centered at -1.5 V and 1.6 V. To draw a correspondence between the STM images and the molecular orbital densities, we consider tunneling events as many-body electronic transitions between different charge states of 5OS. Unexpectedly, another species of 5 was also experimentally observed that exhibited a closedshell ground state. We experimentally infer its closed-shell state first by using qualitative bond order discrimination by AFM. In 5para, the bonds b and d exhibit a higher bond order than a and c, respectively. This pattern is reversed for 5ortho, while the bond orders of the bonds a-d are all similar and small for 5 OS. Furthermore, the bond labeled g appears almost point-like and with enhanced Δf contrast compared to its neighboring bonds, indicative of a high bond order for height-dependent measurements. These observations concur with the calculated 5 para geometry (Fig. 1) which is consistent with the 5OS geometry ( Fig. 2) The bonds e and f exhibit similar apparent length and contrast at one terminal benzenoid rings, while t-d at the pentagon apices appear with similar contrast and apparent length at the t-agon rings. In the AFM images of 5, a and f appear brighter (that is, with higher frequency shift Δf) due to stronger repulsive forces, and they appear shorter . In Fig. 1, we label seven bonds whose bond orders show significant qualitative differences in the calculated5ortho and 5OS geometries. The AFM image of assigned 5Para shown in Fig. 3 indicates higher bond orders in bonds b, d and f compared to a, c and e, respectively compared to 5OS, and in 5ortha and 5 OS it is the same as a, d, and e. The observation of the orbital density of SOMOs, and not the hybridized HOMO and LUMO, proves the open- shell ground state of assigned5OS. In contrast to 5 OS, where the frontier orbitals correspond to the SOMOs ψ1 and ψ2, DFT calculations predict orbitals of different shapes and symmetries for 5parA and 5orthO, denoted as α and β and shown inFig. 4. The orbitals are inverted in energy and occupation for 5 Para, where β is the HOMo and α is the LUMo, respectively, and the orbitals in 5OS are inverted for 5Paro, where α is HOMM and LOMO. The bonds a and d are similar to those of 5 OS and 5paro, but the bond order of 5 Paro is higher than 5OS and 5 paro. The bond order in 5ParA is also higher than that of 5paror, 5OS or 5Paror, but not 5Pararor, and this difference is not consistent with 5OS' bond order. We conclude that 5 Pararor is a monoradical species with a doublet ground state, and we label it as 5par a. We also label 5OS as 5OS in Supplementary Fig. 5. For more details, see the Supplementary Figs. 1 and 2. For the full study, please see the online version of this article. The geometrical structure of 5 -1 probed by AFM and its electronic structure probed. by STM imaging at the NIR are identical within the measurement accuracy for the charged species of 5OS and 5para. When cycling the charge state of 5 between 5 0 and 5 - 1 several times, we always observed the same state when returning to 5 0. The switching is not directed, that is, we cannot choose which of the two species will be formed when changing the adsorption site. 5 can be considered to exhibit the spin-crossover effect, involving magnetic switching between high-spin (5OS) and low-spin. (5para) states, coupled with a reversible structural transformation. We conclude that indeno[1,2-a]fluorene (5), the last unknown indenofluorene isomer, can be stabilized in and switched between an open-shell (5 OS) and a closed- shell (5 para) state on NaCl. The observation that the switching between open-and closedshell states is not achieved by charge-state cycling alone, indicates that the NaCl surface and local defects of 5 facilitate different configurations of 5. Gas-phase QD-NEVPT2 calculations predict that 5OS is the ground state, and 5ortho states are 011 and 021. The experiments on bilayer NaCl/Ag(111) and Cu(111), on which the chargeState of 5 could be switched (Supplementary Figs. 1 and 2), also showed charge bistability between. 5 0 (5 0 or 5Para) and 5 1 (5 -1) (Fig. and Supplementary Fig. 1). We did not observe molecules with the signatures of 5ortha in our experiments. We observed molecules in open-(5OS, Fig. ) and closed-shell states in similar occurrence after their generation from 6 on the surface. The example presented in Fig. shows a molecule that was switched from 5parA to 5OS. and back to 5par a. The experiment has only been observed in transition-metal-based coordination compounds with a near-octahedral geometry (e.e central bond g appears longer compared to assigned 5paras) ( Fig. and Fig. 2). The experiments have only been seen in transition related compounds with near-Octahedral geometries (eg. octahedral bonds) (fig. 3) and near-Decahedral structures (fig 4) ( fig. 5). The results are consistent with AFM measurements that indicate good correspondence to the calculated 5paran geometry (fig 5) and STM images of 5 para (fig 6) (F.S. and F.C. ) (Fig 7) The experiments were carried out at the University of California, Los Angeles (UCLA) and at the California Institute of Technology (C.L.A.) ( Fig 7) and in the National Institutes of Standards and Technology (N.I.T). (Fig 8) and the results have been published in the Journal of Chemical Technology (NCI) (f.S) and N.E.) (Fig 9) and (fig 10) ( respectively). We are the first to show that 5 paras can be switched between open and closed shell states in this way. We are also the first group to show the switching is related to changes in the Ad-Sorption Site. Switching between open-and closed-shell states in 5 does not require the breaking or formation of covalent bonds. A change of adsorption site on NaCl where the molecule is physisorbed. Our results should have implications for single-molecule devices, capitalizing on the altered electronic and chemical properties of a system in π-diradical open-shell and closed- shell states. For possible future applications, it might be possible to switch between open and closed states by changing the local electric field, such as by using chargeable adsorbates. For more details on the study, please visit http://www.sciencemag.org/cgi-bin/suppliers/sciencemag/article/v2.2/s1/S1/s2/S10/s10/S12/S11/s12/s13/s14/s15/s16/s17/s18/s19/s20/s21/s22/s3/s23/s24/s25/s26/s27/s28/s29/s30/s31/s32/s33/s34/s35/s36/s37/s38/s39/s40/s41/s42/s43/s4/s5/s6/s7/s8/s9/s09/s11/t/s0,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21,s24,s25,s28,s29,s32,s34,s31,s38,s39,s40,s41,s26,s3,s27,s30,s1,s36,s2,s33,s44,s37,s50,s35,s42,s43,s46,s47,s48,s49,s54,s55,s56,s53,s58,s59,s60,s61,s64,s65,s62,s63,s68,s75,s70,s74,s79,s80,s82,s83,s84,s85,s88,s89,s90,s91,s92,s94,s95,s99,s110,s100,s120,s114,s125,s130,s131,s133,s128,s140,s135,s144,s150,s145,s151,s153,s160,s165,s155,s164,s185,s195,s205,s190,s255,s196,s208,s223,s225,s260,s240,s230,s234,s239,s256,s229,s242,s233,s244,s251,s246,s254,s261,s270,s275,s268,s284,s259,s300,s288,s273,s290,s304,s340,s305,s342,s335,s390,s401,s404,s423,s425,s424,s414,s535,s460,s525 Reactions were carried out in flame-dried glassware and under an inert atmosphere of purified Ar using Schlenk techniques. Thin-layer chromatography (TLC) was performed on Silica Gel 60 F-254 plates (Merck)Column chromatography was performed. on silica gel (40-60 µm) Nuclear magnetic resonance (NMR) spectra were recorded on Bruker Varian Mercury 300 or Bruker. Varian Inova 500 spectrometers. Mass spectrometry (MS) data were recorded in a Bruker Micro-TOF spectrometer. The synthesis of compound 6 was developed following the two-step synthetic route shown in Supplementary Fig. , which is based on the preparation of methylene-bridge polyarenes by means of Pd-catalyzed activation of benzylic C-H bonds. The reaction crude was purified by column chromatography. affording compound 6 (8 mg, 11%) as a white solid in AFM imaging due to their reduced adsorption height compared to the rest of the carbon atoms. We found no electronic state signatures of 5 on all three metal surfaces. For each panel, zero of the energy axis has been aligned to the respective highest-energy occupied orbital. Filled and empty circles denote occupied and empty orbitals, respectively. We attribute this observation to the significantly different lattice parameter of Cu(111) (2.57 Å) compared to Au( 111) and Ag(111), such that the apical carbon atoms of the pentagonal rings of 5 adsorb on the on-top atomic sites on Au(111 ) and Ag (111) but not on Cu('111') (Fig. 2). We also found that in the anionic state, ψ2 becomes doubly occupied and ψ1 is the SOMO. (Fig 2) (Figure 2) In the STM set point for AFM images: V = 0. e, Frontier orbital spectrum of 5 -1 . In the an ionic state, the orbitals of 5 become doublyoccupied and ω2 is the. SOMO (SOMO) (Fig 3) (SomO) is the empty orbital. (Figure 3) We found that the structure of 5 is not the same as that of the other two materials, Au('111' and 'Au('111)' (Fig 4) and 'Ag('111'), but the structure is the same (Fig 5) (fig. 3) and (fig 4) (somo) is similar to that of Au ('111') and Ag ('111' (Fig 6) ( Fig. 5) The study was based on a previous study of polymers of 1 on Au(""111) by Di Giovannantonio et al. , where both tilted and planar individual units of 1 were observed depending on whether the apico carbon atoms in 1 adsorbed on the. on- top or hollow sites of the surface."
What field does Danny work in in Tennessee?,"My Aspergers Child: COMMENTS & QUESTIONS [for Feb., 2017]
I emailed you a while back and you mentioned that I could email when I needed to. Thank you. I last wrote you in December that my son became involved in a dispute involving the local police. We have had 3 court dates. It keeps delaying due to not being able to come to an agreement. But the attorney, even though he was just vaguely familiar with Aspergers, has been very good with Craig. He has the compassion and excellence that is needed here. What started out very bad is turning into a good thing. It will probably take another 90 days or more.
But Craig is working hard. Too hard sometimes. He goes to therapy 3 times a week. Doing excellent. He's more focused and can calm down easier. He's got a lot on his plate but has support from his family. From his attorney. From therapy. And from his work.
He has been renting a room from a lady who has a son with ADHD. It is good for him. I'm a little worried though because since she smokes he wants to find his own place. With all the costs he has to balance it out financially. That is good. I can't help him more than I am which is good. He is stepping up and taking responsibility. He is listening much better.
He is going to have an evaluation today to get an accurate diagnosis. I understand that is a little difficult since he is an adult. Also the PTSD may cover it over. The attorney stated it would help to have the diagnosis.
Aware this is a long update, but thanks for reading. I am fighting much guilt still but I have a lot of peace now. My daughter and her 4 year old son also have Aspergers symptoms. So my life chapters may not close for a while. :-)
My name is Mac. I'm sure you're quite busy, so I'll get right to it I just wanted to pass on compliments on My Aspergers Child and your post, How to Implement the GFCF Diet: Tips for Parents of Autistic Children.
Me and my wife absolutely loved it!
I got a facebook message from him today begging to be able to come home saying he misses home and he will change. He says he will follow rules now. I stated to him the simple rules he has to follow which were - No weed in my house, or smoked in my house, coming home at curfew, going to school, no skipping, no drugs at school, and to drop the attitude of I am 17 I can do whatever I want.
I have made it very clear that if I see any drugs in my home I will be calling the police, as well as if I see signs of it being sold by him I will report him. (He has never had selling amounts in my house, ... I believe it's being kept at his ""friends"" which of course I have no proof of....I just know it is not here.
I know my battle is not over by a long shot, I am sure we will have more consequences and possibly another being kicked out, but I am going to think positive and hope that he learned some form of a valuable lesson here.
Thank you so much for the guidance, never in a million years did I ever think I'd be on this side, (the one needing the help, as I am the one who helps.)
I am going to go back to the start of the program like I said earlier and keep notes close by for reference.
Thanks for all you do, helping us all with ODD children/teens
I have a small company providing educational support services to a few families who have children with various disabilities in Ohio. One of the families has multiple adopted children of whom several have significant attachment disorders including RAD. As an experienced teacher and foster parent I have some experience in working with children who have extensive trauma backgrounds. However, I could use additional training. Also working with these children are two staff members with minimal background in attachment disorders who would also benefit from training primarily in behavior management. The primary caregiver to the children does a wonderful job managing their needs. In order to further develop team cohesion, I'm hoping to include her in any training as well.
Is it possible to schedule such a training session with you? If so, please let us know what will work for you including time, place, and cost. Thank you for your assistance.
I just listed to your tapes on dealing with an out of control, defiant teen. I'd like to ask your advice on a particular situation we have. Our 15 year old daughter is smoking pot almost every day at school. Because we had no way to control the situation, we told her, fine, go ahead and smoke weed. However, you will no longer receive the same support from us. You will not have your phone, lunch money to go off campus (she has an account at the school for the cafeteria she can use), and you will be grounded until you can pass a drug test. We will not be testing you except for when you tell us you are ready to be tested. She is now saying she's suicidal because she feels so isolated, yet she continues to smoke weed. In fact, she tried to sneak out last night but was foiled by our alarm system. For the particular drug test we have, I read it takes about 10 days of not smoking to pass the test. What would you do? Please advise.
I am having a problem with my 18 year old son, Danny, with high functioning autism. We finally had him diagnosed when he was 16 years old. I always knew something was going on with him but the doctors misdiagnosed him as bipolar. It's been 2 years now and he will not accept his diagnosis. He won't talk about it and when I try to bring it up he gets very angry. I've tried telling him that it's not a bad thing, that there's been many, many very successful people with Aspergers. He won't tell anyone and refuses to learn about managing life with it. He once shared with me that the other kids at school use it as an insult, like saying someone is so autistic when they do something they don't approve of. So he doesn't want anyone to know. He's turned down services that could help him. He has a girlfriend, going on 8 months. He won't tell her and they're having problems arguing a lot and I wonder if it would help for her to know.
I'm sad that he thinks it's a life sentence to something horrible instead of accepting, embracing it and learning about it more so he maybe can understand why he's struggling. I told him that he doesn't need to shout it out to the whole world but he won't even accept it himself.
I don't know how to help him with it and because he's almost 19 I have limited control now. It's made my life easier knowing what we're dealing with and I think his life would be easier is he accepted it.
Please help me help him.
I am a clinical psychologist in NYC who now has several (!!) children I see who have RAD. In 20 years of practice, I’d seen only one case. Now, I have at least three children with this. I have no training, per se, in working with this children though I know about setting structure, consistency, etc. I do a lot of work with parents about parenting. I work primarily within the school setting in a charter school whose mission is to educate children on the autism spectrum in a mainstream setting. We use Michelle Garcia Winner’s social thinking program with our ASD kids. I also work with gen ed kids in the school who are at-risk; the school is in the inner city from where the majority of our non-ASD kids live.
It would have been so much easier to mention to my adult son that I think (I know he does, but want to ease into the subject)
he has Asperger's when we were living together two years ago. He has since moved to Tennessee working in his field of interest
which is 3-D printing and software development. I am so happy for him that he has found his way into a job that he truly enjoys
even though he's socially isolated.
He's not diagnosed and does not know he has it. How I know is his classic symptoms being sensory issues (fabric feeling like sandpaper)
communication difficulties, meltdowns and much more. Throughout his childhood I just felt he was a bit different. Nothing major stood out and time
just passes, misdiagnosis of ADHD, low frustration, etc. We've talked about his ADHD numerous times (which I now know he doesn't have).
It's so much easier to communicate with him now that I know he has Asperger's. I keep it ""slow and low"" in talking, with long moments
of silence and then we connect. It's really too bad that Asperger's got a diagnostic code back in the 90's, yet all the so called doctors,
physiologist's, etc, didn't know how to diagnose it. Too bad.
There seems to be no one answer to ""should I tell my adult son he has Asperger's"" from a few specialists I asked. He is typical Asperger,
complicated, highly intelligent (high IQ), anxiety at times, socially isolated, hard to make friends. Not knowing how he will react is the hard part.
How will he be better off knowing he has it? Do I wait to tell him in person, or ease into it with him over Skype? He likes direct, honest, concrete communication.
Why is this so hard for me? Maybe because no one know's if he is going to be better off knowing or not. Do you know if people are better off
knowing? I try to get up the courage to just let him know, then I back down.
I have been searching the web looking for advice and came upon your site. I am trying to read blogs, websites, books, and articles to help guide me. I was so happy when you said that I could ask you a question. My husband and I are struggling with my 27 year old son who lives with us.
Kyle is the youngest of 4 sons. He is a college graduate but never could find the ""right"" job. He has always been quiet and never had a lot of friends. Two years ago, his girlfriend broke up with him. Kyle had an online gambling addiction and was using pot all the time. After the breakup, Kyle was very depressed and started using heroin and finally told my husband he was using. He is now seeing a psychiatrist who has him on suboxone and antidepressants. He is also seeing a psychologist weekly for counseling but it does not seem to be helping.
Last October,, Kyle lost his job, got drunk, and was agitated and came home , fighting with us, damaging our home and being verbally abusive. My other son , age 32, who also lives with us called the police and Kyle got arrested. He is currently in the family court system. He went through an anger management course and now is in substance abuse classes. Kyle continues to verbally abusive to me and blame me for everything. He says he ""hates me ""and calls me terrible names. At times, he pushes my husband and intimidates me. My husband and I are so upset. We just hired an attorney for him because since he has been going to these classes, he is getting more depressed and not getting better. Kyle continues to drink while taking his meds prescribed by the psychiatrist and then he has his ""moods."" My husband and I have met once with the psychiatrist just to give him background information when Kyle started with him.
At this point, we do not know what to do. We never thought at this stage of our life, we would be supporting and spending our retirement money on adult children. I do not know why Kyle hates me, I could not have been a better mom. My husband and I have no life and just do not know what it the right path we should take. Kyle does not want anything to do with us. He spends all his time in his room playing football online.We have tried tough love versus caring and love and understanding. Do you have any advice for me?
This whole ODD and ADHD is killing me as a parent. I work in the field of adult psych and addictions so I am well educated. I have been dealing with my teen being like this for almost 3 years and I totally lost my cool today with my 17-year-old son to the point I told him he is out of the house. He can never simple rules, comes and goes as he pleases sometimes doesn't come home, just recently back in school from several suspension for drug related... I am just so exhausted. He has made me hate life, hate being a parent and sometimes I just feel like not even being here. I bought your program in hopes to it would help, I am at week three and I feel things are getting worse... what am I doing wrong??
My partner hasn't been diagnosed yet but I know he has aspergers ..day to day is a struggle . I feel I'm going crazy with how he makes me feel.Feel let down constantly. He lies alot but I've been told they can't but I know he does.I just feel trapped and unloved.We have a 4yr old daughter together and my main worry with how he is that it will effect our daughter ; (his skills as a parent are so weak.He can't disapline at all.Feel so alone .he hides it well too.I just wondered if things will get worse? He's angry so quick in arguments.Scares me etc.I can't leave as he's the main bread winner and our daughter loves him to bits.Don't know why I'm writing this..Sorry if I'm going on and not making sense :(
I wanted to let you know about a research opportunity for children, teens, and young adults with autism. I am studying the effects of Brazilian Jiu Jitsu, and psychotherapy on helping people with autism develop subjective awareness of others.
I am writing you to see if this might help someone in your practice, or to see if you might know of someone with autism who may benefit from participating in this study. The requirements of the study will be:
1. A participant should be between 7-21 years of age and have a diagnosis of Autism Spectrum Disorder.
2. The participant should enroll in an approved Jiu Jitsu Academy and attend at least two sessions a week for a period of six months.
3. The participant should enroll in social skills groups, provided by my office or be in a steady psychotherapeutic relationship in your office, at least once a week, or minimally two to three times a month.
4. The participant will be given a SRS (Social Responsiveness Scale) test at the beginning of the study, at three months, and again at six months.
If you know of anyone who might benefit from this novel approach to helping to develop social awareness in autism, please do not hesitate to contact me for further information.
I have a 10 year old daughter who has outbursts with prolonged crying almost like tantrums that 2 year olds have when they cannot express themselves.
I had her in therapy from age 6-8 years old for the same thing but I feel that the sessions didn't really help much.
She has severe sensitivities to light, sound, vibration, frequencies which trigger irritability and crying.
We changed her diet and tried getting her involved with activities but she is anti-social and prefers reading than being social. She is terrified of change even in daily routine (even that will trigger prolonged crying).
It frustrates me because I don't know what else to do with her behavior.
I've tried acupuncture (she refused at the first session); she refuses massage too.
She is an honor-roll student at school and has very minimal issues at school but if she has had a bad day it does result in a tantrum or crying and defiance.
How can I get her tested for Asperger's Syndrome?
Last night our 24 year old son with Aspergers told his dad and I that he is pulling out of the 4 college classes that he recetnly enrolled in because he has not been attending class or turning in his assignments. He paid $2800 (his own money) for tuition and I reminded him of this when he told us but it did not seem to bother him.
This is the 3rd time he has started college courses and has not completed them. (He also took some concurrent college classes while he was in high school that he failed). This is a son who basically had a 4.0 grade point average through 10th grade and got a 34 on the ACT the first time he took it.
With the news that he was once again not sticking with college courses I did not sleep well. When I got up this mornning I began looking online for help in how to deal with his situation. I found your ""Launching Adult Children With Aspergers"" and purchased it. Most of what is included are things we have done or did with our son throughout his life. I was hoping for more help so I am emailing you now in hopes of more specific ideas.
We noticed some things with our son, Taylor, as a yound child but as we had not heard of Aspergers at that time we just did what we thought would help him. As a toddler and a child at pre-school he generally went off on his own to play. When I talked to his pre-school teacher about my concerns (that I was worried he would end up a hermit) she said she did not see him being a loner and that he seemed to interact fine with others in many situations. We worked with him on making eye contact when talking with others. We explained different emotions in people's faces and mannerisms to help him know how to interact with others. We discussed the fact that people would say things that did not mean what they souneded like - such as ""I'm so hungry I could eat a horse"". As we did these things he worked hard to better understand communication with others.
During his 4th grade year I had a teacher from the gifted program ask me if I had ever heard of Aspergers. I told her that I had not heard of it. She proceeded to read me some of the charateristics and so many of them described my son. So we had him tested by the school district during the summer between 4th and 5th grade and they did find that he had Aspergers but that he was high functioning. We then set him up with and EIP which stayed with him until his sophomore year. We pulled him from it at that time because we had moved and the new district was requiring him to take one class a day that was a study class. This reduced the number of required classes he could take and he was doing fine with his studies at the time.
It was during the 2nd half of his Junior year that we noticed some of his grades going down. Then during his Senior year is when he started skipping classes and not doing assignments. We had not realized it before then but we soon became aware that he was addicted to gaming. He would go to the library or somewhere else on campus and play games on the computer rather than go to class. It was also at this time that he began lying about his actions (so as not to get in trouble).
Based on his grades and his ACT score he received offers from colleges for full tuition scholarships. He chose the college where he had taken concurrent classes during his high school years. But he proceeded to skip class and not turn in assignments so he lost his scholarship and quit attending college. During this time he was only able to find employment through an employment agency where he was mostly sent to manuel labor type jobs (which is not something he enjoys but he did it anyway). It was during this time that at one place had gone to on numerous occasions he was told if he came late one more time they would tell the emplyment agency they did not want him to come there anymore. (This seemed to make an impression on him because he has continued to be reliable and responsbile at his places of employment).
At 19 1/2 he left to serve a 2 year full-time mission for our church. He completed his mission successfully. (I don't think it was without some struggle, stress and depression, but he was able to pick himself up and move on from those times).
When he came home he started working for the employment agency again but began looking for employment elsewhere. He got a job at a local Chick Fil-A where he has worked for 3 years. He started college again shortly after he came home but as before it was short lived. He did finish out the semester but failed most of the classes due to his skipping class and not turning in assignments. When he skipped class he would usually sleep in his car.
Taylor's life consists of working (where to the best of our knowledge) he does well, he is reliable and his employer likes him. When he comes home from work he either sleeps or plays video games or other games - such as kakuro. He spendes most of his time in the basement where his bedroom is and this is where he games. Taylor owns his own car, bought his own laptop and very rarely spends money. He pays us $200 /month to still live at home, unloads the dishwasher on a regular basis and does the weekly garbage. However, his room is a mess and he only cleans his bathroom when I tell him he needs to clean it.
Taylor used to read quite a bit and loved to learn. It has just been in his adult years that he has not read as much - I think because of his gaming addiction. Taylor goes to church on a regular basis but sleeps through the main meeting. In Sunday class room settings he stays awake - I think because he is able to particpate in discussions.
Taylor has only had 2 real friends since entering Junior High school. And as of now he only keeps in contact with one of them who still lives in Georgia. We have lived in Utah since the summer of 2007 and he has never had a friend to do things with since we have lived here. He has two younger siblings, a brother 22 and a sister 20. They love Taylor and spend time with him when they are home. They are both at college and doing well.
Throughout Taylor's school years he has seen a counsleor on a fairly regular basis. One summer during junior high he attended a weekly class where he interacted with other kids with Aspergers. We did see a lot of change in him from this group. After he returned from his mission he went to see a counselor for a short period - this counselor tried to help him with some social skills. His dad and I went with him the first 3 or 4 times but we found out that after we quit going with him he only went a few more times and then scheduled appointments but did not show a couple of the times. We only found this out when a bill came for a ""no show"" appointment.
I don't know if this is too much information but were are in dire need of help for him. In the information that we purchased from you you mentioned that you do coaching for Aspergers adults. I don't know if you can help us but I thought I would check with you just in case.
Alas I think I have found your information too late to save my marriage but I am hoping to save myself.
I am currently going through a very very painful separation after a 27 year relationship with my husband whom I am convinced has aspergers syndrome. It is a long and painful story and I am desperately trying to process it all alongside dealing with a very conflictual separation. My partner is angry non communicative and totally dismissive of me and our long shared history.
He walked out last year after I discovered he had been visiting massage parlours and developed a relationship with an illegal Chinese escourt whom he subsequently moved in with. He had been seeing this woman behind my back for over 18 months. The pain of all this indescribable and his dismissal of my pain and very existence beyond belief.
Leading up to this I had been battling anxiety and depression which my husband found very hard to cope with.
Over the years of our relationship I knew something was off but I just could not put my finger on it. I often felt a complete lack of validation and empathy. Communication was also difficult as my husband was defensive and unwilling to look at issues in our marriage.
Please Mark could you help me validate some of this pain and try and make dense of 27 years of my life without drowning in fear guilt and despair about my future.
Thank you for listening and your site.
I have had problems with drunkenness, being late for school, not handing in school work, buying pot from a dealer etc. I chose to focus on the drinking and did the grounding then (grounding happened 3 times). I also stopped sleep overs at friends 100%. I have stopped handing out money for no reason or even buying treats like chocolate.
I did lose it one evening (and didn't do the poker face) when I was trying to unplug the internet at midnight on a school night (she’s always late for school so I am trying to get her to sleep at a reasonable hour). I was physically stopped and pushed around so I slapped my daughter (it was not hard). This ended up with her saying she didn’t want to come home (the next day after school). By this stage, I also had enough and didn’t go get her. I thought I am not begging. You will run out of money soon. It was quite a relief to have some peace. Daughter’s Dad was in town (from another country) and called a family meeting with the counsellor. To cut a long story short, daughter and her counsellor put it on the table that daughter wants to go live somewhere else (with her friends family) because of the stress at home with me (we live on our own) (i.e. stricter rules and her bucking up against it).
I didn’t really want this but made a compromise that daughter would go there Tues morning – Friday afternoon as the friend is an A student whereas my daughter is failing. They do the same subjects. I made the decision at the end of the day based on what is good for me – some time away from the daughter. I also thought of your book when the child went to live with the grandparents – daughter will dig her own hole over at the friend’s house. They have a week day no going out policy which made me think it is OK. I went and discussed with them the problems experienced (drinking, pot, late nights, not handing in work)
I am also trying to follow the let go of school thing per your book. I find it really difficult to remain calm when I can see daughter on her phone and watching series (when I have her on the weekends) when I know there are projects due. I hired her a private tutor once a week for help with a subject. The tutor has just fired my daughter for not handing in work and being not committed. It’s not the first time private tutoring has not been appreciated. The school give me a report back on a Friday as to whether everything is handed in. The deal is – if the work is not handed in – no pocket money and no Friday night out). Her school is a ""progressive"" school and there are no repercussions for her being late or not handing in work. I would change schools if I could but there are only 8 months left of school (she turns 18 in August).
We have just completed the first week and beginning week two of your material. We are agreeing with your take and see our son and ourselves in most of what you are saying. Prior to finding your material and starting your program we had been having extreme out of control behaviors and had to call the police because he was breaking things in our house and pushed my husband. This happened three weeks ago. After that incident we took away privileges ie. PS4, phone (which had already been taken for a few days), and friends. So, last week while doing your program he already didn’t have privileges and has continued with poor behavior – name calling, throwing things, slamming doors. We are not sure when to give privileges back. He has been given the privilege of playing with friends on occasion. His 13th birthday is tomorrow. This past weekend, for his birthday my husband and he went boar hunting. Of course we debated about it but decided to go ahead since it was his bday. We are cooking some of the meet on the grill tomorrow night for his bday and inviting a couple of his friends over for a cookout. No more gifts other than cards and balloons. We are wondering if we should go ahead and give him his privileges back and not sure how to do it. Last Friday morning we attempted to talk giving him a date to return privileges and that conversation ended with him getting angry but he gathered from our conversation that he is getting his stuff back on his bday. We are starting week 2 assignments today but not sure how to handle what was already in place. Of course, we aren’t seeing the respect and responsibility we are looking for but realize it has been a long time. We were wanting him to pay for his phone and thought it might be a good time to introduce that idea. Allowing him to earn his phone. We expect that he will be angry with this idea and not sure how to implement.
My son and myself are interested in a inpatient Aspergers program. We line in Calif which is preferable. My son is very high functioning and was diagnosed dry late. He was eight years old. He has never been in or attended a full day of class. Partially due to depression,anxiety, and trouble with his ADHD also his aversion and being bullied and of course his Aspergers. He will not attend his freshmen year due to surgery on both Achilles' tendons from walking on his toes. With physical therapy he should be ready by his sophomore year! We all feel he needs in patient therapy to give him the tools on how to work with his issues in a structured setting and a place that will give him tools for the rest of his life.
In my utter desperation to find a way to get some help for my daughter's increasingly challenging behaviour I trawled the internet to see if I could find some strategies that would provide specific methods on dealing with teenagers with Asperger's syndrome. When I came across your website, I couldn't believe that every statement you made was exactly what I have been going through with my daughter. She has just turned 14 last week, and was diagnosed with Asperger's/ Autism Spectrum Disorder 15 months ago. I have already been seeing a child psychologist for the past five months, however the methods she has been advising have not been very effective.
Our main difficulty with our daughter is her overwhelming obsession to use her cell phone (and to a lesser extent her laptop) constantly. Without any restriction, she will be on it every minute of the day, and will be awake until the early hours every day. We have tried to incorporate her input around rules as to when she has to give in her phone, but she is unwilling to compromise on a time that she should give it to us, believing that she should have unlimited use. I believe she is unable to do any adequate study or homework, as she is constantly having to look at the phone. We have tried to put rules in place that she has to give in her phone and laptop on school nights at 22:15. If she is able to do this then she is given rewards, and if she doesn't then she knows that there will be consequences. The consequence has been restricted use the following day. However, this is usually where we fail, because taking her phone away from her results in tantrums, screaming, and even threatening to harm herself. This behaviour is relentless to the point where the whole family becomes deeply distressed, and inevitably results in her getting the phone back.
This obsession is affecting her schoolwork, and more severely her eyesight. She has become very shortsighted, and her eyesight continues to deteriorate as a result of holding the phone or laptop very close, and mostly in the dark without any lights on. My husband and I have a constant battle on our hands daily, in all areas of discipline with our daughter, but our main concern is that we have been unable to find a way to minimise this obsessive behaviour centred around her phone and laptop. Please can you provide some strategies that can help us specifically with this problem.
First of all, I thank you for developing this program and I am only at the first stage of assignment 1. I have loads of books I have bought, attended psychiatrists for my son and myself, family therapy, occupational therapy, begged and prayed for change but have been dealing with behavioural issues for so long I am definitely exhausted and resentful.
I am a mum to a 15 yr old boy with ASD, dyslexia, OCD and ODD. Sorry to focus on the labels but just to give you an idea of what I am dealing with. I also have a 13 yr old son whom finds his brother’s behaviours difficult, embarassing and challenging. My husband whom is not in great health ( he had a cerebral aneurysm clamped two years ago and has two further aneurysms that are inoperable so endures fatigue, headaches and stress). We have however a pet cat that is very social and a calming influence in the home! I was fortunate enough to have loving parents but I lost both my mum and dad in 2008 and 2015. My inlaws are elderly and quite directly say they are too old to help us so it feels we are alone in dealing with the issues we have.
I am desperate for change as the household is one of stress and anger and I feel all the control lies in my son Patrick’s hands. I am hopeful your programme can make life better for all of us but I wonder if it is too early to ask you two questions?
The first lies with what to do when Patrick goes into my other son Brendan’s room and will either turn on a light when he is sleeping, yell when he is on his phone or create some disturbance. He will not leave the room when asked to do so and the situation always escalates into yelling and Brendan attempting to physically remove him. This happens regularly and always ends badly with doors slamming, my husband being woken and myself in tears feeling the lack of control and also I admit I seem to think “Why me?” which rationally I know is of no help.
The second problem is leaving the house for school. Patrick refuses personal hygiene (either morning or night) and any request to even brush his teeth is fraught with swearing and abuse. If I can get him to shower, he will watch the water roll down the drain and turn up the water really high temp (mu husband has had to turn down the thermostat on the hot water service) without so much as getting wet. My husband leaves for work at 6am but I leave at 745 to work as a nurse in a busy outpatients department in the Alfred Hospital (Melbourne). My work is my sanity as it is a paid break from home but most days I am late which is causing considerable stress and anxiety not to mention my responsibility to do my job. Patrick simply refuses to leave the house and as much as I am tempted to just walk out and leave I know the house would be left unlocked and wonder if Patrick would even attend school. The time I need to leave is not negotiable but Patrick uses this to his advantage and seems to delight in stressing me out and subsequently speeding to work in a frazzled mess.
The interesting and frustrating element in all of this is that although he is socially isolated at school (he has no friends) and academically challenged his behaviour at school is not a problem. He is quiet and his teachers report he does his best and is compliant and well mannered. It is like a Jekyll and Hyde situation where another side of him at home is so angry and abusive yet at school this behaviour does not happen.
I’m Jackie, I now work primarily as a freelance tech writer, after starting my career in software development and moving on to teach IT to young adults at a variety of colleges and schools.
My freelance work is pretty varied and looks at many aspects of the computer industry as a whole, and I’ve just recently completed a piece which gives help and advice to anyone wanting to become a game designer, which you can read here: http://www.gamedesigning.org/become-a-game-designer/. It highlights the hard work and effort it takes to get into such a role, and also how you can further your career and continue to learn and improve as you go. I hope you’ll agree it shows that starting work in the industry takes dedication and skill and that becoming a game designer isn’t just a fly-by-night job!
If you’d be interested in sharing a quick mention of my work on your blog that would be really wonderful and I’d appreciate the chance to get my work out there to a wider audience. Alternatively, I’d be happy to write a short blurb or paragraph or two (or a longer piece - just let me know) highlighting the key points because I think some of your readers might get a lot of value from it.
My son just turned 15 and is a freshman in high school. Although this is his first year in a general ed environment, he is struggling with behaviors in school. He has meltdowns and does not express why he would have them until much later. Once we all know what caused it, the school will accommodate him and try to ""change up"" things so as not to cause his meltdown. Once that is resolved, another issue comes up and causes him to melt down. He is a high functioning and academically does well, when he wants to do the work. We battle at home over homework. He does not care how it is done, as long as he hands it in. He thinks failing a test is ok, at least he took the test. Homework is never on his mind when he gets home from school. If I never prompt him, he would never open is backpack. He can be aggressive but is never intentionally trying to hurt anyone. He may push over a chair in school, but it is not directed at anyone. We know how that in itself could hurt someone who gets hit by it though. He is defiant in that he only wants to do what interests him. He does not go out by himself (still immature), or abuse alcohol or drugs and never curses. He is a very funny kid and very talented. His main problems are task avoidance and seeking attention. He can be disrespectful to adults in that he is ""cheeky"" with them, trying to be funny or cute. And he has no ""filters"".
I’ve just finished reading your Living with an Aspergers Partner ebook. I found it so informative, thank you.
You offered some personal advise, and i wanted to run a situation past you and seek your input as to a strategy for what to do next.
I’ve been seeing a guy for about 7 months now who I believe has Aspergers. I came to this conclusion months ago and I don’t think he realizes, (or acknowledges) although he is aware he has some traits.
He’s highly intelligent and successful, a pattern seeker, has a tendency to focus on the project to hand to the total exclusion of all else for as long sit takes (work or home) socially awkward (has learned coping strategies), sensitive to loud noise, high anxiety with control strategies, black and white thinking etc. He’s currently not working and I’ve seen a slow withdrawal over the last 6 weeks, including the need to ‘escape’ and leave a situation at least once.
He also has a bipolar ex overseas who has primary custody one daughter where there has been ongoing patterns of drama which has recently increased.
Over the past couple of months (since stopping work and drama increase) I’ve gone from being ‘wonderful’ in his eyes to him now being sorry and not having the ‘urge’ to spend close/intimate time with me and offering friendship. Since he shared that with me in a message he’s stonewalled and has retreated to the safety of minimal messages and talks about not knowing what best to say and not being able to find the right words somehow.
He’s a good kind man who I feel is struggling. I’m concerned about his anxiety and possibly the risk of depression. I’m fairly resilient and whilst i’m disappointed he doesn’t want to pursue a relationship with me, i’m concerned for him and his well being. One of his very few close friends is also just leaving the country to live overseas.
The strategy I’ve used so far is simply to back off and give him space. I’ve asked to take him up on an original offer he made to talk but haven’t pushed it. I also haven’t been aggressive or accusatory in the few messages i’ve sent.
Any advise you could give would be greatly appreciated,
Carli who is 10 years old and has had behavioral issues her whole life. The other night she came home very upset after having a conflict with a friend. She was at her friend's house and her and her friend wanted to get on the computer and the older sister was using it. Carli made up a story that someone was at the door to get the older sister off the computer. Her friend didn't understand that she was making up a story to get the sister off the computer. She got excited that someone was at the door and ran downstairs to answer the door. In the process of getting the door, she fell and yelled at Carli. Carli became extremely upset. She was able to control her feelings at her friend's house, but when she came home, she proceeded to cry extremely loudly for over an hour. Her dad spent most of that time with her, talking to her and trying to calm her down. After an hour, I asked him if he could please tell her to be more quiet because the other members of the household were trying to go to sleep.
My question is....how do I as the girlfriend, handle this? He did not like that I asked her to be quiet. We have a rule that if she is having bad behavior, and can't calm down in 5 minutes, he takes her out of the house because her yelling doesn't stop for a long time and is very upsetting to everyone in the household. I would like to ask him to do this with this kind of situation as well. Is this a reasonable request? His thought was that she shouldn't be made to calm down, because everyone handles being upset in a different way. But, she was literally sobbing and wailing very loudly.
My other question is should she have been told that if she wouldn't have lied, this wouldn't have happened? She has a history of lying and of not accepting responsibility for her actions. My boyfriend became very upset with me when I brought this up. He was being very sympathetic and understanding to her. I feel like he was giving her negative attention, and being an over indulgent parent by not putting his foot gown and saying, ""you can't carry on like this, even though you are upset"". Please let me know how we can handle these situations better.
I am contacting you for help with adult AS. I am taking initiative to pre screen potential therapists to help my current boyfriend get therapy and help with Adult AS.
He has seen many therapists, but it seems like they aren’t really helping him with his problems. They don’t seem to understand how his (undiagnosed) AS would affect therapy approaches. For example, he may not share enough in therapy session and I’m assuming an AS therapist would recognize that is part of the AS and employ strategies to get information from him that helps with treatment. Sometime he tunes out when he is processing something heavy or that he doesn’t want to hear necessarily, or he gets distracted and I’m hoping an As therapist would recognize that and get that he may need repeated something for example, if this is happening.
He is currently suffering from depression that appears clinical in nature as well as reoccurring negative thoughts about something specific that has been worrying him about our relationship. Today he told me these reoccurring thoughts happen during all waking hours unless he watches TV, he never gets a break from them and they make him feel like he is going crazy. As his girlfriend, I am extremely concerned that he cannot get relief from these thoughts and that the therapists he is seeing are unable to help him with his problems. Therefore, I am taking initiative to try and help him find better therapy options, because I want to see him someone who can better help him get to the bottom of things and help him with the challenges he is facing. He really needs an advocate that will help him go deep to figure things out and not just assume therapies are working well, without seeing changes or getting supporting feedback from him in that regard.
Here are some questions I am trying to ask in advance to find the right people to help us with this. As you may know, insurance for these therapies are not often available. We don’t have a lot of money to go from therapist to therapist to find the right person and are hoping prescreening will help.
I recently downloaded your e-book and listened to your talks and your information is by far the most helpful I have been able to find to date. It's very accurately describes my situation as an NT wife married to a very probable AS husband. I think you for taking the time to write this and sharing your insights as well as the experiences of many of your clients. It has really helped me understand the last 32 years of our marriage and get a grasp on how to move forward.
One area that is of primary concern to me, that I did not see addressed, is stimming. I believe that is the behavior my husband is showing through constant vocal singing, repetition of words, shouting out, as well as slapping himself in the chest and general nervous activity. It is very loud and disruptive to our household and it is often a relief when he is not at home. I think there may be a level of Tourette's syndrome as well.
I did some searches on the Internet and could not find anything that really describes his behavior. Most of what I found was flapping or children's behavior. I understand that it is a release of nervous tension but I am really trying to find some strategies to help him stop this behavior as it is extremely frustrating and builds my resentment in dealing with it daily. A lot of it is embarrassing as well and sounds childish to me.
He usually does this when close family members are around and will reign himself in if he is around other people besides us. When we are home it is constant. He also has a lot of anger, mostly at himself, and blows up at unimportant things, it is as if he has a ton of negative energy inside him that need to get out and stimming is one outlet.
I will try to build my acceptance of it, but I also would just like him to stop especially the loudest and most annoying portions. Would you have any resources you could point me to?",['3-D printing and software development.'],8501,multifieldqa_en,en,,0f101ee4ea9d1d88998598e0405367ad95912c92a7d7a31d," My Aspergers Child: COMMENTS & QUESTIONS [for Feb., 2017]                 ""I emailed you a while back and you mentioned that I could email when I needed to. Thank you. I last wrote you in December that my son became involved in a dispute involving the local police. We have had 3 court dates. It keeps delaying due to not being able to come to an agreement""                 ""He is going to have an evaluation today to get an accurate diagnosis. I understand that is a little difficult since he is an adult.""                 ""My daughter and her 4 year old son also have Asperger's symptoms. So my life chapters may not close for a while. :-)                 ""Thanks for all you do, helping us all with ODD children/teens""ipient ""Is it possible to include any training to her in her primary caregiver to do a better job managing their needs?""ipient ""I am going to go back to the start of the program like I said earlier and keep notes close by for reference. I am fighting much guilt still but I have a lot of peace now""vin ""I have made it very clear that if I see any drugs in my home I will be calling the police, as well as if I seeing signs of it being sold by him I will report him""vin. ""I believe it's being kept at his ""friends"" which of course I have no proof of....I just know it is not here. ... I believe it is being kept in my house, ... I have never had selling amounts in my House, (He has never had sold amounts in his house).""vin.""I am sure we will have more consequences and possibly another being kicked out, but I'm going to think positive and hope that he learned some form of a valuable lesson here"" v. ""Never in a million years did I ever think I'd be on this side, (the one needing the help, as I am the one who helps.) :-)                "" ""I'm a little worried though because since she smokes he wants to find his own place. With all the costs he has to balance it out financially. That is good. I can't help him more than I am which is good.""vin.�I have a small company providing educational support services to a few families who have children with various disabilities in Ohio"" vi. ""Can you help me with my daughter's 4-year-old son who has Asperberger's syndrome?"" vi.""Can you give me some advice on how to help my son with his ADHD?""vi. ""What started out very bad is turning into a good thing. It will probably take another 90 days or more or more but it's good for him to work on his self-esteem and self-confidence"" v, ""I don't want him to think he's a bad person because he's not. He's got a lot on his plate but has support from his family. From therapy. From his attorney. And from his work. He has the compassion and excellence that is needed here"" My daughter is smoking pot almost every day at school. She is now saying she's suicidal because she feels so isolated, yet she continues to smoke weed. She tried to sneak out last night but was foiled by our alarm system. For the particular drug test we have, I read it takes about 10 days of not smoking to pass the test. What would you do? Please advise. I am having a problem with my 18 year old son, Danny, with high functioning autism. It's been 2 years now and he will not accept his diagnosis. He won't talk about it and when I try to bring it up he gets very angry. I told him that he doesn't need to shout it out to the whole world but he won't even accept it himself. I don't know how to help him with it and because he's almost 19 I have limited control now. I think his life would be easier is he accepted it.Please help me help him. I have no training, per se, in working with this children though I know about setting structure, consistency, etc. I work primarily within the school setting in a charter school whose mission is to educate children on the autism spectrum in a mainstream setting. We use Michelle Garcia Winner’s social thinking program with our ASD kids. I also work with gen ed kids in the school who are at-risk; the school is in the inner city from where the majority of our non-ASD kids live. It would have been so much easier to mention to my adult son that he thinks (I know he does, but want to ease into the subject)he has Asperger's when we were living together two years ago. He has since moved to Tennessee working in his field of interest which is 3-D printing and software development. I'm so happy for him that. he has found his way into a job that he truly enjoys. Even though he's socially isolated. He's not diagnosed and does not know he has it. How I know is his classic symptoms (sensory issues, meltdowns and much more) Throughout childhood I just felt a bit different time to time. Nothing stood out and he was a major major major stood out. We've talked abut ADHD, low frustration, low anger, low frustrations, etc., etc. If so, please let us know what will work for you including time, place, and cost. Thank you for your assistance. I just listed to your tapes on dealing with an out of control, defiant teen. I'd like to ask your advice on a particular situation we have. We will not be testing you except for when you tell us you are ready to be tested. You will no longer have your phone, lunch money to go off campus (she has an account at the school for the cafeteria she can use), and you will be grounded until you can pass a drug test. I've tried telling him that it's not a bad thing, that there's been many, many very successful people with Aspergers. The other kids at school use it as an insult, like saying someone is so autistic when they do something they don't approve of. I wonder if it would help for her to know. Asperger's got a diagnostic code back in the 90's, yet all the so called doctors,physiologist's, etc, didn't know how to diagnose it. Do you know if people are better off knowing they have it? Do I wait to tell him in person, or ease into it with him over Skype? He likes direct, honest, concrete communication with his parents. I have been dealing with my teen being like this for almost 3 years. I totally lost my cool today with my 17-year-old son to the point I told him he is out of the house. He can never simple rules, comes and goes as he pleases. He has made me feel exhausted, and sometimes I just like not even being here. I bought your program at three and a half dollars a week in hopes to help, I am so happy to have you as a friend. You can contact me at: jennifer.glanfield@mailonline.co.uk or on Twitter @jenniferGLanfield. I would also like to hear from you if you have any questions or concerns about Asperger’s. Please email: jenninger@mailOnline.com or on Facebook at: www.facebook.com/jennifersanninger. If you have a personal essay to share, please send it to jenniferglanville@mail Online.com. I’m looking forward to hearing from you. Thank you for your time. Back to Mail Online home. back to the page you came from. The page you arrived from. Your message will be updated with the latest news from CNN.com and the CNN iReport app. Click here to see the latest from CNN and CNN iReporters on the ""Next Generation"" channel. The Next Generation is a new series on CNN, Sundays at 10 p.m. ET/PT. Visit the Next Generation channel to see what else is trending on CNN and HLN. The next series will be on the NextGen channel on CNN on Sunday, July 14 at 10:30pm ET /PT. The series will feature a panel of experts from CNN, HLN, and ABC News on the topic of ""The Next Generation"" The next installment is on Monday, July 15 at 11:30am ET / 9:00am ET. The panel will feature an expert on the subject of the next generation, ""The New Generation"" from CNN's John Sutter. The first episode will air on July 16 at 9:30 am ET/9:30 a.m., PT. The second episode will be Tuesday, July 17 at 10 a.M. ET. Click HERE to watch the next installment on CNN. and the third episode on July 18 at 11 a. m. ET / 10:00 am. The fourth and final episode is on Thursday, July 19 at 10am ET/11:00 a. M.E. / 9pm ET. It is on ""The View From The Couch,"" a series on parenting for adults with developmental disabilities. The sixth and final installment on the View from The View is on the Family Discussion Channel on CNN TV. I am studying the effects of Brazilian Jiu Jitsu, and psychotherapy on helping people with autism develop subjective awareness of others. I wanted to let you know about a research opportunity for children, teens, and young adults with autism. I have a 10 year old daughter who has outbursts with prolonged crying almost like tantrums that 2 year olds have when they cannot express themselves. I had her in therapy from age 6-8 years old for the same thing but I feel that the sessions didn't really help much. How can I get her tested for Asperger's Syndrome? Last night our 24 year old son with Aspergers told his dad and I that he is pulling out of the 4 college classes that he recetnly enrolled in because he has not been attending class or turning in his assignments. He paid $2800 (his own money) for tuition and I reminded him of this when he told us but it did not seem to bother him. This is a son who basically had a 4.0 grade point average through 10th grade and got a 34 on the ACT the first time he took it. He also took some concurrent college classes while he was in high school that he failed. Most of what we have done throughout his life with our son is included in this book. I was hoping for more help so I am emailing you now in hopes of more spergers support. I hope you can help me with some of the things that I did or did not do throughout my life with my son or did with my daughter. I just wondered if things will get worse? He's angry so quick in arguments.Scares me etc. I can't leave as he's the main bread winner and our daughter loves him to bits. I don't know why I'm writing this..Sorry if I'm going on and not making sense :( Please do not hesitate to contact me for further information about this novel approach to helping to develop social awareness in autism, if you know of anyone with autism who may benefit from participating in this study. The requirements of the study will be: A participant should be between 7-21 years of age and have a diagnosis of Autism Spectrum Disorder. The participant should enroll in an approved Jiu J Jiu Academy and attend at least two sessions a week for a period of six months. The participants will be given a SRS (Social Responsiveness Scale) test at the beginning of theStudy, at three months, and again at six month. They will be in social skills groups, provided by my office or be in a steady psychotherapeutic relationship in your office, at least once a week, or minimally two to three times a month. It is hoped that this might help someone in your practice, or to see if you might know of someone with Autism who may need help with social skills. The study is funded by the Autism Society of the U.S. (ASUS) and the Autism Association of the UK (ASUK) The study will take place over a six-month period. It will be funded by a grant from the National Institute of Mental Health (NIMH) We noticed some things with our son, Taylor, as a yound child but as we had not heard of Aspergers at that time we just did what we thought would help him. We then set him up with and EIP which stayed with him until his sophomore year. Then during his Senior year is when he started skipping classes and not doing assignments. At 19 1/2 he left to serve a 2 year full-time mission for our church. He completed his mission successfully. When he comes home from work he either sleeps or plays video games or other games - such as kakuro. He spendes most of his time in the basement where his bedroom is and this is where he games. Taylor owns his own car, bought his own laptop and very rarely spends his own money. He pays uceific ideas.ecific ideas to help you understand your son's condition. The Asperger's Foundation is a non-profit organization that provides support and resources to families of children with autism. For more information on the Asperberger's Foundation, visit their website at: www.aspergersfoundation.org. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. For support in the U.S., call the National Suicide Prevention Line on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For help in the UK, visit the National suicide Prevention Lifeline at http:// www.suicidalprevention Lifeline.com/. For support on suicide matters in the Middle East, call the Salvation Army on 0845 90 90 95 or visit the Samaritan's Lifeline on 0844 90 90. For help on suicide issues in the United States, visit http:\/www.sending.it/sending-help.html. For information on how to help a child with autism in the developing world, visit www.sickKids.org or go to www.justgiving.com/help-children-with-autism-in-the-developing-world. For info on helping a child in need of help, visit the Samaritans on their page on their page. For information on how to help a child with autism in the developing world or the world,  visit www.justGiving.org/help/autistic-children. For more information about how to help a child with Aspergers in the United States or others, visit the National Institute of Autism and Other Helping Children on its Site. For details on How To Help A Child With Autism In The United World, Visit The National Institution on the Home and the Site of the World on Their Homepage. Taylor has only had 2 real friends since entering Junior High school. He has never had a friend to do things with since we have lived in Utah. Taylor goes to church on a regular basis but sleeps through the main meeting. He only cleans his bathroom when I tell him he needs to clean it. His room is a mess and he only keeps in contact with one of them who still lives in Georgia. He spends $200 /month to still live at home, unloads the dishwasher on a regularly basis and does the weekly garbage. The pain of all this is indescribable and his dismissal of my pain and very existence beyond belief. I am desperately trying to process it all alongside dealing with a very conflictual separation. My partner is angry non communicative and totally dismissive of me and our long shared history. He walked out last year after I discovered he had been visiting massage parlours and developed a relationship with an illegal Chinese escourt. He had been seeing this woman behind my back for over 18 months. I don't know if this is too much information but were are in dire need of help for him. In the information that we purchased from you you mentioned that you do coaching for Aspergers adults. I think I have found your information too late to save my marriage but I am hoping to save myself. Please Mark could you help me validate some of this pain and try and make it through the next 27 years of my life without drowning in fear guilt and despair about my future. Thank you for listening and your site.ă.ę.ĝ.Ě.ĉ.ą.ā.Ć.ē.ļ.Ģ.Ĝ.ġ.ğ.Ę.Ą.ľ.ĥ.ė.Ľ.Ī.ĳ.Ā.ċ.Į.Ħ.Ĩ.Ŀ.ĭ.Ă.Ĺ.Ğ.Ļ.ķ.Ĭ.Ď.ĩ.ć.İ.Č.ě.Ĥ.ĸ.Ġ.ħ.ď.ĵ.Ċ.ī�.į.č.Ķ.Ė.Ē.ı.ģ.đ.ĺ.Đ.ĕ.Ĉ.Ĵ.ī.Ĕ.Ĳ.Ă�.Ĭ�.ą�.Ą�.ę�.Ļ�.ę�.Ć�.Ŀ�.Ċ�.Ę�.Ķ�.ĭ�.Ĉ�.ĥ�.Ă�.ģ�.ā�.ē�.ĭ�.Ĳ�.ĥ�.Ė�.ā�.Ă�.  “I have had problems with drunkenness, being late for school, not handing in school work, buying pot from a dealer etc. I chose to focus on the drinking and did the grounding. I also stopped sleep overs at friends 100%. I have stopped handing out money for no reason or even buying treats like chocolate. Daughter's Dad was in town (from another country) and called a family meeting with the counsellor. Daughter wants to go live somewhere else (with her friends family) because of the stress at home with me. I made the decision at the end of the day based on what is good for me – some time away from the daughter. Her school is a ""progressive"" school and there are no repercussions for her being late or not handing in work. I would change schools if I could but there are only 8 months left of school (she turns 18 in August).We are agreeing with your take and see our son and ourselves in most of what you are saying. We are starting week 2 assignments today but not sure how to handle what was already in place. Of course, we aren’t seeing the respect and responsibility we are looking for but realize it has been a long time. We expect that he will be angry with this idea. We want him to pay for his phone and thought it might be a good time to introduce that idea. Allowing him to earn his phone. No more gifts other than cards and balloons. He has been given the privilege of playing with friends on occasion. This past weekend, for his birthday my husband and he went boar hunting. We're cooking some of the meet on the grill tomorrow night for his bday and inviting a couple of his friends over for a cookout. We have just completed the first week and beginning week two of your material. We line up a line line program and to implement and implement it and we are interested in a inpatient Aspergers program whic WHic whic whim whic is whic we want to implement. We don't know how to do it but we are hoping it will be a positive experience for both of us. We hope it will help our son with his Asperger’s and his parents with their parenting. We’re hoping that it will also help our daughter with her anxiety. We also hope that this will help us with our daughter’S anxiety and depression. We all hope that it helps our daughter and her parents with all of their parenting issues. We will be praying for all of you as we go through this together. We love you all! We hope you are all doing well and are looking forward to seeing you all in the next few weeks. Back to Mail Online home. back to the page you came from. Click here for more information on our parenting tips and tricks. We would like to hear from you about your experiences with your children and how you have dealt with the issues you have faced with them so far. Please email us at editorial@dailymail.co.uk or call us at 1-800-287-8255. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or see www.samaritans.org. In the U.S. call the National Suicide Prevention Lifeline on 1-877-273-TALK (8255). My daughter, 14, was diagnosed with Asperger's/ Autism Spectrum Disorder 15 months ago. Our main difficulty with our daughter is her overwhelming obsession to use her cell phone (and to a lesser extent her laptop) constantly. We have tried to put rules in place that she has to give in her phone and laptop on school nights at 22:15. If she is able to do this then she is given rewards, and if she doesn't then she knows that there will be consequences. The consequence has been restricted use the following day. However, this is usually where we fail, because taking her phone away from her results in tantrums, screaming, and even threatening to harm herself. This behaviour is relentless to the point where the whole family becomes deeply distressed, and inevitably results in her getting the phone back. Please can you provide some strategies that can help us specifically with this problem. I am only at the first stage of assignment 1. I have loads of books I have bought, attended psychiatrists for my son and myself, family therapy, occupational therapy, begged and prayed for change but have been dealing with behavioural issues for so long I am definitely exhausted and resentful. I also have a 13 yr old son whom finds his brother’s behaviours difficult, embarassing and challenging. My husband whom is not in great health ( he had a cerebral aneurysm clamped two years ago and has two furtherAneurysms that are inoperable so endures fatigue, headaches and stress). We have however a pet cat that is very social and a calming influence in the home! I was fortunate enough to have loving parents but I lost both my mum and dad in 2008 and 2015. We all feel he needs in patient therapy to give him the tools on how to work with his issues in a structured setting and a place that will give him tools for the rest of his life. It is preferable. to have a patient therapist. to help him with his ADHD and his aversion and being bullied and of course his Aspergers. He will not attend his freshmen year due to surgery on both Achilles' tendons from walking on his toes. With physical therapy he should be ready by his sophomore year! He has never been in or attended a full day of class. He has become very shortsighted, and her eyesight continues to deteriorate as a result of holding the phone or laptop very close, and mostly in the dark without any lights on. I believe she is unable to do any adequate study or homework, as she is constantly having to look at the phone. Sorry to focus on the labels but just to give you an idea of what I am dealing with. I'm a mum to a 15-year-old boy with ASD, dyslexia, OCD and ODD. He is very high functioning and was diagnosed dry late. He was eight years old when he was 8 years old. He's never been to school. Jackie is desperate for change as the household is one of stress and anger. She feels all the control lies in her 15-year-old son Patrick's hands. Patrick refuses personal hygiene (either morning or night) and any request to even brush his teeth is fraught with swearing and abuse. Although he is socially isolated at school (he has no friends) and academically challenged his behaviour at school is not a problem. He is quiet and his teachers report he does his best and is compliant and well mannered. It is like a Jekyll and Hyde situation where another side of him at home is so angry and abusive yet at school this behaviour does not happen. If you’d be interested in sharing a quick mention of my work on your blog that would be really wonderful. Alternatively, I’m happy to write a short blurb or paragraph or two (or a longer piece - just let me know) highlighting the key points because I think some of your readers might get a lot of value from it. I hope you will agree it shows that starting work in the industry takes dedication and skill and that becoming a game designer isn’t just a fly-by-night job! If you would like to share your story with the rest of the world, please email it to jennifer.smith@mailonline.co.uk with the subject line ‘How do I help my son with his anger and tantrums?’ and a photo of your child. I would also like to hear from you about how you are coping with your son’s behaviour at home and at school. I am hopeful your programme can make life better for all of us but I wonder if it is too early to ask you two questions? The first lies with what to do when Patrick goes into my other son Brendan’S room and will either turn on a light when he is sleeping or create some disturbance. He will not leave the room when asked to do so and the situation always escalates into yelling and Brendan attempting to physically remove him. This happens regularly and always ends badly with doors slamming, my husband being woken and myself in tears feeling the lack of control and also I admit I seem to think “Why me?” which rationally I know is of no help. The second problem is leaving the house for school. My husband leaves for work at 6am but I leave at 745 to work as a nurse in a busy outpatients department in the Alfred Hospital (Melbourne) Most days I am late which is causing considerable stress and anxiety not to mention my responsibility to do my job. The time I need to leave is not negotiable but Patrick uses this to his advantage and seems to delight in stressing me out and subsequently speeding to work in a frazzled mess. If I can get him to shower, he will watch the water roll down the drain and turn up the water really high temp (mu husband has had to turn down the thermostat on the hot water service) without so much as getting wet. He has meltdowns and does not express why he would have them until much later. Once we all know what caused them, the school will accommodate him and try to accommodate him. I’ve been seeing a guy for about 7 months now who I believe has Aspergers. I don’t think he realizes, (or acknowledges) although he is aware he has some traits. He’s highly intelligent and successful, a pattern seeker, has a tendency to focus on the project to hand to the total exclusion of all else for as long as it takes (work or home) socially awkward (has learned coping strategies), sensitive to loud noise, high anxiety with control strategies, black and white thinking etc. He also has a bipolar ex overseas who has primary custody one daughter where there has been ongoing patterns of drama which has recently increased. I’m concerned about his anxiety and possibly the risk of depression. Any advise you could give would be greatly appreciated, Carli who is 10 years old and has had behavioral issues her whole life. Carli was able to answer at the door at her friend's house, but when she came home she proceeded to cry extremely loudly for over an hour. Her dad spent most of that time with her, talking to her and trying to cajole her to calm down. She got excited that someone was at the front door and ran downstairs to answer it. In the process of getting the door, she fell and yelled and yelled. She was extremely upset and yelled at her dad, who spent an hour with her trying to calm her down. Her friend didn't understand that she was making up a story to get the older sister off the computer. She then proceeded to make up another story to try and get her sister to stop using it. She made up the whole thing to get her friend to take her off it and she was so upset that she couldn't stop crying. She is now able to go back to her friends house and play with her friends. She has been able to control her anger for the most of the time. She's also able to get a lot of her anger out by talking to other people. She can be aggressive but is never intentionally trying to hurt anyone. He can be disrespectful to adults in that he is ""cheeky"" with them, trying to be funny or cute. And he has no ""filters"". He is a very funny kid and very talented. He does not go out by himself (still immature), or abuse alcohol or drugs and never curses. He thinks failing a test is ok, at least he took the test. Homework is never on his mind when he gets home from school. If I never prompt him, he would never open is backpack. We battle at home over homework. Once that is resolved, another issue comes up and causes him to melt down. He's a high functioning and academically does well, when he wants to do the work. He doesn't care how it is done, as long that he hands it in. He is defiant in that He is still immature and does not want to do what interests him. I'm disappointed he doesn't want to pursue a relationship with me, i'm concerned for him and his well being. I've gone from being ‘wonderful’ in his eyes to him now being sorry and not having the ‘urge’ to spend close/intimate time with me and offering friendship. I am contacting you for help with adult AS. I am taking initiative to pre screen potential therapists to help my current boyfriend get therapy and help with Adult AS. As his girlfriend, I am extremely concerned that he cannot get relief from these thoughts and that the therapists he is seeing are unable to help him with his problems. He really needs an advocate that will help him go deep to figure things out and not just assume therapies are working well, without seeing changes or getting supporting feedback from him in that regard. As you may know, insurance for these therapies are not often available. We don’t have a lot of money to go from therapist to therapist to find the right person and are hoping prescreening will help. It's very accurately describes my situation as an NT wife married to a very probable AS husband. I think you for taking the time to write this and sharing your insights as well as the experiences of many of your clients. It has really helped me understand the last 32 years of our marriage and get a grasp on how to move forward. I believe that is the behavior my husband is showing through constanan. He did not like that I asked her to be quiet. His thought was that she shouldn't be made to calm down, because everyone handles being upset in a different way. But, she was literally sobbing and wailing very loudly. He was being very sympathetic and understanding to her. I feel like he was giving her negative attention, and being an over indulgent parent by not putting his foot gown and saying, ""you can't carry on like this, even though you are upset"". Please let me know how we can handle these situations better. I would like to ask him to do this with this kind of situation as well. Is this a reasonable request? His other question is should she have been told that if she wouldn't have lied, this wouldn't has happened? She has a history of lying and of not accepting responsibility for her actions. Today he told me these reoccurring thoughts happen during all waking hours unless he watches TV, he never gets a break from them and they make him feels like he is going crazy. Sometime he tunes out when he is processing something heavy or that he doesn't want to hear necessarily, or he gets distracted and I’m hoping an As therapist would recognize that and get that he may need repeated something for example, if this is happening. He is currently suffering from depression that appears clinical in nature. He told me he wants to see him someone who can better help him get to the bottom of things and help him deal with the challenges he is facing. He has seen many therapists, but it seems like they aren't really helping him with their problems. I'm hoping an AS therapist wouldrecognize that and employ strategies to get information from him that helps with treatment. He may not share enough in therapy session and I'm assuming an AS therapists would recognize this is part of the AS. Vocal singing, repetition of words, shouting out, as well as slapping himself in the chest and general nervous activity. It is very loud and disruptive to our household and it is often a relief when he is not at home. He also has a lot of anger, mostly at himself, and blows up at unimportant things, it is as if he has a ton of negative energy inside him that need to get out and stimming is one outlet. I think there may be a level of Tourette's syndrome as well. I understand that it is a release of nervous tension but I am really trying to find some strategies to help him stop this behavior as it is extremely frustrating and builds my resentment in dealing with it daily. A lot of it is embarrassing as well and sounds childish to me. I will try to build my acceptance of it, but I also would just like him to stop especially the loudest and most annoying portions. Would you have any resources you could point me to?"
What is the recommended daily intake of vitamin K for adult women and men?,"Vitamin K - Wikipedia
(Redirected from Vitamin k)
This article needs more medical references for verification or relies too heavily on primary sources. Please review the contents of the article and add the appropriate references if you can. Unsourced or poorly sourced material may be challenged and removed. (November 2015)
This article is about the family of vitamers. For vitamin K1 the form usually used as a supplement, see Phytomenadione.
Vitamin K structures. MK-4 and MK-7 are both subtypes of K2.
Vitamin K deficiency, Warfarin overdose
Vitamin K is a group of structurally similar, fat-soluble vitamins the human body requires for complete synthesis of certain proteins that are prerequisites for blood coagulation and which the body also needs for controlling binding of calcium in bones and other tissues. The vitamin K-related modification of the proteins allows them to bind calcium ions, which they cannot do otherwise. Without vitamin K, blood coagulation is seriously impaired, and uncontrolled bleeding occurs. Low levels of vitamin K also weaken bones and promote calcification of arteries and other soft tissues[citation needed].
Chemically, the vitamin K family comprises 2-methyl-1,4-naphthoquinone (3-) derivatives. Vitamin K includes two natural vitamers: vitamin K1 and vitamin K2.[1] Vitamin K2, in turn, consists of a number of related chemical subtypes, with differing lengths of carbon side chains made of isoprenoid groups of atoms.
Vitamin K1, also known as phylloquinone, is made by plants, and is found in highest amounts in green leafy vegetables because it is directly involved in photosynthesis. It may be thought of as the plant form of vitamin K. It is active as a vitamin in animals and performs the classic functions of vitamin K, including its activity in the production of blood-clotting proteins. Animals may also convert it to vitamin K2.
Bacteria in the gut flora can also convert K1 into vitamin K2. In addition, bacteria typically lengthen the isoprenoid side chain of vitamin K2 to produce a range of vitamin K2 forms, most notably the MK-7 to MK-11 homologues of vitamin K2. All forms of K2 other than MK-4 can only be produced by bacteria, which use these forms in anaerobic respiration. The MK-7 and other bacterially derived forms of vitamin K2 exhibit vitamin K activity in animals, but MK-7's extra utility over MK-4, if any, is unclear and is a matter of investigation.
Three synthetic types of vitamin K are known: vitamins K3, K4, and K5. Although the natural K1 and all K2 homologues and synthetic K4 and K5 have proven nontoxic, the synthetic form K3 (menadione) has shown toxicity.[2]
1.2 Cardiovascular health
1.4 Coumarin poisoning
4.1 Conversion of vitamin K1 to vitamin K2
4.2 Vitamin K2
6 Absorption and dietary need
7 Dietary reference intake
10 Biochemistry
10.1 Function in animals
10.2 Gamma-carboxyglutamate proteins
10.3 Methods of assessment
10.4 Function in bacteria
11 Injection in newborns
11.3 Controversy
A review of 2014 concluded that there is positive evidence that monotherapy using MK-4, one of the forms of Vitamin K2, reduces fracture incidence in post-menopausal women with osteoporosis, and suggested further research on the combined use of MK-4 with bisphosphonates. In contrast, an earlier review article of 2013 concluded that there is no good evidence that vitamin K supplementation helps prevent osteoporosis or fractures in postmenopausal women.[3]
A Cochrane systematic review of 2006 suggested that supplementation with Vitamin K1 and with MK4 reduces bone loss; in particular, a strong effect of MK-4 on incident fractures among Japanese patients was emphasized.[4]
A review article of 2016 suggested to consider, as one of several measures for bone health, increasing the intake of foods rich in vitamins K1 and K2.[5]
Cardiovascular health[edit]
Adequate intake of vitamin K is associated with the inhibition of arterial calcification and stiffening,[6] but there have been few interventional studies and no good evidence that vitamin K supplementation is of any benefit in the primary prevention of cardiovascular disease.[7]
One 10-year population study, the Rotterdam Study, did show a clear and significant inverse relationship between the highest intake levels of menaquinone (mainly MK-4 from eggs and meat, and MK-8 and MK-9 from cheese) and cardiovascular disease and all-cause mortality in older men and women.[8]
Vitamin K has been promoted in supplement form with claims it can slow tumor growth; there is however no good medical evidence that supports such claims.[9]
Coumarin poisoning[edit]
Vitamin K is part of the suggested treatment regime for poisoning by rodenticide (coumarin poisoning).[10]
Although allergic reaction from supplementation is possible, no known toxicity is associated with high doses of the phylloquinone (vitamin K1) or menaquinone (vitamin K2) forms of vitamin K, so no tolerable upper intake level (UL) has been set.[11]
Blood clotting (coagulation) studies in humans using 45 mg per day of vitamin K2 (as MK-4)[12] and even up to 135 mg per day (45 mg three times daily) of K2 (as MK-4),[13] showed no increase in blood clot risk. Even doses in rats as high as 250 mg/kg, body weight did not alter the tendency for blood-clot formation to occur.[14]
Unlike the safe natural forms of vitamin K1 and vitamin K2 and their various isomers, a synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells.[2]
Phylloquinone (K1)[15][16] or menaquinone (K2) are capable of reversing the anticoagulant activity of the anticoagulant warfarin (tradename Coumadin). Warfarin works by blocking recycling of vitamin K, so that the body and tissues have lower levels of active vitamin K, and thus a deficiency of vitamin K.
Supplemental vitamin K (for which oral dosing is often more active than injectable dosing in human adults) reverses the vitamin K deficiency caused by warfarin, and therefore reduces the intended anticoagulant action of warfarin and related drugs.[17] Sometimes small amounts of vitamin K are given orally to patients taking warfarin so that the action of the drug is more predictable.[17] The proper anticoagulant action of the drug is a function of vitamin K intake and drug dose, and due to differing absorption must be individualized for each patient.[citation needed] The action of warfarin and vitamin K both require two to five days after dosing to have maximum effect, and neither warfarin or vitamin K shows much effect in the first 24 hours after they are given.[18]
The newer anticoagulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K, and may be taken with supplemental vitamin K.[19][20]
Vitamin K2 (menaquinone). In menaquinone, the side chain is composed of a varying number of isoprenoid residues. The most common number of these residues is four, since animal enzymes normally produce menaquinone-4 from plant phylloquinone.
A sample of phytomenadione for injection, also called phylloquinone
The three synthetic forms of vitamin K are vitamins K3 (menadione), K4, and K5, which are used in many areas, including the pet food industry (vitamin K3) and to inhibit fungal growth (vitamin K5).[21]
Conversion of vitamin K1 to vitamin K2[edit]
Vitamin K1 (phylloquinone) – both forms of the vitamin contain a functional naphthoquinone ring and an aliphatic side chain. Phylloquinone has a phytyl side chain.
The MK-4 form of vitamin K2 is produced by conversion of vitamin K1 in the testes, pancreas, and arterial walls.[22] While major questions still surround the biochemical pathway for this transformation, the conversion is not dependent on gut bacteria, as it occurs in germ-free rats[23][24] and in parenterally-administered K1 in rats.[25][26] In fact, tissues that accumulate high amounts of MK-4 have a remarkable capacity to convert up to 90% of the available K1 into MK-4.[27][28] There is evidence that the conversion proceeds by removal of the phytyl tail of K1 to produce menadione as an intermediate, which is then condensed with an activated geranylgeranyl moiety (see also prenylation) to produce vitamin K2 in the MK-4 (menatetrione) form.[29]
Vitamin K2[edit]
Main article: Vitamin K2
Vitamin K2 (menaquinone) includes several subtypes. The two subtypes most studied are menaquinone-4 (menatetrenone, MK-4) and menaquinone-7 (MK-7).
Vitamin K1, the precursor of most vitamin K in nature, is a stereoisomer of phylloquinone, an important chemical in green plants, where it functions as an electron acceptor in photosystem I during photosynthesis. For this reason, vitamin K1 is found in large quantities in the photosynthetic tissues of plants (green leaves, and dark green leafy vegetables such as romaine lettuce, kale and spinach), but it occurs in far smaller quantities in other plant tissues (roots, fruits, etc.). Iceberg lettuce contains relatively little. The function of phylloquinone in plants appears to have no resemblance to its later metabolic and biochemical function (as ""vitamin K"") in animals, where it performs a completely different biochemical reaction.
Vitamin K (in animals) is involved in the carboxylation of certain glutamate residues in proteins to form gamma-carboxyglutamate (Gla) residues. The modified residues are often (but not always) situated within specific protein domains called Gla domains. Gla residues are usually involved in binding calcium, and are essential for the biological activity of all known Gla proteins.[30]
At this time[update], 17 human proteins with Gla domains have been discovered, and they play key roles in the regulation of three physiological processes:
Blood coagulation: prothrombin (factor II), factors VII, IX, and X, and proteins C, S, and Z[31]
Bone metabolism: osteocalcin, also called bone Gla protein (BGP), matrix Gla protein (MGP),[32] periostin,[33] and the recently discovered Gla-rich protein (GRP).[34][35]
Vascular biology: growth arrest-specific protein 6 (Gas6)[36]
Unknown function: proline-rich γ-carboxyglutamyl proteins (PRGPs) 1 and 2, and transmembrane γ-carboxy glutamyl proteins (TMGs) 3 and 4.[37]
Like other lipid-soluble vitamins (A, D and E), vitamin K is stored in the fatty tissue of the human body.
Absorption and dietary need[edit]
Previous theory held that dietary deficiency is extremely rare unless the small intestine was heavily damaged, resulting in malabsorption of the molecule. Another at-risk group for deficiency were those subject to decreased production of K2 by normal intestinal microbiota, as seen in broad spectrum antibiotic use.[38] Taking broad-spectrum antibiotics can reduce vitamin K production in the gut by nearly 74% in people compared with those not taking these antibiotics.[39] Diets low in vitamin K also decrease the body's vitamin K concentration.[40] Those with chronic kidney disease are at risk for vitamin K deficiency, as well as vitamin D deficiency, and particularly those with the apoE4 genotype.[41] Additionally, in the elderly there is a reduction in vitamin K2 production.[42]
The National Academy of Medicine (NAM) updated an estimate of what constitutes an adequate intake (AI) for vitamin K in 2001. The NAM does not distinguish between K1 and K2 – both are counted as vitamin K. At that time there was not sufficient evidence to set the more rigorous estimated average requirement (EAR) or recommended dietary allowance (RDA) given for most of the essential vitamins and minerals. The current daily AIs for vitamin K for adult women and men are 90 μg and 120 μg respectively. The AI for pregnancy and lactation is 90 μg. For infants up to 12 months the AI is 2–2.5 μg, and for children aged 1 to 18 years the AI increases with age from 30 to 75 μg. As for safety, the FNB also sets tolerable upper intake levels (known as ULs) for vitamins and minerals when evidence is sufficient. In the case of vitamin K no UL is set, as evidence for adverse effects is not sufficient. Collectively EARs, RDAs, AIs and ULs are referred to as dietary reference intakes.[43] The European Food Safety Authority reviewed the same safety question and did not set an UL.[44]
For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percentage of daily value (%DV). For vitamin K labeling purposes the daily value was 80 μg, but as of May 2016 it has been revised upwards to 120 μg. A table of the pre-change adult daily values is provided at reference daily intake. Food and supplement companies have until 28 July 2018 to comply with the change.
See also: Vitamin K2 § Dietary sources
K1 (μg)[45]
Kale, cooked
Collards, cooked
Collards, raw
Swiss chard, cooked
Swiss chard, raw
Turnip greens, raw
Romaine lettuce, raw
Table from ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"", Clinical Center, National Institutes of Health Drug Nutrient Interaction Task Force.[46]
Vitamin K1 is found chiefly in leafy green vegetables such as dandelion greens (which contain 778.4 μg per 100 g, or 741% of the recommended daily amount), spinach, swiss chard, lettuce and Brassica vegetables (such as cabbage, kale, cauliflower, broccoli, and brussels sprouts) and often the absorption is greater when accompanied by fats such as butter or oils; some fruits, such as avocados, kiwifruit and grapes, are also high in vitamin K. By way of reference, two tablespoons of parsley contains 153% of the recommended daily amount of vitamin K.[47] Some vegetable oils, notably soybean oil, contain vitamin K, but at levels that would require relatively large calorie consumption to meet the USDA-recommended levels.[48] colonic bacteria synthesize a significant portion of humans' vitamin K needs; newborns often receive a vitamin K shot at birth to tide them over until their colons become colonized at five to seven days of age from the consumption of breast milk.
The tight binding of vitamin K1 to thylakoid membranes in chloroplasts makes it less bioavailable. For example, cooked spinach has a 5% bioavailability of phylloquinone, however, fat added to it increases bioavailability to 13% due to the increased solubility of vitamin K in fat.[49]
Main article: Vitamin K deficiency
Average diets are usually not lacking in vitamin K, and primary deficiency is rare in healthy adults. Newborn infants are at an increased risk of deficiency. Other populations with an increased prevalence of vitamin K deficiency include those who suffer from liver damage or disease (e.g. alcoholics), cystic fibrosis, or inflammatory bowel diseases, or have recently had abdominal surgeries. Secondary vitamin K deficiency can occur in people with bulimia, those on stringent diets, and those taking anticoagulants. Other drugs associated with vitamin K deficiency include salicylates, barbiturates, and cefamandole, although the mechanisms are still unknown. Vitamin K1 deficiency can result in coagulopathy, a bleeding disorder.[50]Symptoms of K1 deficiency include anemia, bruising, nosebleeds and bleeding of the gums in both sexes, and heavy menstrual bleeding in women.
Osteoporosis[51][52] and coronary heart disease[53][54] are strongly associated with lower levels of K2 (menaquinone). Vitamin K2 (as menaquinones MK-4 through MK-10) intake level is inversely related to severe aortic calcification and all-cause mortality.[8]
Function in animals[edit]
Mechanism of action of vitamin K1.
The function of vitamin K2 in the animal cell is to add a carboxylic acid functional group to a glutamate (Glu) amino acid residue in a protein, to form a gamma-carboxyglutamate (Gla) residue. This is a somewhat uncommon posttranslational modification of the protein, which is then known as a ""Gla protein"". The presence of two −COOH (carboxylic acid) groups on the same carbon in the gamma-carboxyglutamate residue allows it to chelate calcium ions. The binding of calcium ions in this way very often triggers the function or binding of Gla-protein enzymes, such as the so-called vitamin K-dependent clotting factors discussed below.
Within the cell, vitamin K undergoes electron reduction to a reduced form called vitamin K hydroquinone, catalyzed by the enzyme vitamin K epoxide reductase (VKOR).[55] Another enzyme then oxidizes vitamin K hydroquinone to allow carboxylation of Glu to Gla; this enzyme is called gamma-glutamyl carboxylase[56][57] or the vitamin K-dependent carboxylase. The carboxylation reaction only proceeds if the carboxylase enzyme is able to oxidize vitamin K hydroquinone to vitamin K epoxide at the same time. The carboxylation and epoxidation reactions are said to be coupled. Vitamin K epoxide is then reconverted to vitamin K by VKOR. The reduction and subsequent reoxidation of vitamin K coupled with carboxylation of Glu is called the vitamin K cycle.[58] Humans are rarely deficient in vitamin K1 because, in part, vitamin K1 is continuously recycled in cells.[59]
Warfarin and other 4-hydroxycoumarins block the action of VKOR.[60] This results in decreased concentrations of vitamin K and vitamin K hydroquinone in tissues, such that the carboxylation reaction catalyzed by the glutamyl carboxylase is inefficient. This results in the production of clotting factors with inadequate Gla. Without Gla on the amino termini of these factors, they no longer bind stably to the blood vessel endothelium and cannot activate clotting to allow formation of a clot during tissue injury. As it is impossible to predict what dose of warfarin will give the desired degree of clotting suppression, warfarin treatment must be carefully monitored to avoid overdose.
Gamma-carboxyglutamate proteins[edit]
Main article: Gla domain
The following human Gla-containing proteins (""Gla proteins"") have been characterized to the level of primary structure: blood coagulation factors II (prothrombin), VII, IX, and X, anticoagulant proteins C and S, and the factor X-targeting protein Z. The bone Gla protein osteocalcin, the calcification-inhibiting matrix Gla protein (MGP), the cell growth regulating growth arrest specific gene 6 protein (Gas6), and the four transmembrane Gla proteins (TMGPs), the function of which is at present unknown. Gas6 can function as a growth factor to activate the Axl receptor tyrosine kinase and stimulate cell proliferation or prevent apoptosis in some cells. In all cases in which their function was known, the presence of the Gla residues in these proteins turned out to be essential for functional activity.
Gla proteins are known to occur in a wide variety of vertebrates: mammals, birds, reptiles, and fish. The venom of a number of Australian snakes acts by activating the human blood-clotting system. In some cases, activation is accomplished by snake Gla-containing enzymes that bind to the endothelium of human blood vessels and catalyze the conversion of procoagulant clotting factors into activated ones, leading to unwanted and potentially deadly clotting.
Another interesting class of invertebrate Gla-containing proteins is synthesized by the fish-hunting snail Conus geographus.[61] These snails produce a venom containing hundreds of neuroactive peptides, or conotoxins, which is sufficiently toxic to kill an adult human. Several of the conotoxins contain two to five Gla residues.[62]
Methods of assessment[edit]
Vitamin K status can be assessed by:
The prothrombin time (PT) test measures the time required for blood to clot. A blood sample is mixed with citric acid and put in a fibrometer; delayed clot formation indicates a deficiency. This test is insensitive to mild deficiency, as the values do not change until the concentration of prothrombin in the blood has declined by at least 50%.[63]
Undercarboxylated prothrombin (PIVKA-II); in a study of 53 newborns, found ""PT (prothrombin time) is a less sensitive marker than PIVKA II"",[64] and as indicated above, PT is unable to detect subclinical deficiencies that can be detected with PIVKA-II testing.
Plasma phylloquinone was found to be positively correlated with phylloquinone intake in elderly British women, but not men,[65] but an article by Schurgers et al. reported no correlation between FFQ[further explanation needed] and plasma phylloquinone.[66]
Urinary γ-carboxyglutamic acid responds to changes in dietary vitamin K intake. Several days are required before any change can be observed. In a study by Booth et al., increases of phylloquinone intakes from 100 μg to between 377 and 417 μg for five days did not induce a significant change. Response may be age-specific.[67]
Undercarboxylated osteocalcin (UcOc) levels have been inversely correlated with stores of vitamin K[68] and bone strength in developing rat tibiae. Another study following 78 post-menopausal Korean women found a supplement regimen of vitamins K and D, and calcium, but not a regimen of vitamin D and calcium, was inversely correlated with reduced UcOc levels.[69]
Function in bacteria[edit]
Many bacteria, such as Escherichia coli found in the large intestine, can synthesize vitamin K2 (menaquinone-7 or MK-7, up to MK-11),[70] but not vitamin K1 (phylloquinone). In these bacteria, menaquinone transfers two electrons between two different small molecules, during oxygen-independent metabolic energy production processes (anaerobic respiration).[71] For example, a small molecule with an excess of electrons (also called an electron donor) such as lactate, formate, or NADH, with the help of an enzyme, passes two electrons to menaquinone. The menaquinone, with the help of another enzyme, then transfers these two electrons to a suitable oxidant, such fumarate or nitrate (also called an electron acceptor). Adding two electrons to fumarate or nitrate converts the molecule to succinate or nitrite plus water, respectively.
Some of these reactions generate a cellular energy source, ATP, in a manner similar to eukaryotic cell aerobic respiration, except the final electron acceptor is not molecular oxygen, but fumarate or nitrate. In aerobic respiration, the final oxidant is molecular oxygen (O2), which accepts four electrons from an electron donor such as NADH to be converted to water. E. coli, as facultative anaerobes, can carry out both aerobic respiration and menaquinone-mediated anaerobic respiration.
Injection in newborns[edit]
The blood clotting factors of newborn babies are roughly 30–60% that of adult values; this may be due to the reduced synthesis of precursor proteins and the sterility of their guts. Human milk contains 1–4 μg/L of vitamin K1, while formula-derived milk can contain up to 100 μg/L in supplemented formulas. Vitamin K2 concentrations in human milk appear to be much lower than those of vitamin K1. Occurrence of vitamin K deficiency bleeding in the first week of the infant's life is estimated at 0.25–1.7%, with a prevalence of 2–10 cases per 100,000 births.[72] Premature babies have even lower levels of the vitamin, so they are at a higher risk from this deficiency.
Bleeding in infants due to vitamin K deficiency can be severe, leading to hospitalization, blood transfusions, brain damage, and death. Supplementation can prevent most cases of vitamin K deficiency bleeding in the newborn. Intramuscular administration is more effective in preventing late vitamin K deficiency bleeding than oral administration.[73][74]
As a result of the occurrences of vitamin K deficiency bleeding, the Committee on Nutrition of the American Academy of Pediatrics has recommended 0.5–1 mg of vitamin K1 be administered to all newborns shortly after birth.[74]
In the UK vitamin K supplementation is recommended for all newborns within the first 24 hours.[75] This is usually given as a single intramuscular injection of 1 mg shortly after birth but as a second-line option can be given by three oral doses over the first month.[76]
Controversy arose in the early 1990s regarding this practice, when two studies suggested a relationship between parenteral administration of vitamin K and childhood cancer,[77] however, poor methods and small sample sizes led to the discrediting of these studies, and a review of the evidence published in 2000 by Ross and Davies found no link between the two.[78] Doctors reported emerging concerns in 2013,[79] after treating children for serious bleeding problems. They cited lack-of newborn vitamin K administration, as the reason that the problems occurred, and recommended that breastfed babies could have an increased risk unless they receive a preventative dose.
In the early 1930s, Danish scientist Henrik Dam investigated the role of cholesterol by feeding chickens a cholesterol-depleted diet.[80] He initially replicated experiments reported by scientists at the Ontario Agricultural College (OAC).[81] McFarlane, Graham and Richardson, working on the chick feed program at OAC, had used chloroform to remove all fat from chick chow. They noticed that chicks fed only fat-depleted chow developed hemorrhages and started bleeding from tag sites.[82] Dam found that these defects could not be restored by adding purified cholesterol to the diet. It appeared that – together with the cholesterol – a second compound had been extracted from the food, and this compound was called the coagulation vitamin. The new vitamin received the letter K because the initial discoveries were reported in a German journal, in which it was designated as Koagulationsvitamin. Edward Adelbert Doisy of Saint Louis University did much of the research that led to the discovery of the structure and chemical nature of vitamin K.[83] Dam and Doisy shared the 1943 Nobel Prize for medicine for their work on vitamin K (K1 and K2) published in 1939. Several laboratories synthesized the compound(s) in 1939.[84]
For several decades, the vitamin K-deficient chick model was the only method of quantifying vitamin K in various foods: the chicks were made vitamin K-deficient and subsequently fed with known amounts of vitamin K-containing food. The extent to which blood coagulation was restored by the diet was taken as a measure for its vitamin K content. Three groups of physicians independently found this: Biochemical Institute, University of Copenhagen (Dam and Johannes Glavind), University of Iowa Department of Pathology (Emory Warner, Kenneth Brinkhous, and Harry Pratt Smith), and the Mayo Clinic (Hugh Butt, Albert Snell, and Arnold Osterberg).[85]
The first published report of successful treatment with vitamin K of life-threatening hemorrhage in a jaundiced patient with prothrombin deficiency was made in 1938 by Smith, Warner, and Brinkhous.[86]
The precise function of vitamin K was not discovered until 1974, when three laboratories (Stenflo et al.,[87] Nelsestuen et al.,[88] and Magnusson et al.[89]) isolated the vitamin K-dependent coagulation factor prothrombin (factor II) from cows that received a high dose of a vitamin K antagonist, warfarin. It was shown that, while warfarin-treated cows had a form of prothrombin that contained 10 glutamate (Glu) amino acid residues near the amino terminus of this protein, the normal (untreated) cows contained 10 unusual residues that were chemically identified as γ-carboxyglutamate (Gla). The extra carboxyl group in Gla made clear that vitamin K plays a role in a carboxylation reaction during which Glu is converted into Gla.
The biochemistry of how vitamin K is used to convert Glu to Gla has been elucidated over the past thirty years in academic laboratories throughout the world.
^ ""Vitamin K Overview"". University of Maryland Medical Center. ^ a b Higdon, Jane (Feb 2008). ""Vitamin K"". Linus Pauling Institute, Oregon State University. Retrieved 12 Apr 2008. ^ Hamidi, M. S.; Gajic-Veljanoski, O.; Cheung, A. M. (2013). ""Vitamin K and bone health"". Journal of Clinical Densitometry (Review). 16 (4): 409–413. doi:10.1016/j.jocd.2013.08.017. PMID 24090644. ^ Cockayne, S.; Adamson, J.; Lanham-New, S.; Shearer, M. J.; Gilbody, S; Torgerson, D. J. (Jun 2006). ""Vitamin K and the prevention of fractures: systematic review and meta-analysis of randomized controlled trials"". Archives of Internal Medicine (Review). 166 (12): 1256–1261. doi:10.1001/archinte.166.12.1256. PMID 16801507. ^ O'Keefe, J. H.; Bergman, N.; Carrera Bastos, P.; Fontes Villalba, M.; Di Nicolantonio, J. J.; Cordain, L. (2016). ""Nutritional strategies for skeletal and cardiovascular health: hard bones, soft arteries, rather than vice versa"". Open Heart (Review). 3 (1): e000325. doi:10.1136/openhrt-2015-000325. PMC 4809188. PMID 27042317. ^ Maresz, K. (Feb 2015). ""Proper Calcium Use: Vitamin K2 as a Promoter of Bone and Cardiovascular Health"". Integrative Medicine (Review). 14 (1): 34–39. PMC 4566462. PMID 26770129. ^ Hartley, L.; Clar, C.; Ghannam, O.; Flowers, N.; Stranges, S.; Rees, K. (Sep 2015). ""Vitamin K for the primary prevention of cardiovascular disease"". The Cochrane Database of Systematic Reviews (Systematic review). 9 (9): CD011148. doi:10.1002/14651858.CD011148.pub2. PMID 26389791. ^ a b Geleijnse, J. M.; Vermeer, C.; Grobbee, D. E.; Schurgers, L. J.; Knapen, M. H.; van der Meer, I. M.; Hofman, A.; Witteman, J. C. (Nov 2004). ""Dietary intake of menaquinone is associated with a reduced risk of coronary heart disease: the Rotterdam Study"". Journal of Nutrition. 134 (11): 3100–3105. PMID 15514282. ^ Ades, T. B., ed. (2009). ""Vitamin K"". American Cancer Society Complete Guide to Complementary and Alternative Cancer Therapies (2nd ed.). American Cancer Society. pp. 558–563. ISBN 978-0-944235-71-3. ^ Lung, D. (Dec 2015). Tarabar, A., ed. ""Rodenticide Toxicity Treatment & Management"". Medscape. WebMD. ^ Rasmussen, S. E.; Andersen, N. L.; Dragsted, L. O.; Larsen, J. C. (Mar 2006). ""A safe strategy for addition of vitamins and minerals to foods"". European Journal of Nutrition. 45 (3): 123–135. doi:10.1007/s00394-005-0580-9. PMID 16200467. ^ Ushiroyama, T.; Ikeda, A.; Ueki, M (Mar 2002). ""Effect of continuous combined therapy with vitamin K2 and vitamin D3 on bone mineral density and coagulofibrinolysis function in postmenopausal women"". Maturitas. 41 (3): 211–221. doi:10.1016/S0378-5122(01)00275-4. PMID 11886767. ^ Asakura, H.; Myou, S.; Ontachi, Y.; Mizutani, T.; Kato, M.; Saito, M.; Morishita, E.; Yamazaki, M.; Nakao, S. (Dec 2001). ""Vitamin K administration to elderly patients with osteoporosis induces no hemostatic activation, even in those with suspected vitamin K deficiency"". Osteoporosis International. 12 (12): 996–1000. doi:10.1007/s001980170007. PMID 11846334. ^ Ronden, J. E.; Groenen-van Dooren, M. M.; Hornstra, G.; Vermeer, C. (Jul 1997). ""Modulation of arterial thrombosis tendency in rats by vitamin K and its side chains"". Atherosclerosis. 132 (1): 61–67. doi:10.1016/S0021-9150(97)00087-7. PMID 9247360. ^ Ansell, J.; Hirsh, J.; Poller, L.; Bussey, H.; Jacobson, A.; Hylek, E (Sep 2004). ""The pharmacology and management of the vitamin K antagonists: the Seventh ACCP Conference on Antithrombotic and Thrombolytic Therapy"". Chest. 126 (3 Suppl.): 204S–233S. doi:10.1378/chest.126.3_suppl.204S. PMID 15383473. ^ Crowther, M. A.; Douketis, J. D.; Schnurr, T.; Steidl, L.; Mera, V.; Ultori, C.; Venco, A.; Ageno, W. (Aug 2002). ""Oral vitamin K lowers the international normalized ratio more rapidly than subcutaneous vitamin K in the treatment of warfarin-associated coagulopathy. A randomized, controlled trial"". Annals of Internal Medicine. 137 (4): 251–254. doi:10.7326/0003-4819-137-4-200208200-00009. PMID 12186515. ^ a b ""Important Information to Know When You Are Taking: Warfarin (Coumadin) and Vitamin K"" (PDF). National Institute of Health Clinical Center Drug-Nutrient Interaction Task Force. Retrieved 17 Apr 2015. ^ ""Guidelines For Warfarin Reversal With Vitamin K"" (PDF). American Society of Health-System Pharmacists. Retrieved 17 Apr 2015. ^ ""Pradaxa Drug Interactions"". Pradaxapro.com. 19 Mar 2012. Retrieved 21 Apr 2013. ^ Bauersachs, R.; Berkowitz, S. D.; Brenner, B.; Buller, H. R.; Decousus, H.; Gallus, A. S.; Lensing, A. W.; Misselwitz, F.; Prins, M. H.; Raskob, G. E.; Segers, A.; Verhamme, P.; Wells, P.; Agnelli, G.; Bounameaux, H.; Cohen, A.; Davidson, B. L.; Piovella, F.; Schellong, S. (Dec 2010). ""Oral rivaroxaban for symptomatic venous thromboembolism"". New England Journal of Medicine. 363 (26): 2499–2510. doi:10.1056/NEJMoa1007903. PMID 21128814. ^ McGee, W. (1 Feb 2007). ""Vitamin K"". MedlinePlus. Retrieved 2 Apr 2009. ^ Shearer, M. J.; Newman, P. (Oct 2008). ""Metabolism and cell biology of vitamin K"". Thrombosis and Haemostasis. 100 (4): 530–547. doi:10.1160/TH08-03-0147. PMID 18841274. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). ""Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria"". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). ""Intestinal flora is not an intermediate in the phylloquinone–menaquinone-4 conversion in the rat"". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Thijssen, H. .H.; Drittij-Reijnders, M. J. (Sep 1994). ""Vitamin K distribution in rat tissues: dietary phylloquinone is a source of tissue menaquinone-4"". The British Journal of Nutrition. 72 (3): 415–425. doi:10.1079/BJN19940043. PMID 7947656. ^ Will, B. H.; Usui, Y.; Suttie, J. W. (Dec 1992). ""Comparative metabolism and requirement of vitamin K in chicks and rats"". Journal of Nutrition. 122 (12): 2354–2360. PMID 1453219. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). ""Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria"". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). ""Intestinal flora is not an intermediate in the phylloquinone-menaquinone-4 conversion in the rat"". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Al Rajabi, Ala (2011). The Enzymatic Conversion of Phylloquinone to Menaquinone-4 (PhD thesis). Tufts University, Friedman School of Nutrition Science and Policy. ^ Furie, B.; Bouchard, B. A.; Furie, B. C. (Mar 1999). ""Vitamin K-dependent biosynthesis of gamma-carboxyglutamic acid"". Blood. 93 (6): 1798–1808. PMID 10068650. ^ Mann, K. G. (Aug 1999). ""Biochemistry and physiology of blood coagulation"". Thrombosis and Haemostasis. 82 (2): 165–174. PMID 10605701. ^ Price, P. A. (1988). ""Role of vitamin-K-dependent proteins in bone metabolism"". Annual Review of Nutrition. 8: 565–583. doi:10.1146/annurev.nu.08.070188.003025. PMID 3060178. ^ Coutu, D. L.; Wu, J. H.; Monette, A.; Rivard, G. E.; Blostein, M. D.; Galipeau, J (Jun 2008). ""Periostin, a member of a novel family of vitamin K-dependent proteins, is expressed by mesenchymal stromal cells"". Journal of Biological Chemistry. 283 (26): 17991–18001. doi:10.1074/jbc.M708029200. PMID 18450759. ^ Viegas, C. S.; Simes, D. C.; Laizé, V.; Williamson, M. K.; Price, P. A.; Cancela, M. L. (Dec 2008). ""Gla-rich protein (GRP), a new vitamin K-dependent protein identified from sturgeon cartilage and highly conserved in vertebrates"". Journal of Biological Chemistry. 283 (52): 36655–36664. doi:10.1074/jbc.M802761200. PMC 2605998. PMID 18836183. ^ Viegas, C. S.; Cavaco, S.; Neves, P. L.; Ferreira, A.; João, A.; Williamson, M. K.; Price, P. A.; Cancela, M. L.; Simes, D. C. (Dec 2009). ""Gla-rich protein is a novel vitamin K-dependent protein present in serum that accumulates at sites of pathological calcifications"". American Journal of Pathology. 175 (6): 2288–2298. doi:10.2353/ajpath.2009.090474. PMC 2789615. PMID 19893032. ^ Hafizi, S.; Dahlbäck, B. (Dec 2006). ""Gas6 and protein S. Vitamin K-dependent ligands for the Axl receptor tyrosine kinase subfamily"". The FEBS Journal. 273 (23): 5231–5244. doi:10.1111/j.1742-4658.2006.05529.x. PMID 17064312. ^ Kulman, J. D.; Harris, J. E.; Xie, L.; Davie, E. W. (May 2007). ""Proline-rich Gla protein 2 is a cell-surface vitamin K-dependent protein that binds to the transcriptional coactivator Yes-associated protein"". Proceedings of the National Academy of Sciences of the United States of America. 104 (21): 8767–8772. doi:10.1073/pnas.0703195104. PMC 1885577. PMID 17502622. ^ ""Vitamin K"". MedlinePlus. US National Library of Medicine, National Institutes of Health. Sep 2016. Retrieved 26 May 2009. ^ Conly, J; Stein, K. (Dec 1994). ""Reduction of vitamin K2 concentrations in human liver associated with the use of broad spectrum antimicrobials"". Clinical and Investigative Medicine. 17 (6): 531–539. PMID 7895417. ^ Ferland, G.; Sadowski, J. A.; O'Brien, M. E. (Apr 1993). ""Dietary induced subclinical vitamin K deficiency in normal human subjects"". Journal of Clinical Investigation. 91 (4): 1761–1768. doi:10.1172/JCI116386. PMC 288156. PMID 8473516. ^ Holden, R. M.; Morton, A. R.; Garland, J. S.; Pavlov, A.; Day, A. G.; Booth, S. L. (Apr 2010). ""Vitamins K and D status in stages 3-5 chronic kidney disease"". Clinical Journal of the American Society of Nephrology. 5 (4): 590–597. doi:10.2215/CJN.06420909. PMC 2849681. PMID 20167683. ^ Hodges, S. J.; Pilkington, M. J.; Shearer, M. J.; Bitensky, L.; Chayen, J (Jan 1990). ""Age-related changes in the circulating levels of congeners of vitamin K2, menaquinone-7 and menaquinone-8"". Clinical Science. 78 (1): 63–66. PMID 2153497. ^ ""Vitamin K"". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press. 2001. p. 162–196. ^ Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006 ^ a b Rhéaume-Bleue, p. 42
^ ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"" (PDF). National Institutes of Health Clinical Center. ^ ""Nutrition Facts and Information for Parsley, raw"". Nutritiondata.com. Retrieved 21 Apr 2013. ^ ""Nutrition facts, calories in food, labels, nutritional information and analysis"". Nutritiondata.com. 13 Feb 2008. Retrieved 21 Apr 2013. ^ ""Vitamin K"". Vivo.colostate.edu. 2 Jul 1999. Retrieved 21 Apr 2013. ^ ""Vitamin K"". Micronutrient Data Centre. ^ Ikeda, Y.; Iki, M.; Morita, A.; Kajita, E.; Kagamimori, S.; Kagawa, Y.; Yoneshima, H. (May 2006). ""Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women: Japanese Population-Based Osteoporosis (JPOS) Study"". Journal of Nutrition. 136 (5): 1323–1328. PMID 16614424. ^ Katsuyama, H.; Ideguchi, S.; Fukunaga, M.; Saijoh, K.; Sunami, S. (Jun 2002). ""Usual dietary intake of fermented soybeans (Natto) is associated with bone mineral density in premenopausal women"". Journal of Nutritional Science and Vitaminology. 48 (3): 207–215. doi:10.3177/jnsv.48.207. PMID 12350079. ^ Sano, M.; Fujita, H.; Morita, I.; Uematsu, H.; Murota, S. (Dec 1999). ""Vitamin K2 (menatetrenone) induces iNOS in bovine vascular smooth muscle cells: no relationship between nitric oxide production and gamma-carboxylation"". Journal of Nutritional Science and Vitaminology. 45 (6): 711–723. doi:10.3177/jnsv.45.711. PMID 10737225. ^ Gast, G. C ; de Roos, N. M.; Sluijs, I.; Bots, M. L.; Beulens, J. W.; Geleijnse, J. M.; Witteman, J. C.; Grobbee, D. E.; Peeters, P. H.; van der Schouw, Y. T. (Sep 2009). ""A high menaquinone intake reduces the incidence of coronary heart disease"". Nutrition, Metabolism, and Cardiovascular Diseases. 19 (7): 504–510. doi:10.1016/j.numecd.2008.10.004. PMID 19179058. ^ Oldenburg, J.; Bevans, C. G.; Müller, C. R.; Watzka, M. (2006). ""Vitamin K epoxide reductase complex subunit 1 (VKORC1): the key protein of the vitamin K cycle"". Antioxidants & Redox Signaling. 8 (3–4): 347–353. doi:10.1089/ars.2006.8.347. PMID 16677080. ^ Suttie, J. W. (1985). ""Vitamin K-dependent carboxylase"". Annual Review of Biochemistry. 54: 459–477. doi:10.1146/annurev.bi.54.070185.002331. PMID 3896125. ^ Presnell, S. R.; Stafford, D. W. (Jun 2002). ""The vitamin K-dependent carboxylase"". Thrombosis and Haemostasis. 87 (6): 937–946. PMID 12083499. ^ Stafford, D. W. (Aug 2005). ""The vitamin K cycle"". Journal of Thrombosis and Haemostasis. 3 (8): 1873–1878. doi:10.1111/j.1538-7836.2005.01419.x. PMID 16102054. ^ Rhéaume-Bleue, p. 79.
^ Whitlon, D. S.; Sadowski, J. A.; Suttie, J. W. (Apr 1978). ""Mechanism of coumarin action: significance of vitamin K epoxide reductase inhibition"". Biochemistry. 17 (8): 1371–1377. doi:10.1021/bi00601a003. PMID 646989. ^ Terlau, H.; Olivera, B. M. (Jan 2004). ""Conus venoms: a rich source of novel ion channel-targeted peptides"". Physiological Reviews. 84 (1): 41–68. doi:10.1152/physrev.00020.2003. PMID 14715910. ^ Buczek, O.; Bulaj, G.; Olivera, BM (Dec 2005). ""Conotoxins and the posttranslational modification of secreted gene products"". Cellular and Molecular Life Sciences. 62 (24): 3067–3079. doi:10.1007/s00018-005-5283-0. PMID 16314929. ^ ""Prothrombin Time"". WebMD. ^ Dituri, F.; Buonocore, G.; Pietravalle, A.; Naddeo, F.; Cortesi, M; Pasqualetti, P; Tataranno M. L.; R., Agostino (Sep 2012). ""PIVKA-II plasma levels as markers of subclinical vitamin K deficiency in term infants"". Journal of Maternal, Fetal & Neonatal Medicine. 25 (9): 1660–1663. doi:10.3109/14767058.2012.657273. PMID 22280352. ^ Thane, C. W.; Bates, C. J.; Shearer, M. J.; Unadkat, N; Harrington, D. J.; Paul, A. A.; Prentice, A.; Bolton-Smith, C. (Jun 2002). ""Plasma phylloquinone (vitamin K1) concentration and its relationship to intake in a national sample of British elderly people"". British Journal of Nutrition. 87 (6): 615–622. doi:10.1079/BJNBJN2002582. PMID 12067432. ^ McKeown, N. M.; Jacques, P. F.; Gundberg, C. M.; Peterson, J. W.; Tucker, K. L.; Kiel, D. P.; Wilson, P. W.; Booth, SL (Jun 2002). ""Dietary and nondietary determinants of vitamin K biochemical measures in men and women"" (PDF). Journal of Nutrition. 132 (6): 1329–1334. PMID 12042454. ^ Yamano, M.; Yamanaka, Y.; Yasunaga, K.; Uchida, K. (Sep 1989). ""Effect of vitamin K deficiency on urinary gamma-carboxyglutamic acid excretion in rats"". Nihon Ketsueki Gakkai Zasshi. 52 (6): 1078–1086. PMID 2588957. ^ Matsumoto, T.; Miyakawa, T.; Yamamoto, D. (Mar 2012). ""Effects of vitamin K on the morphometric and material properties of bone in the tibiae of growing rats"". Metabolism. 61 (3): 407–414. doi:10.1016/j.metabol.2011.07.018. PMID 21944271. ^ Je, S.-H.; Joo, N.-S.; Choi, B.-H.; Kim, K.-M.; Kim, B.-T.; Park, S.-B.; Cho, D.-Y.; Kim, K.-N.; Lee, D.-J. (Aug 2011). ""Vitamin K supplement along with vitamin D and calcium reduced serum concentration of undercarboxylated osteocalcin while increasing bone mineral density in Korean postmenopausal women over sixty-years-old"". Journal of Korean Medical Science. 26 (8): 1093–1098. doi:10.3346/jkms.2011.26.8.1093. PMC 3154347. PMID 21860562. ^ Bentley, R.; Meganathan, R. (Sep 1982). ""Biosynthesis of vitamin K (menaquinone) in bacteria"" (PDF). Microbiological Reviews. 46 (3): 241–280. PMC 281544. PMID 6127606. ^ Haddock, B. A.; Jones, C. W. (Mar 1977). ""Bacterial respiration"" (PDF). Bacteriological Reviews. 41 (1): 47–99. PMC 413996. PMID 140652. ^ Shearer, M. J. (Jan 1995). ""Vitamin K"". Lancet. 345 (8944): 229–234. doi:10.1016/S0140-6736(95)90227-9. PMID 7823718. ^ Greer, J. P.; Foerster, J.; Lukens, J. N.; Rodgers, G. M.; Paraskevas, F.; Glader, B. (eds.). Wintrobe's Clinical Hematology (11th ed.). Philadelphia, Pennsylvania: Lippincott, Williams and Wilkens. ^ a b American Academy of Pediatrics Committee on Fetus Newborn. (Jul 2003). ""Controversies concerning vitamin K and the newborn. American Academy of Pediatrics Committee on Fetus and Newborn"" (PDF). Pediatrics. 112 (1.1): 191–192. doi:10.1542/peds.112.1.191. PMID 12837888. ^ Logan, S.; Gilbert, R. (1998). ""Vitamin K For Newborn Babies"" (PDF). Department of Health. Retrieved 12 Oct 2014. ^ ""Postnatal care: Routine postnatal care of women and their babies [CG37]"". www.nice.org.uk. NICE. Jul 2006. Retrieved 12 Oct 2014. ^ Parker, L.; Cole, M.; Craft, A. W.; Hey, E. N. (1998). ""Neonatal vitamin K administration and childhood cancer in the north of England: retrospective case-control study"". BMJ (Clinical Research Edition). 316 (7126): 189–193. doi:10.1136/bmj.316.7126.189. PMC 2665412. PMID 9468683. ^ McMillan, D. D. (1997). ""Routine administration of vitamin K to newborns"". Paediatric Child Health. 2 (6): 429–431. ^ ""Newborns get rare disorder after parents refused shots"". Having four cases since February just at Vanderbilt was a little bit concerning to me ^ Dam, C. P. H. (1935). ""The Antihaemorrhagic Vitamin of the Chick: Occurrence And Chemical Nature"". Nature. 135 (3417): 652–653. doi:10.1038/135652b0. ^ Dam, C. P. H. (1941). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize Laureate Lecture. ^ McAlister, V. C. (2006). ""Control of coagulation: a gift of Canadian agriculture"" (PDF). Clinical and Investigative Medicine. 29 (6): 373–377. ^ MacCorquodale, D. W.; Binkley, S. B.; Thayer, S. A.; Doisy, E. A. (1939). ""On the constitution of Vitamin K1"". Journal of the American Chemical Society. 61 (7): 1928–1929. doi:10.1021/ja01876a510. ^ Fieser, L. F. (1939). ""Synthesis of Vitamin K1"". Journal of the American Chemical Society. 61 (12): 3467–3475. doi:10.1021/ja01267a072. ^ Dam, C. P. H. (12 Dec 1946). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize lecture. ^ Warner, E. D.; Brinkhous, K. M.; Smith, H. P. (1938). ""Bleeding Tendency of Obstructive Jaundice"". Proceedings of the Society of Experimental Biology and Medicine. 37 (4): 628–630. doi:10.3181/00379727-37-9668P. ^ Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). ""Vitamin K dependent modifications of glutamic acid residues in prothrombin"". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730–2733. doi:10.1073/pnas.71.7.2730. PMC 388542. PMID 4528109. ^ Nelsestuen, G. L.; Zytkovicz, T. H.; Howard, J. B. (Oct 1974). ""The mode of action of vitamin K. Identification of gamma-carboxyglutamic acid as a component of prothrombin"" (PDF). Journal of Biological Chemistry. 249 (19): 6347–6350. PMID 4214105. ^ Magnusson, S.; Sottrup-Jensen, L.; Petersen, T. E.; Morris, H. R.; Dell, A. (Aug 1974). ""Primary structure of the vitamin K-dependent part of prothrombin"". FEBS Letters. 44 (2): 189–193. doi:10.1016/0014-5793(74)80723-4. PMID 4472513. Bibliography[edit]
Rhéaume-Bleue, Kate (2012). Vitamin K2 and the Calcium Paradox. John Wiley & Sons, Canada. ISBN 1-118-06572-7. External links[edit]
""Vitamin K: Another Reason to Eat Your Greens"". v
TPP / ThDP (B1)
FMN, FAD (B2)
NAD+, NADH, NADP+, NADPH (B3)
Coenzyme A (B5)
PLP / P5P (B6)
THFA / H4FA, DHFA / H2FA, MTHF (B9)
AdoCbl, MeCbl (B12)
Phylloquinone (K1), Menaquinone (K2)
non-vitamins
Coenzyme B
Heme / Haem (A, B, C, O)
Molybdopterin/Molybdenum cofactor
THMPT / H4MPT
Fe2+, Fe3+
vitamins: see vitamins
Antihemorrhagics (B02)
(coagulation)
Phytomenadione (K1)
Menadione (K3)
intrinsic: IX/Nonacog alfa
VIII/Moroctocog alfa/Turoctocog alfa
extrinsic: VII/Eptacog alfa
common: X
II/Thrombin
I/Fibrinogen
XIII/Catridecacog
combinations: Prothrombin complex concentrate (II, VII, IX, X, protein C and S)
Carbazochrome
thrombopoietin receptor agonist (Romiplostim
Eltrombopag)
Tetragalacturonic acid hydroxymethylester
Epinephrine/Adrenalone
amino acids (Aminocaproic acid
Aminomethylbenzoic acid)
serpins (Aprotinin
Alfa1 antitrypsin
Camostat).",['90 μg for women and 120 μg for men.'],7142,multifieldqa_en,en,,b3f3be2f0b46c0df08868f749519635186e6e22cf054ca79," This article needs more medical references for verification or relies too heavily on primary sources. Unsourced or poorly sourced material may be challenged and removed. For vitamin K1 the form usually used as a supplement, see Phytomenadione. MK-4 and MK-7 are both subtypes of K2. Vitamin K1, also known as phylloquinone, is made by plants, and is found in highest amounts in green leafy vegetables because it is directly involved in photosynthesis. It is active as a vitamin in animals and performs the classic functions of vitamin K, including its activity in the production of blood-clotting proteins. Animals may also convert it to vitamin K2, which can only be produced by bacteria. The natural K1 and all K2 homologues and synthetic K4 and K5 have proven nontoxic, but the synthetic form K3 (menadione) has shown toxicity. A review of 2014 concluded that there is positive evidence that monotherapy using one of the forms of Vitamin K2 reduces fracture incidence in post-menopausal women with osteoporosis. In contrast, an earlier review article of 2013 concluded there is no good evidence that vitamin K supplementation helps prevent osteoarthritis or fractures in post menopausal women. A Cochrane systematic review of 2006 suggested that supplementation with Vitamin K 1 and with MK4 reduces bone loss; in particular, a strong effect ofMK-4 on incident fractures among Japanese patients was emphasized. The vitamin K family comprises 2-methyl-1,4-naphthoquinone (3-) derivatives. It also comprises vitamins K3, K4, andK5. It includes two natural vitamers: vitamin K3 and vitamin K4. The synthetic vitamin K5 has proven to be toxic, but there have been few interventional studies on it. It has also been shown to be associated with the inhibition of arterial calcification and stiffening. It was suggested to consider, as one of several measures for health, increasing the intake of foods rich in vitamins K1. and K2.[2]Cardiovascular health is associated with increased vitamin K intake. The body also needs vitamin K for controlling binding of calcium in bones and other tissues. The human body requires vitamin K to complete synthesis of certain proteins that are prerequisites for blood coagulation and which it also needs for controlling calcium ions, which they cannot do otherwise. Low levels ofitamin K also weaken bones and promote calcification of arteries and other soft tissues[citation needed].Chemically, the vitamin KFamily comprises 2,methyl- 1,4,naphthylquinone-3- derivatives. The K2 subtype is the only one that can be made by bacteria, which use these forms in anaerobic respiration. In addition, bacteria typically lengthen the isoprenoid side chain of vitamin. K2 to produce a range of vitaminK2 forms, most notably theMK-7 to MK-11 homologue of vitamin k2. It can also be found in the gut flora, but its utility is unclear and is a matter of investigation. Injection in newborns is a controversial issue, and there has been little research on the combined use of MK- 4 with bisphosphonates. No good evidence that vitamin K supplementation is of any benefit in the primary prevention of cardiovascular disease. Synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. Vitamin K is part of the suggested treatment regime for poisoning by rodenticide (coumarin poisoning).[10] No known toxicity is associated with high doses of the phylloquinone (vitamin K1) or menaquinone (Vitamin K2) forms of Vitamin K. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells. Sometimes small amounts of vitaminK are given orally to patients taking warfarin so that the action of the drug is more predictable. The proper anticoagulant action is a function of vitamink intake and drug dose, and due to differing absorption must be individualized for each patient. The action of warfar in and vitamin K both require two to five days after dosing to have maximum effect. Neither warfarins or vitamin K shows much effect in the first 24 hours after they are given. The newer anticoageulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K. VitaminK is produced by conversion of vitamin k1 to vitamin K2. It is also used in many areas, including the pet food industry (v vitamin K5) and to inhibit fungal growth (v vitamins K3 and K4) in rats. It can slow tumor growth, but there is no good medical evidence that supports such claims. It also reverses the vitamin K deficiency caused by Warfarin, and therefore reduces the intended anticogeulantaction of warFarin and related drugs. It has been shown that vitaminK can be used to treat cancer. It's also used to prevent heart attacks and strokes in some people with heart conditions, such as atrial fibrillation and atrial torsion, and to treat certain forms of cancer in humans and animals. It may be used in the treatment of cancer of the pancreas and other organs. It could also be used as a treatment for cancer in the lungs and other parts of the digestive system, and as an anti-viral drug in the skin. It does not have a known side-effect in humans, but some studies have shown that it may have a positive effect on the immune system in some patients. It cannot be used for cancer treatment in humans or in animals, however, as it is not known if it has any effect on cancer-causing side-effects in the gut or in other organs such as the liver or in the liver. It was used in a study to test the safety and effectiveness of the anti-tumor drug Cytotecan in humans in the 1980s and 1990s, but it has since been ruled out by the FDA as not being safe for use in humans. There is no safe upper intake level (UL) for vitamin K1 (K1) and K2 (K2) in human adults. The most common number of isoprenoid residues is four, since animal enzymes normally produce menaquin one-4 from plant phyllaquinone. The conversion is not dependent on gut-free rats[23] as it occurs in germ-free Rats. Vitamin K1 is a stereoisomer of phylloquinone, an important chemical in green plants. It is found in large quantities in the photosynthetic tissues of plants (green leaves, and dark green leafy vegetables such as romaine lettuce, kale and spinach) It occurs in far smaller quantities in other plant tissues (roots, fruits, etc.) Iceberg lettuce contains relatively little. Vitamin K (in animals) is involved in the carboxylation of certain glutamate residues in proteins to form gamma-carboxyglutamate (Gla) residues. The modified residues are usually involved in binding calcium, and are essential for the biological activity of all known Gla proteins. 17 human proteins with Gla domains have been discovered, and they play key roles in the regulation of three physiological processes: blood coagulation, bone metabolism and vascular biology. Diets low in vitamin K also decrease the body's vitamin K concentration. Those with chronic kidney disease are at risk for vitamin K deficiency, as well as vitamin D deficiency, and particularly those with the apoE4 genotype. The National Academy of Medicine updated an estimate of what constitutes an adequate intake of vitamin K in 2001. The NAM does not distinguish between K1 and K2 – both are counted as vitamin K. The current daily requirement for Vitamin K for adult women and men is 90 μg for pregnancy and 120 μg for infants. For infants up to 12 months, the AI is 2–5 μg for lactation, and for children aged 1 to 18 years the AI increases with age from 30 to 75 μg. As for safety, the FNB sets also uppeable tolerable tolerability sets also set for safety for infants aged up to 1 to 5 years old. The FNB also sets a safe tolerable level for children from 1 to 15 years of age, and also for adults aged 30 to 70 years old, as for older children from 15 to 75 years old and adults aged 80 to 90 years and older. The average daily intake for adults and men for pregnancy is 90 igrams for women and 120 igram for men for infants and children aged 2 to 18 months. The AI for infants is up to 2–2.5 mg for infants, and up to 3 mg for children ages 1 to 13 years. The recommended daily requirement is 90 mg for adult men and 120 mg for women for pregnancy, and 90mg for infants age 12 months and older for children up to 18 years old. For pregnant women and women, the recommended daily intake is 90 ng/day for pregnancy. For babies aged 1-12 months, it is 90g/day. For children aged 13-18 years, it's 90g/. For infants aged 12 months to 14 years, and older, it’s 90g+. For infants under 18 years, the average daily allowance is 90mg/day (for women and children under 13 years old). For infants over the age of 2 years, there is a recommended daily allowance of 90 mg/day in the U.S. for vitamins A, D, E, and K. and K1 (for men and women).[26] For infants, the current daily allowance for vitamins K is 90ng/day, for women, and 120g/year, for men and men over the ages of 18 years. Vitamin K1 is found chiefly in leafy green vegetables such as dandelion greens, spinach, swiss chard, lettuce and Brassica vegetables. Vitamin K1 deficiency can result in coagulopathy, a bleeding disorder. Secondary vitamin K deficiency can occur in people with bulimia, those on stringent diets, and those taking anticoagulants. Osteoporosis[51] and coronary heart disease are strongly associated with lower levels of vitamin K2 (menaquinone) Vitamin K2 intake level is inversely related to severe aortic calcification and all-cause mortality.[8]Function in animals is to add a carboxylic acid functional group to a glutamate (Glu) amino acid residue in a animal cell. This is a somewhat uncommon modification of the protein, which is then known as a post-translational modification. The presence of two protein groups on the same carb is known as the ""COOH"" (carboxyl acid) effect. For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percentage of daily value (%DV) For vitamin K labeling purposes the daily value was 80 μg, but as of May 2016 it has been revised upwards to 120 μg. A table of the pre-change adult daily values is provided at reference daily intake. The European Food Safety Authority reviewed the same safety question and did not set an UL. No UL is set, as evidence for adverse effects is not sufficient. In the case of vitamins and minerals there is no set intake level for vitamins or minerals when evidence is sufficient, as no evidence is needed to set an intake level in the first place. For vitamins and vitamins there is a set of intake levels known as intake levels (known as ULs) for vitamins and Minerals, which are referred to as the EARs, RDAs, AIs and ULs. The EARs and AIs are collectively known as dietary reference intakes. The ULs are used for food and supplement label labeling purposes in the United States and other countries. For example, in the UK, vitamin K is listed as a nutrient in the ""Dietary Reference Intake"" section of the Dietary Reference Guide (DRC) for the U.K. and Europe. The DRC does not include vitamin K in the definition of the AIs or ULs, but does include Vitamin K as an ingredient in the DRC's DRC definition of Vitamin K. For more information on Vitamin K, see Vitamin K and Vitamin K-rich foods. For Vitamin K supplements, see the Vitamin K Supplements section of this article. for more details on vitamin K and other nutrients, see vitamin K-related topics. For the full list of nutrients and supplements, visit the VitaminK Supplements page. for the UK and Europe, or click here for more information about Vitamin K in Europe and the rest of the world, or see the vitamin K Supplement Guide for the United Kingdom and the European Union, for the European and European countries, or the Vitamink supplement guide for the EU and the EU, respectively. The Vitamin K content in each serving of food and supplements is based on the recommended daily intake for each nutrient. for vitamin K1, a serving of spinach, kale, collards, and spinach, is 741% of recommended daily amount. Gla proteins are known to occur in a wide variety of vertebrates: mammals, birds, reptiles, and fish. The venom of a number of Australian snakes acts by activating the human blood-clotting system. In some cases, activation is accomplished by snake Gla-containing enzymes that bind to the endothelium of human blood vessels and catalyze the conversion of procoagulant clotting factors into activated ones. Warfarin and other 4-hydroxycoumarins block the action of VKOR. This results in decreased concentrations of vitamin K and vitamin K hydroquinone in tissues. It is impossible to predict what dose of warfarin will give the desired degree of clotting suppression, so it must be carefully monitored to avoid overdose. The prothrombin time (PT) test measures the time required for blood to clot. A blood sample is mixed with citric acid and put in a fibrometer. This test is insensitive to mild deficiency, as the values do not change until the blood has declined at least 50%.[63] In a study of 53 newborns, 53% found ""prothrom bin time"" to be less than 53.5 minutes. In a newborn baby with a Gla deficiency, this is less than 50%. In a baby with no Gla, it can be as little as 20 minutes before the blood clotting process is triggered. The Gla protein osteocalcin, the calcification-inhibiting matrix GlA protein (MGP), the cell growth arrest specific gene 6 protein (Gas6), and the four transmembrane Gla proteins (TMGPs), the function of which is at present unknown, can function as a growth factor to activate the Axl receptor tyrosine kinase and stimulate cell proliferation or prevent apoptosis in some cells. In all cases in which their function was known, the presence of the Gla residues in these proteins turned out to be essential for functional activity. Without Gla on the amino termini of these factors, they no longer bind stably to the blood vessel endothelia and cannot activate clotting to allow formation of a clot during tissue injury. The presence of Gla is essential for clotting in some animals, such as the fish-hunting snail Conus geographus, which produces a venom containing hundreds of neuroactive peptides, or conotoxins, which is sufficiently toxic to kill an adult human. In humans, Gla can be found in blood coagulation factors II, VII, IX, and X, anticoagulante proteins C and S, and the factor X-targeting protein Z. Gla also plays a role in the development of the human immune system, including the fight against infections and cancer. The protein Gla has been linked to the development and survival of some cancers, including thrombotic hemangioblastoma, a type of blood cancer. It can also play a key role in fighting infections and infections in humans and other animals. It has been shown that Gla plays a key part in the formation of blood clots in the lungs and other parts of the immune system in humans. It also plays an important role in preventing cancerous cells from developing. Plasma phylloquinone was found to be positively correlated with vitamin K intake in elderly British women, but not men. Undercarboxylated osteocalcin (UcOc) levels have been inversely correlated with stores of vitamin K and bone strength in developing rat tibiae. The blood clotting factors of newborn babies are roughly 30–60% that of adult values. Vitamin K deficiency bleeding in the first week of the infant's life is estimated at 0.25–1.7%, with a prevalence of 2–10 cases per 100,000 births. Premature babies have even lower levels of the vitamin, so they are at a higher risk from this deficiency. Supplementation is recommended for all newborns within the first 24 hours of birth. Intramuscular administration is more effective in preventing late vitamin K deficiency bleed than oral administration. The Committee on Nutrition of the American Academy of Pediatrics has recommended 0.5–1 mg of vitamins K1 be administered to all newbornS shortly after birth. This single intramususus is usually given as a second-line injection of 1.5 mg over the first month of the baby's life but can be given by three oral doses over the course of the first year of the child's life. The vitamin K2 concentrations in human milk appear to be much lower than those of vitaminK1. In these bacteria, menaquinone transfers two electrons between two different small molecules, during oxygen-independent metabolic energy production processes (anaerobic respiration). In aerobic respiration, the final oxidant is molecular oxygen (O2), which accepts four electrons from an electron donor such as NADH to be converted to water. E. coli, as facultative anaerobes, can carry out both aerobic resppiration and menaquin one-mediated anaerobic resppiration. The final electron acceptor is not molecular oxygen, but fumarate or nitrate, which converts succinate or nitrite plus water to succinate and succinate plus water, respectively. This is similar to the way eukaryotic cell respiration generates ATP, except the final electrons are not molecular Oxygen (oxygen) and are taken by nitrate or succinate, which is the final acceptor of ATP, not molecularoxygen (M2). This is the same process that takes place in the human body, but in a more complex way in the gut. In the gut, the electrons are transferred between small molecules such as lactate, formate, or NADH, with the help of an enzyme, to formate and nitrate. This process is known as ""menaquinone-7 or MK-7, up to MK-11"" It is also known as the ""Menaquinone"" process, and it is thought to be responsible for the development of the human immune system. It is thought that vitamin K1 is the most important vitamin in the body, followed by vitamin K 2 and vitamin K3. It can be found in the National Institutes of Health (NIH) and the Food and Drug Administration (FDA) lists of vitamins and minerals. It has been shown that Vitamin K1 and K2 are more important than K2 and K3 in preventing blood clots in babies. In the early 1930s, Danish scientist Henrik Dam investigated the role of cholesterol by feeding chickens a cholesterol-depleted diet. Dam found that these defects could not be restored by adding purified cholesterol to the diet. It appeared that – together with the cholesterol – a second compound had been extracted from the food, and this compound was called the coagulation vitamin. Several laboratories synthesized the compound(s) in 1939. The precise function of vitamin K was not discovered until 1974, when three laboratories (Stenflo et al.,[87] Nelsestuen et al. and Magnusson et al.) isolated the vitamin K-dependent coagulating factor prothrombin (factor II) from cows that received a high dose of a vitamin K antagonist, warfarin. The extra carboxyl group in Gla made clear that vitamin K plays a role in a carboxyglutamate reaction during which Glu is converted into Gla. The biochemistry of how vitamin K is used to convert Glu to Gla has been elucidated over the past thirty years in academic laboratories throughout the world. In 2013, doctors reported emerging concerns in 2013 after treating children for serious bleeding problems. They cited lack-of newborn vitamin K administration, as the reason that the problems occurred, and recommended that breastfed babies could have an increased risk unless they receive a preventative dose. The new vitamin received the letter K because the initial discoveries were reported in a German journal, in which it was designated as Koagulationsvitamin. In the early 1990s, two studies suggested a relationship between parenteral administration of Vitamin K and childhood cancer. However, poor methods and small sample sizes led to the discrediting of these studies, and a review of the evidence published in 2000 by Ross and Davies found no link between the two. The first published report of successful treatment with vitamin K of life-threatening hemorrhage in a jaundiced patient with prothrosbin deficiency was made in 1938 by Smith, Warner, and Brinkhous.[86] In 2008, the University of Maryland Medical Center published an article entitled ""Vitamin K Overview"". The article was published by the Linus Pauling Institute, Oregon State University. It was the only method of quantifying vitamin K in various foods. The extent to which blood coagulated was restored by the diet was taken as a measure for its vitamin K content. For several decades, the Vitamin K-deficient chick model was theOnly method ofquantifying vitaminK in various food was the. Vitamin K was used to measure the extent towhich blood coageulation was restored. For more information on vitamin K, see: http://www.jocd.org/vitamin-k/Vitamin-K- Overview.html. For a list of studies that have shown vitamin K to be beneficial for humans, see http://jocc.org/. For more details about vitamin K and its role in bone health, see http://j.cnn.com/2013/02/08/23/science-news/v Vitamin-K.html#v-k- overview. Vitamin K has been linked to a reduced risk of coronary heart disease. Vitamin K administration to elderly patients with osteoporosis induces no hemostatic activation, even in those with suspected vitamin K deficiency. The American Cancer Society has a guide to Complementary and Alternative Cancer Therapies (2nd ed.) with information on vitamin K and other minerals. The National Institutes of Health has a program to promote vitamin K use in the U.S. and abroad. The U.N. International Agency for Research on Cancer has a programme to promote the use of vitamin K in the treatment of cancer and other cancer-causing conditions. The World Health Organization has a Program to promote Vitamin K use. The European Food Standards Agency has a Programme to Promote Vitamin K Use in the Treatment of Cancer and Other Conditions. The International Organization for Vitamin K is a not-for-profit organization that promotes vitamin K usage in the United States and other countries. It also provides information on Vitamin K-rich foods and supplements for cancer patients and their families. For more information about vitamin K, visit: www.vitamink.org.uk/. For more on vitamin D, see: http://www.vitaminsd.org/d/d-3/vitamin-d-4/v-3.html. For a list of Vitamin K supplements, see the: http:/www.nhs.uk/drugs/supplements/v Vitamin D, Vitamin K, Vitamin D3, and Vitamin D2-D3-D4-D5-D6-D7-D8-D9-D1-D2- D3- D4- D5- D6- D7- D8- D9- D10- D12- D11- D13- D14- D15- D1- D17- D18- D19- D20- D21- D22- D23- D24- D25- D26- D28- D27- D29- D30- D31- D34- D35- D36- D37- D38- D39- D41- D44- D47- D48- D50- D55- D56- D57- D58- D59- D60- D61- D62- D63- D65- D66- D68- D67- D69- D70- D74- D75- D77- D78- D79- D80- D81- D82- D83- D84- D85- D87- D86- D88- D90- D89- D92- D93- D94- D95- D96- D97- D98- D99- D91- D100- D104- D107- D106- D111- D108- D110- D112- D113- D114- D115- D118- D130- D109- D105- D120- D160- D150- D157- D165- D158- D155- D170- D205- D206- D159- D214- D215- D167- D207- D208- D210- D209- D200- D211- D220- D230- D225- D222- D227- D229- D240- D235- D250- D260- D280- D233- D238- D255- D239- D244- D245- D242- D246- D265- D267- D237- D268- D263- D259- D300- D264- D251- D315- D266- D318- D261- D270- D334- D340- D345- D285- D305- D325- D326- D346- D333- D337- D ""Oral rivaroxaban for symptomatic venous thromboembolism"" New England Journal of Medicine 363 (26): 2499–2510. ""Intestinal flora is not an intermediate in the phylloquinone–menaquinone-4 conversion in the rat"" Journal of Nutrition 128 (2): 220–223. ""Vitamin Kynthesis of gamma-carboxyamic acid"" Blood 93 (6): 98–1808. ""Role of vitamin-K-dependent proteins in bone metabolism"" Annual Review of Nutrition 8: 565–583. ""The Enzymatic Conversion of Phylloquinone to Menaquin one-4 (PhD thesis)"". Tufts University, Friedman School of Nutrition Science and Policy. (Mar 1999) ""V vitamin K-dependent biosynthesis of Gamma-Carboxyglamic acid"". Blood 93, K650 (6: 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94 and 94. ""Metabolism and cell biology of vitamin K"". Thrombosis and Haemostasis. 100 (4): 530–547. PMID 18841274. ""Conversion of dietary phylloquinone to tissue menaquinone.-4 in rats is not dependent on gut bacteria"" (Jan 1998). ""Int intestinal flora isnot an intermediate"" (1): 69–75. ""Dietary phyllaquinone is a source of tissue menaquin-4"" (Sep 1994). ""V Vitamin K distribution in rat tissues: dietary. phylaquinone. is a Source of tissue Menaquin One-4"". The British Journal of. Nutrition 72 (3): 415–425. PMid 7947656. ""Remicin and Vitamin K"" (Dec 1992). ""Comparative metabolism and requirement of. vitamin K in chicks and rats"". Journal. of Nutrition 122 (12): 2354–2360. PM ID 1453219. ""Efficacy and safety of Vitamin K in humans"" (Oct 2008). ""Empiracy and effectiveness of vitaminK in humans"". The American Journal of Clinical Nutrition. (Nov 2008) ""Emmetry and Efficacy of Vitamin. K in Humans"" ( Nov 2008). “Vitamin. K. in Humans’ Blood’s Role in Blood Coagulation’ (Nov 2007) “Empires and physiology of blood coagulation.” “Remicine and blood. coagulations.’ “ “. “”. ‘‘’’.  ’, ‘. ’,’,. ‘,”, ’,.’.,’;. ”, ”,. ,‘,  , “, ”,. ’” and ‘;’ , ‘,.”;”’ and. ‚’ .’,'’',’'’'.’%.’ Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women. Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006. ""Vitamin K"". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press 2001. ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"" (PDF) National Institutes of Health Clinical Center. ""Nutrition facts, calories in food, labels, nutritional information and analysis"". Nutritiondata.com. 13 Feb 2008. http://www.nhs.gov/nutritional-information/vitamin-k/index.html?title=Vitamin-K&action=vitamin.K&linkCode=v.K2&source=http%3A/v.v.k2%20k2k.html%20Vitamin%20K2k%20ketoic%20acid%20inhibitory%20protein%20and%20reactive%20phenomenon%20of%20eosin%20disease%20Eosin%. In the U.S., vitamin K2 is linked to a reduced risk of cancer. In the UK, vitamin K levels are associated with a lower risk of osteoporosis in post-menopausal men and women. Vitamin K is linked with a higher risk of bone mineral density in premenopausal and older women. It is also associated with an increased risk of prostate cancer in men and younger women. The link between vitamin K and bone density is not well-described but is thought to be related to the fact that vitamin K is found in higher levels in the body than in the blood. The study was published in the Journal of Nutritional Science and Vitaminology (J.Nutritional Science & Vitaminology): 48 (23): 5231–5244. It was the first study of its kind to look at the link between Vitamin K2 and bone mineral densities in humans. It also found that Vitamin K was associated with higher levels of vitamin D in the human body. It has been shown that the presence of vitamin K in the urine of premenopause patients can be linked to increased bone density in later life. It can also be associated with increased levels of Vitamin D in older women and men. It's unclear whether the link is due to the presence or absence of vitaminK in the diet. It could also be due to vitamin K or other factors, such as the use of probiotics or probiotics in the first place. It may also be a result of the high level of Vitamin K found in soybeans in the Japanese diet. The Journal of Nutrition. and Vitamin.ology (Jun 2002) reported that dietary intake of fermented. soybeans (Natto) is associated. with a reduced bone mineral. density (3.3–3.4%) in pre-menopopausal women (Jun. 2002). Vitamin K (V2menetone) induces no nitric oxide production in muscle cells (N.O.S) in iNetone (menetrozine) (207-207) and gamma-carboxylic acid (Noxon) (208-207). The study found no relationship between nitricoxide production and muscle. smooth muscle cells no nitrozone (NOC) production. . ""Vitamin K-dependent carboxylase"". Annual Review of Biochemistry. 54: 459–477. PMID 3896125. ""Prothrombin Time"". WebMD. ""Effect of vitamin K deficiency on urinary gamma-carboxyglutamic acid excretion in rats"". Nihon Ketsueki Gakkai Zasshi. 52 (6): 1078–1086. ""The vitamin K cycle"". Journal of Thrombosis and Haemostasis. 3 (8): 1873–1878.. ""PIVKA-II plasma levels as markers of subclinical vitamin K. deficiency in term infants"" Journal of Maternal, Fetal & Neonatal Medicine. 25 (9): 1660–1663.. “Vitamin D’s effect on the morphometric and material properties of bone in the tibiae growing rats"". Metabolism. 61 (3): 407–414, PMID 2194444, p. 79.. The vitamin K supplement along with vitamin D reduced calcium and reduced calcium concentration of under-over-menopausal women while increasing bone density in postmenopausal. women.. Vitamin D reduced the concentration of over-over calcium and increased bone density. in post-menopause. women while. increasing bone. density in women over over 55.. vitamin K reduced the calcium and calcium concentration in post menopausal. over-65-year-olds while increasing osteoporosis. in over-70-year old women while decreasing bone density and increasing bone mineral density in. women over 65. Vitamin K increased the bone density of post menopauseopausal women but not in women under 65. It also increased bone. mineral. density while increasing. bone mineral. D-diluted calcium and vitamin D increased. bone density while under over 65-years-old. It increased. the bone mineral concentration of women over 70-years old while under-65. years old. It decreased. the. bone.density of women under 70- years-old while under 65-year -year-old women. It. increased the. calcium and. vitamin D concentration. in women and men over 75-years. It was found to be more effective than vitamin K in the treatment of osteoarthritis of the knee and back in the elderly. It has been shown to improve bone density, bone mass and bone. function. in those who took it for at least 10 years. It is also thought to have a protective effect on bone density as well. It can also have an anti-oxidant effect in the prevention of cancer. of the colon. and bone metastasis. It may also have a role in the development of bone. and cartilage. of bone marrow and bone marrow. cancers. In the elderly, it has been found to have an important role in reducing the risk of cancer of the bladder. and bowel. of developing cancer. in older people and in the. elderly of all ages. It helps to take vitamin K-rich foods such as fish, meat, eggs, vegetables, and milk, as well as vitamins B, C, D, E and E2, E3, B6, B12, and B12. and K-complex. and B-12. It could also help to reduce risk of cardiovascular disease and bone damage in older adults and children who have already suffered a stroke or are at risk of developing a stroke. It's thought that vitamin K may play a key role in protecting against cancer. Having four cases since February just at Vanderbilt was a little bit concerning to me. Dam, C. P. H. (1941). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize Laureate Lecture. McAlister, V. C. (2006). ""Control of coagulation: a gift of Canadian agriculture"" ( PDF). Clinical and Investigative Medicine. 29 (6): 373–377. Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). ""Vitamin K dependent modifications of glutamic acid residues in prothrombin"". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730–2733. Vitamin K Paradox: Another Reason to Eat Your Greens. John Wiley & Sons, Canada. (2012) v.65-72-0-7. v.7.2 and 4.4-2 and B2 and. B. (2013) v65-65-7-2. v1.2.1. v2.3. v3.4. v4.5. v5. V6. V7. V8. V9. V10. V11. V12. V13. V14. V15. V16. V17. V18. V19. V20. V21. V2. V3. V4. V5. The Vitamin K paradox: Another reason to eat your Greens. NICE.org.uk. Jul 2006. Retrieved 12 Oct 2014. V1.4 and V5 and V6 and V7 and V8 and V9 and V10 and V11 and V12 and V13 and V14 and V15 and V16 and V17 and V18 and V19 and V20 and V21 and V22 and V2 and V3 and V1 and V4 and K and K are all listed in NICE's list of recommended vitamins and minerals for pregnant women and babies. V.3 and K is listed in the list of essential nutrients for mothers and children. V 2. V 3. V 4. V 5. V 6. V 7. V 8. V 9. V 10. V 11. V 12. V 13. V 14. V 15. V 16. V 17. V 18. V 19. V 20. V 21. V 22. V 23. V22. V23. V24. V25. V26. V27. V28. V29. V30. V31. V32. V33. V34. V35. V36. V37. V38. V39. V40. V41. V46. V47. V48. V49. V51. V52. V53. V54. V55. V56. V57. V58. V59. V60. V61. V62. V63. V64. V68. V69. V70. V71. V72. V73. V74. V75. V78. V79. V80. V81. V82. V83. V84. V77. V87. V88. V89. V90. V91. V92. V93. V94. V103. V104. V105. V107. V108. V110. V111. V112. V113. V114."
What is the SI unit of power?,"For other uses, see Electricity (disambiguation).
""Electric"" redirects here. For other uses, see Electric (disambiguation).
Lightning is one of the most dramatic effects of electricity.
Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. In early days, electricity was considered as being not related to magnetism. Later on, many experimental results and the development of Maxwell's equations indicated that both electricity and magnetism are from a single phenomenon: electromagnetism. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.
The presence of an electric charge, which can be either positive or negative, produces an electric field. The movement of electric charges is an electric current and produces a magnetic field.
When a charge is placed in a location with a non-zero electric field, a force will act on it. The magnitude of this force is given by Coulomb's law. Thus, if that charge were to move, the electric field would be doing work on the electric charge. Thus we can speak of electric potential at a certain point in space, which is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts.
electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies.
Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then, practical applications for electricity were few, and it would not be until the late nineteenth century that electrical engineers were able to put it to industrial and residential use. The rapid expansion in electrical technology at this time transformed industry and society, becoming a driving force for the Second Industrial Revolution. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.
Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the ""Thunderer of the Nile"", and described them as the ""protectors"" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning ra‘ad (رعد) applied to the electric ray.
Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.
Benjamin Franklin conducted extensive research on electricity in the 18th century, as documented by Joseph Priestley (1767) History and Present Status of Electricity, with whom Franklin carried on extended correspondence.
Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word electricus (""of amber"" or ""like amber"", from ἤλεκτρον, elektron, the Greek word for ""amber"") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words ""electric"" and ""electricity"", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.
Further work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.
In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his ""On Physical Lines of Force"" in 1861 and 1862.
While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.
In 1887, Heinrich Hertz:843–44 discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for ""his discovery of the law of the photoelectric effect"". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially.
The first solid-state device was the ""cat's-whisker detector"" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.
The solid-state device came into its own with the invention of the transistor in 1947. Common solid-state devices include transistors, microprocessor chips, and RAM. A specialized type of RAM called flash RAM is used in USB flash drives and more recently, solid-state drives to replace mechanically rotating magnetic disc hard disk drives. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED).
The presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity.:457 A lightweight ball suspended from a string can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: like-charged objects repel and opposite-charged objects attract.
The force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them.:35 The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 1042 times that of the gravitational attraction pulling them together.
Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. The most familiar carriers of electrical charge are the electron and proton. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact, or by passing along a conducting material, such as a wire.:2–5 The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.
The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. By convention, the charge carried by electrons is deemed negative, and that by protons positive, a custom that originated with the work of Benjamin Franklin. The amount of charge is usually given the symbol Q and expressed in coulombs; each electron carries the same charge of approximately −1.6022×10−19 coulomb. The proton has a charge that is equal and opposite, and thus +1.6022×10−19 coulomb. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle.
The movement of electric charge is known as an electric current, the intensity of which is usually measured in amperes. Current can consist of any moving charged particles; most commonly these are electrons, but any charge in motion constitutes a current. Electric current can flow through some things, electrical conductors, but will not flow through an electrical insulator.
By historical convention, a positive current is defined as having the same direction of flow as any positive charge it contains, or to flow from the most positive part of a circuit to the most negative part. Current defined in this manner is called conventional current. The motion of negatively charged electrons around an electric circuit, one of the most familiar forms of current, is thus deemed positive in the opposite direction to that of the electrons. However, depending on the conditions, an electric current can consist of a flow of charged particles in either direction, or even in both directions at once. The positive-to-negative convention is widely used to simplify this situation.
The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. Examples of electric currents include metallic conduction, where electrons flow through a conductor such as metal, and electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. While the particles themselves can move quite slowly, sometimes with an average drift velocity only fractions of a millimetre per second,:17 the electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires.
Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. Their work was greatly expanded upon by Michael Faraday in 1833. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840.:23–24 One of the most important discoveries relating to current was made accidentally by Hans Christian Ørsted in 1820, when, while preparing a lecture, he witnessed the current in a wire disturbing the needle of a magnetic compass. He had discovered electromagnetism, a fundamental interaction between electricity and magnetics. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment.
In engineering or household applications, current is often described as being either direct current (DC) or alternating current (AC). These terms refer to how the current varies in time. Direct current, as produced by example from a battery and required by most electronic devices, is a unidirectional flow from the positive part of a circuit to the negative.:11 If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. Alternating current is any current that reverses direction repeatedly; almost always this takes the form of a sine wave.:206–07 Alternating current thus pulses back and forth within a conductor without the charge moving any net distance over time. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. Alternating current is affected by electrical properties that are not observed under steady state direct current, such as inductance and capacitance.:223–25 These properties however can become important when circuitry is subjected to transients, such as when first energised.
The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.
A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body.:88 This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects.
The principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh.
A pair of AA cells. The + sign indicates the polarity of the potential difference between the battery terminals.
The concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity.:494–98 This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points. An electric field has the special property that it is conservative, which means that the path taken by the test charge is irrelevant: all paths between two specified points expend the same energy, and thus a unique value for potential difference may be stated.:494–98 The volt is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage.
For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable.
Electric potential is a scalar quantity, that is, it has only magnitude and not direction. It may be viewed as analogous to height: just as a released object will fall through a difference in heights caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. As relief maps show contour lines marking points of equal height, a set of lines marking points of equal potential (known as equipotentials) may be drawn around an electrostatically charged object. The equipotentials cross all lines of force at right angles. They must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface.
Ørsted's discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. Moreover, the interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The force on the compass needle did not direct it to or away from the current-carrying wire, but acted at right angles to it. Ørsted's slightly obscure words were that ""the electric conflict acts in a revolving manner."" The force also depended on the direction of the current, for if the flow was reversed, then the force did too.
Ørsted did not fully understand his discovery, but he observed the effect was reciprocal: a current exerts a force on a magnet, and a magnetic field exerts a force on a current. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other: two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The interaction is mediated by the magnetic field each current produces and forms the basis for the international definition of the ampere.
This relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 1821. Faraday's homopolar motor consisted of a permanent magnet sitting in a pool of mercury. A current was allowed through a wire suspended from a pivot above the magnet and dipped into the mercury. The magnet exerted a tangential force on the wire, making it circle around the magnet for as long as the current was maintained.
Experimentation by Faraday in 1831 revealed that a wire moving perpendicular to a magnetic field developed a potential difference between its ends. Further analysis of this process, known as electromagnetic induction, enabled him to state the principle, now known as Faraday's law of induction, that the potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Faraday's disc was inefficient and of no use as a practical generator, but it showed the possibility of generating electric power using magnetism, a possibility that would be taken up by those that followed on from his work.
Italian physicist Alessandro Volta showing his ""battery"" to French emperor Napoleon Bonaparte in the early 19th century.
The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses.
Electrochemistry has always been an important part of electricity. From the initial invention of the Voltaic pile, electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells.
A basic electric circuit. The voltage source V on the left drives a current I around the circuit, delivering electrical energy into the resistor R. From the resistor, the current returns to the source, completing the circuit.
An electric circuit is an interconnection of electric components such that electric charge is made to flow along a closed path (a circuit), usually to perform some useful task.
Electric power is the rate at which electric energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.
Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries or by other means from a wide variety of sources of energy. Electric power is generally supplied to businesses and homes by the electric power industry. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using electricity meters, which keep a running total of the electric energy delivered to a customer. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency.
Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes, optoelectronics, sensors and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing, telecommunications, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.
Thus, the work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances.
Early 20th-century alternator made in Budapest, Hungary, in the power generating hall of a hydroelectric station (photograph by Prokudin-Gorsky, 1905–1915).
In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. While this method, now known as the triboelectric effect, can lift light objects and generate sparks, it is extremely inefficient. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The voltaic pile, and its modern descendant, the electrical battery, store energy chemically and make it available on demand in the form of electrical energy. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines.
Electrical power is usually generated by electro-mechanical generators driven by steam produced from fossil fuel combustion, or the heat released from nuclear reactions; or from other sources such as kinetic energy extracted from wind or flowing water. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on his electromagnetic principle that a conductor linking a changing magnetic field induces a potential difference across its ends. The invention in the late nineteenth century of the transformer meant that electrical power could be transmitted more efficiently at a higher voltage but lower current. Efficient electrical transmission meant in turn that electricity could be generated at centralised power stations, where it benefited from economies of scale, and then be despatched relatively long distances to where it was needed.
Since electrical energy cannot easily be stored in quantities large enough to meet demands on a national scale, at all times exactly as much must be produced as is required. This requires electricity utilities to make careful predictions of their electrical loads, and maintain constant co-ordination with their power stations. A certain amount of generation must always be held in reserve to cushion an electrical grid against inevitable disturbances and losses.
Electricity is a very convenient way to transfer energy, and it has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Although electrification brought with it its own dangers, replacing the naked flames of gas lighting greatly reduced fire hazards within homes and factories. Public utilities were set up in many cities targeting the burgeoning market for electrical lighting. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector.
The resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of resistive electric heating in new buildings. Electricity is however still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand for heating and cooling, the effects of which electricity utilities are increasingly obliged to accommodate.
Electricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first intercontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process.
The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership.
Electronic devices make use of the transistor, perhaps one of the most important inventions of the twentieth century, and a fundamental building block of all modern circuitry. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square.
A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the greater the current. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. Death caused by an electric shock is referred to as electrocution. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times.
Electricity is not a human invention, and may be observed in several forms in nature, a prominent manifestation of which is lightning. Many interactions familiar at the macroscopic level, such as touch, friction or chemical bonding, are due to interactions between electric fields on the atomic scale. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, or even sugar, generate a potential difference across their faces when subjected to external pressure. This phenomenon is known as piezoelectricity, from the Greek piezein (πιέζειν), meaning to press, and was discovered in 1880 by Pierre and Jacques Curie. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place.
§Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon.
Some organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants.
In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. ""Revitalization"" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored Frankenstein (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.
As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who ""finger death at their gloves' end as they piece and repiece the living wires"" in Rudyard Kipling's 1907 poem Sons of Martha. Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the Tom Swift books. The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.
With electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it stops flowing, an event that usually signals disaster. The people who keep it flowing, such as the nameless hero of Jimmy Webb’s song ""Wichita Lineman"" (1968), are still often cast as heroic, wizard-like figures.
Ampère's circuital law, connects the direction of an electric current and its associated magnetic currents.
^ Diogenes Laertius. R.D. Hicks (ed.). ""Lives of Eminent Philosophers, Book 1 Chapter 1 "". Perseus Digital Library. Tufts University. Retrieved 5 February 2017. Aristotle and Hippias affirm that, arguing from the magnet and from amber, he attributed a soul or life even to inanimate objects.
^ Aristotle. Daniel C. Stevenson (ed.). ""De Animus (On the Soul) Book 1 Part 2 (B4 verso)"". The Internet Classics Archive. Translated by J.A. Smith. Retrieved 5 February 2017. Thales, too, to judge from what is recorded about him, seems to have held soul to be a motive force, since he said that the magnet has a soul in it because it moves the iron.
^ a b c Guarnieri, M. (2014). ""Electricity in the age of Enlightenment"". IEEE Industrial Electronics Magazine. 8 (3): 60–63. doi:10.1109/MIE.2014.2335431.
^ Srodes, James (2002), Franklin: The Essential Founding Father, Regnery Publishing, pp. 92–94, ISBN 0-89526-163-4 It is uncertain if Franklin personally carried out this experiment, but it is popularly attributed to him.
^ a b Guarnieri, M. (2014). ""The Big Jump from the Legs of a Frog"". IEEE Industrial Electronics Magazine. 8 (4): 59–61, 69. doi:10.1109/MIE.2014.2361237.
^ Hertz, Heinrich (1887). ""Ueber den Einfluss des ultravioletten Lichtes auf die electrische Entladung"". Annalen der Physik. 267 (8): S. 983–1000. Bibcode:1887AnP...267..983H. doi:10.1002/andp.18872670827.
^ ""The Nobel Prize in Physics 1921"". Nobel Foundation. Retrieved 2013-03-16.
^ John Sydney Blakemore, Solid state physics, pp. 1–3, Cambridge University Press, 1985 ISBN 0-521-31391-0.
^ Richard C. Jaeger, Travis N. Blalock, Microelectronic circuit design, pp. 46–47, McGraw-Hill Professional, 2003 ISBN 0-07-250503-6.
^ ""The repulsive force between two small spheres charged with the same type of electricity is inversely proportional to the square of the distance between the centres of the two spheres."" Charles-Augustin de Coulomb, Histoire de l'Academie Royal des Sciences, Paris 1785.
^ Sewell, Tyson (1902), The Elements of Electrical Engineering, Lockwood, p. 18 . The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge'.
^ a b Berkson, William (1974), Fields of Force: The Development of a World View from Faraday to Einstein, Routledge, p. 370, ISBN 0-7100-7626-6 Accounts differ as to whether this was before, during, or after a lecture.
^ ""Lab Note #105 EMI Reduction – Unsuppressed vs. Suppressed"". Arc Suppression Technologies. April 2011. Retrieved March 7, 2012.
^ Almost all electric fields vary in space. An exception is the electric field surrounding a planar conductor of infinite extent, the field of which is uniform.
^ Paul J. Nahin (9 October 2002). Oliver Heaviside: The Life, Work, and Times of an Electrical Genius of the Victorian Age. JHU Press. ISBN 978-0-8018-6909-9.
^ ""The Bumpy Road to Energy Deregulation"". EnPowered. 2016-03-28.
^ a b c d e f g h Van Riper, op.cit., p. 71.
Look up electricity in Wiktionary, the free dictionary.
Basic Concepts of Electricity chapter from Lessons In Electric Circuits Vol 1 DC book and series.","['Watt, one joule per second.']",6197,multifieldqa_en,en,,4c7891d780eb3f45e8c4e5bf14fd9ed6c0bf898fb159b329," Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others. In early days, electricity was considered as being not related to magnetism. Later on, many experimental results and the development of Maxwell's equations indicated that both electricity and magnetism are from a single phenomenon: electromagnetism. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society. People were aware of shocks from electric fish before any knowledge of electricity existed. Ancient Egyptian texts referred to these fish as the ""Thunderer of the Nile"", and described them as ""protectors"" of all other fish. Several ancient writers attested to the numbing effect of electric shocks delivered by catfish and electric rays. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. According to a controversial theory, the Parthians knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. The movement of electric charges is an electric current and produces a magnetic field. The presence of an electric charge, which can be either positive or negative, produces an electric field. Electric potential is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts. Electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies, are referred to as ""electronics"" The term ""electricity"" is used to refer to the process of creating electricity from a non-electric source such as water, gas, electricity, or radioactivity. The word ""electric"" is also used for electrical devices such as light bulbs, transformers, and other devices that use electricity to power their functions. The use of the word ""electronic"" is a contraction of the words ""electrical"" and ""electron"" which means ""electric power"" or ""electro-mechanical"" in the UK and the U.S. It is also the name given to a class of technology known as electrical devices known as electronic technologies, which include vacuum tubes, transistors, diodes, integrated circuits and other passive interconnection technologies. Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete. Benjamin Franklin conducted extensive research on electricity in the 18th century, as documented by Joseph Priestley. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. In 1887, Heinrich Hertz:843–44 discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets. This discovery led to the quantum revolution. The first solid-state device was the ""cat's-whisker detector"" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a ge ge) and this is frequently used to make electricity commercially. It is unclear whether the Baghdad Battery, which resembles a galvanic cell, was electrical in nature, though it is uncertain whether the artifact was electric in nature. In June 1752, Benjamin Franklin is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from theKey to the back of his hand showed that lightning was indeed electrical in Nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his ""On Physical Lines of Force"" in 1861 and 1862. The word ""electric"" and ""electricity"" made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646. In the 17th and early 18th centuries, Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay conducted extensive work in electricity, selling his possessions to fund his work. The presence of charge gives rise to an electrostatic force: charges exert a force on each other. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges. The most familiar carriers of electrical charge are the electron and proton. Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. Experiment has shown charge to be a conserved quantity, that is, the net charge within an electrically isolated system will always remain constant regardless of any changes taking place within that system. The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.manium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. The building material is most often a crystalline semiconductor. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED)solid state devices came into its own with the invention of the transistor in 1947. A specialized type of RAM called flash RAM is used in USB flash drives and more recently, solid- state drives to replace mechanically rotating magnetic disc hard disk drives. It is used to store data in the form of a microchip or other type of memory chip, such as in a PC or a mobile phone. It can also be used to transfer data from one device to another, for example, to transfer information from a PC to a Mac or a Macbook, or even a smartphone to a PC. It has been used in the U.S. to detect radio signals since the 1980s. It was also used to detect signals in the 1990s and 2000s by the University of California, Los Angeles, and the California Institute of Technology, among other places. The use of solid state devices has led to the creation of the Department of Energy’s ‘Solid State Technology’ program, which aims to reduce the amount of energy needed to transmit data by up to 50 per cent. It also allows scientists to study the effects of quantum mechanics on the environment. It allows them to study how atoms and molecules interact with each other in a more realistic way. Electric current can consist of any moving charged particles. Most commonly these are electrons, but any charge in motion constitutes a current. Electric current can flow through some things, electrical conductors, but will not flow through an electrical insulator. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle. Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840. Current is often described as being either direct current (DC) or alternating current (AC) These terms refer to how the current varies in time, and are used in engineering or household applications. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. These properties however are not observed under steady state direct current, such as inductance and capacitance, which is why they are called alternating current. Current can be measured in amperes, which are usually measured in seconds. The intensity of an electric current is usually measured as a ratio of ampere to second. The current can also be expressed as a square root of its magnitude, or as a Hertz constant, or a factor of 1.6022×10−19. The electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires. The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. This is known as metallic conduction or electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. It is also known as alternating current, which takes the form of a sine wave, and is used in electrical engineering and other fields. It can be used to refer to any current that reverses direction repeatedly; almost always this takes the shape of a straight line or a line, or even a line or an arc. The term current is widely used to simplify this situation and is often used to describe current in general. The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the gravitational field, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electricField can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric Field at a distance is usually zero. A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body.:88 This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects. The + sign indicates the polarity of the potential difference between the battery terminals. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises electric field in the air to greater than it can withstand. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker. There is a finite limit to theelectric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. The principles of electrostatics are important when designing items of high-voltage equipment. become important when circuitry is subjected to transients, such as when first energised. For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed. to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable. It may be viewed as analogous to height: just as a released object will fall through a difference in heigh. It is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage. The volt is the term used to describe the energy required to move a unit charge between two specified points. This definition of potential, while formal, has little practical application. Discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. The interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 18 21. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells. The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses. It is the basis for the international definition of the ampere.s caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. A basic electric circuit. drives a current I around the circuit, delivering electrical energy into the resistor R. From the resistor, the current returns to the source, completing the circuit. The effect is mediated by the magnetic field each current produces and forms the basis. for the International definition. of the Ampere, which is the ratio of the current to thevoltage in a given circuit. An electric circuit is an intercon, with the voltage source V on the left driving the current into the circuit and the resistor on the right delivering the energy back into the source. An electrostatically charged object may be drawn around a conductor at right angles. The equipotentials must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other. Two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Further analysis of this process, known as electromagnetic induction, enabled. him to state the principle, now known as Faraday’s law of induction, that the potential difference in a close circuit is inversely proportional to magnetic flux. The. potential difference between a wire moving perpendicular to a. magnetic field developed a possible difference between its ends. in the early 19th century. Italian physicist Alessandro Volta showing his ""battery"" to French emperor Napoleon Bonaparte in the late 1800s. Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on h such generators. The ability of electronic devices to act as switches makes digital information processing possible. Today, most electronic devices use semiconductor components to perform electron control. The work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances. The SI unit of power is the watt, one joule per second. Electric utilities measure power using electricity meters, which keep a running total of electric energy delivered to a customer. The current cost of electricity in the UK is around £1,000 per hour. It is estimated that in the U.S. it is about £2,500 per hour to run a typical home or office for a short period of time. The cost of running a business in the United States is about $3,000 to $4,000 a year for a large number of homes and businesses. The average cost of a home in the USA is around $1,800 per year. It can be as little as £200 per hour for small businesses. Electricity is a very convenient way to transfer energy, and has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Electrically powered vehicles are used in public transportation, such as electric buses and trains, and an increasing number of battery-powered electric cars in private ownership. Electricity is still a highly practical energy source for heating and refrigeration, with air conditioning/heat pumps representing a growing sector for electricity demand. The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the great the currere. The resistive Joule heating effect employed in filament light bulbs also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries have issued legislation restricting or banning the use of resistive electric heating in new buildings. In the late 20th century and in modern times, the trend has started to flow in the direction of deregulation in the electrical power sector. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square. The effect of the transistor, perhaps one of most important inventions of the twentieth century, and a fundamental building block of all modern circuitry, is to make electronic devices more and more efficient. The power of the transistors is to be found in the design of modern computers and other electronic devices such as the iPhone, iPad, and the iPad mini, which are powered by a combination of alternating current and transistors. The transistor is the most powerful device in the world, with a current of more than 100,000 volts per square centimetre. It is also the most efficient way to transmit electrical power, as it is able to do so at a much lower voltage than other methods such as radio or television. It can also be used to power a large number of electronic devices, including computers, phones, computers, printers and the internet, and even the internet itself. Electricity is not a human invention, and may be observed in several forms in nature. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, generate a potential difference across their faces when subjected to external pressure. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the Tom Swift books. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times. The revitalization of monsters with electricity later became a stock theme in horror films. Some organisms, like sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception. Others, termed electrogenic, can generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes.Bioelectrogenesis in microbial life is a prominent phenomenon in soils and sediment ecology resulting from anaerobic respiration. The microbial fuel cell mimics this ubiquitous natural phenomenon. The ability to detect electric fields is known as Electroreception, which is used by some species of fish to detect their prey and to stun them. It is also used by certain plants to communicate with each other and to coordinate activities in certain plants. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. It can be used to torture people, and is still used as a means of execution in many jurisdictions. It has also been used to resuscitate dead or drowned persons, as in the case of Frankenstein. Ampère's circuital law, connects the direction of an electric current and its associated magnetic currents. The people who keep it flowing, such as the nameless hero of Jimmy Webb’s song ""Wichita Lineman"" (1968), are still often cast as heroic, wizard-like figures. It is uncertain if Franklin personally carried out this experiment, but it is popularly attributed to him. Aristotle and Hippias affirm that, arguing from the magnet and from amber, he attributed a soul or life even to inanimate objects. The Q originally stood for 'quantity of electricity', the term 'electricity' now more commonly expressed as 'charge' and is used to refer to the state of electricity in the United States. The term 'Electricity in the age of Enlightenment' was coined by Thomas Edison in his book ""The Electric Power of the World"" in 1776. The word electricity is still used to describe electricity in many parts of the world, including the U.S., Australia, Canada, New Zealand, and the United Kingdom. It can also be used as a name for the electric field surrounding a planar conductor of infinite space, in which the field is uniform. It has also been used as the name of a type of conductor, the Planar Conductor, or a conductor of infinitely infinite space. The name of the conductor is also used for the Planarsar Conductor, which has a field surrounding it that is uniform in all directions. The field surrounding the planar conductor is also uniform in space, but the extent of the field around it varies from person to person. The most common electric field is the one surrounding the conductor of unlimited space, which is inversely proportional to the square of the distance between the centres of the two spheres. This is the so-called 'electric field' or 'field of force' (or 'electric charge' in modern usage). It is also known as the field of force (or electric charge) or the 'electric current' ( or electric current) or 'electric voltage' (electricity) It is the force that drives an electrical current or current. The power of the electric current can be measured in volts, amperes, kph, volts, ohms, femtodes, tensiles, tensors, tensile strength, and so on. The electric current is also called the ""electric current"" or ""electricity"" or the ""electrical current"" by some scientists. The electricity that powers a device is known as 'the electricity of the moment' or ""the electric current"". It is used in a variety of scientific fields, including those such as electricity, magnetism, radioactivity, and magnetism. It was also used to explain the origin of the word electricity, and to explain how electricity is created. It also has been used in the popular culture to describe the nature of electricity and the way it is produced. It's been used by scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla to explain some of the principles of modern electricity. In the past, electricity has been seen as a symbol of power, but now it is more commonly referred to as a ""force"" or a ""power source"" or an ""energy source"" by many people. This can be seen in the Wikipedia definition of electricity. The Wikipedia definition also includes the term ""electric power"" as well as other terms such as ""electronic current"" and ""electro-magnetism"""
What is the main advantage of a horizontal business model for mobile devices?,"The future of mobile CPUs, part 1: Today’s fork in the road | Ars Technica
2013 may be a big year for the evolution of smartphones and tablets.
Mobile computing's rise from niche market to the mainstream is among the most significant technological trends in our lifetimes. And to a large extent, it's been driven by the bounty of Moore’s Law—the rule that transistor density doubles every 24 months. Initially, most mobile devices relied on highly specialized hardware to meet stringent power and size budgets. But with so many transistors available, devices inevitably grew general-purpose capabilities. Most likely, that wasn't even the real motivation. The initial desire was probably to reduce costs by creating a more flexible software ecosystem with better re-use and faster time to market. As such, the first smartphones were very much a novelty, and it took many years before the world realized the potential of such devices. Apple played a major role by creating innovative smartphones that consumers craved and quickly adopted.
To some extent, this is where we still stand today. Smartphones are still (relatively) expensive and primarily interesting to the developed world. But over the next 10 years, this too will change. As Moore’s Law rolls on, the cost of a low-end smartphone will decline. At some point, the incremental cost will be quite minimal and many feature phones of today will be supplanted by smartphones. A $650 unsubsidized phone is well beyond the reach of most of the world compared to a $20 feature phone, but a $30 to $40 smartphone would naturally be very popular.
In this grand progression, 2013 will certainly be a significant milestone for mobile devices, smartphones and beyond. It's likely to be the first year in which tablets out-ship notebooks in the US. And in the coming years, this will lead to a confluence of high-end tablets and ultra-mobile notebooks as the world figures out how these devices co-exist, blend, hybridize, and/or merge.
Against this backdrop, in this two-part series, we'll explore the major trends and evolution for mobile SoCs. More importantly, we'll look to where the major vendors are likely going in the next several years.
Tablet and phone divergence
While phones and tablets are mobile devices that often share a great deal of software, it's becoming increasingly clear the two are very different products. These two markets have started to diverge and will continue doing so over time.
From a technical perspective, smartphones are far more compact and power constrained. Smartphone SoCs are limited to around 1W, both by batteries and by thermal dissipation. The raison d’etre of a smartphone is connectivity, so a cellular modem is an absolute necessity. For the cost sensitive-models that make up the vast majority of the market, the modem is integrated into the SoC itself. High-end designs favor discrete modems with a greater power budget instead. The main smartphone OSes today are iOS and Android, though Windows is beginning to make an appearance (perhaps with Linux or BlackBerry on the horizon). Just as importantly, phone vendors like HTC must pass government certification and win the approval of carriers. There is very much a walled-garden aspect, where carriers control which devices can be attached to their networks, and in some cases devices can only be sold through a certain carrier. The business model places consumers quite far removed from the actual hardware.
In contrast, tablets are far more akin to the PC both technically and economically. The power budget for tablet SoCs is much greater, up to 4W for a passively cooled device and as high as 7-8W for systems with fans. This alone means there is a much wider range of tablet designs than smartphones. Moreover, the default connectivity for tablets is Wi-Fi rather than a cellular modem. The vast majority of tablets do not have cellular modems, and even fewer customers actually purchase a wireless data plan. As a result, cellular modems are almost always optional discrete components of the platform. The software ecosystem is relatively similar, with Microsoft, Apple, and Google OSes available. Because tablets eschew cellular modems, the time to market is faster, and they are much more commonly sold directly to consumers rather than through carriers. In terms of usage models, tablets are much more PC-like, with reasonable-sized screens that make games and media more attractive.
Looking forward, these distinctions will likely become more pronounced. Many tablets today use high-end smartphone SoCs, but the difference in power targets and expected performance is quite large. As the markets grow in volume, SoCs will inevitably bifurcate to focus on one market or the other. Even today, Apple is doing so, with the A6 for phones and the larger A6X for tablets. Other vendors may need to wait a few years to have the requisite volume, but eventually the two markets will be clearly separate.
Horizontal business model evolution
Another aspect of the mobile device market that is currently in flux and likely to change in the coming years is the business model for the chip and system vendors. Currently, Apple is the only company truly pursuing a vertically integrated model, where all phones and tablets are based on Apple’s own SoC designs and iOS. The tight integration between hardware and software has been a huge boon for Apple, and it has yielded superb products.
Samsung is one of the few others companies that takes a vertically integrated approach to phones and tablets, although in truth its strategy seems to be ambivalent on that point. Unlike Apple, Samsung’s SoCs are readily available to third parties, and some Samsung devices, such as the S7562 Galaxy S Duos, use SoCs from competitors. More recently though, there has been a trend of Samsung devices using Samsung SoCs, at least for the premier products. For the moment, Samsung’s approach is best characterized as a hybrid, particularly as the company lacks a bespoke OS.
The rest of the major SoC vendors (e.g., Intel, Qualcomm, Nvidia, TI, Mediatek, etc.) have stayed pretty far away from actual mobile devices. These companies tend to focus on horizontal business models that avoid competing with customers or suppliers.
In the long term, mobile devices are likely to evolve similarly to the PC and favor a horizontal business model. The real advantage is one of flexibility; as costs drop and the market expands, it will be increasingly necessary for vendors like HTC to offer a wide range of phones based on radically different SoCs. While a vertically integrated company like Apple can focus and maintain leadership in a specific (and highly lucrative) niche, it would be very difficult to expand in many growing areas of the market. The differences between an iPhone 6 and a $20 feature phone are tremendous and would be very difficult for a single company to bridge.
However, SoC vendors will attempt to reap the benefits of vertical integration by providing complete reference platforms to OEMs. Conceptually, this is a form of ""optional"" system integration, where the phone vendor or carrier can get the entire platform from the SoC supplier. This has the principal advantages of reducing time to market while also providing a baseline quality and experience for consumers. Currently, this approach has mostly been tested in emerging markets, but it's likely to become more common over time. There is a crucial distinction between reference platforms and vertical integration. Namely, OEMs can always choose to customize a platform to differentiate, and the SoC vendor avoids dealing with consumers directly. Typically, most of the customization is in terms of software on top of a base operating system.
Quote:Moreover, that will make the transition to a 10nm node even more difficult, as the foundries will have to move from 20nm interconnects to 10nm interconnects and skip a generation.The advances in technology lately allowing components on such a small scale to even be envisioned, much less planned for, are truly amazing.
Off topic: show
I present the first generation 'non-technical' rock:
I don't think your horizontal market development theory is supported by facts. Samsung and Apple are more vertically oriented than their competition, for starters. I know this article is narrowly focused on the hardware, but MS and Intel getting into hardware, Amazon getting into hardware, Google buying Moto, this is all vertical integration. How can you support the idea that this trend will be reversed with no real justification? I'm sure mobile chips will continue to specialize, but I don't think this means what you think it means. Automobile companies started making their own engines and with rare exceptions, never went back to being more horizontal. Same with retail and their store brands. Same with cloud companies and their servers. Same with mobile companies and their OSs. The horizontal market of PCs created by long-lasting standards and loose hegemony is the exception, not the norm.
Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?
I'm not so sure about several things:1- Moore's law's relevance. Moore's Law is about ICs. ICs are not as big a part of mobile computers as they are of desktops, even of laptops: screens, batteries, radios are a huge part of tablets' and phones' costs, as opposed to the bare SoC + RAM.2- The tablet vs phone dichotomy. For some reason (probably price insensitivity due to subsidies), Phones have a tendency to be more powerful than Tablets, ie phone SoCs are more than good enough for tablets. Since the OS and peripherals are the same, it makes more sense to design and build just one type of SoC, and just disable the phone-modem part of it (even the other radios are still required: BT, Wifi, GPS...), same as Intel disable cache and cores for their entry-level CPUs. Once you're fabbing a SoC, it makes more sense to make more of the same than to setup a separate run of a cut-down SoC on an older process, unless volumes are huge. We might still be getting previous-generation, well amortized SoCs in cheaper tablets, though.3- On the contrary, I see a tablet and phone convergence (the ugly phablet). I'm patiently waiting for the new 6""+ phones to replace my Nook Color and Galaxy Note 1 with a single device.4- The advantage of diversity ? Software is becoming ever more important than hardware. Multiplying SoCs means multiplying product development costs, making support and updates more difficult... Again, unless volumes are huge, OEMs are probaly better off going the way of the car industry and using modular ""platforms"" housed in different chassis with various screen sizes, keyboards, radios, digitizers...I'm wondering why the ""single device"" trend does not figure in your analysis. Is it stillborn ? Does it have no impact nor dependency on/with SoCs ?
Samsung has its own bespoke OS: Bada and it is used on an extensive line of devices. I think there are numbers somewhere that it outsold Windows Phone 7 for a time.
gypsumfantastic wrote:Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?First mover advantage.
SoC? System on a Chip I guess?
You're way off on the Moore's Law/cost of smartphones point. The processors used in today's high-end smartphones are already cheap, around $25. And there are less expensive options if you want a lower end product. In fact, the hardware in the whole smartphone is relatively cheap. Analyst's estimate the Z10's materials cost around $160, the iPhone 5 around $140. They're using expensive glass and metals, then there's the battery, memory, etc. which means the processor is a small factor of the cost.And then there's the jump from $140 in materials to the unsubsidized costs. The reason these phones cost $650 is because of the high margins these companies are able to get and the high cost of hardware design and/or software development. But the point is that making the processors 4 times better/cheaper isn't going to change the economics of the smartphone. What will change the economics is commoditized designs and software and cheaper materials all around. Then you'll have a $50 smartphone that's decent.
Last edited by ggeezz on Wed Feb 13, 2013 9:17 am
bigterp wrote:SoC? System on a Chip I guess?Yup.
gypsumfantastic wrote:Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?Money and momentum, the x86 market is a huge money maker for Intel so it is able to recoup its huge investments for advanced foundries.
Quote:Currently, the only products using 3D integration are FPGAs from Xilinx,Doesn't Sony use it in the PS Vita? I thought I read somewhere that they had the CPU, main memory (2 dies) and video memory, so 4 dies in total, sitting on top of each other all on the same chip.
renoX wrote:gypsumfantastic wrote:Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?Money and momentum, the x86 market is a huge money maker for Intel so it is able to recoup its huge investments for advanced foundries.Exactly and I would clarify that it's all about margins, the difference between what it costs to make a chip and what it sells for. The margins for desktop and server processors is huge because a) the whole product is expensive so $200 to $1000 for the chip is acceptable, and b) Intel has huge advantages in that space and little competition.So Intel can afford to do the R&D to stay ahead of the curve and keep their position. When your smartphone chip sells for $25 you can't do the R&D to leapfrog a company that sells Xeons for $1000 and Core i5's for $200.
I am happy to see Kanter here at Ars, I like his writing and he maintains Real World Tech, where Linus Torvalds often shows up to comment on CPU arch and other interesting topics.
ggeezz wrote:renoX wrote:gypsumfantastic wrote:Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?Money and momentum, the x86 market is a huge money maker for Intel so it is able to recoup its huge investments for advanced foundries.Exactly and I would clarify that it's all about margins, the difference between what it costs to make a chip and what it sells for. The margins for desktop and server processors is huge because a) the whole product is expensive so $200 to $1000 for the chip is acceptable, and b) Intel has huge advantages in that space and little competition.So Intel can afford to do the R&D to stay ahead of the curve and keep their position. When your smartphone chip sells for $25 you can't do the R&D to leapfrog a company that sells Xeons for $1000 and Core i5's for $200.Spot on.Intel are able to piggyback other development efforts off the highly lucrative mainstream x86 market which generates the huge sums of money to fund their amazing fab technology.The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each?The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process. In that scenario, Intel may not totally abandon the market but they might just stick to manufacturing SoCs on nodes that are a step or two behind the state of the art.
solomonrex wrote:I don't think your horizontal market development theory is supported by facts. Samsung and Apple are more vertically oriented than their competition, for starters. I know this article is narrowly focused on the hardware, but MS and Intel getting into hardware, Amazon getting into hardware, Google buying Moto, this is all vertical integration. How can you support the idea that this trend will be reversed with no real justification? I'm sure mobile chips will continue to specialize, but I don't think this means what you think it means. Automobile companies started making their own engines and with rare exceptions, never went back to being more horizontal. Same with retail and their store brands. Same with cloud companies and their servers. Same with mobile companies and their OSs. The horizontal market of PCs created by long-lasting standards and loose hegemony is the exception, not the norm.Yea, each year Amazon, MS, Apple and Google look more and more the same.
Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward.But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that.
gypsumfantastic wrote:Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?Intel's called Chipzilla for a reason up
Lagrange wrote:The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each?The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process. In that scenario, Intel may not totally abandon the market but they might just stick to manufacturing SoCs on nodes that are a step or two behind the state of the art.I think the processing is a bigger advantage than many realize. If Intel can stay ahead in process design - which this article seems to indicate - they should have a major advantage. All else being equal a 14nm chip should be significantly faster and more efficient than the same chip at 22nm. Add in the fact that yields increase geometrically - you can fit a lot more 14nm chips on a given wafer size vs 22nm (or 32nm for the other manufacturers.) and you have a very appealing proposition. And then add in the fact that Intel actually has a pretty good graphics stack and IP. It's not a sure thing by any means, but I suspect ARM may have just prodded a sleeping giant.edit: Also worth noting, Intel, TSMC, and Samsung are the only manufacturers who are building out 450nm wafers. This will increase yields dramatically. Of course Samsung and TSMC will build ARM out, but it definitely puts quite a bit of pressure on all other manufacturers. As the article mentions Intel and Samsung are the only ones who control production top to bottom, and Samsung must share some of the benefits with ARM.
As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy.The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP.
Last edited by paul5ra on Wed Feb 13, 2013 11:06 am
introiboad wrote:I am happy to see Kanter here at Ars, I like his writing and he maintains Real World Tech, where Linus Torvalds often shows up to comment on CPU arch and other interesting topics.Indeed. Most tech writing in this area is atrocious. This piece is one of the few well informed articles I've read in a long time.
ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward.But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that.The word you're looking for is Haswell, as far as I know.
Mabsark
ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward.But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that.Actually, that trend will not simply keep increasing going forward. The reason desktop/laptop sales are stagnating/decreasing is due to the fact that most people already have one and therefore don't need to buy another one. The exact same thing will happen with tablets as well. Sales are increasing now because people without tablets are buying them. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/decrease.The PC market is saturated and in a couple of years, the tablet market will be saturated too. Basically, in order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product.
gypsumfantastic wrote:Why wouldn't the foundries be able close the process gap with Intel? Is it a matter of money? Scale?Probably a mix of a lot of things. One big thing was during this recession, Intel was the ONLY fab company that didn't scale back their R&D. That alone gave Intel a large advantage.Intel has almost always been ahead. One of the reasons could be that Intel works with much higher margins than many of the commodity companies like Samsung and TSMC.Outside of the P4 flop and some of the monopolistic abuses, Intel has typically been selling to high end customers that are willing to pay a premium for ""the best"".Intel has a large benefit of having a relatively ""good name"" when it comes to CPUs, so they can effectively charge a brand-name premium.I'm sure there are other reasons, and probably better reasons, but these are the main ones that I think of.
Mabsark wrote:Actually, that trend will not simply keep increasing going forward. The reason desktop/laptop sales are stagnating/decreasing is due to the fact that most people already have one and therefore don't need to buy another one. The exact same thing will happen with tablets as well. Sales are increasing now because people without tablets are buying them. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/decrease.The PC market is saturated and in a couple of years, the tablet market will be saturated too. Basically, in order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product.That's true as long as most people are still buying both a tablet and a laptop when each needs to be replaced. I think the assumption is that, as you say, the tablet market will saturate, with people just replacing existing ones, but the desktop/laptop market could decrease much farther than that, if most people stop replacing them at all. I'm not sure of the likelihood of that, but I think that's where this idea comes from.
ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward.But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that.The upcoming Haswell chip is showing to consume 1/3 the power of IvyBridge at peak, consumes 1/20th the power at idle, all the while maintaining Identical or better performance.This chip should actually compete with ARM CPUs on both power/performance and idle.I am expecting a large war.
Apple once again is dictating the performance in the mobile industry. Nice to see others being able to keep the pace, as well.
paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy.The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple evolutionary path by the SoC providers since then.Yeah, and most of the innovation in the automobile industry came about before Henry Ford came into the business. Doesn't change the fact that cars would probably have been an asterisk in the history books under ""toys for rich people"" if it weren't for him.The same applies to to mobile computing for Apple, Samsung, et al.
SheldonRoss wrote:Lagrange wrote:The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each?The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process. In that scenario, Intel may not totally abandon the market but they might just stick to manufacturing SoCs on nodes that are a step or two behind the state of the art.I think the processing is a bigger advantage than many realize. If Intel can stay ahead in process design - which this article seems to indicate - they should have a major advantage. All else being equal a 14nm chip should be significantly faster and more efficient than the same chip at 22nm. Add in the fact that yields increase geometrically - you can fit a lot more 14nm chips on a given wafer size vs 22nm (or 32nm for the other manufacturers.) and you have a very appealing proposition. And then add in the fact that Intel actually has a pretty good graphics stack and IP. My point was that Intel might have a one or two process advantage over the rest of the industry at the cutting edge but that doesn't mean that they can afford to manufacture on those processes for very low margin parts. If the SoC market becomes increasingly commoditised, there isn't going to be the money to justify making them in a state of the art fab.Remember that one of the big selling points of Itanium was that it would make use of process advantages that were effectively paid for by the mainstream x86 market. That didn't quite work out in practice and Itanium processors were often well behind Xeons in process technology.
paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy.The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP.Just goes to show that people who have worked in an industry for a long time don't always understand what that industry is doing.You haven't been working in it long enough to seem to know that it was Acorn and Apple that invented the mobile ARM CPU in the first place. All those companies you've mentioned have just been working off Acorn and Apple's pioneering work. Now, Apple is back at it again, very successfully, and all the companies you mentioned that produce chips with ARM IP in them are licensing them from the company Acorn and Apple formed—ARM.
Last edited by melgross on Wed Feb 13, 2013 11:13 am
Mark Havel wrote:ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward.But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that.The word you're looking for is Haswell, as far as I know.If tablets move into the $100-200 range, is there going to be room for Haswell?So long as there is a higher-end tablet market, then Haswell will be able to shine, but it's going to be a much more powerful and costly part than the sort of ARM based hardware that often runs tablets. If we see a race to the bottom where price is the dominant motivator behind purchases, then a high performance SoC will struggle to make its mark.
melgross wrote:paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy.The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP.Just goes to show that people who have worked in an industry for a long time don't always understand what that industry is doing.You haven't been working in it long enough to seem to know that it was Acorn and Apple that invented the mobile ARM CPU in the first place. All those companies you've mentioned have just been working off Acorn and Apple's pioneering work. Now, Apple is back at it again, very successfully, and all the companies you mentioned that produce chips with ARM IP in them are licensing them from the company Acorn and Apple formed—ARM.Of course I realise ARM IP has indeed been a major driving factor too (though only one if several architectures before ARM became dominant), though I see ARM's influence on the mobile industry as having nothing to do with modern day Apple and only one small piece of the puzzle. My point is that the hard electrical engineering, mathematics, DSP, semiconductor physics/chemistry, RF engineering, analogue design, CAD etc. that make modern telecommunications possible has very little to do with the fashion companies who consumers (and unfortunately much of the tech media) associate with it and give the credit (though in this respect Samsung does deserve a bit more credit for their work on NAND flash and displays). The industry simply would not exist TODAY without the overwhelming horizontal integration that already dominates.
Quote:In the long term, mobile devices are likely to evolve similarly to the PC and favor a horizontal business model. The real advantage is one of flexibility; as costs drop and the market expands, it will be increasingly necessary for vendors like HTC to offer a wide range of phones based on radically different SoCs. You don't mention in the article that each SoC necessarily requires a bit of parallel dev work unlike the PC. In the PC world there is a standard BIOS and HW architecture that allows for pluggable designs. On a highly integrated SoC this is untrue. HTC suffers because it has to support radically different SoCs, their drivers and boot loaders, etc. Quote:While a vertically integrated company like Apple can focus and maintain leadership in a specific (and highly lucrative) niche, it would be very difficult to expand in many growing areas of the market. The differences between an iPhone 6 and a $20 feature phone are tremendous and would be very difficult for a single company to bridge.It's only difficult because Apple chooses to ignore that market, not because they can't. If they can release a $99 Apple TV, they can surely cobble together a $20 feature phone if they chose to eschew 8GB of NAND, BT, WiFi, a specialized dock connector, LTE, and their specialized processors. In other words, build the equivalent of an iPod shuffle with a horrible screen and no OS to speak of.
paul5ra wrote:melgross wrote:paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy.The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST.The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP.Just goes to show that people who have worked in an industry for a long time don't always understand what that industry is doing.You haven't been working in it long enough to seem to know that it was Acorn and Apple that invented the mobile ARM CPU in the first place. All those companies you've mentioned have just been working off Acorn and Apple's pioneering work. Now, Apple is back at it again, very successfully, and all the companies you mentioned that produce chips with ARM IP in them are licensing them from the company Acorn and Apple formed—ARM.Of course I realise ARM IP has indeed been a major driving factor too (though only one if several architectures before ARM became dominant), though I see ARM's influence on the mobile industry as having nothing to do with modern day Apple and only one piece of the puzzle. My point is that the hard electrical engineering, mathematics, DSP, semiconductor physics/chemistry, RF engineering, analogue design,etc. that make modern telecommunications possible has very little to do with the fashion companies who consumers (and unfortunately much of the tech media) associate with it and give the credit (though in this respect Samsung does deserve a bit more credit for their work on NAND flash and displays). The industry simply would not exist TODAY without the overwhelming horizontal integration that already dominates.Yes the efforts of these companies getting cellular communications standardized were immense. And the technology matured. And then they didn't do much with it. It took some youngin's to look at the problem fresh and add the UI that make today's smartphones work. As we have all seen, the moment your technology has matured is the moment you are screwed because someone else now has the opportunity to look at it as a black box and make something new. Each of those manufacturers knew that smartphones would eventually be awesome, but none of them had the UI and software design to make a truly breakout product. Imagine if Motorola would have been smart enough to buy the Android guys instead of Google. Instead, Google bought a bunch of patents on that cellular black box to try to defend it's platform.And when you think about it, which consumes more man years of engineering effort per year at this point.... iterating that cellular black box or developing the OS, services and apps that power today's smartphones?
Intel had better decide that they are competing in this space ""for real"", or they are screwed. They've already let the Atom languish for five years, during which ARM has completely caught up in performance.Just like Tim Cook said, if you don't cannibalize your own markets someone else will do it for you.Whether Intel will embrace that concept in time remains to be seen. Personally, I hope they don't; if Intel transforms into a chipless Fab company (like TSMC) everyone benefits.
I still think Samsung has the advantage long term because they have both the SOC and the memory products. As mentioned in the article, TSV's (Through Silicon Via's) are going to be quite a disruption. Today, people normally stack an LPDDR2 package on top of their SOC package (POP or Package On Package). Within the LPDDR2 package, you could have a stack of DRAM die typically with wire bonding connecting the die within the package.Once you more to TSV's, you can have a LOT more connections between the SOC and its DRAM's. While this is being standardized through JEDEC (http://www.jedec.org/category/technolog ... a/3d-ics-0), Samsung has all the pieces in house to do whatever they want. You could see a 512 bit or higher bus from the SOC to the memory. The trick is that the memory and the SOC need to line up with each other when you stack them. This gives Samsung an inherent advantage.This isn't just going to impact mobile either. Take a look at that JEDEC link. It also lists High Bandwidth Memory (HBM). This is a 1024 bit bus that provides 128GBytes/s to 256GBytes/s of bandwidth to a stack of up to 8 DRAM's. Here is your processor that includes 8-16 cores and 4GBytes of really, really, fast DRAM... No DIMMs required. How many of them do you want in your server rack?If I was Intel or Apple, I would be thinking seriously about making some investments in Micron to guarantee they make some compelling DRAM's to integrate with their SOC's and processors... otherwise Samsung is going to blow them out of the water on bandwidth.
Great_Scott wrote:Intel had better decide that they are competing in this space ""for real"", or they are screwed. They've already let the Atom languish for five years, during which ARM has completely caught up in performance.Just like Tim Cook said, if you don't cannibalize your own markets someone else will do it for you.Whether Intel will embrace that concept in time remains to be seen. Personally, I hope they don't; if Intel transforms into a chipless Fab company (like TSMC) everyone benefits.It's true that Atom has stood still for too long, but honestly it's pretty amazing how Atom is still competitive with current ARM chips. The Z2760 is even 32nm vs 28nm of the latest Krait and A15 chips.But that's all changing with Atom moving to the tick tock schedule this year. It wouldn't even surprise me to see Apple move to Intel chips for IOS.And I don't see how Intel moving to a chipless Fab company would help everyone. It certainly wouldn't help Intel.
Mabsark wrote:ggeezz wrote:Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward.But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they're going to have to up their game in the tablet space to even be able to do that.Actually, that trend will not simply keep increasing going forward. The reason desktop/laptop sales are stagnating/decreasing is due to the fact that most people already have one and therefore don't need to buy another one. The exact same thing will happen with tablets as well. Sales are increasing now because people without tablets are buying them. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/decrease.The PC market is saturated and in a couple of years, the tablet market will be saturated too. Basically, in order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product.Yes and no. I'm not sure the tablet market will saturate in a ""couple of years."" It may be more like 5 years. But that's a quibble.Here's the real issue. Right now Apple wants you to own an iPhone AND iPad AND Macbook AND iWatch AND Apple TV. Microsoft, OTOH, is making the Surface so that you could ditch your laptop and just use a Surface. Not everyone, but some people.If 5 years from now, we're in a world where a significant number of people use a Surface-type device instead of a laptop, then the PC market is going to contract significantly. Maybe some of the tablet-like devices will use moderately expensive Intel chips, but some of them are going to use cheaper chips.
GravyGraphics wrote:I still think Samsung has the advantage long term because they have both the SOC and the memory products. As mentioned in the article, TSV's (Through Silicon Via's) are going to be quite a disruption. Today, people normally stack an LPDDR2 package on top of their SOC package (POP or Package On Package). Within the LPDDR2 package, you could have a stack of DRAM die typically with wire bonding connecting the die within the package.Once you more to TSV's, you can have a LOT more connections between the SOC and its DRAM's. While this is being standardized through JEDEC (http://www.jedec.org/category/technolog ... a/3d-ics-0), Samsung has all the pieces in house to do whatever they want. You could see a 512 bit or higher bus from the SOC to the memory. The trick is that the memory and the SOC need to line up with each other when you stack them. This gives Samsung an inherent advantage.This isn't just going to impact mobile either. Take a look at that JEDEC link. It also lists High Bandwidth Memory (HBM). This is a 1024 bit bus that provides 128GBytes/s to 256GBytes/s of bandwidth to a stack of up to 8 DRAM's. Here is your processor that includes 8-16 cores and 4GBytes of really, really, fast DRAM... No DIMMs required. How many of them do you want in your server rack?If I was Intel or Apple, I would be thinking seriously about making some investments in Micron to guarantee they make some compelling DRAM's to integrate with their SOC's and processors... otherwise Samsung is going to blow them out of the water on bandwidth.Why not AMD? Last I checked they still made memory...and processors/GPUs.",['Flexibility.'],7565,multifieldqa_en,en,,04b588ff2dea15f4a9c4fdbaabc55aaad1ba3446114d6741," 2013 may be a big year for the evolution of smartphones and tablets. As Moore’s Law rolls on, the cost of a low-end smartphone will decline. It's likely to be the first year in which tablets out-ship notebooks in the US. And in the coming years, this will lead to a confluence of high-end tablets and ultra-mobile notebooks as the world figures out how these devices co-exist, blend, hybridize, and/or merge. The power budget for tablet SoCs is much greater, up to 4W for a passively cooled device and as high as 7-8W for systems with fans. This alone means there is a much wider range of tablet designs than smartphones. The default connectivity for tablets is Wi-Fi rather than a cellular modem. The vast majority of tablets do not have cellular modems, and even fewer customers actually purchase a wireless data plan. The main smartphone OSes today are iOS and Android, though Windows is beginning to make an appearance (perhaps with Linux or BlackBerry on the horizon). Just as importantly, phone vendors like HTC must pass government certification and win the approval of carriers. There is very much a walled-garden aspect, where carriers control which devices can be attached to their networks, and in some cases devices can only be sold through a certain carrier. The business model places consumers quite far removed from the actual hardware. In this two-part series, we'll explore the major trends and evolution for mobile SoCs. More importantly, we will look to where the major vendors are likely going in the next several years. We'll also look at the future of mobile CPUs, part 1: Today's fork in the road, and part 2: The future of the mobile SoC in the future, part 3: The next generation of SoC technology, part 4: What's next for the mobile processor and SoC industry? We'll end the series with the second part of the series, part 5: The evolution of mobile processors, part 6: What the future holds for mobile processors and SoCs, part 7: What is next for mobile chips and processors, and what the future looks like for mobile devices and chips? We will end with the third part, the fourth part of this series, the fifth and final installment, the sixth and final part of our series on mobile chips, part 8: The mobile chip revolution, part 9: Mobile chips and chips that will dominate the next decade and beyond. The final part will be the seventh and final piece, the final part: The development of the next-generation mobile processor, part 10: Mobile processors and chips for tablets and smartphones, part 11: Mobile computing and chips, and the next step in the mobile computing evolution, part 12: Mobile SoCs for PCs and tablets, part 13: Mobile compute and chips. Apple is the only company truly pursuing a vertically integrated model. Samsung’s approach is best characterized as a hybrid, particularly as the company lacks a bespoke OS. In the long term, mobile devices are likely to evolve similarly to the PC and favor a horizontal business model. As costs drop and the market expands, it will be increasingly necessary for vendors like HTC to offer a wide range of phones based on radically different SoCs. The advances in technology lately allowing components on such a sma sma.ost always optional discrete components of the platform. The software ecosystem is relatively similar, with Microsoft, Apple, and Google OSes available. Because tablets eschew cellular modems, the time to market is faster, and they are much more commonly sold directly to consumers rather than through carriers. The difference in power targets and expected performance is quite large. The differences between an iPhone 6 and a $20 feature phone are tremendous and would be very difficult for a single company to bridge. The real advantage is one of flexibility; as costs drop, it'll be increasingly needed for vendors to offer radically different phones. It will be difficult for foundries to move from 20nm to 10nm interconnects and skip a generation. This will make the transition to a 10nm node even more difficult, as the foundries will have to move to 20nm interConnects and skipping a generation to get to the new node. It's likely to become more common over time, but it's mostly been tested in emerging markets, but the approach has been more common in the past few years. It has been a trend of Samsung devices using Samsung SoCs, at least for the premier products. More recently though, there has been an increase in the use of Intel and Qualcomm SoCs in phones and tablets. This is a form of ""optional"" system integration, where the phone vendor or carrier can get the entire platform from the SoC supplier. This has the principal advantages of reducing time toMarket while also providing a baseline quality and experience for consumers. It is a crucial distinction between reference platforms and vertical integration. Namely, OEMs can always choose to customize a platform to differentiate, and the soC vendor avoids dealing with consumers directly. It would be difficult to expand in many growing areas of the market, such as the mobile gaming and gaming market, without a horizontal approach to the mobile device market. It could take a few years to have the requisite volume, but eventually the two markets will be clearly separate. There is a real advantage to the horizontal approach, as it allows you to offer phones with radically different processors. It also gives you the flexibility to offer devices on a range of different platforms, including Android, iOS, Windows, and other operating systems. It can also reduce the overall cost of the device. Mobile chips will continue to specialize, but I don't think this means what you think it means. Moore's Law is about ICs. ICs are not as big a part of mobile computers as they are of desktops, even of laptops. Software is becoming ever more important than hardware. Multiplying SoCs means multiplying product development costs, making support and updates more difficult. I see a tablet and phone convergence (the ugly phablet) I'm patiently waiting for the new 6""+ phones to replace my Nook Color and Galaxy Note 1 with a single device. I'm wondering why the ""single device"" trend does not figure in your analysis. Is it stillborn ? Does it have no impact nor dependency on/with SoCs ?Samsung has its own bespoke OS: Bada and it is used on an extensive line of devices. I think there are numbers somewhere that it outsold Windows Phone 7 for a time. Amazon getting into hardware, Google buying Moto, this is all vertical integration. How can you support the idea that this trend will be reversed with no real justification? The horizontal market of PCs created by long-lasting standards and loose hegemony is the exception, not the norm. The processors used in today's high-end smartphones are already cheap, around $25. And there are less expensive options if you want a lower end product. In fact, the hardware in the whole smartphone is relatively cheap. Analyst's estimate the Z10's materials cost around $160, the iPhone 5 around $140. And then there's the jump from $140 in the unsized to the high cost of hardware. The reason these phones cost $650 is because of the high margins these companies are able to get and the design and/or development.l scale to even be envisioned, much less planned for, are truly amazing. You're way off on the Moore's law/cost of smartphones point. And you're wrong on the price insensitivity due to subsidies. The price of smartphones is already $25, and there are more expensive options to choose from if you're looking for a lower-end product. If you want to make a phone that costs $650, you're going to have to spend a lot of money on materials, batteries, memory, etc. which means the processor is a small factor of the cost. And if you don't want to spend that much on the processor, you'll have to pay more for the battery, memory and other parts of the phone, such as the screen. You can't do that with a cheap phone. You need to pay $650 for the whole phone, not just the processor. You also need to spend $650 on the phone itself. You don't need to do that for the screen, you need to buy the parts that make up the phone that make the phone. That's the difference between a phone and a tablet. You have to choose between the screen and the battery. You must choose the screen over the battery and the memory, and then the battery over the memory. You've got to choose the battery to make your phone a phone. Intel are able to piggyback other development efforts off the highly lucrative mainstream x86 market which generates the huge sums of money to fund their amazing fab technology. When your smartphone chip sells for $25 you can't do the R&D to leapfrog a company that sells Xeons for $1000 and Core i5's for $200. The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that scenario, Intel may not totally abandon the market but they might just stick to manufacturing SoCs on nodes that are a step or two behind the state of the art. The only products using 3D integration are FPGAs from Xilinx,Doesn't Sony use it in the PS Vita? I thought I read somewhere that they had the CPU, main memory (2 dies) and video memory, so 4 dies in total, sitting on top of each other all on the same chip. The margins for desktop and server processors is huge because a) the whole product is expensive so $200 to $1000 for the chip is acceptable, and b) Intel has huge advantages in that space and little competition. The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoC on a cutting edge, or near cutting edge process. I don't think your horizontal market development theory is supported by facts. Samsung and Apple are more vertically oriented than their hardware rivals. I know this is narrowly focused on the hardware, but MS Intel getting into hardware, getting into Amazon, and getting into the hardware market, is a different story. It's all about margins, the difference between what it costs to make a chip and what it sells for. When you have processors selling for a only a few dollars each, you don't need to go all-out with the design and development. Then you'll have a $50 smartphone that's decent. That's what will change the economics is commoditized designs and software and cheaper materials all around. You don't have to be Intel or AMD to be a big player in the smartphone market. You just need to be in the right place at the right time with the right technology and the right people at the wrong price. It doesn't matter if you're Apple or Samsung, you need to have a product that's at least as good as the one you're selling. You can't compete on price with Apple and Samsung. You have to compete on quality and price. You need to compete with each other on price and quality. You've got to be at the top of the food chain, not just the bottom of the pile. You're not going to be able to compete against each other for the same thing. You'll have to work together to get the best of both worlds. You won't have the same level of quality and the same price points. You will have to do it in different ways to compete. It won't be easy, but it's worth the effort. It will be worth it. Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Google buying Moto, this is all vertical integration. How can you support the idea that this trend will be reversed with no real justification? I'm sure mobile chips will continue to specialize, but I don't think this means what you think it means. The horizontal market of PCs created by long-lasting standards and loose hegemony is the exception, not the norm. It's not a sure thing by any means. I suspect ARM may have just prodded a sleeping giant.edit: Also worth noting, Intel, TSMC and Samsung are the only manufacturers who are building out 450nm wafers. This will increase yields dramatically. Of course Samsung and TSMC will build ARM out, but it definitely puts quite a bit of pressure on all other manufacturers. As the article mentions Intel and Samsung is the only ones who control production top to bottom, andSamsung must share some of the benefits with ARM. Under the hood most stuff has been on a simple linear evolutionary path by the SoC by the soC. And then add in the fact that Intel actually has a pretty good graphics stack and IP. The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each? The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process. All else being equal a 14nm chip should be significantly faster and more efficient than the same chip at 22nm. And you can fit a lot more 14nm chips on a given wafer size vs 22nm (or 32nm for the other manufacturers.) and you've got a very appealing proposition. If Intel can stay ahead in process design - which this article seems to indicate - they should have a major advantage. But you're right, they're going to have to use their fabs that are a step or two behind the cutting the edge. But it's not the same as making a phone chip on a 22nm process, it's making a tablet chip on 14nm. That's a different story. And it's the same with cloud companies and their servers. Same with mobile companies and its OSs. It doesn't matter if you're a cloud company or a mobile company, you're in the cloud. Intel cannot abandon the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward. The PC market is saturated and in a couple of years, the tablet market will be saturated too. In order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product. That's true as long as most people still still buy a laptop and a tablet when each needs to be replaced. I think the assumption is that, as you say, the iPad market will become saturated in the next few years. The word you're looking for is Haswell, as far as I know. The phone manufacturers have then mainly been simply sticking these off-the-shelf SoCs (and their included low-level software stacks) in a box, a job made all the easier with the SoC manufacturer collaboration providing the bulk of the work for the AOSP.Indeed. Most tech writing in this area is atrocious. This piece is one of the few well informed articles I've read in a long time. Back to Mail Online home.Back to the page you came from. Back To the pageyou came from, back to the site you were from.. You can now sign up for the Daily Discussion to help other readers understand today's featured news stories. Follow us on Twitter @ArsTechnica and @wiredtrends. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. For information on suicide prevention in the United States, call the suicide prevention Lifeline at 1-844-788-88 or visit http://www.suicidesprevention.org/. For confidential help in the UK, contact Samaritans at 08457 909090 or http:// www. Samaritans. org/. Back to the pages you came From. ArsTechnica.com/news/2013/02/13/paul-5ra-kanter-says-Intel-can't-abandon-tablet-market-and-that-it-will-become-saturated-in-a-matter-of-months-or-years-ago-because-people-are-buying-more-laptops-to-replace-them-with-tablets-instead-of  tablets-and that-they-can-be-skeptical.html. You're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they'regoing to have to up their game in the tablet space to even be able to do that. The same thing will happen with tablets as well. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/ decrease. Intel's Haswell chip is showing to consume 1/3 the power of IvyBridge at peak, consumes 1/20th the power at idle, all the while maintaining identical or better performance. If Intel can stay ahead in process design - which this article seems to indicate - they should have a major advantage. The question for the future is how the economics will stack up when overall device costs fall significantly and there is a big downward pressure on SoC prices. In that situation, can Intel still justify bringing their A-game to a market where products are essentially commoditised and you have processors selling for a only a few dollars each? The lesson from their previous involvement in the DRAM market is that they probably won't want to be involved because there isn't enough money to be made to justify manufacturing phone SoCs on a cutting edge, or near cutting edge process.Yeah, and most of the innovation in the automobile industry came about before Henry Ford came into the business. Doesn't change the fact that cars would probably have been an asterisk in the history books under ""toys for rich people"" if it weren't for him.The same applies to mobile computing for Apple, Samsung, et al. Nice to see others being able to keep the pace, as well. I am expecting a large war between Apple and Samsung in the mobile industry. And then add in the fact of Intel actually has a pretty good graphics stack and IP. My point was that Intel might have a one or two process advantage over the rest of the industry at the cutting edge but that doesn't mean that they can afford to manufacture on those processes for very low margin parts. It's a very appealing proposition - you can fit a lot more 14nm chips on a given wafer size vs 22nm (or 32nm for the other manufacturers.) and you can have a very attractive proposition. I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. They're better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung and Apple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST. The true innovation in mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Samsung had little to do with it, and it's a long time ago that Apple has been dictating the performance in the phone industry. That's where this idea comes from, but I'm not sure of the likelihood of that, if most people stop replacing them at all. I think the trend is only going to increase going forward.saturate, with people just replacing existing ones, but the desktop/laptop market could decrease much farther than that. The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. Under the hood most stuff has been on a simple linear evolutionary path by the SoC providers since then. If we see a race to the bottom where price is the dominant motivator behind purchases, then a high performance SoC will struggle to make its mark. The word you're looking for is Haswell, as far as I know. If tablets move into the $100-200 range, is there going to be room for Haswell? So long as there is a higher-end tablet market, then Haswell will be able to shine, but it will be a much more powerful and costly part than the sort of ARM based hardware that often runs tablets. The true innovation is in the phone/tablet market. Desktop/laptop sales are stagnating/decreasing and phones/tablets are on the rise. This trend is only going to increase going forward. But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they'regoing to have to up their game in the tablet space to even be able. to do that. The real innovation is going to come in the next few years with the release of the iPhone 5 and 5S and the launch of the next generation of Intel's next-generation processor, the Intel Core 2 Duo. It's going to take a long time to get to that point, but I think it's coming soon. I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. You haven't been working in it long enough to seem to know that it was Acorn and Apple that invented the mobile ARM CPU in the first place. All those companies you've mentioned have just been working off AcORN and Apple's pioneering work. Now, Apple is back at it again, very successfully, and all the companies you mentioned that produce chips with ARM IP in them are licensing them from the company Acorn. and Apple formed—ARM. The mobile industry moved to horizontal integration a long. time ago. It is a simple stuff stuff. The phone manufufu is a phone. It doesn't need a state of the art fab. It needs a phone that can run on a standard operating system. It can't do that with an ARM-based SoC. It has to be a phone with an Intel-based processor that runs on an ARM chip. It must be a smartphone that runs a standard OS that runs an ARM processor. It cannot be a tablet that runs the same software as an Intel chip that uses ARM chips. And it can't be a laptop that runs ARM chips that run on an Intel processor that uses a different chip that has a different operating system that uses different software. It will have to do all of the work for the AOSP. Acorn and Apple invented the mobile ARM CPU in the first place. The industry simply would not exist TODAY without the overwhelming horizontal integration that already dominates. The true innovation in the mobile market was done a decade or more ago mainly by Moto/Nokia/Ericsson/TI/Qualcomm, and Samsung and Apple had little to do with it. If they can release a $99 Apple TV, they can surely cobble together a $20 feature phone if they chose to eschew 8GB of NAND, BT, WiFi, a specialized dock connector, LTE, and their specialized processors. In other words, build the equivalent of an iPod shuffle with a horrible screen and no OS to speak of. You don't mention in the article that each SoC necessarily requires a bit of parallel dev work unlike the PC. In the PC world there is a standard BIOS and HW architecture that allows for pluggable designs. On a highly integrated SoC this is untrue. HTC suffers because it has to support radically different SoCs, their drivers and boot loaders, etc. The real advantage is one of flexibility; as costs drop and the market expands, it will be increasingly necessary for vendors like HTC to offer a wide range of phones based on radically different soCs. If a vertically integrated company like Apple can focus and maintain leadership in a specific (and highly lucrative) niche, it would be very difficult to expand in many growing areas of the market. It's only difficult because Apple chooses to ignore that market, not because they can't.melgross wrote:paul5ra wrote:As someone who has worked in the semiconductor industry for longer than contemporary fanboys have existed, I'm getting a bit fed-up seeing many of these articles which rewrite history with analyses distorted by the lens of the contemporary Apple or Samsung fanboy. The mobile industry moved to horizontal integration a long time ago. Better indicators than these odd contemporary obsessions with the relatively non-innovative Samsung andApple where when Motorola spun out its semiconductor division as Freescale, Nokia stopped making it's own custom designs with TI and ST, and Ericsson spun out it's Ericsson Mobile Platforms division and formed ST-Ericsson with ST. The differences between an iPhone 6 and a $50 feature phone are tremendous and would bevery difficult for a single company to bridge. On the other hand, a $30 feature phone would be much easier to make than a $100 iPhone 6. That's not to say Apple can't do it, it's just that it would have to work much harder to do it. It would take a lot of time and effort to get to the point where it can do what Apple can do with the iPhone 6 in a matter of weeks. That would be a huge leap in terms of development time, and a huge amount of money. Acorn and Apple invented the mobile ARM CPU in the first place. All those companies you've mentioned have just been working off Acorn's pioneering work. It took some youngin's to look at the problem fresh and add the UI that make today's smartphones work. As we have all seen, the moment your technology has matured is the moment you are screwed. If you don't cannibalize your own markets someone else will do it for you.Whether Intel will embrace that concept in time remains to be seen. If Intel transforms into a chipless Fab company (like TSMC) everyone benefits. I still think Samsung has the advantage long term because they have both the SOC and the memory products. As mentioned in the article, TSV's (Through Silicon Via's) are going to be quite a disruption. While this is being standardized through JEDEC (http://www.jedec.org/category/technolog ... a/3d-ics-0), Samsung has all the pieces in house to do whatever they want. You could see a 512 bit or higher bus from the SOC to the memory. The trick is that the memory and the SOC need to line up with each other when you stack them. This givin you more to TSV’s, you can have a LOT more connections between the SOC's and its DRAM's. And when you think about it, which consumes more man years of engineering effort per year at this point.... iterating that cellular black box or developing the OS, services and apps that power today's smartphone?Intel had better decide that they are competing in this space ""for real"", or they are screwed, they've already let the Atom languish for five years, during which ARM has completely caught up in performance. Imagine if Motorola would have been smart enough to buy the Android guys instead of Google. Instead, Google bought a bunch of patents on that cellularBlack box to try to defend it's platform. Instead of Google buying Motorola, Motorola bought Google instead of Motorola, and Google bought Motorola to defend its Android platform. It's the same thing with Apple and Samsung. The industry simply would not exist TODAY without the overwhelming horizontal integration that already dominates. The hard electrical engineering, mathematics, DSP, semiconductor physics/chemistry, RF engineering, analogue design, etc. that make modern telecommunications possible has very little to do with the fashion companies who consumers (and unfortunately much of the tech media) associate with it and give the credit (though in this respect Samsung do deserve a bit more credit for their work on NAND flash and displays) Samsung does deserve a lot of credit for this. i.e. Samsung has a lot more control over what happens in the mobile industry than Apple does. I.e then. Apple is back at it again, very successfully, and all the companies that produce chips with ARM IP in them are licensing them from the company Acorn andApple formed—ARM. Intel's Atom has stood still for too long, but honestly it's pretty amazing how Atom is still competitive with current ARM chips. The Z2760 is even 32nm vs 28nm of the latest Krait and A15 chips. It wouldn't even surprise me to see Apple move to Intel chips for IOS. I don't see how Intel moving to a chipless Fab company would help everyone. It certainly wouldn't help Intel. I still think Samsung has the advantage long term because they have both the SOC and the memory products. As mentioned in the article, TSV's (Through Silicon Via's) are going to be quite a disruption. If I was Intel or Apple, I would be thinking seriously about making some investments in Micron to guarantee they make some compelling DRAM's to integrate with their SOC's and processors... otherwise Samsung is going to blow them out of the water on bandwidth. I'm not sure the tablet market will saturate in a ""couple of years"" It may be more like 5 years. When most people already own a tablet, they won't be buying a new one every year and therefore sales will stagnate/decrease. If 5 years from now, we're in a world where a significant number of people use a Surface-type device instead of a laptop, then the PC market will contract significantly. Maybe some of the tablet-like devices will use moderately expensive Intel chips, but some of them will use cheaper chips. But that's a quibble. Right now Apple wants you to own an iPhone AND iPad AND Macbook AND iWatch AND Apple TV. Microsoft, OTOH, is making the Surface so that you could ditch your laptop and just use a Microsoft Surface. Not everyone, butsome people. If you don't cannibalize your own markets someone else will do it for you. The PC market is saturated and in a couple of years, the tabletmarket will be saturated too. In order to increase sales in a saturated market, you need to increase the population growth or decrease the longevity of the product. The reason desktop/laptop sales are stagnating is due to the fact that most people Already have one and therefore don't need to buy another one. The exact same thing will happen with tablets as well. Sales are increasing now because people without tablets are buying them. While this can have a LOT of more connections between the SOC. and its DRAM, you typically typically have a lot more connections within the die within the SOC ( typically with the LPDDR2 package) than within the DRAM package. This isn't just going to impact mobile either. It also lists High Bandwidth Memory (HBM). This is a 1024 bit bus that provides 128Gbytes/s to 256GBytes/s of bandwidth to a stack of up to 8 DRAMs. How many of them do you want in your server rack? How many DIMMs required. It's all changing with Atom moving to the tick tock schedule this year. But you're right, they're going to have use their fabs that are a step or two behind the cutting the edge. But they'regoing to have to up their game in the tablet space to even be able to do that. Samsung has all the pieces in house to do whatever they want. You could see a 512 bit or higher bus from the SOC to the memory. The trick is that the memory and the SOC need to line up with each other when you stack them. This isn't just going to impact mobile either. It also lists High Bandwidth Memory (HBM) This is a 1024 bit bus that provides 128Gbytes/s to 256GBytes/s of bandwidth to a stack of up to 8 DRAM's. Here is your processor that includes 8-16 cores and 4GBytes of really, really, fast DRAM... No DIMMs required. How many of them do you want in your server rack? If I was Intel or Apple, I would be thinking seriously about making some investments in Micron. Otherwise Samsung is going to blow them out of the water on bandwidth.Why not AMD? Last I checked they still made memory...and processors/GPUs."
Who was Brooksley Elizabeth's first husband?,"Brooksley Elizabeth Born (born August 27, 1940) is an American attorney and former public official who, from August 26, 1996, to June 1, 1999, was chair of the Commodity Futures Trading Commission (CFTC), the federal agency which oversees the U.S. futures and commodity options markets. During her tenure on the CFTC, Born lobbied Congress and the President to give the CFTC oversight of off-exchange markets for derivatives, in addition to its role with respect to exchange-traded derivatives, but her warnings were ignored or dismissed, and her calls for reform resisted by other regulators.<ref name=""nytimes"">Goodman, Peter S. The Reckoning - Taking Hard New Look at a Greenspan Legacy, The New York Times, October 9, 2008.</ref> Born resigned as chairperson on June 1, 1999, shortly after Congress passed legislation prohibiting her agency from regulating derivatives.

In 2009, Born received the John F. Kennedy Profiles in Courage Award, along with Sheila Bair of the Federal Deposit Insurance Corporation, in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis.

Early life and education
Born graduated from Abraham Lincoln High School (San Francisco, California) at the age of 16. She then attended Stanford University, where she majored in English and was graduated with the class of 1961.  She initially wanted to become a doctor, but a guidance counsellor at Stanford advised her against medicine, so she majored in English literature instead.

She then attended Stanford Law School, one of only seven women in her class.  She was the first female student ever to be named president of the Stanford Law Review. She received the ""Outstanding Senior"" award and graduated as valedictorian of the class of 1964.

Legal career
Immediately after law school Born was selected as a law clerk to judge Henry Edgerton of the U.S. Court of Appeals for the District of Columbia Circuit. It was during this time that she met her first husband, Jacob C. Landau, who was a journalist covering the Federal courts at the time. Following her clerkship, she became an associate at the Washington, D.C.-based international law firm of Arnold & Porter. Born was attracted to Arnold & Porter because it was one of the few major law firms to have a woman partner at that time, Carolyn Agger, who was the head of the tax practice. Born took a two-year leave of absence from Arnold & Porter to accompany her first husband to Boston, where he had received a fellowship. During that time she worked as a research assistant to law professor Alan Dershowitz.

Born's early career at Arnold & Porter focused on international trade law, in which she represented a number of Swiss industries and the government of Switzerland. She developed a practice representing clients in numerous complex litigation and arbitration cases involving financial market transactions. Among her high-profile cases was the matter of the Hunt Brothers attempt to corner the market in silver in the 1970s. She made partner at Arnold & Porter, after moving to a three-day schedule to help raise her second child, and eventually rose to be the head of the firm's derivatives practice.

Born was among the first female attorneys to systematically address inequities regarding how the laws treated women. Born and another female lawyer, Marna Tucker, taught what is considered to have been the first ""Women and the Law"" course at Catholic University’s Columbus School of Law. The class exclusively concerned prejudicial treatment of women under the laws of the United States, past and present.  Born and Tucker were surprised to discover that there was no textbook on the issue at the time.  Born is also one of the co-founders of the National Women's Law Center. Born also helped rewrite the American Bar Association rules to make it possible for more women and minorities to sit on federal bench.

During her long legal career, and into her retirement, Born did much pro bono and other types of volunteer work. She was active in the American Bar Association, the largest professional organization of lawyers in the United States.  Initially Born was named a member of the governing council of the ABA's Individual Rights Section, eventually becoming chairperson.  Born and Tucker founded the ABA Women's Caucus, the first organization of female lawyers in the ABA.  She held several other senior positions in the ABA, including being named the first woman member of the ABA's Standing Committee on the Federal Judiciary.  As a member of the Judiciary Committee, Born provided testimony and opinion on persons nominated for federal judgeships. In 1980 she was named chair of the committee.  As chair of the committee, Born was invited to address the U.S. Congress regarding the nomination of Judge Sandra Day O'Connor to the U.S. Supreme Court.

In 1993, Born's name was floated as a possible candidate for Attorney General of the United States, but Janet Reno was nominated.

In July 2009, Nancy Pelosi appointed Brooksley Born as a commissioner to the Financial Crisis Inquiry Commission (FCIC).

Born and the OTC derivatives market
Born was appointed to the CFTC on April 15, 1994, by President Bill Clinton. Due to litigation against Bankers Trust Company by Procter and Gamble and other corporate clients, Born and her team at the CFTC sought comments on the regulation of over-the-counter derivatives, a first step in the process of writing CFTC regulations to supplement the existing regulations of the Federal Reserve System,  the Options Clearing Corporation, and the National Association of Insurance Commissioners. Born was particularly concerned about swaps, financial instruments that are traded over the counter between banks, insurance companies or other funds or companies, and thus have no transparency except to the two counterparties and the counterparties' regulators, if any.  CFTC regulation was strenuously opposed by Federal Reserve chairman Alan Greenspan, and by Treasury Secretaries Robert Rubin and Lawrence Summers. On May 7, 1998, former SEC Chairman Arthur Levitt joined Rubin and Greenspan in objecting to the issuance of the CFTC's concept release. Their response dismissed Born's analysis and focused on the hypothetical possibility that CFTC regulation of swaps and other OTC derivative instruments could create a ""legal uncertainty"" regarding such financial instruments,  hypothetically reducing the value of the instruments. They argued that the imposition of regulatory costs would ""stifle financial innovation"" and encourage financial capital to transfer its  transactions offshore. The disagreement between Born and the Executive Office's top economic policy advisors has been described not only as a classic Washington turf war, but also a war of ideologies,  insofar as it is possible to argue that Born's actions were consistent with Keynesian and neoclassical economics while Greenspan, Rubin, Levitt, and Summers consistently espoused neoliberal, and neoconservative policies.

In 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse.  Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions.  The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures.  Born stated, ""I thought that LTCM was exactly what I had been worried about"".  In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance.  After intervention by the Federal Reserve, the crisis was averted.  In congressional hearings into the crisis, Greenspan acknowledged that language had been introduced into an agriculture bill that would prevent CFTC from regulating the derivatives which were at the center of the crisis that threatened the US economy.  U.S. Representative Maurice Hinchey (D-NY) asked ""How many more failures do you think we'd have to have before some regulation in this area might be appropriate?""  In response, Greenspan brushed aside the substance of Born's warnings with the simple assertion that ""the degree of supervision of regulation of the over-the-counter derivatives market is quite adequate to maintain a degree of stability in the system"". Born's warning was that there wasn't any regulation of them.  Born's chief of staff, Michael Greenberger summed up Greenspan's position this way: ""Greenspan didn't believe that fraud was something that needed to be enforced, and he assumed she probably did. And of course, she did.""  Under heavy pressure from the financial lobby, legislation prohibiting regulation of derivatives by Born's agency was passed by the Congress.  Born resigned on June 1, 1999.

The derivatives market continued to grow yearly throughout both terms of George W. Bush's administration. On September 15, 2008, the bankruptcy of Lehman Brothers forced a broad recognition of a financial crisis in both the US and world capital markets.  As Lehman Brothers' failure temporarily reduced financial capital's confidence, a number of newspaper articles and television programs suggested that the failure's possible causes included the conflict between the CFTC and the other regulators.Faiola, Anthony, Nakashima, Ellen and Drew, Jill. The Crash: Risk and Regulation - What Went Wrong, The Washington Post, October 15, 2008.

Born declined to publicly comment on the unfolding 2008 crisis until March 2009, when she said: ""The market grew so enormously, with so little oversight and regulation, that it made the financial crisis much deeper and more pervasive than it otherwise would have been."" She also lamented the influence of Wall Street lobbyists on the process and the refusal of regulators to discuss even modest reforms.

An October 2009 Frontline documentary titled ""The Warning""  described Born's thwarted efforts to regulate and bring transparency to the derivatives market, and the continuing opposition thereto. The program concluded with an excerpted interview with Born sounding another warning: ""I think we will have continuing danger from these markets and that we will have repeats of the financial crisis -- may differ in details but there will be significant financial downturns and disasters attributed to this regulatory gap, over and over, until we learn from experience.""

In 2009 Born, along with Sheila Bair of the FDIC, was awarded the John F. Kennedy Profiles in Courage Award in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis.  According to Caroline Kennedy, ""Brooksley Born recognized that the financial security of all Americans was being put at risk by the greed, negligence and opposition of  powerful and well connected interests.... The catastrophic financial events of recent months have  proved them [Born and Sheila Bair] right.""  One member of the President's working group had a change of heart about Brooksley Born.  SEC Chairman Arthur Levitt stated ""I've come to know her as one of the most capable, dedicated, intelligent and committed public servants that I have ever come to know"", adding that ""I could have done much better. I could have made a difference"" in response to her warnings.

In 2010, a documentary film Inside Job further alleged that derivatives regulation was ineffective from the Clinton administration on. Along with fellow whistleblower, former IMF Chief Economist Raghuram Rajan, who was also scorned by the economic establishment, Brooksley Born was cited as one of the authorities arguing that financial derivatives increase economic risk.

 Personal life 
Born is married to Alexander E. Bennett (also retired from Arnold & Porter).  She has five adult children - two from a previous marriage to Jacob Landau and three stepchildren. Notably, Born was named a partner at Arnold & Porter while working part-time so she could raise her two young children.  When both of her children were school-age, Born returned to practice full-time.

References

External links
Attorney profile at Arnold & Porter
Brooksley Born (2009 Winner) of the Profiles in Courage Award, with acceptance speech transcript and NECN video

Profile at MarketsWiki
Speeches and statements
""Testimony Of Brooksley Born Chairperson of the CFTC Concerning The Over-The-Counter Derivatives Market"", before the House Committee On Banking And Financial Services, July 24, 1998.
""The Lessons of Long Term Capital Management L.P."", Remarks of Brooksley Born, Chairperson of the CFTC, Chicago-Kent-IIT Commodities Law Institute, Chicago, Illinois, October 15, 1998.
 Interview: Brooksley Born for ""PBS Frontline: The Warning"", PBS, (streaming VIDEO 1 hour), October 20, 2009.
Articles
Manuel Roig-Franzia. ""Credit Crisis Cassandra:Brooksley Born's Unheeded Warning Is a Rueful Echo 10 Years On"", The Washington Post, May 26, 2009
 Taibbi, Matt. ""The Great American Bubble Machine"", Rolling Stone'', July 9–23, 2009

1940 births
American women lawyers
Arnold & Porter people
Clinton administration personnel
Columbus School of Law faculty
Commodity Futures Trading Commission personnel
Heads of United States federal agencies
Lawyers from San Francisco
Living people
Stanford Law School alumni
21st-century American women
Stanford University alumni.",['Jacob C. Landau.'],2085,multifieldqa_en,en,,470018af720bc15decf8f7a9643250c9a6548c8efeb394cd," Brooksley Elizabeth Born was chair of the Commodity Futures Trading Commission (CFTC) from August 26, 1996, to June 1, 1999. During her tenure on the CFTC, Born lobbied Congress and the President to give the agency oversight of off-exchange markets for derivatives. Her warnings were ignored or dismissed, and her calls for reform resisted by other regulators. In 2009, Born received the John F. Kennedy Profiles in Courage Award, along with Sheila Bair of the Federal Deposit Insurance Corporation, in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis. Born is one of the co-founders of the National Women's Law Center. She helped rewrite the American Bar Association rules to make it possible for more women and minorities to sit on federal bench. She is married to Jacob C. Landau, who was a journalist covering the Federal courts at the time. She has two children, a son and a daughter. She was the first female student ever to be named president of the Stanford Law Review and graduated as valedictorian of the class of 1964. She also taught the first ""Women and the Law"" course at Catholic University’s Columbus School of Law, which exclusively concerned prejudicial treatment of women under the laws of the United States, past and present. Born was among the first women to systematically address inequities regarding how the laws treated women. She made partner at Arnold & Porter, after moving to a three-day schedule to help raise her second child, and eventually rose to be the head of the firm's derivatives practice. She represented a number of Swiss industries and the government of Switzerland. Among her high-profile cases was the matter of the Hunt Brothers attempt to corner the market in silver in the 1970s. Born took a two-year leave of absence to accompany her first husband to Boston, where he had received a fellowship. During that time she worked as a research assistant to law professor Alan Dershowitz. She became an associate at the Washington, D.C.-based international law firm of Arnold and Porter. She developed a practice representing clients in numerous complex litigation and arbitration cases involving financial market transactions. She later became a partner at the law firm, and represented clients in several international trade law cases, including the Swiss government and the Swiss canton of Basel. She retired from the law practice in 1999, shortly after Congress passed legislation prohibiting her agency from regulating derivatives. She currently lives in San Francisco with her husband, Jacob Landau. Her son, son and daughter live in California, and she has a daughter and a son-in-law who lives in New York, New York and Washington, DC. Her husband and daughter have three children, both of whom were born in the U.S. and live in the San Francisco area. Brooksley Born was appointed to the CFTC on April 15, 1994, by President Bill Clinton. Born was particularly concerned about swaps, financial instruments that are traded over the counter between banks, insurance companies or other funds or companies. In 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse. In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance. After intervention by the Federal Reserve, the crisis was averted. In July 2009, Nancy Pelosi appointed Brooksley Born as a commissioner to the Financial Crisis Inquiry Commission (FCIC). Born and the OTC derivatives market were at the center of the crisis that threatened the U.S. economy. She was named the first woman member of the ABA's Standing Committee on the Federal Judiciary in 1980. In 1980 she was named chair of the committee. In 1993, Born's name was floated as a possible candidate for Attorney General of the United States, but Janet Reno was nominated instead. In 1995, Born was invited to address the Congress regarding the nomination of Judge Sandra Day O'Connor to the Supreme Court. In 1997, Born and Tucker founded the A BA Women's Caucus, the first organization of female lawyers inThe ABA. She held several other senior positions in the American Bar Association, including being named the head of the Individual Rights Section. In 2000, she became chairperson of the Criminal Defense Section of the American Lawyer's Association, the largest professional organization of lawyers in the US. In 2002, Born became the first female member of the ABA Governing Council, the governing council of which she was the chairperson for two years. In 2003, she was elected to the American Bar Association's Board of Trustees, the highest level of membership for women in the association. In 2008, she was named the chair of its Individual Rights Committee, a position she held until her retirement in 2010. In 2010, she served as the chairwoman of the ABA's Criminal Defense Committee, the council's governing council for the first time. In 2011, she resigned from the board to take up a post at the University of California, Los Angeles. In 2012, she wrote a book about her experiences as a CFTC commissioner, ""Brooke Born: A Memoir of a Regulator and a Lawyer"" The book was published by Simon & Schuster, a division of Penguin Books, and is available on Amazon.com for $20.99. For more information, or to order a copy of the book, go to: http://www.samaritans.com/Brooke-Born-A-Memoirs-of-a-Regulator. Brooksley Born was the head of the CFTC, which regulated the over-the-counter derivatives market. In 1999, she warned that there wasn't any regulation of them. The derivatives market continued to grow yearly throughout both terms of George W. Bush's administration. On September 15, 2008, the bankruptcy of Lehman Brothers forced a broad recognition of a financial crisis in both the US and world capital markets. Born declined to publicly comment on the unfolding 2008 crisis until March 2009, when she said: ""The market grew so enormously, with so little oversight and regulation, that it made the financial crisis much deeper and more pervasive than it otherwise would have been"" In 2009 Born, along with Sheila Bair of the FDIC, was awarded the John F. Kennedy Profiles in Courage Award in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis. In 2010, a documentary film alleged that derivatives regulation was ineffective from the Clinton administration on. Along with fellow whistleblower, former IMF Chief Economist Raghuram Rajan, who was also scorned by the economic establishment, Born was cited as one of the authorities arguing that financial derivatives increase economic risk. She is married to Alexander E. Bennett (also retired from Arnold & Porter).  She has five adult children - two from a previous marriage to Jacob Landau and three stepchild stepchildren. She also has two stepchildren from her first marriage to Alexander Landau. The SEC Chairman Arthur Levitt stated ""I could have done much better. I could have made a difference"" in response to her warnings. In an excerpt of an October 2009 Frontline documentary titled ""The Warning""  described Born's thwarted efforts to regulate and bring transparency to the derivatives market, and the continuing opposition thereto. The program concluded with an excerpted interview with Born sounding another warning: ""I think we will have continuing danger from these markets ... There will be significant financial downturns and disasters attributed to this regulatory gap, over and over, until we learn from experience"" She is survived by her husband, three stepchildren and a daughter from her second marriage. She has a son from her third marriage, and a stepdaughter from her fourth marriage. The CFTC is now headed by former CFTC Chairman Michael Greenberger, who is also a former FDIC chief. The agency is based in Washington, D.C. and has offices in New York, Washington, and San Francisco. It was created in 1998 by President George H.W. Bush to oversee the creation of the Federal Deposit Insurance Corporation (FDIC) and the Securities and Exchange Commission (SEC). The agency was created by President Bill Clinton to protect the U.S. financial system. Brooksley Born was named a partner at Arnold & Porter while working part-time so she could raise her two young children. When both of her children were school-age, Born returned to practice full-time. Born is the winner of the 2009 Profiles in Courage Award. He is married to the former head of the U.S. Commodity Futures Trading Commission. He has two daughters and a son. He lives in San Francisco with his wife and two daughters. Born has served as Chair of the CFTC since 1998. He was a member of the House Committee on Banking and Financial Services from 1997 to 2001. He also served as Chairman of the Securities and Exchange Commission from 2001 to 2007. He currently serves as the chairman of the Federal Reserve Bank of San Francisco. He previously served as the chair of the New York Mercantile Bank from 1995 to 1998. His wife is a former president of the American College of Law, where she was a dean. She was a partner in the firm's San Francisco office from 1988 to 1998, and is now a partner of Arnold &porter. She has two children, one of whom is a junior partner at the firm. She is the mother of two daughters, both of whom are currently in school."
What is the main methodology used in the research?,"Paper Info

Title: On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement Learning
Publish Date: Unkown
Author List: Seth Karten, Siva Kailas, Huao Li, Katia Sycara

Figure

Figure1.By using contrastive learning, our method seeks similar representations between the state-message pair and future states while creating dissimilar representations with random states.Thus satisfying the utility objective of the information bottleneck.The depicted agents are blind and cannot see other cars.
Figure 2.An example of two possible classes, person and horse, from a single observation in the Pascal VOC game.
Figure 3. Blind Traffic Junction Left: Our method uses compositional complexity and contrastive utility to outperform other baselines in terms of performance and sample complexity.The legend provides the mean ± variance of the best performance.Right: Top: success, contrastive, and complexity losses for our method.Right, Bottom: success, autoencoder loss for ae-comm with supervised pretraining.
Figure 4. Pascal VOC Game Representing compositional concepts from raw pixel data in images to communicate multiple concepts within a single image.Our method significantly outperforms ae-comm and no-comm due to our framework being able to learn composable, independent concepts.
Figure 5. Blind Traffic Junction Social shadowing enables significantly lower sample complexity when compared to traditional online MARL.
Beta ablation: Messages are naturally sparse in bits due to the complexity loss.Redundancy measures the capacity for a bijection between the size of the set of unique tokens and the enumerated observations and intents.Min redundancy is 1.0 (a bijection).Lower is better.

abstract

Explicit communication among humans is key to coordinating and learning. Social learning, which uses cues from experts, can greatly benefit from the usage of explicit communication to align heterogeneous policies, reduce sample complexity, and solve partially observable tasks. Emergent communication, a type of explicit communication, studies the creation of an artificial language to encode a high task-utility message directly from data.
However, in most cases, emergent communication sends insufficiently compressed messages with little or null information, which also may not be understandable to a third-party listener. This paper proposes an unsupervised method based on the information bottleneck to capture both referential complexity and task-specific utility to adequately explore sparse social communication scenarios in multi-agent reinforcement learning (MARL).
We show that our model is able to i) develop a natural-language-inspired lexicon of messages that is independently composed of a set of emergent concepts, which span the observations and intents with minimal bits, ii) develop communication to align the action policies of heterogeneous agents with dissimilar feature models, and iii) learn a communication policy from watching an expert's action policy, which we term 'social shadowing'.

INTRODUCTION

Social learning agents analyze cues from direct observation of other agents (novice or expert) in the same environment to learn an action policy from others. However, observing expert actions may not be sufficient to coordinate with other agents. Rather, by learning to communicate, agents can better model the intent of other agents, leading to better coordination.
In humans, explicit communication for coordination assumes a common communication substrate to convey abstract concepts and beliefs directly , which may not be available for new partners. To align complex beliefs, heterogeneous agents must learn a message policy that translates from one theory of mind to another to synchronize coordination.
Especially when there is complex information to process and share, new agent partners need to learn to communicate to work with other agents. Emergent communication studies the creation of artificial language. Often phrased as a Lewis game, speakers and listeners learn a set of tokens to communicate complex observations .
However, in multi-agent reinforcement learning (MARL), agents suffer from partial observability and non-stationarity (due to unaligned value functions) , which aims to be solved with decentralized learning through communication. In the MARL setup, agents, as speakers and listeners, learn a set of tokens to communicate observations, intentions, coordination, or other experiences which help facilitate solving tasks .
Agents learn to communicate effectively through a backpropagation signal from their task performance . This has been found useful for applications in human-agent teaming , multirobot navigation , and coordination in complex games such as StarCraft II . Communication quality has been shown to have a strong relationship with task performance , leading to a multitude of work attempting to increase the representational capacity by decreasing the convergence rates .
Yet these methods still create degenerate communication protocols , which are uninterpretable due to joined concepts or null (lack of) information, which causes performance degradation. In this work, we investigate the challenges of learning a arXiv:2302.14276v1 LG] 28 Feb 2023 messaging lexicon to prepare emergent communication for social learning (EC4SL) scenarios.
We study the following hypotheses: H1) EC4SL will learn faster through structured concepts in messages leading to higher-quality solutions, H2) EC4SL aligns the policies of expert heterogeneous agents, and H3) EC4SL enables social shadowing, where an agent learns a communication policy while only observing an expert agent's action policy.
By learning a communication policy, the agent is encouraged to develop a more structured understanding of intent, leading to better coordination. The setting is very realistic among humans and many computer vision and RL frameworks may develop rich feature spaces for a specific solo task, but have not yet interacted with other agents, which may lead to failure without alignment.
We enable a compositional emergent communication paradigm, which exhibits clustering and informativeness properties. We show theoretically and through empirical results that compositional language enables independence properties among tokens with respect to referential information. Additionally, when combined with contrastive learning, our method outperforms competing methods that only ground communication on referential information.
We show that contrastive learning is an optimal critic for communication, reducing sample complexity for the unsupervised emergent communication objective. In addition to the more human-like format, compositional communication is able to create variable-length messages, meaning that we are not limited to sending insufficiently compressed messages with little information, increasing the quality of each communication.
In order to test our hypotheses, we show the utility of our method in multi-agent settings with a focus on teams of agents, high-dimensional pixel data, and expansions to heterogeneous teams of agents of varying skill levels. Social learning requires agents to explore to observe and learn from expert cues.
We interpolate between this form of social learning and imitation learning, which learns action policies directly from examples. We introduce a 'social shadowing' learning approach where we use first-person observations, rather than third-person observations, to encourage the novice to learn latently or conceptually how to communicate and develop an understanding of intent for better coordination.
The social shadowing episodes are alternated with traditional MARL during training. Contrastive learning, which works best with positive examples, is apt for social shadowing. Originally derived to enable lower complexity emergent lexicons, we find that the contrastive learning objective is apt for agents to develop internal models and relationships of the task through social shadowing.
The idea is to enable a shared emergent communication substrate (with minimal bandwidth) to enable future coordi-nation with novel partners. Our contributions are deriving an optimal critic for a communication policy and showing that the information bottleneck helps extend communication to social learning scenarios.
In real-world tasks such as autonomous driving or robotics, humans do not necessarily learn from scratch. Rather they explore with conceptually guided information from expert mentors. In particular, having structured emergent messages reduces sample complexity, and contrastive learning can help novice agents learn from experts.
Emergent communication can also align heterogeneous agents, a social task that has not been previously studied.

Multi-Agent Signaling

Implicit communication conveys information to other agents that is not intentionally communicated . Implicit signaling conveys information to other agents based on one's observable physical position . Implicit signaling may be a form of implicit communication such as through social cues or explicit communication such as encoded into the MDP through ""cheap talk"" .
Unlike implicit signaling, explicit signaling is a form of positive signaling that seeks to directly influence the behavior of other agents in the hopes that the new information will lead to active listening. Multi-agent emergent communication is a type of explicit signaling which deliberately shares information.
Symbolic communication, a subset of explicit communication, seeks to send a subset of pre-defined messages. However, these symbols must be defined by an expert and do not scale to particularly complex observations and a large number of agents. Emergent communication aims to directly influence other agents with a learned subset of information, which allows for scalability and interpretability by new agents.

Emergent Communication

Several methodologies currently exist to increase the informativeness of emergent communication. With discrete and clustered continuous communication, the number of observed distinct communication tokens is far below the number permissible . As an attempt to increase the emergent ""vocabulary"" and decrease the data required to converge to an informative communication ""language"", work has added a bias loss to emit distinct tokens in different situations .
More recent work has found that the sample efficiency can be further improved by grounding communication in observation space with a supervised reconstruction loss . Information-maximizing autoencoders aim to maximize the state reconstruction accuracy for each agent. How-ever, grounding communication in observations has been found to easily satisfy these input-based objectives while still requiring a myriad more samples to explore to find a task-specific communication space .
Thus, it is necessary to use task-specific information to communicate informatively. This will enable learned compression for task completion rather than pure compression for input recovery. Other work aims to use the information bottleneck to decrease the entropy of messages . In our work, we use contrastive learning to increase representation similarity with future goals, which we show optimally optimizes the Q-function for messages.

Natural Language Inspiration

The properties of the tokens in emergent communication directly affect their informative ability. As a baseline, continuous communication tokens can represent maximum information but lack human-interpretable properties. Discrete 1-hot (binary vector) tokens allow for a finite vocabulary, but each token contains the same magnitude of information, with equal orthogonal distance to each other token.
Similar to word embeddings in natural language, discrete prototypes are an effort to cluster similar information together from continuous vectors . Building on the continuous word embedding properties, VQ-VIB , an information-theoretic observation grounding based on VQ-VAE properties , uses variational properties to provide word embedding properties for continuous emergent tokens.
Like discrete prototypes, they exhibit a clustering property based on similar information but are more informative. However, each of these message types determines a single token for communication. Tokens are stringed together to create emergent ""sentences"".

Preliminaries

We formulate our setup as a decentralized, partially observable Markov Decision Process with communication (Dec-POMDP-Comm). Formally, our problem is defined by the tuple, S, A, M, T , R, O, Ω, γ . We define S as the set of states, A i , i ∈ [1, N ] as the set of actions, which includes task-specific actions, and M i as the set of communications for N agents.
T is the transition between states due to the multi-agent joint action space T : S × A 1 , ..., A N → S. Ω defines the set of observations in our partially observable setting. Partial observability requires communication to complete the tasks successfully. O i : M 1 , ..., M N × Ŝ → Ω maps the communications and local state, Ŝ, to a distribution of observations for each agent.
R defines the reward function and γ defines the discount factor.

Architecture

The policy network is defined by three stages: Observation Encoding, Communication, and Action Decoding. The best observation encoding and action decoding architecture is task-dependent, i.e., using multi-layer perceptrons (MLPs), CNNs , GRUs , or transformer layers are best suited to different inputs.
The encoder transforms observation and any sequence or memory information into an encoding H. The on-policy reinforcement learning training uses RE-INFORCE or a decentralized version of MAPPO as specified by our experiments. Our work focuses on the communication stage, which can be divided into three substages: message encoding, message passing (often considered sparse communication), and message decoding.
We use the message passing from . For message decoding, we build on a multiheaded attention framework, which allows an agent to learn which messages are most important . Our compositional communication framework defines the message encoding, as described in section 4.

Objective

Mutual information, denoted as I(X; Y ), looks to measure the relationship between random variables, which is often measured through Kullback-Leibler divergence , I(X; Y ) = D KL (p(x, y)||p(x) ⊗ p(y)). The message encoding substage can be defined as an information bottleneck problem, which defines a tradeoff between the complexity of information (compression, I(X, X)) and the preserved relevant information (utility, I( X, Y )).
The deep variational information bottleneck defines a trade-off between preserving useful information and compression . We assume that our observation and memory/sequence encoder provides an optimal representation H i suitable for sharing relevant observation and intent/coordination information. We hope to recover a representation Y i , which contains the sufficient desired outputs.
In our scenario, the information bottleneck is a trade-off between the complexity of information I(H i ; M i ) (representing the encoded information exactly) and representing the relevant information I(M j =i ; Y i ), which is signaled from our contrastive objective. In our setup, the relevant information flows from other agents through communication, signaling a combination of the information bottleneck and a Lewis game.
We additionally promote complexity through our compositional independence objective, This is formulated by the following Lagrangian, where the bounds on mutual information Î are defined in equations 1, 2, and 10. Overall, our objective is,

Complexity through Compositional Communication

We aim to satisfy the complexity objective, I(H i , M i ), through compositional communication. In order to induce complexity in our communication, we want the messages to be as non-random as possible. That is, informative with respect to the input hidden state h. In addition, we want each token within the message to share as little information as possible with the preceding tokens.
Thus, each additional token adds only informative content. Each token has a fixed length in bits W . The total sequence is limited by a fixed limit, L l W l ≤ S, of S bits and a total of L tokens. We use a variational message generation setup, which maps the encoded hidden state h to a message m; that is, we are modeling the posterior, π i m (m l |h).
We limit the vocabulary size to K tokens, e j ∈ R D , j ∈ [1, K] ⊂ N, where each token has dimensionality D and l ∈ [1, L] ⊂ N. Each token m l is sampled from a categorical posterior distribution, 0 otherwise such that the message m l is mapped to the nearest neighbor e j . A set of these tokens makes a message m.
To satisfy the complexity objective, we want to use m i to well-represent h i and consist of independently informative m i l .

Independent Information

We derive an upper bound for the interaction information between all tokens. Proposition 4.1. For the interaction information between all tokens, the following upper bound holds: The proof is in Appendix A.1. Since we want the mutual information to be minimized in our objective, we minimize,

Input-Oriented Information

In order to induce complexity in the compositional messages, we additionally want to minimize the mutual information I(H; M ) between the composed message m and the encoded information h. We derive an upper bound on the mutual information that we use as a Lagrangian term to minimize. Proposition 4.2. For the mutual information between the composed message and encoded information, the following upper bound holds:
The proof is in Appendix A.1. Thus, we have our Lagrangian term, Conditioning on the input or observation data is a decentralized training objective.

Sequence Length

Compositional communication necessitates an adaptive limit on the total length of the sequence. Corollary 4.3. Repeat tokens, w, are redundant and can be removed. Suppose one predicts two arbitrary tokens, w k and w l . Given equation 1, it follows that there is low or near-zero mutual information between w k and w l .
A trivial issue is that the message generator will predict every available token as to follow the unique token objective. Since the tokens are imbued with input-oriented information (equation 2), the predicted tokens will be based on relevant referential details. Thus, it follows that tokens containing irrelevant information will not be chosen.
A nice optimization objective that follows from corollary 4.3 is that one can use self-supervised learning with an end-ofsequence (EOS) token to limit the variable total length of compositional message sequences. (3) Algorithm 1 Compositional Message Gen.(h t ) m i ∼ N ( ĥ; µ, σ) 9: end for 10: return m

Message Generation Architecture

Now, we can define the pipeline for message generation. The idea is to create an architecture that can generate features to enable independent message tokens. We expand each compressed token into the space of the hidden state h (1-layer linear expansion) since each token has a natural embedding in R |h| .
Then, we perform attention using a softmin to help minimize similarity with previous tokens and sample the new token from a variational distribution. See algorithm 1 for complete details. During execution, we can generate messages directly due to equation 1, resolving any computation time lost from sequential compositional message generation.

Utility through Contrastive Learning

First, note that our Markov Network is as follows: H j → M j → Y i ← H i . Continue to denote i as the agent identification and j as the agent ID such that j = i. We aim to satisfy the utility objective of the information bottleneck, I(M j ; Y i ), through contrastive learning as shown in figure 1. Proposition 5.1.
Utility mutual information is lower bounded by the contrastive NCE-binary objective, The proof is in Appendix A.1. This result shows a need for gradient information to flow backward across agents along communication edge connections.

Experiments and Results

We condition on inputs, especially rich information (such as pixel data), and task-specific information. When evaluating an artificial language in MARL, we are interested in referential tasks, in which communication is required to complete the task. With regard to intent-grounded communication, we study ordinal tasks, which require coordination information between agents to complete successfully.
Thus, we consider tasks with a team of agents to foster messaging that communicates coordination information that also includes their observations. To test H1, structuring emergent messages enables lower complexity, we test our methodology and analyze the input-oriented information and utility capabilities.
Next, we analyze the ability of heterogeneous agents to understand differing communication policies (H2)). Finally, we consider the effect of social shadowing (H3), in which agents solely learn a communication policy from an expert agent's action policy. We additionally analyze the role of offline reinforcement learning for emergent communication in combination with online reinforcement learning to further learn emergent communication alongside an action policy.
We evaluate each scenario over 10 seeds.

Environments

Blind Traffic Junction We consider a benchmark that requires both referential and ordinal capabilities within a team of agents. The blind traffic junction environment requires multiple agents to navigate a junction without any observation of other agents. Rather, they only observe their own state location.
Ten agents must coordinate to traverse through the lanes without colliding into agents within their lane or in the junction. Our training uses REINFORCE . Pascal VOC Game We further evaluate the complexity of compositional communication with a Pascal VOC . This is a two-agent referential game similar to the Cifar game but requires the prediction of multiple classes.
During each episode, each agent observes a random image from the Pascal VOC dataset containing exactly two unique labels. Each agent must encode information given only the raw pixels from the original image such that the other agent can recognize the two class labels in the original image. An agent receives a reward of 0.25 per correctly chosen class label and will receive a total reward of 1 if both agents guess all labels correctly.
See figure 2. Our training uses heterogeneous agents trained with PPO (modified from MAPPO repository). For simplicity of setup, we consider images with exactly two unique labels from a closed subset of size five labels of the original set of labels from the Pascal VOC data. Furthermore, these images must be of size 375 × 500 pixels.
Thus, the resultant dataset comprised 534 unique images from the Pascal VOC dataset.

Baselines

To evaluate our methodology, we compare our method to the following baselines: (1) no-comm, where agents do not communicate; (2) rl-comm, which uses a baseline communication method learned solely through policy loss ; (3) ae-comm, which uses an autoencoder to ground communication in input observations ; (4) VQ-VIB, which uses a variational autoencoder to ground discrete communication in input observations and a mutual information objective to ensure low entropy communication .
We provide an ablation of the loss parameter β in table 1 in the blind traffic junction scenario. When β = 0, we use our compositional message paradigm without our derived loss terms. We find that higher complexity and independence losses increase sample complexity. When β = 1, the model was unable to converge.
However, when there is no regularization loss, the model performs worse (with no guarantees about referential representation). We attribute this to the fact that our independence criteria learns a stronger causal relationship. There are fewer spurious features that may cause an agent to take an incorrect action.
In order to understand the effect of the independent concept representation, we analyze the emergent language's capacity for redundancy. A message token m l is redundant if there exists another token m k that represents the same information. With our methodology, the emergent 'language' converges to the exact number of observations and intents required to solve the task.
With a soft discrete threshold, the independent information loss naturally converges to a discrete number of tokens in the vocabulary. Our β ablation in table 1 yields a bijection between each token in the vocabulary and the possible emergent concepts, i.e., the enumerated observations and intents. Thus for β = 0.1, there is no redundancy.
Sparse Communication In corollary 4.3, we assume that there is no mutual information between tokens. In practice, the loss may only be near-zero. Our empirical results yield independence loss around 1e − 4. In table 1, the size of the messages is automatically compressed to the smallest size to represent the information.
Despite a trivially small amount of mutual information between tokens, our compositional method is able to reduce the message size in bits by 2.3x using our derived regularization, for a total of an 8x reduction in message size over non-compositional methods such as ae-comm. Since the base unit for the token is a 32-bit float, we note that each token in the message may be further compressed.
We observe that each token uses three significant digits, which may further compress tokens to 10 bits each for a total message length of 20 bits.

Communication Utility Results

Due to coordination in MARL, grounding communication in referential features is not enough. Finding the communication utility requires grounding messages in ordinal information. Overall, figure shows that our compositional, contrastive method outperforms all methods focused on solely input-oriented communication grounding.
In the blind traffic junction, our method yields a higher average task success rate and is able to achieve it with a lower sample complexity. Training with the contrastive update tends to spike to high success but not converge, often many episodes before convergence, which leaves area for training improvement.
That is, the contrastive update begins to find aligned latent spaces early in training, but it cannot adapt the methodology quickly enough to converge. The exploratory randomness of most of the early online data prevents exploitation of the high utility f + examples. This leaves further room for improvement for an adaptive contrastive loss term.
Regularization loss convergence After convergence to high task performance, the autoencoder loss increases in order to represent the coordination information. This follows directly from the information bottleneck, where there exists a tradeoff between utility and complexity. However, communication, especially referential communication, should have an overlap between utility and complexity.
Thus, we should seek to make the complexity loss more convex. Our compositional communication complexity loss does not converge before task performance convergence. While the complexity loss tends to spike in the exploratory phase, the normalized value is very small. Interestingly, the method eventually converges as the complexity loss converges below a normal- ized 0.3.
Additionally, the contrastive loss tends to decrease monotonically and converges after the task performance converges, showing a very smooth decrease. The contrastive f − loss decreases during training, which may account for success spikes prior to convergence. The method is able to converge after only a moderate decrease in the f + loss.
This implies empirical evidence that the contrastive loss is an optimal critic for messaging. See figure 3.

Heterogeneous Alignment Through Communication

In order to test the heterogeneous alignment ability of our methodology to learn higher-order concepts from highdimensional data, we analyze the performance on the Pascal VOC game. We compare our methodology against ae-comm to show that concepts should consist of independent information directly from task signal rather than compression to reconstruct inputs.
That is, we show an empirical result on pixel data to verify the premise of the information bottleneck. Our methodology significantly outperforms the observation-grounded ae-comm baseline, as demonstrated by figure 4. The ae-comm methodology, despite using autoencoders to learn observation-grounded communication, performs only slightly better than no-comm.
On the other hand, our methodology is able to outperform both baselines significantly. It is important to note that based on figure 4, our methodology is able to guess more than two of the four labels correctly across the two agents involved, while the baseline methodologies struggle to guess exactly two of thew four labels consistently.
This can be attributed to our framework being able to learn compositional concepts that are much more easily discriminated due to mutual independence.

Social Shadowing

Critics of emergent communication may point to the increased sample complexity due to the dual communication and action policy learning. In the social shadowing scenario, heterogeneous agents can learn to generate a communication policy without learning the action policy of the watched expert agents. To enable social shadowing, the agent will alternate between a batch of traditional MARL (no expert) and (1st-person) shadowing an expert agent performing the task in its trajectory.
The agent only uses the contrastive objective to update its communication policy during shadowing. In figure , the agent that performs social shadowing is able to learn the action policy with almost half the sample complexity required by the online reinforcement learning agent. Our results show that the structured latent space of the emergent communication learns socially benevolent coordination.
This tests our hypothesis that by learning communication to understand the actions of other agents, one can enable lower sample complexity coordination. Thus, it mitigates the issues of solely observing actions.

Discussion

By using our framework to better understand the intent of others, agents can learn to communicate to align policies and coordinate. Any referential-based setup can be performed with a supervised loss, as indicated by the instant satisfaction of referential objectives. Even in the Pascal VOC game, which appears to be a purely referential objective, our results show that intelligent compression is not the only objective of referential communication.
The emergent communication paradigm must enable an easy-to-discriminate space for the game. In multi-agent settings, the harder challenge is to enable coordination through communication. Using contrastive communication as an optimal critic aims to satisfy this, and has shown solid improvements. Since contrastive learning benefits from good examples, this method is even more powerful when there is access to examples from expert agents.
In this setting, the communication may be bootstrapped, since our optimal critic has examples with strong signals from the 'social shadowing' episodes. Additionally, we show that the minimization of our independence objective enables tokens that contain minimal overlapping information with other tokens.
Preventing trivial communication paradigms enables higher performance. Each of these objectives is complementary, so they are not trivially minimized during training, which is a substantial advantage over comparative baselines. Unlike prior work, this enables the benefits of training with reinforcement learning in multi-agent settings.
In addition to lower sample complexity, the mutual information regularization yields additional benefits, such as small messages, which enables the compression aspect of sparse communication. From a qualitative point of view, the independent information also yields discrete emergent concepts, which can be further made human-interpretable by a post-hoc analysis .
This is a step towards white-box machine learning in multi-agent settings. The interpretability of this learned white-box method could be useful in human-agent teaming as indicated by prior work . The work here will enable further results in decision-making from high-dimensional data with emergent concepts.
The social scenarios described are a step towards enabling a zero-shot communication policy. This work will serve as future inspiration for using emergent communication to enable ad-hoc teaming with both agents and humans.

Appendix

A.1. Proofs Proposition 4.1 For the interaction information between all tokens, the following upper bound holds: Proof. Starting with the independent information objective, we want to minimize the interaction information, which defines the conditional mutual information between each token and, Let π i m (m l |h) be a variational approximation of p(m l |h), which is defined by our message encoder network.
Given that each token should provide unique information, we assume independence between m l . Thus, it follows that our compositional message is a vector, m = [m 1 , . . . , m L ], and is jointly Gaussian. Moreover, we can define q( m|h) as a variational approximation to p(m|h) = p(m 1 ; . . . , m L |h).
We can model q with a network layer and define its loss as || m − m|| 2 . Thus, transforming equation 4 into variational form, we have, it follows that q( m|h) log q( m|h)d m ≥ q( m|h) log Thus, we can bound our interaction information, Proposition 4.2 For the mutual information between the composed message and encoded information, the following upper bound holds:
Proof. By definition of mutual information between the composed messages M and the encoded observations H, we have, Substituting q( m|h) for p( m|h), the same KL Divergence identity, and defining a Gaussian approximation z( m) of the marginal distribution p( m), it follows that, In expectation of equation 1, we have,
This implies that, for m = [m 1 , . . . , m L ], there is probabilistic independence between m j , m k , j = k. Thus, expanding, it follows that, where z(m l ) is a standard Gaussian. Proposition 5.1. Utility mutual information is lower bounded by the contrastive NCE-binary objective, Proof. We suppress the reliance on h since this is directly passed through.
By definition of mutual information, we have, Our network model learns π R + (y|m) from rolled-out trajectories, R + , using our policy. The prior of our network state, π R − (y), can be modeled from rolling out a random trajectory, R−. Unfortunately, it is intractable to model π R + (y|m) and π R − (y) directly during iterative learning, but we can sample y + ∼ π R + (y|m) and y − ∼ π R − (y) directly from our network during training.
It has been shown that log p(y|m) provides a bound on mutual information , with the expectation over l p(m l , y l ). However, we need a tractable understanding of the information Y . In the information bottleneck, Y represents the desired outcome. In our setup, y is coordination information that helps create the desired output, such as any action a − .
This implies, y =⇒ a − . Since the transition is known, it follows that a − =⇒ s − f , a random future state. Thus, we have, π This is similar to the proof for lemma A.5, but requires assumptions on messages m from the emergent language. We note that when m is random, the case defaults to lemma A.5. Thus, we assume we have at least input-oriented information in m given sufficiently satisfying equation 2. Given a sufficient emergent language, it follows that y =⇒ a + , where a + is an intention action based on m.
Similarly, since the transition is known, a + =⇒ s + f , a desired goal state along the trajectory. Thus, we have, π R + (y|m) = p(s = s + f |y, m). Recall the following (as shown in ), which we have adapted to our communication objective, Proposition A.3 (rewards → probabilities). The Q-function for the goal-conditioned reward function r g (s t , m t ) = (1 − γ)p(s = s g |y t ) is equivalent to the probability of state s g under the discounted state occupancy measure:
and Lemma A.4. The critic function that optimizes equation 8 is a Q-function for the goal-conditioned reward function up to a multiplicative constant 1 The critic function f (s, m, s f ) = y enc(s f ) represents the similarity between the encoding y = enc(s, m) and the encoding of the future rollout s f .
Given lemmas A.5 A.6 A.8 and proposition A.7, it follows that equation 8 is the NCE-binary (InfoMAX ) objective, Î(M j , Y i ) = log σ(f (s, m, s + f )) + log 1 − σ(f (s, m, s − f )) which lower bounds the mutual information, I(M j , Y i ) ≥ Î(M j , Y i ). The critic function is unbounded, so we constrain it to [0, 1] with the sigmoid function, σ( * ).
We suppress the reliance on h since this is directly passed through. By definition of mutual information, we have, Our network model learns π R + (y|m) from rolled-out trajectories, R + , using our policy. The prior of our network state, π R − (y), can be modeled from rolling out a random trajectory, R−.
Unfortunately, it is intractable to model π R + (y|m) and π R − (y) directly during iterative learning, but we can sample y + ∼ π R + (y|m) and y − ∼ π R − (y) directly from our network during training. It has been shown that log p(y|m) provides a bound on mutual information , with the expectation over l p(m l , y l ).
However, we need a tractable understanding of the information Y . Lemma A.5. π R − (y) = p(s = s − f |y). In the information bottleneck, Y represents the desired outcome. In our setup, y is coordination information that helps create the desired output, such as any action a − . This implies, y =⇒ a − . Since the transition is known, it follows that a − =⇒ s − f , a random future state.
Thus, we have, π R − (y) = p(s = s − f |y). Lemma A.6. π R + (y|m) = p(s = s + f |y, m). This is similar to the proof for lemma A.5, but requires assumptions on messages m from the emergent language. We note that when m is random, the case defaults to lemma A.5. Thus, we assume we have at least input-oriented information in m given sufficiently satisfying equation 2. Given a sufficient emergent language, it follows that y =⇒ a + , where a + is an intention action based on m.
Similarly, since the transition is known, a + =⇒ s + f , a desired goal state along the trajectory. Thus, we have, π R + (y|m) = p(s = s + f |y, m). Recall the following (as shown in ), which we have adapted to our communication objective, Proposition A.7 (rewards → probabilities). The Q-function for the goal-conditioned reward function r g (s t , m t ) = (1 − γ)p(s = s g |y t ) is equivalent to the probability of state s g under the discounted state occupancy measure:
and Lemma A.8. The critic function that optimizes equation 8 is a Q-function for the goal-conditioned reward function up to a multiplicative constant 1 p(s f ) : exp(f * (s, m, s f ) = 1 p(s f ) Q π s f (s, m). The critic function f (s, m, s f ) = y enc(s f ) represents the similarity between the encoding y = enc(s, m) and the encoding of the future rollout s f .
Given lemmas A.5 A.6 A.8 and proposition A.7, it follows that equation 8 is the NCE-binary (InfoMAX ) objective, which lower bounds the mutual information, I(M j , Y i ) ≥ Î(M j , Y i ). The critic function is unbounded, so we constrain it to [0, 1] with the sigmoid function, σ( * ).",['An unsupervised method based on the information bottleneck and contrastive learning.'],6235,multifieldqa_en,en,,120cb783c796fbedbc76f04cf9be3318e54a63cd642c4401," Paper: On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement LearningPublish Date: Unkown.Author List: Seth Karten, Siva Kailas, Huao Li, Katia Sycara.Figure 1. By using contrastive learning, our method seeks similar representations between the state-message pair and future states while creating dissimilar representations with random states. Figure 2. An example of two possible classes, person and horse, from a single observation in the Pascal VOC game. Figure 3. Our method uses compositional complexity and contrastive utility to outperform other baselines in terms of performance and sample complexity. Figure 4. Social shadowing enables significantly lower sample complexity when compared to traditional online MARL.Figure 5. Blind Traffic Junction Social shadowed enables significantly higher sample complexity than traditional MARL when compared with ae-comm, no-comm and supervised pretraining. Figure 6. The effect of social shadowing on the performance of MARL is shown in the figure below, which shows the mean ± variance of the best performance for our method. Figure 7. The impact of socialshadowing on MARL can be seen in the results of the study, which show that the method significantly outperforms other baseline methods. The results also show that our method significantly reduces the sample complexity for MARL, which is a key part of the learning process for new agents to learn how to work with other agents in a team. The paper is published in the open-access journal, The open-source version of the paper can be downloaded for free from the arXiv.org/zoom/zr/17061. The article is based on a version of this paper published by the University of California, San Diego, in which the author argues that social learning agents analyze cues from direct observation of other agents (novice or expert) in the same environment to learn an action policy from others. However, observing expert actions may not be sufficient to coordinate with other agent. Rather, by learning to communicate, agents can better model the intent of other agent, leading to better coordination. This paper proposes an unsupervised method based on the information bottleneck to capture both referential complexity and task-specific utility to adequately explore sparse social communication scenarios in multi-agent reinforcement learning (MARL) The paper also suggests that emergent communication can be used to align heterogeneous policies, reduce sample complexity, and solve partially observable tasks, and to synchronize coordination between new and old agents. It is published as a prelude to a forthcoming book on the subject, ‘Social Learning in a Team-less World’. The book will be published by The University of San Diego. Agents learn to communicate effectively through a backpropagation signal from their task performance. This has been found useful for applications in human-agent teaming, multirobot navigation, and coordination in complex games such as StarCraft II. In multi-agent reinforcement learning (MARL), agents suffer from partial observability and non-stationarity (due to unaligned value functions) This can be solved with decentralized learning through communication. In this work, we investigate the challenges of learning a messaging lexicon to prepare emergent communication for social learning (EC4SL) scenarios. We introduce a 'social shadowing' learning approach where we use first-person observations, rather than third-person observation, to encourage the novice to learn latently or conceptually how to communicate and develop an understanding of intent for better coordination. We show that contrastive learning is an optimal critic for communication, reducing sample complexity for the unsupervisedEmergent communication objective. By learning a communication policy, the agent is encouraged to develop a more structured understanding ofintent, leading to better coordination in the game. The setting is very realistic among humans and many computer vision and RL frameworks may develop rich feature spaces for a specific solo task, but have not yet interacted with other agents, which may lead to failure without alignment. The method outperforms competing methods that only ground communication on referential information. In addition to the more human-like format, compositional communication is able to create variable-length messages, meaning that we are not limited to sending insufficiently compressed messages with little information, increasing the quality of each communication. It is also apt for agents to develop internal models and relationships of the task. Originally derived to enable lower complexity emergent lexicons, we find that the contrastivelearning objective is apt for Agents to learn action policies directly from examples. The social shadowing episodes are alternated with traditional MARL during training. Contrastive learning, which works best with positive examples, isapt for social shadowed. It can be used to test the utility of our method in multi- agent settings with a focus on teams of agents, high-dimensional pixel data, and expansions to heterogeneous teams of Agents of varying skill levels. It has been shown that the method is best suited for teams of heterogeneous agents with different skill levels, such as teams of humans and robots. It also has the potential to be used in the future to train teams of robots in complex environments such as augmented reality and virtual reality. It could also be used as a training tool to train robots for complex tasks. through social shadowing. The idea is to enable a shared emergent communication substrate (with minimal bandwidth) to enable future coordi-nation with novel partners. Our contributions are deriving an optimal critic for a communication policy and showing that the information bottleneck helps extend communication to social learning scenarios. Having structured emergent messages reduces sample complexity, and contrastive learning can help novice agents learn from experts. The properties of the tokens inEmergent communication directly affect their informative ability. As a baseline, continuous communication tokens can represent maximum information but lack human-interpretable properties. Discrete 1-hot (binary vector) tokens allow for a finite vocabulary, but each token contains the same magnitude of information, with equal orthogonal distance to each other token. The number of observed distinct communication tokens is far below the number permissible . Building on the continuous word embeddings properties, VQ-VIB is an information-theoretic observation grounding based on  VQ-vIB. This will enable learned compression for task completion rather than pure compression for input recovery. Other work aims to use the Information bottleneck to decrease the entropy of messages . In our work, we use contrastivelearning to increase representation similarity with future goals, which we show optimally optimizes the Q-function for messages. We show that the sample efficiency can be further improved by grounding communication in observation space with a supervised reconstruction loss. We hope that our work will be of interest to the field of social robotics and autonomous driving in the future, and to the researchers at the University of California, San Diego, who are currently working on a similar project in the area of autonomous driving and robotics in the U.S. and Canada. The study was published in the open-access journal, Proceedings of the Royal Society of the United States of America (RSU) (http://www.russi.org/2013/01/07/26/17/17-17-18/13-18-vq-vib-social-robotics-and-autonomous-driving-in-the-united-states.html) It is the first of its kind to be published by the RSU. It is also the first time the authors have used the MDP to study heterogeneous agents, a social task that has not been previously studied. The authors hope that their work will help develop new tools for social robots and autonomous vehicles. They also hope to use their findings to improve social robots in other areas. n VQ-VAE properties , uses Variational properties to provide word embedding properties for continuous emergent tokens. Each of these message types determines a single token for communication. Tokens are stringed together to create emergent ""sentences"" The on-policy reinforcement learning training uses RE-INFORCE or a decentralized version of MAPPO. The best observation encoding and action decoding architecture is task-dependent, i.e., using multi-layer perceptrons (MLPs), CNNs , GRUs , or transformer layers are best suited to different inputs. The message encoding substage can be defined as an information bottleneck problem, which defines a tradeoff between the complexity of information (compression) and the preserved relevant information (utility) We aim to satisfy the complexity objective, I(H i , M i ), through compositional communication. In our setup, the relevant information flows from other agents through communication, signaling a combination of the information bottleneck and a Lewis game. We assume that our observation and memory/sequence encoder provides an optimal representation H i suitable for sharing relevant observation and intent/coordination information. We hope to recover a representation Y i , which contains the sufficient desired outputs. Overall, our objective is to induce complexity through our compositional independence objective, This is formulated by the following Lagrangian, where the bounds on mutual information Î are defined in equations 1, 2, and 10. We want the messages to be as non-random as possible, with the input hidden with respect to the hidden hidden input. That is to say, we want them to be informative with hidden input hidden. We use the message passing from . For message decoding, we build on a multiheaded attention framework, which allows an agent to learn which messages are most important. For message encoding, we use the messages from . We build on the message passed from . In our scenario, the message encoding problem is a trade-off between. I(X; Y ), which looks to measure the relationship between random variables, which is often measured through Kullback-Leibler divergence, and I(x; Y ) = D KL (p(x, y) ⊗ p(y) (or p(x) & p(X) + p(Y) ( or p (x) + P (x, Y) . We hope that our encoder transforms observation and any sequence or memory information into an encoding H i which is suitable to share relevant information with other agents. For example, H i could be used to share information about the location and intent of an action, or to send a message to another agent about a task. We also assume that H i is suitable for sending a message about a specific task, such as the location of a goal, or the intent of a task, and that the message can be encoded into a message that can be sent to other agents using a message-passing technique. We conclude by saying that our work focuses on the communication stage, which can be divided into three substages: message encoding. Each token has a fixed length in bits W . The total sequence is limited by a fixed limit, L l W l ≤ S, of S bits and a total of L tokens. The message generator will predict every available token as to follow the unique token objective. We aim to satisfy the utility objective of the information bottleneck, I(M j ; Y i ), through contrastive learning as shown in figure 1. (3) Algorithm 1 Compositional Message Gen. (h t ) m i ∼ N ( ĥ; µ, σ) 9: end for 10: return m m i. (4) Message Generation Architecture. (5) The idea is to create an architecture that can generate features to enable independent message tokens. (6) We use a variational message generation setup, which maps the encoded hidden state h to a message m; that is, we are modeling the posterior, π i m (m l |h). (7) We limit the vocabulary size to K tokens, e j ∈ R D , j ∉ [1, K] ⊂ N, where each token has dimensionality D and l ∉ L ⊁ N. (8) We perform attention using a softmin to help minimize similarity with previous tokens and sample the new token from a Variational distribution. See algorithm 1 for complete details. (9) The proof is in Appendix A and the proof for the algorithm is in the Appendix A section of the paper, as well as the code for the proof of the algorithm in the appendix, the code in the book, and the software for the message generation architecture in the chapter, the software in the journal, the book and the video, the DVD, the video and the audio, the slides and the slides, the videos, the books and the posters, the lectures and the lectures, the papers and the presentations, the talk and the talk, the lecture and the talks, the talks and the seminars, the speeches and the workshops, the seminars and the competitions, the workshops and the retreats, the classes and the conferences, the sessions and the classes, the conferences and the sessions, the training and the training. The lectures and seminars are the parts of the book that are the focus of this article. The talks and lectures are the sections that follow the lecture, the seminar and the conference, the presentations and the papers, the chapters and the videos. The talk and slides are the talk transcripts and the PowerPoint slides. The slides are also the talk transcript and the presentation, the presentation and the slide show, the PowerPoint and the transcript, the latter of which is the slide presentation. The lecture is the talk version, which is available in the online version of this report. The slide show is available on the University of California, San Diego, California, website. The presentation is the online edition of this paper. The book is the hard-to-read version, available on http://www.reuters.com/2013/01/07/14/science/science-and-technology/article/topics/compositional-message-generation-algorithm-1.html#storylink. shows a need for gradient information to flow backward across agents along communication edge connections. To test H1, structuring emergent messages enables lower complexity, we test our methodology and analyze the input-oriented information and utility capabilities. We consider a benchmark that requires both referential and ordinal capabilities within a team of agents. The blind traffic junction environment requires multiple agents to navigate a junction without any observation of other agents. We additionally analyze the role of offline reinforcement learning for emergent communication in combination with online reinforcement learning to further learn emergent communications alongside an action policy. We find that higher complexity and independence losses increase sample complexity. When there is no regularization loss, the model performs worse. There are fewer spurious features that may cause an agent to tak a different action than the one it is trained to take. We provide an ablation of the loss parameter β in table 1 in the blind Traffic Junction scenario. When β = 0, we use our compositional message paradigm without our derived loss terms. Whenβ = 1, the models perform worse (with no guarantees about referentially representation). We attribute this to the fact that our independence criteria learns a stronger causal relationship. The model performs better when there is a higher level of independence loss, such as when the model is trained with a different type of message than when it is used with the original message. We conclude that our method can be used to train agents to communicate with each other in a variety of environments. We hope that this will lead to a better understanding of how agents learn to communicate in the real world. We are currently working on a version of this article that will be published in the Journal of Machine Learning. The manuscript is open-source and can be downloaded as a PDF for $20.00 (or $25.00 in the U.S. and $30 in the UK). For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential help, contact the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or the National Suicide Prevention Helpline at http:// www.samaritans.org or the Samaritans in the United States. For confidential assistance, call the National Suicide prevention helpline on 1-888-273 8255. For support in the U.S., call the United States on 1-877-9255 or call the National suicide prevention Lifeline on 1-800 9255. For confidential help in the United Kingdom, call 08457 909090. With our methodology, the emergent 'language' converges to the exact number of observations and intents required to solve the task. In table 1, the size of the messages is automatically compressed to the smallest size to represent the information. Despite a trivially small amount of mutual information between tokens, our compositional method is able to reduce the message size in bits by 2.3x using our derived regularization, for a total of an 8x reduction in message size over non-compositional methods such as ae-comm. Finding the communication utility requires grounding messages in ordinal information. The contrastive update begins to find aligned latent spaces early in training, but it cannot adapt the methodology quickly enough to converge. The exploratory randomness of most of the early online data prevents exploitation of the high utility f + examples. This leaves further room for improvement for an adaptive contrastive loss term. See figure 3 for more details on our methodology. See also the paper on how to learn higher-order concepts from highdime concepts. The paper was published in the Proceedings of the National Academy of Sciences of the United States of America (PNAS) on November 14, 2013. It is available for download from the NPS website here: http://www.pnas.org/content/early/2013/09/14/08/07/0726/0826/0526/0626/0725/0724/0723/0722/0720/0623/0622/0620/0713/0613/0715/0615/0714/0614/077/071/061/072/073/074/076/075/078/079/0710/0711/0712/0716/0717/0718/0719/0709/062/063/067/081/087/069/086/082/083/084/085/066/0611/089/0612/088/060/0610/064/097/096/091/094/068/092/093/095/080/0708/06/07 /07/11/09 /06/098/0805/065/090/051/056/057/054/058/055/05/070/08 /076 /077 /071 /07 /08/0905/052/059/053/050/099/0907/0909/0705/0906/0806/0605/0807/0607/0809/0908/0511/0609/0808/0811/0505/03/0700/0904/0707/05 /0705 /0805 /09/0500/0701/0903/0902/0900/0803/0804/09090/0706 We show that concepts should consist of independent information directly from task signal rather than compression to reconstruct inputs. Our methodology significantly outperforms the observation-grounded ae-comm baseline. The agent that performs social shadowing is able to learn the action policy with almost half the sample complexity required by the online reinforcement learning agent. By using our framework to better understand the intent of others, agents can learn to communicate to align policies and coordinate. Our results show that the structured latent space of the emergent communication learns socially benevolent coordination. Unlike prior work, this enables the benefits of training with reinforcement learning in multi-agent settings. This method is even more powerful when there is access to examples from expert agents. It mitigates the issues of solely observing actions. The mutual information regularization yields additional benefits, such as small messages, which enables the compression aspect of sparse communication. It is important to note that the minimization of our independence objective enables tokens that contain minimal overlapping information with other tokens. Each of these objectives is complementary, so they are not trivially minimized during training, which is a substantial advantage over comparative baselines. Even in the Pascal VOC game, which appears to be a purely referential objective, we show that intelligent compression is not the only objective of referentional communication. We have shown solid improvements. Using contrastive communication as an optimal critic aims to satisfy this, and has shown good results. We show an empirical result on pixel data to verify the premise of the information bottleneck. Preventing trivial communication paradigms enables higher performance. It can be performed with a supervised loss, as indicated by the instant satisfaction of refereNTial objectives. It's possible that the communication may be bootstrapped, since our optimal critic has examples with strong signals from the 'social shadowing' episodes. From a qualitative point of view, the independent i could be used to improve the performance of our methodology. It could also be used as a tool to improve performance in other areas of computer science. It has shown solid results in the field of machine learning, including the development of artificial neural networks, computer vision, and other cognitive tools. It also has shown that it is possible to use the contrastive objective to update its communication policy during shadowing. It may be used in the future to improve communication in other fields, including computer science and other fields such as finance and finance. It was also used in a study of the use of computer vision in a business setting. The results were published in the Journal of Computer Science and Information Systems (JCSI) in December 2013. formation also yields discrete emergent concepts, which can be further made human-interpretable by a post-hoc analysis. The interpretability of this learned white-box method could be useful in human-agent teaming as indicated by prior work. The social scenarios described are a step towards enabling a zero-shot communication policy. This work will serve as future inspiration for using emergent communication to enable ad-Hoc teaming with both agents and humans. The work here will enable further results in decision-making from high-dimensional data with emergent concept. The results of the study have been published in the open-access issue of the Proceedings of the National Academy of Sciences of the United States of America (PNAS) (http://www.pnas.org/content/early/2014/10/29/151515/pennas.html#storylink=cpy. We note that when the input is random, defaults to lemma A.5, but we have at least one input that is at least at least least-oriented in satisfying equation 2.5. Given a sufficient language, it follows that y =⇒ a + , where a is an intention based on the transition along the desired state along the trajectory. Recall the following: s + f |y, m(s) = p(m l |h), p(s + f, m) = s + s + m(f, m), f(s, m, f) = f (s, f, f), f (f, s) = m (s) + f (m) + m (f,. f (y, y, y), y (y) = y + (y|m), y(y) + y(f,. y) =y (y), y('s' + f,. f('s', f', f,. 's' , 'f', 's', 'f' });. We conclude that the results of this study could be used to train machine learning in multi-agent settings. We have also shown that this method can be used in the future to teach agents to communicate with each other in a non-zero-shot fashion. We also show that this technique can be applied in the real world to train agents to talk to each other with a single message. The findings are presented in the next section of the paper, which also includes an explanation of how the method was used to learn the social scenarios in the first place and how it was applied to agents in a real-world setting. The paper is also available in the online version of the book, which includes an analysis of the scenarios and a discussion of how to apply the method in the field of agent-to-agent communication. The book is available for free on Amazon.com (US) and on Google Play (UK) for $1.99. The online version includes an online version with an interactive version for $2.99, as well as the print version for £1, £1.49, £3.99 and £4, £5, £6, £7, £8, £9, £10, £11, £12, & £8. The print version also includes a video version of this article, which is available on the company’s website and on the Google Play store for £3, £2, £4 and £5. Given lemmas A.5 A.6 A.8 and proposition A.7, it follows that equation 8 is the NCE-binary (InfoMAX ) objective. The critic function f (s, m, s f ) = y enc(s f ) represents the similarity between the encoding y and the encoding of the future rollout s f. By definition of mutual information, we have, Our network model learns π R + (y|m) from rolled-out trajectories, R + , using our policy. The Q-function for the goal-conditioned reward function r g (s t , m t ) = (1 − γ)p(s = s g |y t ) is equivalent to the probability of state s g under the discounted state occupancy measure:. We note that when m is random, the case defaults to lemma A5.5. We assume we have at least input-oriented information in m given sufficiently satisfying equation 2. Given a sufficient emergent language, it following that y =⇒ a + , where a + is an intention action based on m. This is similar to the proof for lemma a.5, but requires assumptions on messages m from the emergentlanguage. We have adapted to our communication objective, Proposition A. 7 (rewards → probabilities) to make our argument more tractable. We conclude that our argument is more compelling than the previous one, and that we can use our network to predict the outcome of a game. We also show that our network model can be used to predict future outcomes of games in a game-like way. We are now ready to use our model to predict outcomes in the real world. We will publish our results in the next issue of the open-access journal, Machine Learning and Cognition (June 2014). We will also be releasing a book, The Brain and Machine Learning, in which we present our results on a number of fronts. We hope to use these results to help people understand the power of machine learning in a variety of ways. We can also use them to improve the design of game-based games in the future, such as in the game of Go, for example. We plan to release a new version of the book in the fall 2014, which will be available in the U.S. and other countries. The book will be published in the spring 2014, and we hope to publish it in the UK in the summer 2014, as well as in Europe and elsewhere in the Middle East. We would like to thank our readers for their help with this project. We thank them for their support and support in the development of the paper. We look forward to seeing how this project will affect the field of computer science in the years to come, and to the potential uses of machine-learning in the 21st century, including in the fields of medicine, medicine, and other fields such as nursing and nursing care, among other fields. We wish to thank you for your support and comments. We appreciate your feedback. We welcome your comments and suggestions for future work. We believe that this article will have a significant impact on the future of computer-based gaming in the United States and around the world. Back to the page you came from. Click here to read the rest of the article. The original version of this article stated that we had used a model that was based on a random trajectory. In fact, the prior of our network state, ρ R − (y), can be modeled from rolling out arandom trajectory, R−. This implies, y =  y =  a − ."
How is the function beta(r) determined in the derivation?,"\section{Introduction}
The Schwarzschild solution plays a key role in teaching about general relativity: It describes the simplest version of a black hole. By Birkhoff's theorem, it more generally describes the gravitational field around any spherical mass distribution, such as the Sun in our own Solar system. As one of two particularly simple, yet physically relevant examples of a non-trivial metric (the other being the FLRW spacetime of an expanding universe), it is particularly well-suited for teaching about general techniques of ``reading'' and interpreting a spacetime metric.

Consider undergraduate courses where students are introduced to selected concepts and results from general relativity without exposing them to the full mathematical formalism. Such courses have the advantage of introducing students to one of the two great fundamental theories of 20th century physics early on (the other being quantum mechanics); they also profit from subject matter that meets with considerable interest from students.\cite{Hartle2006} Using the terminology of Christensen and Moore,\cite{Christensen2012} in the ``calculus only'' approach pioneered by Taylor and Wheeler,\cite{Taylor2001,Taylor2018} spacetime metrics are not derived, but taken as given, and the focus is on learning how to interpret a given spacetime metric. Similar presentations can be found in the first part of the ``physics first'' approach exemplified by Hartle's text book,\cite{Hartle2003} where the concepts of the metric and of geodesics are introduced early on, and their physical consequences explored, while the mathematics necessary for the Einstein equations is only introduced at a later stage. 

Whenever the approach involves an exploration of simple metrics such as the Schwarzschild solution, but stops short of the formalism required for the full tensorial form of Einstein's equations, access to a simple derivation of the Schwarzschild solution that does not make use of the advanced formalism can be a considerable advantage.

Simplified derivations of the Schwarzschild solution have a long tradition within general relativity education,\cite{Schiff1960,Harwit1973} although specific simplifications have met with criticism.\cite{Rindler1968} This article presents a derivation which requires no deeper knowledge of the formalism of differential geometry beyond an understanding of how to interpret a given spacetime metric $\mathrm{d} s^2$. The derivation avoids the criticism levelled at attempts to derive the Schwarzschild solution from the Einstein equivalence principle in combination with a Newtonian limit,\cite{Gruber1988} relying as it does on a simplified version of the vacuum Einstein equation.

More specifically, I combine the restrictions imposed by the symmetry with the simple form of Einstein's equations formulated by Baez and Bunn.\cite{BaezBunn2005} That same strategy was followed by Kassner in 2017,\cite{Kassner2017} but in this text, I use the ``infalling coordinates'' that are commonly associated with the Gullstrand-Painlev\'e form of the Schwarzschild metric,\cite{Martel2001,Visser2005,HamiltonLisle2008} not the more common Schwarzschild coordinates. That choice simplifies the argument even further. In the end, what is required is no more than the solution of an ordinary differential equation for a single function, which yields to standard methods, to obtain the desired result.

\section{Coordinates adapted to spherical symmetry and staticity}
\label{SymmetriesCoordinates}

Assume that the spacetime we are interested in is spherically symmetric and static. In general relativity, a symmetry amounts to the possibility of being able to choose coordinates that are adapted to the symmetry, at least within a restricted sub-region of the spacetime in question. That the spacetime is static is taken to mean that we can introduce a (non-unique) time coordinate ${t}$ so that our description of spacetime geometry does not depend explicitly on ${t}$, and that space and time are completely separate --- in the coordinates adapted to the symmetry, there are no ``mixed terms'' involving $\mathrm{d} {t}$ times the differential of a space coordinate in the metric. If we use ${t}$ to slice our spacetime into three-dimensional hyperplanes, each corresponding to ``space at time ${t}$,'' then each of those 3-spaces has the same spatial geometry. A mixed term would indicate that those slices of space would need to be shifted relative to another in order to identify corresponding points. The mixed term's absence indicates that in adapted coordinates, there is no need for such an extra shift. In those coordinates, we can talk about the 3-spaces as just ``space,'' without the need for specifying which of the slices we are referring to.

In the case of spherical symmetry, we can introduce spherical coordinates that are adapted to the symmetry: a radial coordinate $r$ and the usual angular coordinates $\vartheta,\varphi$, so that the spherical shell at constant $r$ has the total area $4\pi r^2$. In consequence, the part of our metric involving $\mathrm{d}\vartheta$ and $\mathrm{d}\varphi$ will have the standard form
\begin{equation}
r^2(\mathrm{d}\vartheta^2+\sin^2\theta\mathrm{d}\varphi^2) \equiv r^2\mathrm{d}\Omega^2,
\end{equation}
where the right-hand side defines $\mathrm{d}\Omega^2$, the infinitesimal solid angle corresponding to each particular combination of $\mathrm{d}\vartheta$ and $\mathrm{d}\varphi$.

The radial coordinate slices space into spherical shells, each corresponding to a particular value $r=const.$ The rotations around the origin, which are the symmetry transformations of spherical symmetry, map each of those spherical shells onto itself, and they leave all physical quantities that do not explicitly depend on $\vartheta$ or $\varphi$ invariant.

In what follows, we will use the basic structures introduced in this way --- the slices of simultaneous ${t}$, the radial directions within each slice, the angular coordinates spanning the symmetry--adapted spherical shells of area $4\pi r^2$ --- as auxiliary structures for introducing spacetime coordinates. For now, let us write down the shape that our metric has by simple virtue of the spherical symmetry, the requirement that the spacetime be static, and the adapted coordinates, namely
\begin{equation}
\mathrm{d} s^2 = -c^2F(r) \mathrm{d} {t}^2 + G(r) \mathrm{d} r^2 + r^2\:\mathrm{d}\Omega^2. 
\label{StaticForm}
\end{equation}
Students familiar with  ``reading'' a spacetime metric will immediately recognize the sign difference between the parts describing space and describing time that is characteristic for spacetime, and the speed of light $c$ that gives us the correct physical dimensions. That there is no explicit dependence on $\varphi$ and $\vartheta$ in the remaining functions $F$ and $G$ is a direct consequence of spherical symmetry. That the factor in front of $\mathrm{d}\Omega^2$ is $r^2$ is a consequence of our coordinate choice, with spherical angular coordinates so that the area of a spherical surface of constant radius $r$ is $4\pi r^2$. That there is no explicit dependence on ${t}$ is one consequence of the spacetime being static; the absence of the mixed term $\mathrm{d} {t}\cdot \mathrm{d} r$ is another. We are left with two unknown functions $F(r)$ and $G(r)$. In the following, let us call ${t}$ and $r$ the {\em static coordinates}. 
 
Note that, since $G(r)$ is as yet undefined, we have not yet chosen a specific physical meaning for the length measurements associated with our $r$ coordinate. But because of the $\mathrm{d}\Omega^2$ part, it is clear that whatever choice we make, the locally orthogonal lengths $r\cdot\mathrm{d}\vartheta$ and $r\cdot\sin\vartheta\cdot\mathrm{d}\varphi$ will have the same physical interpretation as for the length measurement corresponding to $\mathrm{d} r$.

\section{Infalling observer coordinates}
\label{Sec:InfallingObservers}

Now that we know what the radial directions are, at each moment of time ${t}$, we follow Visser\cite{Visser2005} as well as Hamilton and Lisle\cite{HamiltonLisle2008} in defining a family of radially infalling observers.  Observers in that family are in free fall along the radial direction, starting out at rest at infinity: In mapping each observer's radial progression in terms of the static coordinate time ${t}$, we adjust initial conditions, specifically: the choice of initial speed at some fixed time ${t}$, in just the right way that the radial coordinate speed goes to zero for each observer in the same way as $r\to\infty.$

It is true that talking about ``infalling'' observers already reflects our expectation that our solution should describe the spacetime of a spherically symmetric mass.  As we know from the Newtonian limit, such a mass attracts test particles in its vicinity. It should be noted, though, that all our calculations would also be compatible with the limit of no mass being present. In that case, ``infalling'' would be a misnomer, as our family of observers would merely hover in empty space at unchanging positions in $r$. 

We can imagine infinitesimal local coordinate systems associated with our observers --- think of the observer mapping out space and time by defining three orthogonal axes, and by measuring time with a co-moving clock. We assume all such little coordinate systems to be non-rotating --- otherwise, we would break spherical symmetry, since rotation would locally pick out a plane of rotation that is distinguishable from the other planes. The radial direction is a natural choice for the first space axis of those little free-falling systems. The other directions, we take to point to observers falling side by side with our coordinate-defining observer --- and to remain pointed at a specific such other observer, once the choice of direction is made.

We assume our infalling observers' clocks to be synchronised at some fixed radius value $r$. By spherical symmetry, those clocks should then be synchronised at {\em all} values of $r$. Anything else would indicate direction-dependent differences for the infalling observers and their clocks, after all. Hence, at any given static time ${t}$, all the infalling observers who are at radius value $r$ show the same proper time $T$ on the ideal clocks travelling along with them. 

Once our definition is complete, our static, spherically symmetric spacetime is filled with infalling observers from that family: Whenever we consider an event $\cal E$, there will be an observer from that family passing by at that time, at that location. 

Now, consider the coordinate speed of those infalling observers. If we position ourselves at some constant radius value $r$ and watch the falling observers fly by, then we can express both their proper time rate and their coordinate speed in the $r$ direction in terms of $r$ and ${t}$. We can combine the two pieces of information to obtain the rate of change in radial position $r$ with proper time $T$ for those infalling observers. But since the initial conditions for those observers are the same, and since our spacetime is, by assumption, static, the resulting function can only depend on $r$, and not explicitly on ${t}$. Let us rescale that function with the speed of light to make it dimensionless, give it an overall minus sign to make it positive for infalling particles, and call it $\beta(r)$,
\begin{equation}
\beta(r)\equiv -\frac{1}{c}\frac{\mathrm{d} r}{\mathrm{d} T}(r).
\label{betaDefinition}
\end{equation}

Recall from section \ref{SymmetriesCoordinates} that we also still have the freedom to decide on the physical meaning of $r$. We make the choice of making $\mathrm{d} r$ the physical length measured by one of our infalling observers at the relevant location in spacetime, at constant time $T$. Via our angular coordinates, that implies that length measurements orthogonal to the radial direction, $r\cdot\mathrm{d}\vartheta$ and $r\cdot\sin\vartheta\:\mathrm{d}\varphi$ inherit the same physical interpretation.

As a next step, we transform our metric (\ref{StaticForm}) from the static form into the form appropriate for our coordinate choice $r$ and $T$. We do so by writing the static time coordinate as a function ${t}(T,r)$ in terms of infalling observer time and radius value. In consequence,
\begin{equation}
\mathrm{d} {t} = \frac{\partial{t}}{\partial T}\cdot\mathrm{d} T+ \frac{\partial {t}}{\partial r}\cdot\mathrm{d} r,
\end{equation}
and our new metric now has the form
\begin{align}
 \mathrm{d} s^2 = {} & -c^2 F(r)\left(\frac{\partial t}{\partial T}\right)^2\mathrm{d} T^2 \nonumber \\[0.2em]
 & -2c^2F(r)\left(\frac{\partial t}{\partial T}\right)\left(\frac{\partial t}{\partial r}\right)\mathrm{d} T\:\mathrm{d} r \nonumber \\[0.2em]
 & +\left[G(r)-c^2F(r)\left(\frac{\partial t}{\partial r}\right)^2\right]\mathrm{d} r^2+r^2\:\mathrm{d}\Omega^2.
 \end{align}
At face value, this looks like we are moving the wrong way, away from simplification, since we now have more functions, and they depend on two variables instead of one.

But in fact, this new formulation paves the way for an even simpler form of the metric. Consider a specific event, which happens at given radius value $r$. In a small region around that event, we will introduce a new coordinate $\bar{r}$ to parametrize the radial direction. We want this coordinate to be co-moving with our infalling observers at $r$; each such observer then has a position $\bar{r}=const.$ that does not change over time. 

Key to our next step is that we {\em know} the metric for the local length and time measurements made by any one of our free-falling observers. By Einstein's equivalence principle, the metric is that of special relativity. Locally, namely whenever tidal effects can be neglected, spacetime geometry for any non-rotating observer in free fall is indistinguishable from Minkowski spacetime as described by a local inertial system.

Since we have chosen both the time coordinate $T$ and the physical meaning of the radial coordinate $r$ so as to conform with the measurements of the local infalling observer, the transformation between $\bar{r}$ and $r$ is particularly simple: It has the form of a Galilei transformation
\begin{equation}
\mathrm{d}\bar{r}= \mathrm{d} r + \beta(r)c\:\mathrm{d} T.
\label{barRshift}
\end{equation}
In that way, as it should be by definition, radial coordinate differences at constant $T$ are the same in both systems, while for an observer at constant $\bar{r},$ with $\mathrm{d} \bar{r}=0$, the relation between $\mathrm{d} r$ and $\mathrm{d} T$ is consistent with the definition of the function $\beta(r)$ in (\ref{betaDefinition}).

Are you surprised that this is not a Lorentz transformation, as one might expect from special relativity? Don't be. We are not transforming from one local inertial coordinate system to another. The $T$ is already the time coordinate of the infalling observers, so both coordinate systems have the same definition of simultaneity, and time dilation plays no role in this particular transformation. Also, we have chosen $r$ intervals to correspond to length measurements of the infalling observers, so there is no Lorentz contraction, either. It is the consequence of these special choices that gives the relation (\ref{barRshift}) its simple form.

Last but not least, when we analyse specifically an infinitesimal neighbourhood of the point $r,\vartheta,\varphi$, let us make the choice that directly at our point of interest, we make $\bar{r}$ coincide with $r$. Since before, we had only fixed the differential $\mathrm{d} \bar{r}$, we do have the remaining freedom of choosing a constant offset for $\bar{r}$ that yields the desired result.

By Einstein's equivalence principle, the metric in terms of the locally co-moving coordinates $T,\bar{r},\vartheta,\varphi$ is the spherical-coordinate version of the Minkowski metric,
\begin{equation}
\mathrm{d} s^2 = -c^2\mathrm{d} T^2 + \mathrm{d}\bar{r}^2 + \bar{r}^2\mathrm{d}\Omega.
\end{equation}
This version can, of course, be obtained by taking the more familiar Cartesian-coordinate version
\begin{equation}
\mathrm{d} s^2=-c^2\mathrm{d} T^2 + \mathrm{d} X^2 + \mathrm{d} Y^2 + \mathrm{d} Z^2,
\label{CartesianMinkowski}
\end{equation}
applying the definition of Cartesian coordinates $X,Y,Z$ in terms of spherical coordinates $\bar{r},\vartheta,\varphi$
\begin{equation}
x= \bar{r}\:\sin\vartheta\:\cos\varphi, \;\;
y= \bar{r}\:\sin\vartheta\:\sin\varphi, \;\;
z= \bar{r}\:\cos\vartheta,
\end{equation}
to express $\mathrm{d} X, \mathrm{d} Y, \mathrm{d} Z$ in terms of $\mathrm{d} \bar{r}, \mathrm{d}\vartheta, \mathrm{d}\varphi$, and substitute the result into (\ref{CartesianMinkowski}).

By noting that we have chosen $\bar{r}$ so that, at the specific spacetime event where we are evaluating the metric, $\bar{r}=r$, while, for small radial coordinate shifts around that location, we have the relation (\ref{barRshift}), we can now write down the same metric in the coordinates $T, r, \vartheta,\varphi$, namely as
\begin{equation}
\mathrm{d} s^2 = -c^2\left[
1-\beta(r)^2
\right] \mathrm{d} T^2+2c\beta(r)\mathrm{d} r\:\mathrm{d} T
+\mathrm{d} r^2+r^2\mathrm{d}\Omega^2.
\label{preMetric}
\end{equation}
Since we can repeat that local procedure at any event in our spacetime, this result is our general form of the metric, for all values of $r$. This, then is the promised simplification: By exploiting the symmetries of our solutions as well as the properties of infalling observers, we have reduced our metric to a simple form with no more than one unknown function of one variable, namely $\beta(r)$.

So far, what I have presented is no more than a long-form version of the initial steps of the derivation given by Visser in his heuristic derivation of the Schwarzschild metric.\cite{Visser2005} In the next section, we will deviate from Visser's derivation.

\section{$\beta(r)$ from tidal deformations}
\label{TidalSection}

In the previous section, we had exploited symmetries and Einstein's equivalence principle. In order to determine $\beta(r)$, we need to bring in additional information, namely the Einstein equations, which link the matter content with the geometry of spacetime. For our solution, we only aim to describe the spacetime metric outside whatever spherically-symmetric matter distribution resides in (or around) the center of our spherical symmetry. That amounts to applying the {\em vacuum Einstein equations}.

More specifically, we use a particularly simple and intuitive form of the vacuum Einstein equations, which can be found in a seminal article by Baez and Bunn:\cite{BaezBunn2005} Consider a locally flat free-fall system around a specific event $\cal E$, with a time coordinate $\tau$, local proper time, where the event we are studying corresponds to $\tau=0$. In that system, describe a small sphere of freely floating test particles, which we shall call a {\em test ball}. The particles need to be at rest relative to each other at $\tau=0$. Let the volume of the test ball be $V(\tau)$. Then the vacuum version of Einstein's equations states that
\begin{equation}
\left.\frac{\mathrm{d}^2 V}{\mathrm{d}\tau^2}\right|_{\tau=0} = 0.
\label{EinsteinVacuum}
\end{equation}
In words: If there is no matter or energy inside, the volume of such a test ball remains constant in the first order (those were our initial conditions) and the second order (by eq.~[\ref{EinsteinVacuum}]). 

If you are familiar with Wheeler's brief summary of Einstein's equations, ``spacetime grips mass, telling it how to move'' and ``mass grips spacetime, telling it how to curve'',\cite{Wheeler1990} you will immediately recognise that this is a specific way for the structure of spacetime telling the test ball particles how to move. The calculation later in this section provides the second part: It will amount to using (\ref{EinsteinVacuum}) to determine the structure of spacetime, namely the still missing function $\beta(r)$, and that is the way for mass, in this case: for the absence of mass, to tell spacetime how to curve.

Note that equation (\ref{EinsteinVacuum}) also holds true in Newtonian gravity. So in a way, this version of Einstein's equation can be seen as a second-order extension of the usual Einstein equivalence principle: Ordinarily, the equivalence principle is a statement about physics in the absence of tidal forces. Equation (\ref{EinsteinVacuum}) adds to this that the lowest-order correction for tidal forces in a freely falling reference frame is that specified by Newtonian gravity. This makes sense, since by going into a free-fall frame, and restricting our attention to a small spacetime region, we have automatically created a weak-gravity situation. In such a situation, tidal corrections are approximately the same as those described by Newton. This argument can serve as a heuristic justification of (\ref{EinsteinVacuum}).

In 2017, Kassner made use of the Baez-Bunn form of Einstein's vacuum equation to derive the Schwarzschild solution, starting from what we have encountered as the static form of the metric (\ref{StaticForm}).\cite{Kassner2017} We follow the same general recipe, but using the infalling coordinates introduced in section \ref{Sec:InfallingObservers}, which makes our derivation even simpler.

Consider five test particles in a small region of space. Let the motion of each be the same as for the local representative from our coordinate-defining family of infalling observers. We take the central particle $C$ to be at radial coordinate value $r=R$ at the time of the snapshot shown in Fig.~\ref{TestParticlesOutside}. The other four are offset relative to the central particle: As described in the local inertial system that is co-moving with the central particle, one of the particles is shifted by $\Delta l$ upwards in the radial direction, another downward, while two of the particles are offset orthogonally by the same distance.
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.5\linewidth]{01-free-fall-particles.pdf}
\caption{Five test particles in our spherically-symmetric spacetime}
\label{TestParticlesOutside}
\end{center}
\end{figure}
The $\Delta l$ is meant to be infinitesimally small, so while Fig.~\ref{TestParticlesOutside} is of course showing a rather large $\Delta l$ so as to display the geometry of the situation more clearly, we will in the following only keep terms linear in $\Delta l$. 

Consider a generic particle, which moves as if it were part of our coordinate-defining family of infalling observers, and which at the time $T_0$ is at $r=r_0$. By a Taylor expansion, that particle's subsequent movement is given by
\begin{equation}
r(T) = r_0 + \frac{\mathrm{d} r}{\mathrm{d} T}(T_0) \cdot \Delta T +\frac12 \frac{\mathrm{d}^2 r}{\mathrm{d} T^2}(T_0) \cdot \Delta T^2
\label{TaylorREvo}
\end{equation}
where $\Delta T\equiv T-T_0$. We know from (\ref{betaDefinition}) that the derivative in the linear term can be expressed in terms of $\beta(r)$; by the same token,
\begin{equation}
\frac{\mathrm{d}^2 r}{\mathrm{d} T^2} = -c\frac{\mathrm{d}\beta}{\mathrm{d} T}=-c\beta' \frac{\mathrm{d} r}{\mathrm{d} T} = c^2\beta\cdot\beta',
\end{equation}
where the prime denotes differentiation of $\beta$ with respect to its argument. Since, in the following, the product of $\beta$ and its first derivative will occur quite often, let us introduce the abbreviation
\begin{equation}
B(r) \equiv \beta(r)\cdot\beta'(r).
\label{BigBDefinition}
\end{equation}
With these results, can rewrite the Taylor expansion (\ref{TaylorREvo}) as 
\begin{equation}
r(T) = r_0 -c\beta(r_0)\cdot\Delta T + \frac12 c^2B(r_0)\cdot\Delta T^2.
\label{RadialOrbitTime}
\end{equation}
In order to find $r_C(T)$ for our central particle, we simply insert $r_0=R$ into that expression. If, on the other hand, we want to write down the time evolution for particles $U$ and $D$, let us denote it by $r_{U,D}(T)$, we need to evaluate the expression (\ref{RadialOrbitTime}) at the initial location $r_0=R\pm\Delta l$. Since $\Delta l$ is small, we can make a Taylor expansion of $\beta(r)$ and its derivative around $r=R$, and neglect everything beyond the terms linear in $\Delta l$. The result is
\begin{multline}
r_{U,D}(T)=R \pm\Delta l-c\left[
\beta(R)\pm\beta'(R)\Delta l
\right]\Delta T \\[0.2em]
+\frac{c^2}{2}\big[
B(R)\pm B'(R)\Delta l
\big]\Delta T^2
\end{multline}
In consequence, the distance between the upper and lower particle, $d_{\parallel}(T)\equiv r_U(T)-r_D(T),$ changes over time as
\begin{equation}
d_{\parallel}(T) =  2\Delta l\left[
1-c\beta'(R)\Delta T+\frac12c^2 B'(R)\Delta T^2
\right].
\label{dParallel}
\end{equation}
Next, let us look at how the distance between the particles $L$ and $R$ changes over time. The initial radial coordinate value for each of the particles is
\begin{equation}
r(T_0) = \sqrt{R^2+\Delta l^2}=R\left[1+\frac12\left(\frac{\Delta l}{R}\right)^2\right]\approx R,
\end{equation}
that is, equal to $R,$ as long as we neglect any terms that are higher than linear in $\Delta l$. In consequence, $r_{L,R}(t)$ is the same function as for our central particle, given by eq.~(\ref{RadialOrbitTime}) with $r_0=R$. The transversal (in Fig.~\ref{TestParticlesOutside}: horizontal) distance $d_{\perp}(T)$ between the particles $L$ and $R$ changes in proportion to the radius value,
\begin{align}
d_{\perp}(T) &= 2\Delta l\cdot\frac{r_{L}(T)}{R} \nonumber \\
                 &=2\Delta \left[1-\frac{c\beta(R)}{R}\Delta T+\frac{c^2}{2}\frac{B(R)}{R}\Delta T^2\right].
                \label{dPerp}
\end{align}
With these preparations, consider the vacuum Einstein equation (\ref{EinsteinVacuum}) for the volume of a test ball. Initially, our particles $C, U, D, L, R$ define a circle, which is deformed to an ellipse. By demanding rotational symmetry around the radial direction, we can construct the associated ellipsoid, which is initially a spherical surface. That ellipsoid has one axis in radial direction, whose length is $d_{\parallel}(T)$, and two axes that are transversal and each have the length  $d_{\perp}(t)$. But that ellipsoid is not quite yet the test ball we need. After all, the particles of the test ball need to be at rest initially, at time $T_0$, in the co-moving system defined by the central particle $C$. Our defining particles are not, as the terms linear in $\Delta T$ in both (\ref{dParallel}) and (\ref{dPerp}) show, where the coefficients of $\Delta T$ correspond to the particles' initial velocities. 

In order to define our test ball, we need to consider particles at the same location, undergoing the same acceleration, but which are initially at rest relative to the central particle $C$. 

We could go back to the drawing board, back to Fig.~\ref{TestParticlesOutside}, make a more general Ansatz that includes initial velocities which measure the divergence of the motion of our test ball particles from that of the infalling-observer particles, and repeat our calculation while including those additional velocity terms. But there is a short-cut. The only consequence of those additional velocity terms will be to change the terms linear in $\Delta T$ in equations (\ref{dParallel}) and (\ref{dPerp}). And we already know the end result: We will choose the additional terms so as to cancel the terms linear in $\Delta T$ in the current versions of (\ref{dParallel}) and (\ref{dPerp}). But by that reasoning, we can skip the explicit steps in between, and write down the final result right away. The time evolution of the radial-direction diameter of our test ball, let us call it $L_{\parallel}(T)$, must be the same as $d_{\parallel}(T)$, but without the term linear in $\Delta T$. Likewise, the time evolution $L_{\perp}(T)$ of the two transversal diameters must be equal to $d_{\perp}(T)$, but again without the term linear in $\Delta T$. The result is
\begin{align}
L_{\parallel}(T)  &=  2\Delta l \left[1+\frac12c^2B'(R)\Delta T^2\right] \\
L_{\perp}(T) &= 2\Delta l \left[1+\frac{c^2}{2}\frac{B(R)}{R}\Delta T^2\right].
\end{align}
Thus, our test ball volume is
\begin{align}
V(T) &= \frac{\pi}{6}L_{\parallel}(T) L_{\perp}^2(T) \\
       &= \left.\frac{4\pi}{3}\Delta l^3\left[1+{c^2}\left( \frac{B(r)}{r} + \frac{B'(r)}{2}\right)\Delta T^2\right]\right|_{r=R}
\end{align}
For the second time derivative of $V(T)$ to vanish at the time $T=T_0$, we must have
\begin{equation}
\frac{B(r)}{r} + \frac{B'(r)}{2}= 0
\label{VolumeConditionR}
\end{equation}
for all values of $r$. This is readily solved by the standard method of separation of variables: We can rewrite (\ref{VolumeConditionR}) as
\begin{equation}
\frac{\mathrm{d} B}{B} = -2\frac{\mathrm{d} r}{r},
\end{equation}
which is readily integrated to give
\begin{equation}
\ln(B) = -\ln(r^{2}) + const.  \;\; \Rightarrow \;\; \ln(Br^2) = C',
\end{equation}
with a constant $C'$, which upon taking the exponential gives us
\begin{equation}
Br^2= C,
\label{BSolution}
\end{equation}
with a constant $C$. Note that the constant $C$ can be negative --- there is no reason the constant $C'$ needs to be real; only our eventual function $B(r)$ needs to be that, and it is clear that (\ref{BSolution}) satisfies the differential equation
(\ref{VolumeConditionR}) for any constant $C$, positive, zero, or negative. By (\ref{BigBDefinition}), the solution (\ref{BSolution}) corresponds to the differential equation
\begin{equation}
\beta(r)\beta'(r) = \frac{C}{r^2}
\end{equation}
for our function $\beta$; with another separation of variables, we can re-write this as 
\begin{equation}
\beta\cdot\mathrm{d}\beta=C\frac{\mathrm{d} r}{r^2}.
\end{equation}
Both sides are readily integrated up; we can solve the result for $\beta(r)$ and obtain
\begin{equation}
\beta(r) = \sqrt{
-\frac{2C}{r} +2D
},
\end{equation}
where $D$ is the second integration constant, and where we have chosen the proper sign, since we know that $\beta(r)>0$. That brings us to the last step: The requirement that, for large values of $r$, the description provided by our solution should correspond to the results from Newtonian gravity. First of all, we note that our initial condition for the infalling observers, which had those observers start out at zero speed at infinity, means that we must choose $D=0$. Then, as we would expect, $\beta(r)$ for large values of $r$ becomes very small, corresponding to small speeds. But at slow speeds, time and length intervals as measured by the infalling observer will become arbitrarily close to time and length intervals as measured by an observer at rest in our static coordinate system at constant $r$, using the static time coordinate ${t}$. As is usual, we identify these coordinates with those of an approximately Newtonian description. In that description, the radial velocity is
\begin{equation}
v(r) = \sqrt{\frac{2GM}{r}},
\end{equation}
which follows directly from energy conservation for the sum of each observer's kinetic and Newtonian-gravitational potential energy. This fixes the remaining integration constant as
\begin{equation}
C = -\frac{GM}{c^2},
\end{equation}
and the final form of our function $\beta(r)$ becomes
\begin{equation}
\beta(r) = \sqrt{\frac{2GM}{rc^2}}.
\end{equation}
Inserting this result in (\ref{preMetric}), we obtain the metric
\begin{equation}
\mathrm{d} s^2 = -c^2\left[
1-\frac{2GM}{rc^2}
\right]\mathrm{d} T^2+2\sqrt{\frac{2GM}{r}}\mathrm{d} r\:\mathrm{d} T+\mathrm{d} r^2+r^2\mathrm{d}\Omega^2.
\label{GPMetric}
\end{equation}
This is known as the Gullstrand-Painlev\'e version of the Schwarzschild metric.\cite{Martel2001,Visser2005,HamiltonLisle2008} A  last transformation step brings us back to the traditional Schwarzschild form. Recall our discussion in sec.~\ref{SymmetriesCoordinates}, leading up to the explicitly static form (\ref{StaticForm}) of the metric? The main difference between our current form and the static version is the mixed term containing $\mathrm{d} r\:\mathrm{d} T$ in (\ref{GPMetric}). Everything else already has the required shape. Inserting the Ansatz
\begin{equation}
\mathrm{d} T = \mathrm{d} t + \xi(r) \mathrm{d} r
\end{equation}
into the metric (\ref{GPMetric}), it is straightforward to see that the mixed term vanishes iff our transformation is
\begin{equation}
\mathrm{d} T = \mathrm{d} t +\frac{\sqrt{2GM/r}}{c^2\left(1-\frac{2GM}{rc^2}\right)}\mathrm{d} r.
\label{TtTrafo}
\end{equation}
Substitute this into (\ref{GPMetric}), and the result is the familiar form of the Schwarzschild metric in Schwarzschild's original coordinates $t,r,\vartheta,\varphi$, 
\begin{equation}
\mathrm{d} s^2 = -c^2\left(1-\frac{2GM}{c^2 r}
\right)\mathrm{d} t^2 + \frac{\mathrm{d} r^2}{\left(1-\frac{2GM}{c^2 r}
\right)} + r^2\mathrm{d}\Omega^2.
\end{equation}

\section{Conclusion}
Using coordinates adapted to the symmetries, we were able to write down the spherically symmetric, static spacetime metric. On this basis, and using the family of infalling observers that is characteristic for the Gullstrand-Painlev\'e solution, we wrote down the metric in the form (\ref{preMetric}), with a single unknown function $\beta(r)$. From the simplified form (\ref{EinsteinVacuum}) of the vacuum Einstein equations, as applied to a test ball in free fall alongside one of our family of observers, we were able to determine $\beta(r)$, up to two integration constants. By using the Einstein equation, we escape the restrictions imposed on simplified derivations by Gruber et al.\cite{Gruber1988} 

From the initial condition for our infalling observers, as well as from the Newtonian limit at large distances from our center of symmetry, we were able to fix the values of the two intergration constants. Our derivation does not require knowledge of advanced mathematical concepts beyond the ability to properly interpret a given metric line element $\mathrm{d} s^2$. Even our analysis of tidal effects proceeds via a simple second-order Taylor expansion, leading to differential equations for $\beta(r)$ that are readily solved using two applications of the method of separation of variables. 

What is new about the derivation presented here is the combination of the Baez-Bunn equations with the infalling coordinates typical for the Gullstrand-Painlev\'e form of the metric --- this combination is what, in the end, makes our derivation particularly simple. In turn, this simplicity is what should make the derivation particularly useful in the context of teaching general relativity in an undergraduate setting.

The derivation proceeds close to the physics, and gives ample opportunity to discuss interesting properties of Einstein's theory of gravity. Students who are presented with this derivation, either as a demonstration or as a (guided) exercise, will come to understand the way that symmetries determine the form of a metric, the deductions that can be made from Einstein's equivalence principle, and last but not least that we need to go beyond the equivalence principle, and consider tidal forces, to completely define our solution.

\section*{Acknowledgements}

I would like to thank Thomas M\""uller for helpful comments on an earlier version of this text.

",['Using the vacuum Einstein equation and the Baez-Bunn form.'],4982,multifieldqa_en,en,,032ee1448dec7751d00cd9f752fc61c5843a47e49dd7fcb6," Schwarzschild solution plays a key role in teaching about general relativity. It describes the simplest version of a black hole. By Birkhoff's theorem, it more generally describes the gravitational field around any spherical mass distribution, such as the Sun in our own Solar system. It is one of two particularly simple, yet physically relevant examples of a non-trivial metric. This article presents a derivation which requires no deeper knowledge of the formalism of differential geometry beyond an understanding of how to interpret a given spacetime metric. The derivation avoids the criticism levelled at attempts to derive the Schwarzschild solution from the Einstein equivalence principle in combination with a Newtonian limit. It relies as it does on a simplified version of the vacuum Einstein equation. In the end, what is required is no more than the solution of an ordinary differential equation for a single function, which yields to standard methods, to obtain the desired result. In this text, we assume that the spacetime we are interested in is spherically symmetric and static. In general relativity, a symmetry amounts to the possibility of being able to choose coordinates that are adapted to the symmetry, at least within a restricted sub-region of spacetime in question. That spacetime is static is taken to mean that we can introduce a (non-unique) time coordinate ${t}$ so that our description of Spacetime geometry does not depend explicitly on ${t]$, and that spacetime geometry doesn't depend on the symmetry of the region we are talking about. We use the Gullstrand-Painlev\'e form of the metric, but in this text we use the more common Schwarzschild coordinates. We also use the ``infalling coordinates'' that are commonly associated with the Gulstrand and Painlev\'es form of Schwarzschild metric, rather than the more commonly used Gull Strand and Painter coordinates, which we use in the previous section. We conclude with a discussion of the implications of this article for the theory of general relativity in terms of the universe as a whole, as well as the general theory of relativity in its various sub-disciplines. We end with a review of the current state of the art in the field of quantum mechanics, with a focus on how quantum mechanics can be used to understand the universe in the 21st century. We hope that this will provide a useful starting point for students of the theories of relativity and quantum mechanics in the coming years. We will end the article with a look at some of the latest developments in quantum mechanics and the theory's application to the real world. We look forward to hearing from you in the next few months as you continue to develop your understanding of the Theory of Quantum Mechanics. Back to the page you came from.    The original version of this text was published in the journal The Astrophysical Journal, with an Introduction to General Relativity and Quantum Mechanics, published by the University of California, Los Angeles, in September 2013. In the case of spherical symmetry, we can introduce spherical coordinates that are adapted to the symmetry. We are left with two unknown functions $F(r) and $G(r), which we call the static and static coordinates. In those coordinates, there is no need for specifying which of the 3-spaces we are referring to. In consequence, the part of our metric involving $\mathrm{d}\vartheta$ and $varphi$ will have the standard form of $R(R) + G(G(R), since we have not yet chosen a specific coordinate for the measurements associated with our specific measurements. But it is clear that whatever choice we make, the locallyogonal lengths of the lengths $r, $cdot, and $rcdot$ will be the same for the corresponding measurement to the corresponding length of the coordinate in the metric. We will use the basic structures introduced in this way --- the slices of simultaneous ${t}$, the radial directions within each slice, the angular coordinates spanning the symmetry--adapted spherical shells of area $4\pi r^2$ --- as auxiliary structures for introducing spacetime coordinates. For now, let us write down the shape that our metric has by simple virtue of the spherical symmetry and the requirement that the spacetime be static, and the adapted coordinates, namely $R (R) & G (G), which are the static coordinates of $F (R), $G (G) & $r(G), $r$ (R, G, $r), $R, $G, $R$ (r, G), $g, $g', $r'', $g'', 'g', 'h', 'm' is the name of the metric we use to describe the three-dimensional hyperplanes of space and time, and it is based on the fact that time and space are completely separate. We can use $G' and $F' to talk about our three- dimensional hyperplanes as just 'space' and 'time' in the same way as we did in the previous section. In this case, we will use $R' as the name for the metric, and we will refer to it as the 'StaticForm' metric, since it is the only one that can be used to describe our 3-D hyperplanes in the current form. It is the speed of light $c$ that gives us the correct physical dimensions, and that is characteristic for spacetime, so we can call it the 'speed of light' metric. In the coordinates adapted to symmetry, there are no 'mixed terms' involving $t' times the differential of a space coordinate. The absence of the mixed term indicates that in adapted coordinates there isNo need for such an extra shift. That there isno explicit dependence on $t$ is one consequence of the spacTime metric being static. That the factor in front of the 'Omega' coordinate is $r^2' is a consequence of our coordinate choice, with spherical angular coordinates so that the area of a spherical surface of constant radius $ r$ is $4''pi r''2$ is another. That $r''r'' is the same as the length of a coordinate associated with a specific measurement $r,.$ because it is a part of the same measurement, it is not clear that it is that part of it that we want to use for the measurement to be taken. The fact that the coordinate choice is not explicit is a direct consequence of spherical symmetries. t of time ${t}$, we follow Visser and Hamilton in defining a family of radially infalling observers. Observers in that family are in free fall along the radial direction, starting out at rest at infinity. In mapping each observer's radial progression in terms of the static coordinate time $t$ we adjust initial conditions, specifically: the choice of initial speed at some fixed time. We assume all such little coordinate systems to be non-rotating, otherwise, we would break spherical symmetry. The radial direction is a natural choice for the first space axis of those little free-falling systems. The other directions, we take to point to observers falling side by side with our coordinate-defining observer, and to remain pointed at a specific such other observer, once the choice is made. All the infalling. observers who are at radius value $r$ show the same proper time $T$ on the ideal clocks travelling along with them. We can combine the two pieces of information to obtain the rate of change in radial position $ r$ with proper time £t$ for those infallingObservers. The resulting function can only depend on $r$, and not explicitly on £t. Let us rescale that function with the speed of light to make it dimensionless, and call it $beta(r)$, then give it an overall minus sign to makeIt positive for infalling particles. We make the choice to make the physical length measured by one of our observers at the relevant location in spacetime, at constant time £T$. Via angular measurements, that implies that length to the radialdirection $r\cdot\vartota$ and $r:\vartodot$ inherit the same length to $r. The result is that our solution should describe the spacetime of a spherically symmetric mass. It should be noted, though, that all our calculations would also be compatible with the limit of no mass being present. In that case, ``infalling'' would be a misnomer, as our family of observers would merely hover in empty space at unchanging positions in $r$. The solution is then called $r(r), and we can use it to describe the length of a particle at a given location in space and time, at $r/t$ as well as the physical meaning of $r ($r) (or $r-d) for the particle's position in space, and $t' for time, and so on. For example, we can say that the particle is at a location $R$ at the time of a given event $cal E, and that it is at that location at $R/T$ at that time. The solution can then be used to explain the origin of the theory of quantum gravity, and how it relates to the universe as a whole. It can also be used as a basis to explain how quantum gravity is related to the laws of quantum mechanics, such as the so-called quantum electrodynamics. We conclude that the solution is compatible with a theory of gravity that describes the existence of a non-singular, non-deterministic universe. We call this the theory $r (r) and the result is called $R(r). physical interpretation. We write the static time coordinate as a function ${t}(T,r)$ in terms of infalling observer time and radius value. By Einstein's equivalence principle, the metric is that of special relativity. Locally, whenever tidal effects can be neglected, spacetime geometry for any non-rotating observer in free fall is indistinguishable from Minkowski spacetime. We are not transforming from one local inertial coordinate system to another. The $T$ is already the time coordinate of the infalling observers, so both systems have the same definition of simultaneity, and time dilation plays no role in this particular transformation. There is no Lorentz contraction, so there is no special choices that give these special choices. We do have the remaining freedom of choosing the offset of a constant for our point of interest, but we do not have to choose the offset for our infinitesimal neighbourhood of the point $r, let us make the choice directly at our point $varta, instead of choosing an offset of freedom for the infinimal neighbourhood. We have chosen intervals to correspond to infalling intervals. We can now write the metric for the local length and time measurements made by any one of our free-falling observers. The metric is consistent with the definition of the function $beta(r) $ in (\ref{betaDefinition) in the Galilei transformation. It has the form of a Galilei transformed form of the relation (Last but not its simple form), which is the same as the relation for the differential (Rshift) we used before. We now have a simple form of our metric (StaticForm) that is more simple than the previous one (Staticformula) with two variables instead of one. The new metric has the same form as the static form, but with a more simple form, which is more convenient for our purposes. We call the new metric the 'G Galilei metric' (G(r), G(r, T) and G(R, T), and the new form is the 'StaticForm' form (G, T, G, G), which has a more complex form, with two more variables, such as time coordinate $T, radius value $r$ and radial coordinate $R$ We can then write our metric as a simple Galilei form with the form:StaticForm(G(R), StaticForm(T) and StaticForm (T, R), G (R, R, T). The new form gives the same result as the previous form, except that we have more variables to work with, and we can write it as a metric with two functions instead of just one. We then have a new metric that can be written as the 'static form' with two new variables, which we call 'StaticMetric' and 'Staticform' The new formulation paves the way for an even simpler form of this metric, the 'DynamicMetric', which is just a simple transformation of the StaticForm form with two extra variables. We also have the choice of a new coordinate $r' to parametrize the radial direction of the radial movement of $r. We want this coordinate to be co-moving with our inf falling observers at $r $; each such observer then has a position $\bar{r}=const.$ that does not change over time. The result is a metric that is the equivalent of a simple 'G-form' of the 'Galileo transformation. In the previous section, we had exploited symmetries and Einstein's equivalence principle. In order to determine the metric, we need to bring in additional information, namely the Einstein equations, which link the matter content with the geometry of spacetime. For our solution, we only aim to describe the spacetime metric outside whatever spherically-symmetric matter distribution resides in (or around) the center of our spherical symmetry. That amounts to applying the vacuum Einstein equations. In that system, we describe a small sphere of freely floating test particles, which we shall call a ball. The test particles need to be at the rest of the volume of the ball. If there is no matter or energy inside the volume, such as in a vacuum, then the test particle is not a particle. We can now write down the same metric in the coordinates $T, r, \vartheta,\varphi$ in terms of the CartesianMinkowski coordinates. We have reduced our metric to a simple form with no more than one unknown function of one variable, namely $r$ and we can repeat that local procedure at any event in our spacetime, this result is our general form of the metric. In the next section we will deviate from Visser's derivation of the Schwarzschild metric. We will focus on tidal deformations, which can be found in a seminal article by Baez and Bunn. We'll also look at Einstein's equations for the first time in this article. We hope that this will help us understand the theory of quantum gravity more fully, as well as some of the implications of the theory. Back to Mail Online home. back to the page you came from. Click here to read the original article and to share your thoughts on the theory with other readers. Back To the page we came from, click here to share our views on the Theory of Quantum Gravity and the theory behind quantum gravity and other theories of particle physics. Back into the page that you came in from, and share your knowledge of the theories of quantumgravity and particle physics with other researchers. Click back to see the original story. Backto the page where you came to the story about quantum gravity, or share your views on it with other physicists and other academics. CLICK HERE to share the story with other academics and researchers. Back on the page to read more about the theory and theories behind the theory, including how to apply the theory to quantum gravity in the real world. Click HERE for the latest in-depth coverage of the Theory Of Quantum Gravity, including the theory's implications for quantum mechanics and quantum field theory, and to see a video of quantum physics in action in the form of a simulation of a free-fall system around a specific event in space and time. Click there to watch the video of the free fall system in action, and click here for the second part of the article, in which we look at the theory from the perspective of a quantum field in space-time. The third part of this article will look at how to use the theory in a quantum-field model to understand quantum gravity. The fourth section will look into the theory for quantum gravity from the standpoint of quantum mechanics in the universe in general. The final section will be devoted to a theory of the quantum universe, which is based on a theory called quantum entanglement. The last part will be dedicated to the theory on quantum gravity for the universe's gravitational field. The full article will be published in the next few weeks, and will include a discussion of how the theory can be used to explain quantum gravity as a model of the universe. In 2017, Kassner made use of the Baez-Bunn form of Einstein's vacuum equation to derive the Schwarzschild solution. By going into a free-fall frame, and restricting our attention to a small spacetime region, we have automatically created a weak-gravity situation. In such a situation, tidal corrections are approximately the same as those described by Newton. This argument can serve as a heuristic justification of (\ref{EinsteinVacuum}). In the following, we will in the following only keep terms linear in $\Delta l$ so as to display the geometry of the situation more clearly. We take the central particle $C$ to be at radial coordinate value $r=R$ at the time of the snapshot shown in Fig. 1. We follow the same general recipe, but using the infalling coordinates introduced in section \ref{Sec:InfallingObservers}, which makes our derivation even simpler. With these results, we can rewrite the Taylor expansion (TaylorREvo) as (T-T_0) with the product of its derivative and the prime product of the derivative of its first and first derivative. The result is a weak gravitational field of the form T-T-L (T, T, T-L) with a Schwarzschild radius of 1.5. We can see that this is the same result as in the original Einstein equation (Einstein Vacuum) and that it also holds true in Newtonian gravity. In this case, the lowest-order correction for tidal forces in a freely falling reference frame is that specified by Newtoniangravity. We know that the derivative in the linear term can be expressed in terms of $\beta(r)$; by the same token, the prime differentiation of its product and its derivative will occur quite often, let us introduce the abbreviation ""Big abbreviation"" (B abbreviation) for ""big"" and ""beta"" (beta) in the next section. We will also use this abbreviation to refer to the fact that Einstein's equation can be seen as a second-order extension of the usual Einstein equivalence principle: for the absence of mass, to tell spacetime how to curve. For example, in the equations for gravity and mass, the equation for gravity is: (T(R) + T(T (R) - T (T (D) + D) = -c\frac{\mathrm{d}\beta'(r). T(R(T) & T(D) - t (T) + t (D). T (d) + d (t) = t (r) + r (d(T), t (d (D), d (T). T) = r (t(R), t(D), r (D, T) + (t (T), r(D, D) - d (R), d(T). t(R, D, T), r. (T.2) = d (r, T.2, t (R, T). (T1) - r (T2) - (T0) (R (D.2), (T3) (T4) (D2) ( t(T, D2, T (D4) & (T5) (t4) / (T6) (r (D3, D4, T5) & t (t6, T4) + 2 (D6, D3, T6, t4, D6, (T7, D7, T7) (d2) & 2 (T8) (dt(T4), t6, d4, t7, t8, t6 (D7, d7, (D1, D8, D5, D9, D1, T8, T9) & d6 (t2) , (D5, T1, d2, D In order to define our test ball, we need to consider particles $C, U, D, L, R$ which are initially at the same location, but are undergoing the same acceleration. The distance between the upper and lower particle, $d_{\parallel}(T),$ changes over time as $r_0=R. We could go back to the drawing board and repeat our calculation while including those additional velocity terms. But there is a short-cut: The terms in the equations will be only to the linear terms in $\Delta T$ in order to make a more general general equation that includes the initial velocities of the test ball. The equations are shown in Fig. 1, which includes a general equation to measure the divergence of the motion of our test particles from that of the central particle $C$. The equations for the ball can be found at: http://www.einsteinvacuum.org/article/Einstein-Vacuum-Einstein.html#v=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,33-34,34-35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,64-64,67,68,69,70,72,73,74,75,78,79,80,82,83,84,83,.88,88,89,88,.88,.89,90,89,.90,92,93,94,90,.92,94,.93,95,96,98,99,100,100,.96,93,.98,98,.99,99,.99,.92,.98,.93,.99%,98,97,99! (See Fig. 1 for more details on how to calculate the Einstein equation for the volume of a test ball in terms of particles in a vacuum. See Fig. 2 for more detail on the equations for particles in the vacuum in the form of an ellipsoid, which is initially a spherical surface with two axes that are transversal and each have the length of $d_\perp}(t) The initial radial coordinate value for each of the particles is $r(T_0) = \sqrt{R^2+\Delta l^2}=R, given by eq.~(\ref{RadialOrbitTime}) with $r-R$ as the initial coordinate for each particle. After all, the particles of thetest ball need to be at rest initially, at time $t_0$, in the co-moving system defined by the centralicle $C$ Our defining particles are not, as the terms in $DParallel$ in both (\ref{dParallel) and (\Ref{dPerp) show, where the coefficients of $Delta T) correspond to particles that are not linear in the equation. In Fig. 3, we show how the distance between particles $L$ and $R$ changes in proportion to the radius value of the sphere. For the second time derivative of $V(T)$ to vanish at the time $T=T_0$, we must have $B(r)$ for all values of $r$. This is readily solved by the standard method of separation of variables. The description provided by our solution should correspond to the results from Newtonian gravity. We can solve the result for $r$ and obtain the radial velocity of the test ball. In the next section, we will look at how this radial velocity is related to the potential energy of the ball. We will also look at the effect of gravity on the ball, and how this affects the ball's volume. We'll conclude by looking at how the ball behaves in the presence of a large gravitational field, such as a strong magnetic field or a strong gravitational field of up to 1,000,000 times the density of the Earth's atmosphere. The test ball will have a radius of 2,000 by 1,500 by 2,500, and a center of mass of 1,200 by 1.5,000. The ball will be at rest in our static coordinate system at the static time time of $t$ or $r$, and its velocity will be approximately approximately approximately 1/2 the speed of light. We call this the Newtonian description of the velocity of our test ball, or V(T), and it is the same as Newton's description of gravity in terms of kinetic energy and potential energy. The solution (\ref{BSolution) corresponds to the differential equation for $C'$ for any constant $C$, positive, zero, or negative. The final form of our equation is the equation for our function $beta(r), where $D$ is the second integration constant, and where we have chosen the proper sign, since we know that $R$ is a constant of magnitude 1.1. We conclude that the ball will travel at a rate of 1/4 to 1/3 of the distance between us and the infalling observer at rest, or about 1/10,000 of a million miles per hour (1/4,000 km/h) The ball's radius will be around 2,200 feet (1,500 meters) and its speed will be about 1,400 feet (600 meters) per second (or 1,300 meters per second). The ball has a volume of $1,000 kg (2,500 m/s) and a mass of about 3,000 kilograms (4,500 ft/m) or about 2,400 ft/s (2/3,500m/s, or 1/6,000m/h, or 2/4k/s), and its radius is 1,600 m (1) or 1,700 m (2) or 2,600 ft (1), and so on. We have a test ball with a radius that is about 2.5 times the size of our current coordinate system, or the radius of our static time system. The radius of the sphere is about 1.6 times the length of our coordinate system. This gives us the equation: V(V) = 1/5, or $1.6, or ""V"" = ""V(V),"" or ""T"" (or ""T"") = ""T"", or ""R"" = 1.2, or (1-2,2,0,1,5,1) and ""V (T) = ""R"", ""T,"" ""R,"" ""T""; ""T'"", ""R""', ""T"". Using coordinates adapted to the symmetries, we were able to write down the spherically symmetric, static spacetime metric. By using the Einstein equation, we escape the restrictions imposed on simplified derivations by Gruber et al. Our derivation does not require knowledge of advanced mathematical concepts beyond the ability to properly interpret a given metric line element. Even our analysis of tidal effects proceeds via a simple second-order Taylor expansion, leading to differential equations for $\beta(r)$ that are readily solved using two applications of the method of separation of variables. In turn, this simplicity is what should make the derivation particularly useful in the context of teaching general relativity in an undergraduate setting. The derivation proceeds close to the physics, and gives ample opportunity to discuss interesting properties of Einstein's theory of gravity. It is hoped that this article will help students to understand the way that symmetry determine the form of a metric, the deductions that can be made from Einstein's equivalence principle, and last but not least that we need to go beyond the equivalence Principle to completely define our solution. The author would like to thank Thomas M.uller for helpful comments on an earlier version of this text. He would also like to apologize for any confusion caused by the use of the word 'trafo' in the original version of the article, and for any errors that may have been caused by using the term 'Trafo' instead of 'Trafic' in this article. For more information, see the original article, which was published in The Astrophysics of General Relativity, published by MIT Press, on November 6, 2013. The article was updated on November 13, 2013, to include the following comments:  ‘Trafo’ means ‘trafic’ in the English language, and ‘Trafalgar’ is a term used to refer to the force of gravity in general relativity. It was also clarified that the term ‘T’ refers to the fact that the gravitational force of general relativity is ‘t’ rather than ‘force’, rather than the ‘gravitational force’ as in previous versions of this article, as this is a more general term for the theory of relativity. The authors conclude that this is not the case, and instead they use the terms ‘triggered’ and � ‘transformed’ to mean ‘gravity’ or ‘mass’. They also point out that the mixed term vanishes iff our transformation is transformed into the metric (\ref{GPMetric), and the result is the familiar form of the Schwarzschild metric in Schwarzschild’s original coordinates $t,r,\vartheta,’varphi$,  and that this transformation is straightforward to see. They conclude that the metric can be derived from the Baez-Bunn equations."
What is the water depth in the Greater Ekofisk Area?,"Filip Fremo Minge – Ekofisk
Author: Filip Fremo Minge
Posted on 1. October 2019 12. October 2019
— Sunset over Ekofisk. Photo: Husmo Foto/Norwegian Petroleum Museum
The three are operated by ConocoPhillips on behalf of the Ekofisk licensees. The area also embraces former producers Albuskjell, Cod, Edda, Tor, West Ekofisk and Tommeliten G.
These fields all lie within production licence 018 apart from Tommeliten G, which was operated by Statoil from 1976 to 2003.
In all, 31 installations have been positioned in the Greater Ekofisk Area.
First Norwegian offshore field
Ekofisk began production on 15 June 1971, following its discovery in the autumn of 1969. Development of the field has occurred in several phases.
Its central facilities were installed during the early 1970s, with oil initially being buoy-loaded into tankers. From 1975, it has been piped to Teesside in the UK. The gas has been landed by pipeline at Emden in Germany from 1977.
ekofisk i et nøtteskall, engelsk
Jacked up six metres
The water depth in the Greater Ekofisk Area is 70-75 metres. However, declining pressure in the Ekofisk reservoir over the years has caused the seabed to subside.
Efforts began as early as 1985 to safeguard the installations against the effects of this development, and the steel platforms in the Ekofisk Complex were jacked up by six metres in 1987.
In addition, a protective breakwater was installed around the Ekofisk tank in 1989. The rate of seabed subsidence has declined sharply in recent years.
Waterflooding improves recovery
The Ekofisk 2/4 K water injection platform became operational in December 1987 as part of efforts to improve Ekofisk’s recovery factor – the share of petroleum in place actually produced.
Waterflooding capacity on the field to help maintain reservoir pressure was later expanded several times, and had reached just over 500 000 barrels per day by 2019.
Measured in barrels of oil equivalent, the recovery factor on Ekofisk has risen from an original estimate of 17 per cent to over 50 per cent.
Ekofisk I and II plus licence extension
The first phase of development and production on Ekofisk began with initial oil output from the converted Gulftide jack-up rig in 1971 and ended with the start-up of Ekofisk II in 1998.
Large parts of the Greater Ekofisk Area were restructured in the latter year, leading to plans for removing 15 installations – 14 steel platforms and the process facilities on the Ekofisk tank.
plattformer, historie, 2004, driftsenter åpnet,
Embla 2/7 D. Photo: ConocoPhillips/Norwegian Petroleum Museum
Designated Ekofisk I, these redundant structures include Ekofisk 2/4 A, 2/4 B, 2/4 FTP, 2/4 Q, 2/4 H, 2/4 R, 2/4 P and 2/4 T.
In addition come the Edda 2/7 C, Albuskjell 1/6 A, Albuskjell 2/4 F, Cod 7/11 A, West Ekofisk 2/4 D, Norpipe 36/22 A and Norpipe 37/4 A installations.
The concrete part of the tank – Ekofisk 2/4 T – will remain. Gulftide was removed as far back as 1974. Two platforms owned by other companies – Ekofisk 2/4 G and 2/4 S – have also gone.
A new plan for development and operation (PDO) of the field (Ekofisk II) was approved in 1994, at the same time as the Ekofisk licence was extended to 2028.
This creates a new Ekofisk Complex with two structures – the Ekofisk 2/4 X wellhead unit installed in the autumn of 1996 and the Ekofisk 2/4 J processing and transport platform in 1997.
Ekofisk II became operational in August 1998 and is intended to produce until 2028. Ekofisk, Eldfisk and Embla are tied back to the new complex, as was Tor until it shut down in December 2015.
Ekofisk West
historie, forsidebilde, 2003, ekofisk vekst godkjent i statsråd
Ekofisk Growth. Illustration: Ståle Ådland
In December 2002, soon after the Conoco-Phillips merger had been announced, the Ekofisk West project was presented to improve oil and gas recovery. Process capacity and reliability on Ekofisk were also to be enhanced.
This development primarily involved the construction and installation of a new platform, Ekofisk 2/4 M, with processing facilities and 24 new wells drilled over five years.
The latter could contribute to improved recovery both because there were more wells and because they would tap new locations in the reservoir. On stream in 2005, 2/4 M was linked to the Ekofisk Complex with a bridge.
Process capacity for produced water was also to be increased through upgrading on Ekofisk 2/4 J and Eldfisk 2/7 E. A third measure concerned laying a power cable from the Ekofisk Complex to 2/4 K in order to make electricity supplies more efficient.
New developments: Eldfisk II and Ekofisk South
Eldfisk 2/7 S løft
The deck of Eldfisk 2/7 S being mated with the steel jacket. Foto: Øyvind Sætre/ConocoPhillips
The plan for development and operation (PDO) of Eldfisk II, approved by the Storting (parliament) on 9 June 2011, includes a new wellhead, process and accommodation platform – Eldfisk 2/7 S.
In addition come 42 new wells as well as upgrades to existing platforms which extend their commercial life.
The PDO for Ekofisk South involves the construction of a new wellhead platform – Ekofisk 2/4 Z – as well as a new subsea water injection facility and 44 additional wells.
ConocoPhillips Norge, 2004.
Ministry of Petroleum and Energy, press release, “Vekstprosjekt på Ekofisk godkjent”, 6 June 2003.
https://www.stortinget.no/no/Saker-og-publikasjoner/Saker/Sak/?p=50343
https://www.stortinget.no/globalassets/pdf/innstillinger/stortinget/2010-2011/inns-201011-398.pdf
https://www.regjeringen.no/no/aktuelt/klart-for-40-nye-ar-pa-ekofisk-feltet/id642376/)
Published 1. October 2019 • Updated 12. October 2019
— Gassterminalen i Emden. Foto: Husmo Foto/Norsk Oljemuseum
Oil terminal in Teesside
Olje- og gassterminalene, engelsk,
Teesside terminal. Brian Henderson Thynne takes samples of refrigerated propane. Photo: Husmo Foto/Norwegian Petroleum Museum
The terminal at Teesside in north-east England receives oil and natural gas liquids (NGL) by pipeline from the Ekofisk field. It comprises stabilisation, NGL fractionation, storage tanks for crude oil and an export port.
After arriving through the Norpipe Oil line, crude and NGL are separated and the oil goes through a stabilisation process before reaching the 10 storage tanks, which each hold 750 000 barrels.
The NGLs go to the fractionation facility, with a daily capacity of 64 000 barrels, for separation into methane, ethane, propane, and normal and iso butane.
While the methane (natural gas) is used to fuel the plant, the other products (now known as liquefied petroleum gases – LPG) are made liquid by cooling and stored for export by sea.
One reason for the choice of Teesside as the landfall for the Ekofisk pipeline was the opportunity it offered to install deepwater quays.
The terminal has four of these, with those for crude oil able to handle tankers up to 150 000 deadweight tonnes. The LPG quays can accept carriers loading as much as 60 000 cubic metres.
Two of the crude oil quays lie on the main channel of the River Tees, while the others have been installed in dredged docks.
Gas terminal in Emden
Gas arriving at the Emden terminal from the Ekofisk Complex enters nine parallel treatment trains for cleaning, metering and onward distribution to the buyers.
The North Sea gas is very clean, and needs only limited treatment to remove small amounts of sulphur compounds using an absorption process. Impure molecules from the gas accumulate on the surface of small particles, which act as filter spheres.
Each of the nine trains comprises four process columns and a process oven. The gas enters the top of a column and leaves through the base after passing through the filter spheres.
That leaves the gas ready for sale, and it is piped to the fiscal metering station before entering the buyer receiving pipelines and distribution network.
Three separate commercial pipeline systems connect to the terminal, operated by Ruhrgas, BEB and Gastransport Services (previously Gasunie) respectively. They pipe the gas away on behalf of the gas buyers.
The Norsea Gas Terminal in Emden was officially opened in September 1977 by Norwegian industry minister Bjartmar Gjerde and Phillips executive Gordon Goerin.
Ranking as the first gas sales deal for the Norwegian continental shelf, the Ekofisk agreement paved the way for later contracts covering other fields off Norway.
Regularity at the Emden terminal has been very high, with its own equipment never causing shutdowns. Maintenance takes place when other parts of the system are off line.
The terminal has a daily capacity of about 2.1 million cubic feet of gas per day.
Gas transport restructured
Norpipe AS owned the gas pipeline from Ekofisk to Emden until the transport system for the Norwegian offshore sector was restructured at 1 January 2003.
Norsea Gas A/S furthermore served as the formal owner of the Emden facility, with Phillips Petroleum and then ConocoPhillips as operator for both pipeline and terminal.
olje- og gassterminalene,
Teesside gas terminal. Photo: Husmo Foto/Norwegian Petroleum Museum
Since 2007, Norway’s state-owned Gassco company has been responsible for technical operation of the facilities on behalf of their owners.
That included operator responsibility for the H7 and B11 booster platforms along the gas pipeline, which were shut down in 2007 and 2013 respectively and have since been removed.
The Gassled partnership is a project collaboration embracing 10 companies which collective own large parts of the gas infrastructure on the Norwegian continental shelf (NCS).
A substantial proportion of Norway’s gas deliveries to Germany continues to arrive at the Emden terminal, including the volumes piped from Ekofisk.
Preliminary planning for a new terminal in the German port began in 2011, with Gassled taking the investment decision for this development in the autumn of 2012.
Construction work began in the following year, with the new facility being built on an unused part of the existing terminal site.
The new terminal has not expanded export capacity. But its functionality is well adapted to future processing needs for fields in the Greater Ekofisk Area and other parts of the NCS sending gas through the Norpipe system.
It was officially opened on 24 May 2016 by Elisabeth Aspaker, the Norwegian government minister for the EU and the European Economic Area. That closed a chapter in Ekofisk’s history.
Source: ConocoPhillips Norge
— Gas pipes at Ekofisk. Photo: Husmo Foto/Norwegian Petroleum Museum
In addition to ConocoPhillips’ own production from Ekofisk, these pipelines carry gas and oil from the company’s fields in the UK sector and from other fields on the Norwegian and British continental shelves.
The three fields in the Greater Ekofisk Area are also tied together by pipelines.
Oil pipeline to Teesside
rørledningene, engelsk,
Pipes and oil tanks at the Teesside plant. Photo: ConocoPhillips/Norwegian Petroleum Museum
The pipeline linking Ekofisk with the terminal for oil and natural gas liquids (NGL) at Teesside on the north-east English coast became operational in October 1975.
Pumps raise the pressure of the oil and NGL before they start their journey to land. Two pumping stations – 37/4 A and 36/22 A ­– originally stood along the pipeline to maintain this pressure, but have now been disconnected and removed.
The pipeline was installed with the ability to carry a million barrels per day. However, that much capacity has never been required.
In the UK sector, a 24-inch pipeline has been tied in with a Y connection to receive input from several British fields – including the J block developments operated by ConocoPhillips.
Output from the Greater Ekofisk Area is supplemented by crude from Valhall, Hod, Ula and Gyda heading for Teesside, optimising pipeline utilisation and thereby boosting value creation.
The pipeline is owned by Norpipe Oil AS and operated by ConocoPhillips.
Gas pipeline to Emden
Sandbags and gravel were used to cover Norpipe to Emden. Photo: Unknown/Norwegian Petroleum Museum
This pipeline became operational in September 1977. The starting pressure of around 132 bar is provided by compressors on the Ekofisk Complex.
The 443-kilometre distance to Emden was split into three equal sections, with platforms B11 and H7 located at the intermediate points to provide boosting if required.
However, additional compression was seldom needed on the final stage to Emden. H7 was shut down in 2007 and B11 in 2013, and both have since been removed.
These two booster platforms were located in the German sector of the North Sea, while the pipeline also crosses the Danish sector.
The pipeline has been trenched or covered with sand. Its final section passes the island of Juist before making landfall on the coast of East Friesland to the north of Emden.
Its daily capacity is roughly 59.4 million standard cubic metres (2.1 billion cubic feet). In addition to gas from the Greater Ekofisk Area, it carries output from Valhall, Hod, Ula, Gyda and the Statpipe system (primarily Statfjord and Gullfaks).
Posted on 24. June 2017 25. October 2019
Embla 2/7 D
This unmanned wellhead facility is remotely controlled from Eldfisk 2/7 S located 5.2 kilometres to the north, where oil and gas output from the platform is also processed.
Unmanned and remotely operated wellhead platform
On stream 12 May 1993
— Embla 2/7 D. Photo: ConocoPhillips
sokkelkart, illustrasjon, blokker, lisens, forsidebilde, engelsk,
Hand-colored map of the licenses of the first licensing round on the Norwegian continental shelf. Norwegian Continental Shelf Map, 1965.
The Phillips group was awarded block 2/7 as early as 1965, and the Embla reservoir lies in the southern part of this acreage. Drilling began there in 1974 to depths of 4 500-5 000 metres, but pressure and temperature in the wells were too high for testing with the available equipment.
The first production well was not drilled and tested until 1988, followed by a second in 1990. Both yielded very promising results, and the field came on stream in May 1993.
Embla comprises a sandstone reservoir at least 250 million years old. The other fields in the Greater Ekofisk Area comprise fine-grained carbonate rocks deposited about 70 million years ago.
The Embla reservoir has a temperature of 160°C compared with the 125°C normally found in the chalk formations 1 000 metres higher up, and its pressure is almost twice as high.
Fabricated by Heerema in the Netherlands, the Embla 2/7 D jacket (support structure) was installed by the M 7000 crane vessel. It stands 84 metres high and weighs 2 300 tonnes.
A 5.2-kilometre subsea umbilical from Eldfisk comprises three power cables for electricity supply and eight fibreoptic lines handling data transmission and telecommunication.
Eldfisk 2/7 S, embla,
Eldfisk 2/7 S. Photo: ConocoPhillips
The platform has six production wells and an average daily output of roughly 7 000 barrels of oil. All processing and metering took place on Eldfisk 2/7 FTP until 2015, and has now been switched to Eldfisk 2/7 S.
A 14-inch flowline linked 2/7 D with 2/7 FTP and runs today to 2/7 S. Produced at Wick in Scotland, this line was floated out to the field in one piece.
Topside equipment includes the wellhead area, helideck (built by Vindholmen Services in Arendal), crane, control room, workshop, test separator and glycol pump.
Normally unmanned, the platform is maintained as and when required and therefore incorporates a simplified accommodation module with lounge, mess, coffee room, galley, changing room, WC and 12 emergency beds.
More about platforms
Ekofisk 2/4 Z
This installation is a wellhead platform in the Ekofisk Complex.
Gulftide
This four-leg jack-up drilling rig was built in Glasgow during 1967 for Ocean Drilling & Exploration Co.
Posted on 1. September 2019 8. October 2019
— Gulftide with Ekofisk 2/4 A in the background. Photo: Aker Mek. Verksted/Norwegian Petroleum Museum
Gulftide was converted to cope with conditions on Ekofisk in the Åmøy Fjord near Stavanger. This jack-up drilling rig was equipped with process equipment and its derrick, helideck, hangar and legs were reinforced.
To win time, it was decided that the discovery well and three appraisals drilled on Ekofisk by Ocean Viking would be completed for production.
Principles for producing from Gulftide were relatively simple. Output flowed from the subsea wellheads to the platform, where it went through two-stage separation to remove gas and water.
With pressure also reduced, the gas was flared off and the oil sent on by flowlines to two loading buoys where shuttle tankers moored to take on cargo.
utbyggingen,
Tankskipet Donovania laster olje fra lastebøyen på Ekofisk. I bakgrunnen skimtes så vidt Gulftide. Foto: ConocoPhillips/Norsk Oljemuseum
Production could only continue while ships were loading. As soon as one tanker had been filled, the oil stream was diverted to the vessel waiting at the other loading buoy.
The problem with this approach was manifested when weather conditions ­– strong winds and/or high waves – forced the tankers to leave the buoys.
If that happened, production from the wellheads had to be suspended immediately. Given the prevailing weather on Ekofisk, that happened regularly. Output was halted for 20 per cent of the time during the first year.
https://ekofisk.industriminne.no/wp-content/uploads/sites/2/2019/09/Building-Ekofisk.mp4
Gulftide was replaced as the temporary production installation in 1974 by the permanent Ekofisk 2/4 A (Alpha) and 2/4 B (Bravo) platforms for production, drilling and quarters.
In addition came the Ekofisk 2/4 C (Charlie) production, drilling and compression facility, the Ekofisk 2/4 FTP (field terminal platform) for production and risers, and Ekofisk 2/4 Q for accommodation.
Oil and gas were produced by 2/4 A, B and C through their own wells for processing in their separation plants and piping on the 2/4 FTP for a three-stage separation process.
At the same time, the tanker loading buoys were moved further from the platforms and the Ekofisk 2/4 T oil storage tank became operational.
This facility was extremely advantageous, because it allowed production to continue virtually regardless of whether bad weather prevented tankers from connecting to the buoys.
Ekofisktanken ble satt i drift i 1974. Foto: ConocoPhillips/Norsk Oljemuseum
The 2/4 FTP platform, where oil and gas from the three producing facilities was processed, had been planned to handle the level of output estimated for the main field.
Clear restrictions had been imposed by the Norwegian government on the amount of gas Phillips was allowed to flare. That also set a ceiling for oil production, since gas accompanies it up from the reservoir.
The solution was to install two powerful compression packages on 2/4 C in order to inject the gas under pressure back into the producing formation.
Accommodation facilities had to be provided on the two first platforms, 2/4 A and B. Where 2/4 C and FTP were concerned, however, they were tied together with bridges and to 2/4 Q.
Published 1. September 2019 • Updated 8. October 2019
Posted on 9. April 2019 25. October 2019
Jack-up drilling rig
Built 1967 in Glasgow for Ocean Drilling & Exploration Co.
Began test production on Ekofisk 15 June 1971
Produced on Ekofisk until 1974
— Gulftide at theEkofisk field. Photo: Terje Tveit/Norwegian Petroleum Museum
gulftide,
Gulftide. Photo: Unknown/Norwegian Petroleum Museum
A mere 17 months after the Ekofisk discovery was announced in December 1969, Gulftide was ready to come on stream as a temporary production platform.
Its official inauguration took place on 9 June, with initial test output commencing on 15 June. Full production began on 8 July.
The rig was chosen because it was available on the market. Established equipment for processing oil and gas was tailored to the limited space on board. Separate flowlines carried wellstreams from four subsea wells. Oil, gas and water were separated on board, with the gas flared and the oil piped to two buoys for loading into shuttle tankers.
Work on the process equipment was relatively simple. The problem was to tailor it to the rig. The subsea wellheads had to be reinforced to meet the demands posed by the North Sea, and a buoy loading system needed to be developed for waters where this technology had never been used before.
To gain time, it was decided that the three appraisal wells drilled by Ocean Viking to map the extent of the field – in addition to the discovery well – would be completed for production.
Første testflamme tent på Ekofisk. På Gulftide
1973, Teddy Broadhurst, gulftide,
arbeidsliv, hjelpearbeider
Gulftide, separator – på bildet kan man se at det er fire brønner.
arbeidsliv, gulftide, pionerkultur, arbeid, dekk, Norges første havbunnsbrønner, historie, 1971,
The producers would be topped with hydraulically controlled wellheads. Such equipment had been tried out on the seabed earlier, but on a limited scale and not in the deep and rough waters found on Ekofisk. This challenge was overcome by having the wellheads manufactured and then reinforced at the Phillips base in Dusavik outside Stavanger. Flowlines and control cables would also be laid from each well to Gulftide, with production comingled in a single riser to the topsides.
Weather conditions also represented a major problem when designing the loading buoys. Phillips itself had experience with such facilities, but the concept had only been used before in harbour-like conditions and waters no deeper than 27 metres. They were now to stand in 70 metres in the middle of the North Sea.
Gulftide was converted in the Åmøy Fjord outside Stavanger to cope with conditions on Ekofisk. The processing facilities were installed and reinforcements made to the derrick, helideck, hangar and leg structures.
Gulftide, Ekofisk 2/4 A, boretårn, flare, 1971, utbygging,
Gulftide with Ekofisk 2/4 A in the background. Photo: Aker Mek. Verksted/Norwegian Petroleum Museum
Planning began in late 1970, when Phillips received approval to begin laying the flowlines between wellheads and rig. Brown & Root won this contract, with the first oil pipelines on the Norwegian continental shelf laid by the Hugh W Gordon laybarge.
The production principle on Gulftide was relatively simple. Output flowed from the subsea wellheads to the rig, where it passed through two separation levels to be split into oil and gas while the huge pressure was reduced.
Gas was flared off and the oil was piped to one of the loading buoys where a shuttle tanker was moored. Production could only take place when a ship was present.
Offisiell åpning av norsk oljeproduksjon,
The Greek tanker, Theogennitor, unloads crude oil from loading buoys on the Ekofisk field. Gulftide in the background. Photo: ConocoPhillips/Norwegian Petroleum Museum
As soon as one tanker had become fully laden, the oil flow was switched to the other buoy where another ship was waiting to take on cargo.
The problem with this approach arose when weather conditions meant the tankers had to cast off from the buoys because of strong winds or high waves. The rig then had to shut down production from the wellheads immediately.
Given the weather conditions found on Ekofisk, output regularly had to cease. Production was suspended for 20 per cent of the first year for this reason.
Output began cautiously on 8 July 1971 from a single well. The second producer came on stream that September, the third was ready the following month and all four were producing by February 1972. They each flowed 10 000 barrels of oil per day.
Source: Kvendseth, Stig, Giant discovery, 1988.
Published 9. April 2019 • Updated 25. October 2019
Norpipe H-7
This platform served as a pumping/compressor station to maintain pressure in the 443-kilometre Norpipe gas pipeline from Ekofisk to Emden in Germany, which became operational in September 1977.
Kjappe fakta::
Compressor platform on Ekofisk-Emden gas pipeline
Installed 1976
Operational 1977
Shut down 29 October 2007
Removed 2013
— Norpipe GNSC-H7. Photo: Husmo Foto/Norwegian Petroleum Museum
Gas received initial compression to 132 bar at the Ekofisk Complex. The pipeline was divided into three equal lengths, with Norpipe GNSC B11 positioned at the end of the first third to maintain pressure as and when required.
From there, the gas then travelled the next third of the distance to the second and virtually identical compressor platform, H7.
This was also responsible for maintaining pressure, but additional compression was seldom required on this final leg of the journey to Emden.
Both platforms stood on the German continental shelf, but 48 kilometres of the pipeline also ran across the Danish North Sea sector.
The pipeline is trenched or covered with sand. On its final approach to the coast of East Friesland, it passes beneath the island of Juist before making landfall north of Emden.
Capacity in Norpipe is about 60 million standard cubic metres (scm) or 2.1 billion cubic feet per day. In addition to output from the Ekofisk-area fields, it carries gas from Valhall, Ula and the Statpipe system – primarily Statfjord and Gullfaks. Gas was also transported for a time from Hod and Gyda, but that has ceased.
fritid, Norpipe GNSC-H7,
Magnus Refsland and Werner Hein have pulled the crab trap (full of starfish) on the Norpipe H-7 platform. Photo: Husmo Foto/Norwegian Petroleum Museum
Built in 1976, the B11 platform had six decks. Its permanent staffing totalled 14 people, but various service personnel were also often on board. The regular crew included three in catering.
The 11 Phillips employees comprised the offshore installation manager, the nurse/radio operator, eight operators and a roustabout.
In addition to their direct function, the operators covered various other trades which meant the crew was self-sufficient in most circumstances.
Both platforms obtained a satellite antenna in 1986 which allowed them to received Norwegian TV, while the 24-bed accommodation were redecorated in 1981 and upgraded in the summer of 1990.
Work on the upgrading largely comprised converting all cabins to doubles with shower and WC. The galley and changing rooms were renewed and changing facilities for women provided.
A new module with a lounge for non-smokers, a smoking room, gym and pool room was also installed. During this work, the West Gamma accommodation rig was positioned alongside.
Upgrading equipment on the platform was also initiated in 1990. While the pipeline’s original daily capacity had been estimated at 2 100 million standard cubic feet, this was found to have declined after a number of years to 1 975 million.
To return to the original capacity, the compressors needed to be upgraded and power supply from the turbines increased. This was done both on the Ekofisk tank and on the H7 and B11 platforms. Gas coolers on the tank were replaced as well.
Norpipe GNSC-H7, yrker, radiooperatør,
Radio operator Torleif Førland on the platform Norpipe H-7, with his amateur radio. Photo: Husmo Foto/Norwegian Petroleum Museum
The control systems were also upgraded in parallel. Control panels on turbines and compressors were replaced and metering instruments installed to conduct measurements in this equipment.
While the nearest neighbour to B11 was a Danish oil field, H7 stood in the middle of the shipping channel. M/S Hero broke down 15 nautical miles west of the latter platform at around 13.00 on 12 November 1977.
By 21.00, the ship was still adrift and heading directly for H7, and all 14 crew on the platform made ready to evacuate by helicopter – the waves were too high for the lifeboats. The wreck passed at 21.40 with a clearance of 400 metres.
German cargo carrier Reint collided with H7 on 30 September 1995, despite efforts by the standby ship to avert the threat. Production was halted as a safety measure, but the platform luckily suffered only minor damage. The collision was caused by inadequate watchkeeping on the ship’s bridge.
Operator responsibility for B11 and H7 was transferred at the beginning of 2003 to Norway’s state-owned Gassco company, which runs the Norwegian gas transport network.
This change had little significance for operation of the platforms, since the actual work was still carried out by ConocoPhillips as a technical service provider to Gassco.
H7 was shut down in 2007, and removal had been completed in 2013. In connection with preparations to remove the structure, operator responsibility was transferred to Statoil as the company in charge of the project on Gassco’s behalf.
Published 24. August 2016 • Updated 22. October 2019
Phillips inundates Sola with oil revenues
person by Kristin Øye Gjerde
Stavanger and neighbouring Sola were the first Norwegian local authorities to experience fantastic oil-related growth after the award of the first exploration licences in 1965.
— Phillips er i ferd med å etablere seg på Norscobasen nederst til høyre Ca 1972 Foto: Norsk fly og flyfoto/Norsk Oljemuseum
The Shell refinery at Risavika in Sola was completed two years later, while the Norsco base in Tananger became operational as early as 1966.
But things really took off once the Ekofisk field had been discovered in the autumn of 1969 and started trial production on 14 July 1971.
Operator Phillips Petroleum Company moved its offices from the Dusavik base outside Stavanger to Tananger in Sola, and Shell could finally start refining Norwegian rather than imported crude.
Sola’s population now rose steadily from 8 400 in 1965 to 15 000 two decades later, and jobs grew even faster – from about 2 000 in 1970 to almost 8 000 in 1985. That averages 10 per cent annually.
Phillips and Shell became cornerstone companies. A large part of their workforce, particularly in Phillips, worked offshore. In addition came newly established oil supply firms.
More jobs were also created in retail, public administration, education, health and social care, personal services and so forth.
Although traditional agriculture remained important for the local authority, the number of farmers gradually declined as a result of mechanisation.[REMOVE]Fotnote: This article is based on the chapter “Elverket i Oljealderen” in I det regionale spenningsfelt. Sola Energi 1913-1999, Kristin Øye Gjerde.
Boreskipet Drillship ligger ved kai på Norscobasen i Tananger (1968). Foto: NOM/Norsk Fly og Flyfoto
Boreskipet Drillship ligger ved kai på Norscobasen i Tananger (1968). Foto: Norsk Fly og Flyfoto/Norsk Oljemuseum
The “agio tax”
The sharp rise in Sola’s revenues was attributable entirely to the oil industry, and it found itself in an enviable position during this period. Tax revenues rose even faster than population and jobs.
To give an indication, the local authority’s overall income from wealth and income taxes rose from NOK 9.3 million in 1966 to NOK 198 million in 1990. The biggest growth came in 1978-82, when it averaged 39 per cent a year.[REMOVE]Fotnote: Sola local authority, plans.
The secret behind this sharp increase was the tax paid by the oil companies – primarily Phillips – on agio, or the percentage fee charged when exchanging one currency for another.
Under Norwegian law at the time, the companies paid tax on their interest income to the local authority where they had their head office. In making this rule, however, the government had failed to take account of the considerable sums involved.
As operator of the Greater Ekofisk Area, Phillips had placed capital to be used for new investment in banks around the world – particularly the UK.
These deposits yielded substantial interest payments, and tax was payable on converting this income into Norwegian kroner.[REMOVE]Fotnote: Toralv Torstenbø, former chief executive officer in Sola local authority, interviewed by Kristin Øye Gjerde, 22 February 2001.
Sola council is said to have almost gone into shock the first time Phillips paid this agio tax. It suddenly had more money than it could spend.
During the 1970s and early 1980s, Sola’s municipal income always exceeded the budgeted amount. Large sums could be transferred every year to a capital fund.
Since the local authority was in a growth phase, additional funding was needed for the big developments it faced. While the rest of Norway experienced a slump in the late 1970s, Sola continued in top gear without a sign of unemployment.
Net income tax revenues came to NOK 55.5 million in 1978, while net spending was NOK 31.9 million. And these fantastic results went on improving.
By 1982, wealth and income taxes yielded NOK 203.4 million – compared with a budget of NOK 146 million, which was upgraded to NOK 190 million during the year.
According to Toralv Torstensbø, the financial controller, agio tax accounted for almost half this amount – in other words, as much as the tax paid by all other enterprises, private individuals and industry in Sola.
Its chief executive officer became a little overweening. In his comments on the 1982 budget, he declared that it would be “natural for Sola local authority to feel a strong regional responsibility and not to be too strict about the traditional division of costs between state, county and local authority.”
In line with this open-handed policy, Sola paid for both road projects and an upper secondary modern school which the county council was supposed to fund.[REMOVE]Fotnote: Chief executive officer’s budget proposal for Sola local authority covering 1974-85.
Tightening up petroleum tax
This unexpected prosperity undoubtedly created some jealously in the neighbouring local authorities, and the media began to show an interest in the issue.
Local daily Stavanger Aftenblad interviewed Sola’s chief executive and controller in 1981, when its photographer took a shot which illustrated the boundless wealth – Torstensbø stood showering hundred-krone notes over his colleague.
This story was not only read by the paper’s regular subscribers. The following day, 150 copies were distributed to members of the Storting (parliament).
That in turn prompted Centre Party representative Lars Velsand to make a passionate speech in which he described the position as a misuse of tax revenues.
He called on the government to intervene so that individual local authorities were unable to benefit in this way. Nor was he alone in finding it unreasonable that a small community like Sola should get so much money.
The result was an amendment to the Petroleum Tax Act on 11 June 1982, which specified that the proceeds from the agio tax should be transferred in future to central government.
Løfteskipet Uglen i aksjon ved Norscobasen i juli 1980. Foto: NOM/Norsk Fly og Flyfoto
Løfteskipet Uglen i aksjon ved Norscobasen i juli 1980. Foto: Norsk Fly og Flyfoto/Norsk Oljemuseum
Unfortunately, however, Sola had got used to consuming these revenues. It is easy to learn expensive habits, but not so straightforward to shrug them off again.
Matters had become a little unusual when the council’s executive board adopted the style of the oil company chiefs and took a helicopter outing during an ordinary budget meeting.[REMOVE]Fotnote: Oskar Goa, former chief technical officer in Sola local authority, interviewed by Kristin Øye Gjerde, 23 October 2000.
However, most of the tax money benefitted the general public. Paying for Sola upper secondary school and new national and county highways is an example of this.
The council also invested on local authority school buildings and community facilities such as the big sports complex at Åsen, with an outdoor athletics ground and two modern indoor arenas. Dysjaland and Tananger also acquired new sports arenas.
A new cultural centre built in central Sola has a distinctive architecture in brick and glass, with a grassed roof to blend with the surrounding Jæren landscape. With two stages and a public library, this became the community’s main venue for events and so forth.
The local authority thereby built up a very good infrastructure. Power cables were laid in the same trenches as water and sewage pipes, a network of cycle lanes was built and street lighting installed.
On the downside, virtually all these investments boosted operating expenses. The council’s running costs rose by an annual average of 30 per cent in 1978-84, with the biggest growth in the last three years of the period.
So the calls by Storting representatives to transfer agio tax receipts from councils to central government represented a real threat to local politicians.
Sola joined forces with other local authorities in the same position, including Stavanger, Oslo and Bærum as well as Rogaland county council.
A delegation met the Storting’s standing committee on finance to present their case, and secured a commitment to accept a phased reduction in revenues over four years.
The local authorities would receive 80 per cent of agio tax receipts during the first year, then 60 per cent, 40 per cent and finally 20 per cent.[REMOVE]Fotnote: Amendment to the Petroleum Tax Act adopted on 14 May 1982.
In reality, however, the run-down percentages were adjusted to extend over five years in annual steps of 80, 60, 20, 20 and 20 per cent. The total amount going to the local authorities was the same.
The arrangement was controversial to the last, and also uncertain because it had to be approved in each annual government budget.
Living within its means
After the tax change, Sola’s chief executive officer saw the writing on the wall. It seemed “to be unquestionable that [Sola] has seen its best days in purely financial terms and must return to setting tougher priorities for various assignments,” he asserted in connection with the budget process for 1983.[REMOVE]Fotnote: Chief executive officer’s budget proposal for Sola local authority, 1983.
It took the politicians a little longer to accept this reality, but they were forced to reduce investment and operating expenditures in the years which followed.
Cutting back on the new sports arenas and cultural centre was not very desirable. Nor was it pleasant to have to slow down. But savings had to be made, and long-terms spending plans were removed from the budget for possible reintroduction later.
A raft of measures were stripped from the budget in 1985, such as extensions to and modernisation of schools, sports arenas and swimming pools, a new somatic nursing home, housing for the intellectually disabled and sheltered housing. Grants for national and county roads were reduced.[REMOVE]Fotnote: Chief executive officer’s budget proposal for Sola local authority, 1985.
Once the government’s compensation scheme had ended, Torstensbø – now chief executive officer – told Stavanger Aftenblad that he did not want to paint too gloomy a picture.
“But it’s clear that we must set much more moderate financial priorities than we’ve been used to. To sum up the position, we were previously flush with cash and poor in facilities. We’re now flush with facilities and poor in cash.”[REMOVE]Fotnote: Stavanger Aftenblad, ”Alt blir dyrere i det rike Sola”, 19 May 1987.
Sola kulturhus fotografert vinteren 2004
Rogaland county council also raised the question of whether it would be possible to establish a permanent arrangement which allowed local authorities and counties to benefit from some of the tax revenues paid by local oil companies.
The council pointed out that it was otherwise normal practice for Norwegian companies to pay taxes to the local communities they were based in.
This request was turned by Labour finance minister Gunnar Berge because the councils concerned still benefitted from bigger tax payments by oil company employees and on property.[REMOVE]Fotnote: Stavanger Aftenblad, “Rogaland reiser skattekrav på ny”, 16 January 1988.
According to Torstensbø, this was only partly true. The big oil companies were not so significant for Sola’s income once the agio tax was excluded.
About NOK 2 million was received annually from Phillips, primarily in property tax. The most important taxpayers in the local authority were the roughly 90 companies at Aker Base. These were service providers such as Halliburton, Schlumberger and Baker Hughes.
At the same time, Sola acquired a steadily growing number of affluent residents and a growing share of its revenue came from income tax. Despite the cut-backs, it remained prosperous.
Published 29. July 2019 • Updated 29. July 2019
More about economy
Participants in Ekofisk
The question of who “owns” Ekofisk is not straightforward. In simple terms, however, the field and the rest of Norway’s continental shelf (NCS) belongs to the Norwegian state. This was determined on 14 June 1963, when the Storting (parliament) passed the Act Relating to Exploration for and Exploitation of Submarine Natural Resources. This permits licences to be awarded on certain terms.
Riding out the oil crisis
The greatest-ever oil bonanza, with oil prices hitting USD 130 per barrel, came to an abrupt end in 2014, when the cost of a barrel of crude slumped to less than USD 50 from June to December. And the bottom had still not been reached – this was only the start of a new oil crisis which lasted several years. What effect did this have on ConocoPhillips’ financial position off Norway?",['The water depth in the Greater Ekofisk Area is 70-75 meters.'],6625,multifieldqa_en,en,,18ef34b54d2ddc134e1be7cae3d6101432465011d016c77a," Ekofisk began production on 15 June 1971, following its discovery in the autumn of 1969. The water depth in the Greater Ekofisk Area is 70-75 metres. The rate of seabed subsidence has declined sharply in recent years. The recovery factor on the field has risen from an original estimate of 17 per cent to over 50 per cent. The field is intended to produce until it is 2028 and is tied back to the Eldfisk and Embla complex, which was shut down in December 2015, as well as Tor and Tor Embla, which will remain in place until it can be developed. The Ekofisks West project was presented to improve the recovery and recovery capacity on Ek ofisk West in 2003, but it was cancelled in December 2014. The development of the West project is expected to be primarily focused on oil and gas processing. The project is also expected to increase the recovery factor and improve the reliability of the gas recovery process. The total number of fields in the area is 31, including Tommeliten G, Albuskjell, Cod, Edda, Tor, West Ekofsk and Tommel iten G. These fields all lie within production licence 018 apart from TommelIten G which was operated by Statoil from 1976 to 2003. The three are operated by ConocoPhillips on behalf of the EkOfisk licensees. The licence was extended to 2028 in August 1998 and became operational in August 1996 and is intended for production until it was 2028. The first phase of development and production began with initial oil output from the converted Gulftide jack-up rig in 1971 and ended with the start-up of Ekofis II in 1998. The gas has been landed by pipeline at Emden in Germany from 1977. The oil has been piped to Teesside in the UK from 1975, with oil initially being buoy-loaded into tankers. In the early 1970s, the oil was pumped to Ståle Ådland in Norway and then to Emden, where it was delivered by pipeline to the UK. In 1985, the steel platforms were jacked up by six metres to safeguard the installations against the effects of this development, and a protective breakwater was installed around the tank in 1989. Two platforms owned by other companies – Ekofisc 2/4 G and 2/ 4 S – have also gone. In addition come the Edda 2/7 C, Al Buskjell 1/6 A and Cod 7/11 A, Al buskjell 2-4 F, Cod 7-11 A and West Ek ofsk 2/2 D, Norpipe 36/22 A and Norpipe 37/4 A installations. In all, 31 installations have been positioned in theGreater Ekofks Area. The area also embraces former producers Albukjell,. Edda,. Tor, and Tor, which were operated by the Statoil company from 1976-2003. It is intended that the field will be used for oil exploration and production until the end of the 20th century. It will also be used to produce gas and natural gas from the 2030s and 2050s. It has been named the ‘Norway’s largest offshore oil field, with a total of 1.2 billion barrels of oil equivalent (boe/day) in reserves. It also has the highest recovery factor in the world (50 per cent) in terms of the share of petroleum in place actually produced. It was the first Norwegian offshore field to go into production. The terminal at Teesside in north-east England receives oil and natural gas liquids (NGL) by pipeline from the Ekofisk field. It comprises stabilisation, NGL fractionation, storage tanks for crude oil and an export port. NGLs go to the fractionation facility, with a daily capacity of 64 000 barrels, for separation into methane, ethane, propane, and normal and iso butane. The methane (natural gas) is used to fuel the plant, while the other products (now known as liquefied petroleum gases – LPG) are made liquid by cooling and stored for export by sea. The terminal has four of these, with those for crudeOil able to handle tankers up to 150 000 deadweight tonnes. The LPG quays can accept carriers loading as much as 60 000 cubic metres. They are operated by Ruhrgas, BEB and Gastunie Gas Services (previously Gasunie) They connect to the pipeline systems to connect the terminal to the buyer receiving pipelines and distribution network. The gas enters the top of a column and leaves through the base after passing through a filter and oven. That leaves the gas ready for sale, and it is piped to the buyers and the fiscal fiscal station. The North Sea gas is very clean, and needs only limited treatment to remove small amounts of sulphur compounds using an absorption process from the gas accumulate on the surface of small particles, which act as a process filter. The process is called Impure Impure. It is used on the North Sea to remove large amounts of Sulphur molecules using an absorbant process. It can also be used to remove the small amounts on the gas surface from the surface which accumulate on. the surface. It has been used to. remove small. amounts of. sulphur molecules on the north Sea gas. from the north sea gas. The four process columns and ovens act as process filters and act as ovens for the gas to be processed. The natural gas is then piped away on the pipe away on be the gas pipe to be sold on the market. The pipeline systems connect the gas pipeline to the commercial pipeline systems and connect the two commercial systems to the gas receiving network. This means that the gas can be metered, metering and distribution is carried out on a single pipeline. It also means that it can be transported by sea and on land. The terminals are located on the main channel of the River Tees, and the others have been installed in dredged docks. In 2005, 2/4 M was linked to the Ek ofisk Complex with a bridge. The latter could contribute to improved recovery both because there were more wells and because they would tap new locations in the reservoir. In addition come 42 new wells as well as upgrades to existing platforms which extend their commercial life. A third measure concerned laying a power cable to make electricity supplies more efficient. The plan for development and operation (PDO) of Eldfisk II, approved by the Storting (parliament) on 9 June 2011, includes a new wellhead, process and accommodation platform – Eld fisk 2/7 S. M, with processing facilities and 24 new wells drilled over five years. A new subsea water injection facility and 44 additional wells were also planned. The PDO for Ekofsk South involves the construction of a new. wellhead platform – Ekfisk 2-4 Z – aswell as a new Subsea water injections facility. The Norsea Gas Terminal in Emden was officially opened in September 1977. A substantial proportion of Norway’s gas deliveries to Germany continues to arrive at the Emden terminal. The pipeline linking Ekofisk with the terminal for oil and natural gas liquids (NGL) at Teesside on the north-east English coast became operational in October 1975. In the UK sector, a 24-inch pipeline has been tied in with a Y connection to receive input from several British fields – including the J block developments operated by ConocoPhillips. The new terminal has not expanded export capacity. But its functionality is well adapted to future processing needs for fields in the Greater Ek ofisk Area and other parts of the NCS sending gas through the Norpipe system. The terminal has a daily capacity of about 2.1 million cubic feet of gas per day. It is owned by Norpipe Oil AS and operated by Norway Gas A/S. It has a capacity of around 2.5 million barrels of oil per day, but that capacity has never been required. It was opened on 24 May 2016 by Elisabeth Aspaker, the Norwegian government minister for the EU and the European Economic Area. The Gassled partnership is a project collaboration embracing 10 companies which collective own large parts of. the gas infrastructure on the Norwegian continental shelf (NCS). It was formed in 2003 to improve the efficiency of the gas transport system for the Norwegian offshore sector. It also aims to optimise pipeline utilisation and thereby boost value creation. The NCS produces gas and oil from Valhall, Hod, Ula and Gyda, as well as from other fields on the Norway and British continental shelves. The three fields are also tied together by pipelines, which are used to transport oil and gas from the UK to the UK and vice-versa. The E&P company that owns the E&C company, Norsk Gas, is based in Norway and has offices in Germany and the UK. The company was founded in 1973 and is one of the world's largest gas trading companies. It employs more than 2,000 people. It operates a network of pipelines, terminals and other facilities across Europe and the Middle East. In 2013, the company announced plans to expand its operations in the UK by opening a new terminal on the North Sea coast. The project is expected to be completed by the end of 2015. It will be the first new terminal to be built in the EU in more than a decade. The facility will be located in the German port of Emden, near the city of Stuttgart, and will be open to the public by 2017. It can handle up to 2 million barrels per day of oil and NGL at a cost of around £1.5 billion per year. The Emden facility is the only one of its kind in the world and is located in a strategically important region for the gas industry. The German port is also home to a number of other gas-producing fields, such as Vlissingen and Stromsberg. Eldfisk 2/7 S is a remotely operated wellhead facility located 5.2 kilometres to the north of the Ekofisk Complex. The platform has six production wells and an average daily output of roughly 7 000 barrels of oil. The Embla reservoir has a temperature of 160°C compared with the 125°C normally found in the chalk formations 1 000 metres higher up. All processing and metering took place on Eldfisk 1/7 until 2015, and has now been switched to Eld fisk 2-7 S. The platforms are located in the Norwegian section of the North Sea, which is part of the Norwegian Continental Shelf. They are located between the islands of Stavanger and Gulleg in the north and the island of Juist in the south. For more information about platforms in the region, visit the Ek ofisk Complex website or visit the ConocoPhillips Ek of the Sea website or the Norwegian Oil and Gas Association’s Ek of The North Sea website. For information about oil and gas in Norway, see the National Oil and Natural Gas Administration’S Ek ofThe North Sea webpage. For details on oil and natural gas exploration in Norway see the Norwegian National Oil & Natural Gas Authority’ s Ek ofthe North Sea Exploration and Drilling Program (EoNSP) and the Norwegian Offshore Drilling Programme (EODP). For information on gas and oil exploration in the area, visit EoNPS’ website. for details on the National oil and Natural gas exploration and drilling program (EONSP), and for information about gas and gas production in the Greater Ekofisks Area, see EoNNSP’eskofisk Area (EKofisk A, EKOA, E KOA and E KA). For more on oil exploration and production in Norway visit the Norwegian Petroleum Museum. For. more information on oil drilling in Norway and the rest of the world, visit: http://www.nps.org/uk/ oil-and-natural-gas-exploration/ Norway-Oil- and-Gas- Exploration-Pipeline.html. for more details on drilling in the Arctic and the Middle East, see: http:// www.npr.org.uk/news/article/stories/2014/07/08/15/npr-news-top-stories-oil-drilling-pipelines-in-the-north-east-of-nord-sea.html#storylink=cpy. The first production well was not drilled and tested until 1988, followed by a second in 1990. Both yielded very promising results, and the field came on stream in May 1993. The field comprises a sandstone reservoir at least 250 million years old. It stands 84 metres high and weighs 2 300 tonnes. The wellhead area, helideck (built by Vindholmen Services in Arendal), crane, control room, workshop, test separator and glycol pump. It was converted to cope with conditions on Stavavanger near Åmøyjord near Stavang. This jack-up rig was built in 1967 for Ocean Drilling & Exploration Co. in the Gulleg/Norwegian Petroleum Museum (Gulleg Co. Verker Verker) This jack up rig was converted in the 1990s for Oceandrilling & exploration Co. Ekofisk was discovered in December 1969 and production began in June 1971. Gulftide was a temporary production installation until 1974. The Ekofisk 2/4 A (Alpha) and 2-4 B (Bravo) platforms were built in 1973 and 1974. In addition came the Ekofsk 2/ 4 C (Charlie) production, drilling and compression facility, and the Ek ofisk 2.4 FTP (field terminal platform) for production and risers. The rig was equipped with process equipment and its derrick, helideck, hangar and legs were reinforced. Oil and gas were produced by two wells for processing in their separation plants and piping for a three-stage separation process. The gas was flared off and the oil sent on by flowlines to two loading buoys where shuttle tankers moored to take on cargo. The facility was extremely advantageous, because it allowed production to continue virtually regardless of whether bad weather prevented tankers from connecting to the buoys. It was replaced by a permanent installation in 1974, which is still in operation today. It is expected to be fully operational by the end of the year, with output estimated for the main field at around 1.5 million barrels of oil per day. It will be the largest oil field in the world at the time, according to the Norwegian National Oil Company (Norsk Oljemuseum) It will also be one of the biggest oil fields in the history of the world, overtaking Saudi Arabia as the world's largest oil producer in the early 1980s. The field is now home to one of Norway's biggest oil refineries, with production estimated to be around 1 million barrels per day (1.5 billion cubic feet per day) by the time it is fully operational. It also has the largest gas reserves in the country, with more than 1.2 million cubic feet of gas per day, or 1.4 million barrels a day (2.2 billion cubic metres) of natural gas per well. It has been the source of much of the Norwegian oil wealth since the 1970s, when it was discovered by Phillips Oil & Gas. The company is now the world’s largest oil and gas producer, with a market value of more than $100 billion (US$80 billion) per year. The Norwegian National oil company is also the largest natural gas producer in history, with the majority of its output coming from the North Sea oil field, which it has operated since the 1960s and 1970s. It plans to expand its operations in the coming years, with plans for two new platforms and a third in the 2030s and 1940s. In the future, the company plans to build a new oil refinery in the Norwegian city of Stavanger, which will be located on the island of Hordaland, on the east coast of Norway. It hopes to be able to produce 1 million tonnes of oil a year by 2050. It currently has a total production of 1.6 million tonnes per year, or around 2.5million barrels per year (1,000 million tonnes) of oil and natural gas. The project is in its early stages, but the company is still planning to expand it in the next few years. The first of the new platforms is due to be completed in the late 2020s or early 2021. Gulftide was converted in the Åmøy Fjord outside Stavanger to cope with conditions on Ekofisk. Production began cautiously on 8 July 1971 from a single well. The second producer came on stream that September, the third was ready the following month and all four were producing by February 1972. They each flowed 10 000 barrels of oil per day at the time. Production was suspended for 20 per cent of the first year for this reason. The oil was flared off and the oil was piped to one of the loading buoys where a shuttle tanker was moored. Production could only take place when a ship was present. The field was discovered in 1969 and the rig was built in 1970. It is now owned by Norway's state-owned oil company, Norges Petroleum, and is operated by Norway’s second-largest oil producer, Statoil. It has been the source of much of the oil and gas production in the North Sea since the 1970s and 1980s. The company has also been involved in the development of the Norwegian deep sea rig, the Barents Sea rig, and the Norwegian Deepwater Horizon oil rig, which are still in operation today. The rig is still being used as a compressor station to maintain the pressure in the Emden gas pipeline from Germany to Norway. It was also used to pump gas from the Karmen gas pipeline to Norway in the 1980s and 1990s. It also serves as a pump station for the Norpipe gas pipeline, which became operational in September 1977 and shut down in October 2007. The platform is still in use today and is used to transport natural gas to and from the Norwegian port of Kristiansand, where it is being developed into a liquefied natural gas facility. The gas pipeline is still operational and is expected to be fully operational by the end of next year. It will be the first of its kind in the world and will be connected to the rest of the world by a long-distance pipeline that will be built in the 2030s and 2040s. For more information on the Ek ofisk oil field, visit the Norwegian Petroleum Museum’S website. For information on how to get your hands on a copy of the book, visit: http://www.norges-petroleum.org.uk/Ekofisk-Discovery-and-Exploration-Report.html/. For more on the history of the field, see: www.norsk-petrol-museum.com/ekofisk/ekoffisk-discovery.html. For. more information about the company and its history, visit www.Norway-petrochemicals.uk/. For. the Norwegian Oil and Gas Association, see www.norwegianpetrochemical.org/ekoffisk.uk. for more details on the company's history and information on its activities, including how to obtain a copy, visit http:// www.NorgesPetrochemistry.com/. For the full story, see the Norwegian Energy Museum's website: http:/www.NorskPetrochemical-Museum.org/. For a. more detailed account of the history and development of Ekofsk, see here: http:\/www Norwegian Petroleum Museum.com /. For an. overview of the Ek Ofisk field, click here:http:www.NorwegianPetroChem Museum. and http: /www.norway-epm. andhttp:/news/index.php/. Norpipe GNSC B11 and H7 stood on the German continental shelf, but 48 kilometres of the pipeline also ran across the Danish North Sea sector. Both platforms obtained a satellite antenna in 1986 which allowed them to received Norwegian TV, while the 24-bed accommodation were redecorated in 1981 and upgraded in the summer of 1990. The pipeline’s original daily capacity had been estimated at 2 100 million standard cubic feet, but this was found to have declined after a number of years to 1 975 million. In addition to output from the Ekofisk-area fields, it carries gas from Valhall, Ula and the Statpipe system – primarily Statfjord and Gullfaks. German cargo carrier Reint collided with H7 on 30 September 1995, despite efforts by the standby ship to avert the threat. Production was halted as a safety measure, but the platform luckily suffered minor damage. The collision was caused by inadequate watchkeeping on the ship's bridge. The platform was transferred at tOperator responsibility for B11 was transferred to H7 at the end of the 1990s, and the two platforms are still in use today. The H-7 platform stood in the middle of the shipping channel, and all 14 crew on the platform made ready to evacuate by helicopter – the waves were too high for the lifeboats. The wreck passed at 21.40 with a clearance of 400 metres, and it is still not known if the platform will ever be used as an oil rig again. The B11 platform had six decks. Its permanent staffing totalled 14 people, but various service personnel were also often on board. The regular crew included three in catering. Gas was also transported for a time from Hod and Gyda, but that has ceased. Gas coolers on the tank were replaced as well. The galley and changing rooms were renewed and changing facilities for women provided. A new module with a lounge for non-smokers, a smoking room, gym and pool room was also installed. During this work, the West Gamma accommodation rig was positioned alongside. The West Gamma rig is now positioned alongside the H7 platform, and is located in the Danish port of Emden. It has a capacity of 2.1 billion cubic feet per day. It was built in 1976 and has six decks, with a total staffing of 11 Phillips employees, including the offshore installation manager, the nurse/radio operator, eight operators and a roustabout. It is still in service today and is based in the port of Friesland, Norway. Its capacity is about 60 million standard. cubic metres (scm) or 2.2 billion cubic ft per day, and this has been increased to 2.3 billion. It also has a total capacity of 1.9 billion cubicft per day and this was increased to 1.8 billion in the 1980s and 1990s. The two platforms have a total of 14 decks and a permanent staffing of 14 people. The platforms are located in a shipping channel in the Norwegian North Sea, and are in the northern part of the North Sea. Sola and neighbouring Stavanger were the first Norwegian local authorities to experience fantastic oil-related growth after the award of the first exploration licences in 1965. The Ekofisk field had been discovered in the autumn of 1969 and started trial production on 14 July 1971. Sola’s population now rose steadily from 8 400 in 1965 to 15 000 two decades later, and jobs grew even faster – from about 2 000 in 1970 to almost 8 000 in 1985. Tax revenues rose even faster than population and jobs. The secret behind this sharp increase was the tax paid by the oil companies – primarily Phillips – on agio, or the percentage fee charged when exchanging one currency for another. Phillips had placed capital to be used for new investment in banks around the world – particularly the UK – which yielded substantial interest payments, and tax payable on converting into Norwegian kroner. Former chief executive of Sola local authority, Kristin Øye Gjerde, said to have almost gone into shock when Phillips paid this agio tax for the first time in the 1970s and 1980s. The biggest growth came in 1978-82, when it averaged 39 per cent a year. The “agio tax” was attributable entirely to the oil industry, and it found itself in an enviable position during this period. It could suddenly have suddenly spent more money than it could spend on the local authority’Sola council is now planning to build a new city centre. The city centre is due to be completed by the end of next year, and Sola will have a new mayor in place by the start of 2015. The new mayor will be the first female mayor of the city since the 1980s, and the first since the 1990s. It is hoped the city will be able to attract more young people to Sola, which has been hit hard by the downturn in the oil and gas industry. The local authority is also planning a new town centre, which will be built on the site of a former railway station. The town will be named after a former mayor, who died in a car crash in 1998. The current mayor is the son of the former mayor and the daughter of a local council member, who was killed in a road accident in 2000. It will be called Sola Energi, after the town's former chief executive. It has been renamed Sola-Energi since the name was changed in 1999 to reflect the new name and the new mayor's role in the development of the town. It also has a new chief executive, who has been in charge since the beginning of 2003. The mayor is now the head of the Sola City Council, and she is also the chief of the local council. The council plans to build the new city center in the summer of 2015, and a new community centre in the winter of 2016. The community centre is expected to be built in the spring of 2016, and will be completed in the fall of that year. In the past, Sola has been the only city in Norway to have a city centre with a central planning authority, which was established in the 1960s and 1970s. This will be followed by a new regional planning authority in the late 1990s and early 2000s, to be based in the city of Bergen. The Sola city centre will also be the subject of a new project by the Norwegian Ministry of the Environment. In 1978, income tax revenues came to NOK 55.5 million in 1978, while net spending was NOK 31.9 million. By 1982, wealth and income taxes yielded NOK 203.4 million – compared with a budget of NOK 146 million. Sola paid for both road projects and an upper secondary modern school which the county council was supposed to fund. The result was an amendment to the Petroleum Tax Act on 11 June 1982, which specified that the proceeds from the agio tax should be transferred in future to central government. But Sola had got used to consuming these revenues. It is easy to learn expensive habits, but not so straightforward to shrug them off again. A new cultural centre built in central Sola has a distinctive architecture in brick and glass, with a grassed roof to blend with the surrounding Jæren landscape. With two stages and a library, this community’s main venue for events and so forth became a very public public authority. Power cables were laid up in the same trenches and sewage pipes, a network of cycle lanes was built. The local authority built up a very good infrastructure. And these fantastic results went on improving until the 1970s and 1980s, when the local authority was in a growth phase, additional funding was needed for the big developments it faced. In the 1980s and 1990s, Sola was in top gear without a sign of unemployment. But it became a little overweening when the council’'s executive board adopted the style of the oil company chiefs and took a helicopter outing during an ordinary budget meeting. The council also invested on local authority school buildings and community facilities such as the big sports complex at Åsen, with an outdoor athletics ground and two modern indoor arenas. But most of the tax money benefitted the general public, with Sola upper secondary school and new national and county highways is an example of this. In 1981, when its photographer took a shot which illustrated the boundless wealth – Toralv Torstensbø stood showering hundred-krone notes over his colleague. The following day, 150 copies were distributed to members of the Storting (parliament) The story was not only read by the regular subscribers, but also by the Stavanger Aftenblad. In 1982, the government decided to tighten up the tax system. It specified that proceeds from agio taxes should be transfer to the central government in future, and that Sola would not be able to benefit in this way. The Sola local authority became the very public authority that had built up the public infrastructure in the first place. It also invested in a cultural centre, a public library, a cycle network, a library and a sports complex. It was also the first time a local authority had built a large sports complex in the north of the country. In 1983, the council built a new cultural center in centralSola, with two stages, a grass-roofed roof, and a grassing roof, to blend the surrounding landscape. It has since been renamed Sola Uglen i aksjon ved Norscobasen i juli 1980. Sola joined forces with other local authorities in the same position, including Stavanger, Oslo and Bærum as well as Rogaland county council. A delegation met the Storting’s standing committee on finance to present their case, and secured a commitment to accept a phased reduction in revenues over four years. The local authorities would receive 80 per cent of agio tax receipts during the first year, then 60 per cent, 40 per cent and finally 20 per cent. In reality, however, the run-down percentages were adjusted to extend over five years in annual steps of 80, 60, 20, 20 and 20%. The total amount going to the local authorities was the same. The arrangement was controversial to the last, and also uncertain because it had to be approved in each annual government budget. It took the politicians a little longer to accept this reality, but they were forced to reduce investment and operating expenditures in the years which followed. A raft of measures were stripped from the budget in 1985, such as extensions to and modernisation of schools, sports arenas and swimming pools, a new somatic nursing home, housing for the intellectually disabled and sheltered housing. Grants for national and county roads were reduced. The most important taxpayers in Sola were the roughly 90 companies in the local authority were Phillips, primarily in Aker Base. These were service providences at Aker.nd street lighting installed. The council pointed out that it was otherwise normal practice for Norwegian companies to pay taxes to local communities they were based in. The big oil companies were not so significant for Sola's income once the agio. tax was excluded. The biggest growth in the last three years of the period was received annually from Phillips,primarily in NOK 2 million in property tax. It seemed “to be unquestionable that [Sola] has seen its best days in purely financial terms and must return to setting tougher priorities for various assignments,” he asserted in connection with the budget process for 1983. To sum up the position, we were previously flush with cash and poor in facilities. We’re now flush with facilities andpoor in cash.” – Torstensbø – now chief executive officer, Sola local authority, 1983. ‘We must set much more moderate financial priorities than we’ve been used to,’ he said. “We’ll have to cut back on the new sports arenas, cultural centre and nursing home.’ ‘It was not very desirable. Nor was it pleasant to have to slow down. But savings were made, and long-terms spending plans were removed from thebudget for possible reintroduction later.‘ ‘I’m not sure I’d want to go back to the old way of doing things. I don’t think it would be very good for the country to go through the same thing again. It would be a lot of work to get back to where we were in the 1970s and 1980s. ’‘I don't think it’ would be good for Norway to have the same level of oil production as it did in the 1980s and 1990s. ders such as Halliburton, Schlumberger and Baker Hughes. At the same time, Sola acquired a steadily growing number of affluent residents and a growing share of its revenue came from income tax. Despite the cut-backs, it remained prosperous. The greatest-ever oil bonanza, with oil prices hitting USD 130 per barrel, came to an abrupt end in 2014, when the cost of a barrel of crude slumped to less than USD 50 from June to December. And the bottom had still not been reached – this was only the start of a new oil crisis which lasted several years. What effect did this have on ConocoPhillips’ financial position off Norway? The state owns the Ekofisk field and the rest of Norway’s continental shelf (NCS) This was determined on 14 June 1963 when the Storting (parliament) passed the Act Relating to Exploration for and Exploitation of Submarine Natural Resources."
What were the vaccines trialed against?,"A special tribute to Del Bigtree (pictured) and his team at ICAN for his stunning 88 page letter to the HHS regarding vaccine safety. As Del reported - in the latest edition of Highwire - the letter, in response to an earlier reply from the then acting Director National Vaccine Program Office, Melinda Wharton, took virtually a year to compile, and is a meticulous piece of research. Most sensationally they researched the HHS claim through US government archives that at least some pediatric vaccines had been trialed against genuine placebo, and came to a negative conclusion. Not only that, they established that none of the vaccines those vaccines had been trialed against had ever been trialed against genuine placebo either. At the end of the line the toxic products were only being compared with other toxic products, rather than against saline.
Leave aside the sceptics, for any believer in the vaccine program as a necessary intervention in public health, this should be a devastating finding. Fundamentally, the research into the safety of any of the products before marketing was simply not there. The manufacturers apparently had no faith that their proto-products could withstand this scrutiny, and for the rest they just did not care: under the alleged imperative of protecting the population it seems anything went. So even before all the sham monitoring procedures and reviews which Del and his team dismantle in forensic detail we are left with the proposition that none of the present products being given to US children – and frequently other children across most of the developed world – have any meaningful pre-marketing safety data all. If you are believer in the program you have been let down: if you wanted a program with any pretensions to safety - supposing such a thing to be possible - it looks like you would have to start from scratch. The manufacturers did this: the governments, the politicians and the regulators (internationally) let it happen.
This damning document is published simultaneously with a demand in the UK from the Royal Society for Public Health (which I had never heard of) to shut down comment about vaccines on the web. It echoes calls from Seth Berkley of GAVI, Heidi Larson of the Vaccine Confidence Project and the European Parliament. The pamphlet airily dismisses concerns that vaccines have side effects or that you could possibly have too many. It is pure public relations, and if the RSPH claims to be ""independent"" it also admits that the publication was paid for by Merck, a detail which was reported by British Medical Journal and the Guardian, but not true to form by the BBC. We have, in truth, been building to this moment for two decades: as the evidence piles up that every single aspect of the program lacks integrity or is simply rotten to the core all the perpetrators can do is call for the silencing of their critics, and maintain the products are safe because they say so.
Please help give the ICAN letter the widest possible distribution, particularly to politicians.
""The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system.""
Nope. This makes no sense. Lots of people who seemed vibrant will get a very severe case of the same illness that a vulnerable baby overcomes in a day.
And under the germ theory it doesn't matter how strong your immune system *was*. Once it's been overcome by the pathogen it is every bit as weak as anybody else's with that pathogen.
What you say makes no sense. There's no reason for me to reply to you again.
""Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared?""
Why do you keep asking this question when I've already provided the answer hundreds of times? Why are you so desperate to believe the people who you already recognize are harming our children?
Why would Walter Reed be any more trustworthy than Paul Offit or Senator Pan? Why would Jenner or Pasteur?
And you went no way to explaining my arguments against germ theory. If we are attacked by billions of viruses every day then if even a tiny fraction of them are pathogenic then we couldn't possibly survive. And even if we could, we would already be immune rendering every vaccine pointless. Once we had survived our first few days on earth, then we could never get sick again.
If that's wrong then we must conclude that precisely 0% of germs are pathogenic.
Plus your comment about the immune system completely misunderstood my point. The immune system does not allow us to overcome our math problem. In fact, it makes it worse.
You did provide one solitary example of a patient with what are presumably yellow fever symptoms but you didn't say whether they had been given any toxic medical treatments.
And like I said before, the whole ""incubation period"" is more than a little suspicious. Clearly they never found what they thought they would and just rigged the results to tell them what they want to hear.
Like every other germ theorist/vaccine promoter in history.
Many kinds of bacteria are constantly evolving and changing, like flu viruses. Others are more stable over time, like the yellow fever virus. Those that change develop new ways of infiltrating the cells of the organism being attacked (from our point of view, from its unconscious point of view, it's just carrying out its need to replicate, which it can only do inside the cells of its host). The changes which allow it to better infiltrate are more successful and result in more viruses with those traits.
Our immune system is designed to detect and destroy potentially dangerous invading pathogens. Many bacteria are usually harmless and absolutely necessary. The minority are dangerous, and most people's immune systems do a good job of analyzing them and killing them, often with no signs of disease. Others experience a clinical infection, and the immune system usually mounts a successful attack on them.
The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system. Vaccines are usually effective in giving immunity to the targeted diseases. They also have many dangers which everyone should be aware of, and vaccines should be avoided whenever possible. But in the case of the most dangerous diseases, everyone should learn about them and think about what he wants to do to protect himself and his children from them, considering all the factors involved. And no one can have 100% certainty that he has made the right decision, but that's life. But if you live in the Congo and many people around you are currently dying of yellow fever, then that means that you yourself are at risk of being bitten by a loaded mosquito and getting, often dying, of yellow fever. The yellow fever vaccine is very effective at preventing yellow fever. From there, each person must make a choice.
At the end of this stage there is a remission of two or three days. About 80% of those with clinical disease recover at this point, with permanent immunity. The other 20% enter the toxic stage, with a return of the fever, black vomit (coffee-ground emesis), diarrhea, a slowing of the pulse (Faget's sign), jaundice, yellow eyes, yellow skin, and failure of the kidneys, liver, and heart. The patient gets a strange hiccup (like with Ebola, a related disease), falls into a coma, and dies. About half of those patients who enter the toxic stage dies, even now, even with the best of hospital care. The Faget's sign can also occur at the end of the first stage.
You asked specifically about the symptoms of the Americans on Dr. Reed's team who got yellow fever in Cuba in 1900. I'll give the passage from The American Plague (162-5), which describes the course of Jesse Lazear's illness. ""In his logbook, Lazear wrote an unusual entry on September 13. In all cases before those, page after page of records, Lazear had used the soldier's name and simply the date he was bitten, with no other attention to the mosquito. A one-line entry with a name and a date. On that day, however, in his elegant hand, Lazear did not write the soldier's name, but instead wrote 'Guinea Pig No. 1.' He went on to write that this guinea pig had been bitten by a mosquito that developed from an egg laid by a mosquito that developed from an egg laid by a mosquito that fed on a number of yellow fever cases: Suarez, Hernández, De Long, Ferández. It was a precise, detailed history that proved beyond doubt that the mosquito was loaded with the virus when it bit a healthy soldier...(If he had entered his name, then his death would have been considered medical suicide by the insurance company, and his wife and two children would not have gotten any payment.) For the next few days, Lazear's life continued much as it had over the last few months in Cuba. He fed and cared for the mosquitoes in the lab. ..Then he began to lose his appetite. He skipped a few meals in the mess hall. He didn't mention it to anyone, nor did he ask to see one of the yellow fever doctors; instead, he worked hard in the lab trying to ignore the oncoming headache.
""On September 18, he complained of feeling 'out of sorts,' and stayed in his officer's quarters. His head pounded and L. decided to write a letter. ..(he wrote to his mother, and referred to his one-year old son Houston and the baby his wife Mabel was about to have: they were staying with his mother in the US). ..That night, L. started to feel chilled as the fever came on. He never went to sleep but worked at his desk all through the night, trying to get all the information about the mosquitoes organized. By morning, he showed all the signs of a severe attack of yellow fever. The camp doctors made the diagnosis, and L. agreed to go to the yellow fever ward. ..L. was carried by litter out of the two-room, white pine board house in which he had lived since he and Mabel first arrived in Cuba. ..(In the yellow fever ward, in a separate one-room building), Lena Warner (the immune nurse who had survived the yellow fever in 1878, when she was nine, and was found in her boarded-up house by a former slave who first thought she was dead, and carried her to safety) nursed J.L., recording his vitals. (I put up a link to his case record and vital signs last week. The surgeon general required that this record be made for every yellow fever patient.)... (On September 25,) Lena Warner braced L's arms with all of her weight, shouting for help. Still he bolted from the bed, darting around the small frame-wood room as wildly as a trapped insect beating against glass. Two soldiers ran into the ward, pinning L to his bed, tying restraints around his wrists and elbows. ..Warner sponged his body with iced whiskey and water. She recorded his temperature, which had held at 104 degrees for days, on the chart beside his bed. ..(Warner watched him sleep.) But the quiet did not last. L's body began to lurch, and black vomit rolled from his mouth; through the bar hanging above his hospital cot. He writhed in the bed, and his skin grew deep yellow. His 104 temperature slowly fell, leveling out 99 degrees, and JL died at 8:45 p.m. at the age of thirty-four.""
As is obvious, there are many problems with vaccines. But, that being said, most of them usually work for a period of time to prevent the targeted diseases. The basic science behind vaccines is correct. Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared? In the case of the routine childhood diseases, this was a bad thing, but it is a true thing.
Vaccines usually don't cause any obvious reactions. While they usually prevent the diseases, and that's why people continue to get them. With the increasing vaccination schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk.
Your article said as though it had any probative value that 90% of those who get pertussis had been vaxxed. The old DPT vaccine was MUCH more effective at preventing pertussis, but it was so dangerous (again, not to most, but to many), that developed countries replaced it with the acellular version, DTaP. From the beginning about twenty years ago, it was clear that it was not very effective and that huge numbers of vaxxed people got pertussis anyway, including my daughter who got pertussis at eight month old after having gotten three DTaPs. The pertussis vaccine continues to be very dangerous, and I do not recommend that anyone get it. It used to be a killer disease, but evolved to become much milder, to the extent that the disease is very rarely dangerous (usually only to newborns under three months old), while the vaccine is very dangerous. And they're trying to see how they can go back to the old DPT. This does not show that vaccine science has collapsed, but rather that the vaccine they developed to replace the DPT turned out to be much less effective than they first thought, while continuing to be much more dangerous than they first thought.
Your article extrapolated from that that modern medical science in general has collapsed, but that, again, is going too far. A older woman in Mexico City who is like my mother to me had a pacemaker inserted about two months ago to aid her failing heart, and it has restored her to optimism and energy, when she was despondent, weak, and close to death. I took my daughter to the dentist yesterday, who said she has three wisdom teeth coming in and that she said that the lower right one was sore. So, although I am cautious about X-rays, I made an appointment for a panoramic X-ray in a month to assess the wisdom teeth, and, if it seems appropriate, I'll take her to an oral surgeon to have one or more extracted under IV sedation, in his office, if possible (the dentist thought that it would be). And I am confident that there will be no serious problems, but this is thanks to technology and training in modern medicine that haven't been available for that long.
I think that everyone should inform himself on all medical procedures before agreeing to anything, but I also think that he should have access to any medical procedure which is reasonable (and opinions can differ as to that).
One problem is that you have not said how you think people should protect themselves against tetanus, bacterial meningitis, and yellow fever in the relevant cases, for example. These are diseases which healthy, well-nourished people used to die from very readily.
If most people stopped vaxxing and the mortality from these diseases rose to something like pre-vaccine levels, do you think they should just accept dying from them?
I put that in a separate paragraph because it is the crucial issue.
balinaheuchter Air Traffic Control You Tube - Colin Campbell example of - How to ""Fudge a Nudge"" -""Deal"" or ""No Deal"" ""Not in a month of Sundays"" ""No exceptions/no compromise?"" -make a trade off -do an exception- everyone get's a good deal /good outcome!
Hans, you are right that we are looking at one of the biggest crimes in all history. When I read the story of that poor girl who was so healthy and is now confined to a wheelchair after getting her third Gardasil shot I could not believe that Merck could produce such a toxic vaccine and give it out to girls like it was something they absolutely had to have only to be mislead and made into cripples. Merck should be prosecuted for the damage they have done to so many girls who got the Gardasil vaccine and were physically debilitated for life. There is a place for the people who perpetrated this crime on young girls and women and it is called hell. They have destroyed people's lives and gotten away with it. My heart goes out to those who have suffered this damage for no damn good reason except to help make huge profits for Merck!
Here is the reason that the germ theory is nonsense.
1) Everyday we are bombarded with billions of germs. Presumably at least some of them are of the kind that germ theorists believe are dangerous (otherwise we would have to conclude that none of them are dangerous). So how do we survive?
2) Let's just say that we ignore 1 and imagine that, by way of magic, none of the billions of viruses we get bombarded with are pathogenic but all those that are are tucked away somewhere. Ok. But presumably they reside in sick people right? So where are there lots of sick people? Doctor offices and hospitals! So everybody must be dying the moment they enter these places right?
3) I love this one because I have never seen anybody else ever raise it. Under the germ theory there are no negative feedbacks. This makes a stable biological system by definition impossible. The immune system is *not* a negative feedback it is the opposite. It actually reinforces our math problem because the immune system will weaken as the number of pathogens increase.
There is no way of resolving this problem without a discontinuity. A Deus ex Machina as The Almighty Pill so beautifully put it. So the germ theory is quite literally, mathematically impossible.
There is as much chance of it being true as 2+2 = 5.
There are plenty of other massive problems with germ theory such as why did things like SARS and bird flu magically disappear? Why do we have the symptoms that we do? Is our body controlling the symptoms to help fight the germs and if so, why would suppressing the symptoms with antibiotics or Tamiflu be considered a good idea? If the virus is causing the symptoms then why would it cause these kinds of things?",['Other toxic products.'],3141,multifieldqa_en,en,,74334862b5d8e2a02deb3c24aa90d5339e443fbc412453b8," Del Bigtree and his team at ICAN wrote 88 page letter to the HHS regarding vaccine safety. Letter, in response to an earlier reply from the then acting Director National Vaccine Program Office, Melinda Wharton, took virtually a year to compile. Most sensationally they researched the HHS claim through US government archives that at least some pediatric vaccines had been trialed against genuine placebo, and came to a negative conclusion. The manufacturers apparently had no faith that their proto-products could withstand this scrutiny, and for the rest they just did not care. We have, in truth, been building to this moment for two decades: as the evidence piles up that every single aspect of the program lacks integrity or is simply rotten to the core, all the perpetrators can do is call for the silencing of their critics, and maintain the products are safe because they say so. The pamphlet airily dismisses concerns that vaccines have side effects or that you could possibly have too many. It is pure public relations, and if the RSPH claims to be ""independent"" it also admits that the publication was paid for by Merck, a detail which was reported by British Medical Journal and the Guardian, but not true to form by the BBC. It echoes calls from Seth Berkley of GAVI, Heidi Larson of the Vaccine Confidence Project and the European Parliament. The letter is published simultaneously with a demand in the UK from the Royal Society for Public Health (which I had never heard of) to shut down comment about vaccines on the web. Please help give the ICAN letter the widest possible distribution, particularly to politicians. It has been published in the latest edition of Highwire, which is published every week on the 8th and 9th October. For more information on the Highwire series, visit www.highwire.co.uk/vaccine and follow us on Twitter @HighwireVaccine. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. In the UK, contact the National Institute of Health and Social Services on 0800-788-9090. For details on the National Institutes of Health, visit the National Institute for Health and Social Service on the 8th October and or the University of London on the 7th November on 8th November on 08th and 9th November. For information on how to get in touch with the NHS, see www.samaritans.org or  www.nhs.uk. For confidential support on suicide matters call the Samaritans. Vaccines are usually effective in giving immunity to the targeted diseases. They also have many dangers which everyone should be aware of, and vaccines should be avoided whenever possible. But in the case of the most dangerous diseases, everyone should learn about them and think about what he wants to do to protect himself and his children from them. If you live in the Congo and many people around you are currently dying of yellow fever, then that means that you yourself are at risk of being bitten by a loaded mosquito. The yellow fever vaccine is very effective at preventing yellow fever. From there, each person must make a choice about whether or not to get the vaccine. The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system. The immune system is designed to detect and destroy potentially dangerous invading pathogens. Many bacteria are usually harmless and absolutely necessary. The minority are dangerous, and most people's immune systems do a good job of analyzing them and killing them, often with no signs of disease. About 80% of those with clinical disease recover at this point, with permanent immunity. The other 20% enter the toxic stage, with a return of the fever, black vomit (coffee-ground emesis), diarrhea, a slowing of the pulse (Faget's sign), jaundice, yellow eyes, yellow skin, and failure of the kidneys, liver, and heart. The patient gets a strange hiccup (like with Ebola, a related disease), falls into a coma, and dies. About half of those patients who enter the Toxic stage dies, even now, even with the best of hospital care. The Fagett's sign can also occur at the end of the first stage. In all cases before and after the vaccine, the patient has a remission of two or three days. In the end, the person who got yellow fever in Cuba in 1900. ""In his logbook, Lazear wrote an unusual entry on September 13. ""I'll give the passage from The American Plague (162-5), which describes the course of Jesse Lazear's illness. I'll give you the details of his illness,"" he writes. ""He wrote in his log book, ""I've been suffering from yellow fever for a few days, and I've been given a shot of the vaccine."" He wrote in the logbook: ""I'm fine, I'm fine. I've had a good day, I've got a good night, I'll have a good tomorrow."" He writes in the next few days he'll be fine, he says. He says in the last few days of his life he's had a bad day, he's been sick, he'll get better. He's been ill, he writes, and he's going to be fine. He goes on to say that he's having a great day, and then he's sick again. He writes: ""The immune system does not allow us to overcome our math problem. In fact, it makes it worse."" Lazear had used the soldier's name and simply the date he was bitten, with no other attention to the mosquito. If he had entered his name, then his death would have been considered medical suicide by the insurance company, and his wife and two children would not have gotten any payment. The basic science behind vaccines is correct. While they usually prevent the diseases, they don't cause obvious reactions. With increasing vaccininatin, people should continue to get them, and that's why people are still getting them, says Dr. David Frum, author of the book, ""Vaccines: The New Science of Vaccines and the Death of The Vaccine,"" which is published by Simon & Schuster at $24.99. For more information on the book and its author, visit www.vaccines.com. For the full interview with Dr. Frum on ""The Death of the Vaccinate,"" visit CNN.com/Heroes, or go to http://www.cnn.co/heroes/2013/09/26/science/death-of-the-vaccine-death-by-jennifer-frum-1.html. The full interview will be published on October 1, 2013, at 9 p.m. ET on CNN.co.uk/ Heroes, the death of thevaccine, will be broadcast on October 2, 2013 at 9:30 p.M. ET, and on October 3, 2014, at 10:30 a.m., 9:45 a.M., 10:50 a.C.E. and 10:55 a.E., respectively. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255 or visit http:// www.suicidepreventionlifeline.org/. For confidential. support on suicide matters call theNational Suicide Prevention Lifeline at 1-856-788-7255 or click here. For support in the United States, call the Salvation Army on 1 (800) 273-TALK (8255) or  visit http: http: www.sending.it/sending-a-compassionate-message or http://www.-sendinga.org/Sendinga/Suffering-A-Compassion-Lonely-Man-To-The-World-Abandoned-By-Your-Widow-and-Children-Awareness-Affected- By-His-Family-On-This-World.html/. For help in the UK, visit the Samaritans on 08457 9/11/13. For help with suicide matters, visit http:/www.samarsitans.com/. For support on the UK Samaritans, click here.  For help on suicide issues in the US, go to the  Samaritans' page on the website. For help on how to get involved in the fight against cancer, call the National Suicide Helpline on 1(800) 856-9255. on schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk. The pertussis vaccine continues to be very dangerous, and I do not recommend that anyone get it. Merck should be prosecuted for the damage they have done to so many girls who got the Gardasil vaccine and were physically debilitated for life. There is a place for the people who perpetrated this crime on young girls and women called Thriving Life, a place where life can be lived to its full potential and no one can be put in harm's way. The old DPT vaccine was MUCH more effective at preventing pertussi, but it was so dangerous (again, not to most, but to many) that developed countries replaced it with the acellular version, DTaP. This does not show that vaccine science has collapsed, but rather that the vaccine they developed to replace the DPT turned out to be much less effective than they first thought. If most people stopped vaxxing and the mortality from these diseases rose to something like pre-vaccine levels, do you think they should just accept dying from them? You Tube - Colin Campbell example of - How to ""Fudge a Nudge"" -""Deal"" or ""No Deal"" ""Not in a month of Sundays"" ""No exceptions/no compromise?"" -make a trade off -do an exception- everyone get's a good deal /good outcome!Everyone get's the good deal/good outcome- everyone gets the good outcome! The best way to make a deal is to ""fudge a nudge"" or to ""no-nudge"" a deal, which is a way to get people to agree to a deal that doesn't have to be agreed on by all parties at the end of the deal. The best place to start is with the terms ""no deal"" and ""no compromise,"" which can be very different from ""no exception"" or a compromise, depending on the terms of the ""no agreement"" you're talking about. The most successful deals are the ones where everyone gets a good price for what they get, and the best deals are those that don't include a price for the ""good deal"" or the ""bad deal"" The best deals can be found on the internet, and they are usually available for as little as a few hundred dollars per person, or even a few thousand dollars for a small number of people who want to take a chance on the vaccine. The worst deals are usually the ones that are available for a lot of people, such as those who don't want to risk their lives, and who are willing to put their lives on the line for a few months at a time to get a vaccine that they believe will protect them from a disease that is very rarely dangerous. The ""best deals"" are often available on the Internet, and can be bought for much less than $1,000 per person. y have destroyed people's lives and gotten away with it. My heart goes out to those who have suffered this damage for no damn good reason except to help make huge profits for Merck!Here is the reason that the germ theory is nonsense. Everyday we are bombarded with billions of germs. Presumably at least some of them are of the kind that germ theorists believe are dangerous. This makes a stable biological system by definition impossible. The immune system is *not* a negative feedback it is the opposite. It actually reinforces our math problem because the immune system will weaken as the number of pathogens increase. So the germ Theory is quite literally, mathematically impossible. There is as much chance of it being true as 2+2 = 5. There are plenty of other massive problems with germ theory such as why did things like SARS and bird flu magically disappear? Why do we have the symptoms that we do? Is our body controlling the symptoms to help fight the germs and if so, why would suppressing the symptoms with antibiotics or Tamiflu be considered a good idea? If the virus is causing the symptoms then why would it cause these kinds of things? If it is not causing them then why does it cause them at all? It is a massive mystery to me. I have never seen anybody else ever raise it. I love this one because I have ever seen nobody else ever raising it."
Where was Margaret Way born and where did she die?,"Margaret Way (b. Brisbane d. Cleveland, Queensland, Australia ) was an Australian writer of romance novels and women's fiction. A prolific author, Way wrote more than 120 novels since 1970, many through Mills & Boon, a romance imprint of British publisher Harlequin UK Ltd., owned by Harlequin Enterprises.

Biography
Before her marriage, she was a well-known pianist, teacher, vocal coach and accompanist. She began writing when her son, Laurence Way, was born, a friend took a pile of Mills & Boon books to her, she read all and decided that she also could write these types of novels. She began to write and promote her country with her stories set in Australia. She sold her first novels in 1970. Margaret Way lives with her family in her native Brisbane. Beginning in 2013, Margaret began to self-publish, releasing her first ""e-book"" mid-July.

Margaret died on the 10th of August 2022 in Cleveland, Queensland.

Bibliography

Single Novels
King Country (1970)
Blaze of Silk (1970)
The Time of the Jacaranda (1970)
Bauhinia Junction (1971)
Man from Bahl Bahla (1971)
Summer Magic (1971)
Return to Belle Amber (1971)
Ring of Jade (1972)
Copper Moon (1972)
Rainbow Bird (1972)
Man Like Daintree (1972)
Noonfire (1972)
Storm Over Mandargi (1973)
Wind River (1973)
Love Theme (1974)
McCabe's Kingdom (1974)
Sweet Sundown (1974)
Reeds of Honey (1975)
Storm Flower (1975)
Lesson in Loving (1975)
Flight into Yesterday (1976)
Red Cliffs of Malpara (1976)
Man on Half-moon (1976)
Swan's Reach (1976)
Mutiny in Paradise (1977)
One Way Ticket (1977)
Portrait of Jaime (1977)
Black Ingo (1977)
Awakening Flame (1978)
Wild Swan (1978)
Ring of Fire (1978)
Wake the Sleeping Tiger (1978)
Valley of the Moon (1979)
White Magnolia (1979)
Winds of Heaven (1979)
Blue Lotus (1979)
Butterfly and the Baron (1979)
Golden Puma (1980)
Temple of Fire (1980)
Lord of the High Valley (1980)
Flamingo Park (1980)
North of Capricorn (1981)
Season for Change (1981)
Shadow Dance (1981)
McIvor Affair (1981)
Home to Morning Star (1981)
Broken Rhapsody (1982)
The Silver Veil (1982)
Spellbound (1982)
Hunter's Moon (1982)
Girl at Cobalt Creek (1983)
No Alternative (1983)
House of Memories (1983)
Almost a Stranger (1984)
A place called Rambulara (1984)
Fallen Idol (1984)
Hunt the Sun (1985)
Eagle's Ridge (1985)
The Tiger's Cage (1986)
Innocent in Eden (1986)
Diamond Valley (1986)
Morning Glory (1988)
Devil Moon (1988)
Mowana Magic (1988)
Hungry Heart (1988)
Rise of an Eagle (1988)
One Fateful Summer (1993)
The Carradine Brand (1994)
Holding on to Alex (1997)
The Australian Heiress (1997)
Claiming His Child (1999)
The Cattleman's Bride (2000)
The Cattle Baron (2001)
The Husbands of the Outback (2001)
Secrets of the Outback (2002)
With This Ring (2003)
Innocent Mistress (2004)
Cattle Rancher, Convenient Wife (2007)
Outback Marriages (2007)
Promoted: Nanny to Wife (2007)
Cattle Rancher, Secret Son (2007)
Genni's Dilemma (2008)
Bride At Briar Ridge (2009)
Outback Heiress, Surprise Proposal (2009)
Cattle Baron, Nanny Needed (2009)

Legends of the Outback Series
Mail Order Marriage (1999)
The Bridesmaid's Wedding (2000)
The English Bride (2000)
A Wife at Kimbara (2000)

Koomera Crossing Series
Sarah's Baby (2003)
Runaway Wife (2003)
Outback Bridegroom (2003)
Outback Surrender (2003)
Home to Eden (2004)

McIvor Sisters Series
The Outback Engagement (2005)
Marriage at Murraree (2005)

Men Of The Outback Series
The Cattleman (2006)
The Cattle Baron's Bride (2006)
Her Outback Protector (2006)
The Horseman (2006)

Outback Marriages Series
Outback Man Seeks Wife (2007)
Cattle Rancher, Convenient Wife (2007)

Barons of the Outback Series Multi-Author
Wedding At Wangaree Valley (2008)
Bride At Briar's Ridge (2008)

Family Ties Multi-Author
Once Burned (1995)

Hitched! Multi-Author
A Faulkner Possession (1996)

Simply the Best Multi-Author
Georgia and the Tycoon (1997)

The Big Event Multi-Author
Beresford's Bride (1998)

Guardian Angels Multi-Author
Gabriel's Mission (1998)

Australians Series Multi-Author
7. Her Outback Man (1998)
17. Master of Maramba (2001)
19. Outback Fire (2001)
22. Mistaken Mistress (2002)
24. Outback Angel (2002)
33. The Australian Tycoon's Proposal (2004)
35. His Heiress Wife (2004)

Marrying the Boss Series Multi-Author
Boardroom Proposal (1999)

Contract Brides Series Multi-Author
Strategy for Marriage (2002)

Everlasting Love Series Multi-Author
Hidden Legacy (2008)

Diamond Brides Series Multi-Author
The Australian's Society Bride (2008)

Collections
Summer Magic / Ring of Jade / Noonfire (1981)
Wife at Kimbara / Bridesmaid's Wedding (2005)

Omnibus in Collaboration
Pretty Witch / Without Any Amazement / Storm Over Mandargi (1977) (with Lucy Gillen and Margaret Malcolm)
Dear Caliban / Heart of the Eagle / Swans' Reach (1978) (with Jane Donnelly and Elizabeth Graham)
The Bonds of Matrimony / Dragon Island / Reeds of Honey (1979) (with Elizabeth Hunter and Henrietta Reid)
The Man Outside / Castles in Spain / McCabe's Kingdom (1979) (with Jane Donnelly and Rebecca Stratton)
Winds From The Sea / Island of Darkness / Wind River (1979) (with Margaret Pargeter and Rebecca Stratton)
Moorland Magic / Tree of Idleness / Sweet Sundown (1980) (with Elizabeth Ashton and Elizabeth Hunter)
The Shifting Sands / Portrait of Jaime / Touched by Fire (1982) (with Jane Donnelly and Kay Thorpe)
Head of Chancery / Wild Heart / One-Way Ticket (1986) (with Betty Beaty and Doris Smith)
Heart of the Scorpion / The Winds of Heaven / Sweet Compulsion (1987) (with Janice Gray and Victoria Woolf)
One Brief Sweet Hour / Once More With Feeling / Blue Lotus (1990) (with Jane Arbor and Natalie Sparks)
Marry Me Cowboy (1995) (with Janet Dailey, Susan Fox and Anne McAllister)
Husbands on Horseback (1996) (with Diana Palmer)
Wedlocked (1999) (with Day Leclaire and Anne McAllister)
Mistletoe Magic (1999) (with Betty Neels and Rebecca Winters)
The Australians (2000) (with Helen Bianchin and Miranda Lee)
Weddings Down Under (2001) (with Helen Bianchin and Jessica Hart)
Outback Husbands (2002) (with Marion Lennox)
The Mother's Day Collection (2002) (with Helen Dickson and Kate Hoffmann)
Australian Nights (2003) (with Miranda Lee)
Outback Weddings (2003) (with Barbara Hannay)
Australian Playboys (2003) (with Helen Bianchin and Marion Lennox)
Australian Tycoons (2004) (with Emma Darcy and Marion Lennox)
A Mother's Day Gift (2004) (with Anne Ashley and Lucy Monroe)
White Wedding (2004) (with Judy Christenberry and Jessica Steele)
A Christmas Engagement (2004) (with Sara Craven and Jessica Matthews)
A Very Special Mother's Day (2005) (with Anne Herries)
All I Want for Christmas... (2005) (with Betty Neels and Jessica Steele)
The Mills and Boon Collection (2006) (with Caroline Anderson and Penny Jordan)
Outback Desire (2006) (with Emma Darcy and Carol Marinelli)
To Mum, with Love (2006) (with Rebecca Winters)
Australian Heroes (2007) (with Marion Lennox and Fiona McArthur)
Tall, Dark and Sexy (2008) (with Caroline Anderson and Helen Bianchin)
The Boss's Proposal (2008) (with Jessica Steele and Patricia Thayer)
Island Heat / Outback Man Seeks Wife / Prince's Forbidden Virgin / One Night Before Marriage / Their Lost-and-found Family / Single Dad's Marriage Wish (2008) (with Robyn Donald, Marion Lennox, Carol Marinelli, Sarah Mayberry and Anne Oliver)
Australian Billionaires (2009) (with Jennie Adams and Amy Andrews)
Cattle Baron : Nanny Needed / Bachelor Dad on Her Doorstep (2009) (with Michelle Douglas)

External links
Margaret Way at Harlequin Enterprises Ltd

Australian romantic fiction writers
Australian women novelists
Living people
Year of birth missing (living people)
Women romantic fiction writers","['Margaret Way was born in Brisbane and died in Cleveland, Queensland, Australia.']",1203,multifieldqa_en,en,,934162f30867844eb8d74c9d62c1e2aba3fca790b5b1d53e," Margaret Way (b. Brisbane d. Cleveland, Queensland, Australia ) was an Australian writer of romance novels and women's fiction. A prolific author, Way wrote more than 120 novels since 1970, many through Mills & Boon, a romance imprint of British publisher Harlequin UK Ltd. She began writing when her son, Laurence Way, was born, she read all and decided that she also could write these types of novels. She sold her first novels in 1970. Beginning in 2013, Margaret began to self-publish, releasing her first ""e-book"" mid-July. Margaret Way died on the 10th of August 2022 in Cleveland,. Queensland. She was a well-known pianist, teacher, vocal coach and accompanist. Her novels include: The Cattleman's Bride (2000) The Australian Heiress (1997) The Husbands of the Outback (2001)Secrets of the outback (2002) Innocent Mistress (2004) The English Bride at Kimbara ( 2000) The Outback Marriages ( 2007)Promoted: Cattle Rancher, Convenient Wife, Secret Son (2007) Promoted: Briilemma At The Briiilemma (2009) Outback Heiresses (2010) The Bride at the Briiillemma (2011) The Bridesmaids (2012) The Wedding (2013) The Bachelorette (2014) The Girlfriends (2015) The Daughters of the Bride (2016) The Vows (2017) The Wives of the Vows, a series of four books about a young couple in a polygamist community in Queensland, will be released in September. The series will be called The Bodies of the Bodies, a collection of five books about the lives of two young women in the rural Outback. The first of the series, The Bride At The Vow, is expected to be released later this year. The second, The Wedding of the Wives, is scheduled for release in October. The third and the fourth, the Bride at The Bride, a Series of five, is due to be published in November. The fourth and the fifth, The Branches of the Cattle Baron, will come out in December. The fifth and the sixth, The EnglishBride at the Koomera Crossing, are due to come out later in the year. For more information, visit www.millsandboon.co.uk. For the full list of Margaret Way's novels, visit her website: http://www.mw.com/margaret-way/mw-romance-novels-and-women's-fiction.html. For a list of her most recent works, see http://mw/romances/Margaret-Way-Series-of-Wedding-Novels/Outback-Marriages.html/. For a complete list of all of her novels, click here:http://mwa.com.au/books/outback-marriages-and women-fiction-series-of wedding.html#outback marriages of-wedders-series of the-same-year as the Koomera series in 2011 and The English Bride at Kimbara series in 2011. For an overview of some of her books, see her releases on her website and her publications on her websites. Australian romantic fiction is written by women. Women writers include Margaret Way at Harlequin Enterprises Ltd and Marion Lennox. The Australian Tycoon's Proposal (2004) is one of the best-selling romantic fiction books of all time. Australian women writers have also written children's books and a number of children's fiction books. The list of Australian women romantic fiction writers can be found at: http://www.harlequin.co.uk/ Australian-Women-Romantic-Fiction/Women-Writers-of-Australian-Romance/Women Writers of Australian Romantic Fiction and/or Women of the World. For more information on the Women's Romance and Nurseries of Australia, visit: http: www.harlen-way.com/women-romantic-fictions.html. For a list of all the women's romantic fictions in Australia, see:http: http://www harlene-way com/Australian-Woman-Romances/Nursery-Fictions-of Australia and-Countries/Women%20of%20Australia.html%. For a full list of the Australian women's romantic fiction novels, visit http::// www.harnet.com / Australia/Romantic%20Fiction.html#Women%. For more details on the Australian men's fictional books, see:  http: //www.hrn.com%20/Australia%20Men%20Romance.html%20and%20Nursing%20Wives%20Of%20Australian%20Domestics%20&%20Husbands, including%20The%20Billionaire Wife%20Who%20Says%20It%20All%20in%20the%20World%20Is%20Aussie%20Woman%20Will%20Married%20Be%20To%20Me%,%20Her%20Kirsten%20Mistletoe%20Magic%20Magical%20Night%20And%20Swans%20Reeds%20Mistaken Mistaken%20 Mistress%20%20 (2000) (with Day Leclaire and Anne McAllister) The Australian  Tycoon's Proposal (2004) ( with Emma Darcy and Marion Lennox) The  Australian Tycoons  (2004), (with Emma Darcy and Sarah Mayberry) and the Australian Billionaires (2007) are some of the most popular romantic fiction works of the past 20 years. For the rest of the decade, the focus has been on the brides and weddings in Australia. The books include: The Australian Society Bride (2008), The Australian's Society Bride (2009) and The Mother's Day Collection (2002) and The Australian Nights (2003) (With Miranda Lee) The books are written by Margaret Way at Harlequin Equinox, Marina Lennox and Jessica Steele. The Australian Billionaires and the Australian Playboys (2003) are among the best selling romantic fiction books of the last decade."
What size chains were used in the benchmarking?,"Paper Info

Title: Compressed quantum error mitigation
Publish Date: 10 May 2023
Author List: Maurits Tepaske (from Physikalisches Institut, Universität Bonn), David Luitz (from Physikalisches Institut, Universität Bonn)

Figure

FIG.3.The out-of-time-ordered correlator C otoc i=L/2,j (t) as a function of the operator position j and time t, for the infinite temperature initial state, for a denoised second-order Trotter supercircuit with Trotter depth Mtrot = 32 and denoiser depth M = 2.We consider evolution times t = 0.5, 1, ..., 5, for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarizing noise with p = 0.01.
FIG. 4. The complex eigenvalues λ of the noisy second-order Trotter supercircuit with Mtrot = 16 at time t = 1 (left), the corresponding optimized denoiser with M = 4 (center), and the denoised Trotter supercircuit (right).The Trotter circuit is for a L = 6 Heisenberg model with PBC, and all twoqubit channels are affected by depolarizing noise with p = 0.0046.The unit circle, on which unitary eigenvalues must lie, is shown in black, and the noiseless eigenvalues are shown as blue bars.It is evident that the denoiser recovers all the noiseless eigenvalues from the noisy circuit.
FIG. 2. The complex eigenvalues λ of the noisy second-order Trotter supercircuit with Mtrot = 16 at time t = 1 (left), the corresponding optimized denoiser with M = 4 (center), and the denoised Trotter supercircuit (right).The Trotter circuit is for a L = 6 Heisenberg model with PBC, and all twoqubit channels are affected by depolarizing noise with p = 0.036.The unit circle, on which unitary eigenvalues must lie, is shown in black, and the noiseless eigenvalues are shown as blue bars.It is clear that the denoiser recovers with high accuracy the noiseless eigenvalues from the noisy circuit.
FIG. 3. The half-chain channel entanglement entropy S at different two-qubit depolarizing noise strengths p, for a secondorder Trotter supercircuit with Mtrot = 16 and t = 2, for a M = 4 denoiser.The Trotter circuit is for a Heisenberg model with PBC of size L = 6.The different curves correspond to the different supercircuits, i.e. the noisy supercircuit, the denoiser, the corresponding denoised supercircuit, and the noiseless variant.
FIG. 4. The out-of-time-ordered correlator C otoc i=L/2,j (t) as a function of the operator position j and stacked time t, for the infinite temperature initial state, for a denoised secondorder Trotter supercircuit with Trotter depth Mtrot = 32 and denoiser depth M = 2.It is optimized at t = 2 and stacked up to ten times.The calculations are for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarization with p = 0.01.The denoiser is affected by the same noise.
FIG.6.The distribution of the ZZ angle α of M = 2 denoisers (top panels) and M = 8 denoisers (bottom panels), with the lightest color corresponding to the denoiser for the Trotter supercircuit with t = 0.5, and the darkest color with t = 5.As usual, we consider the Heisenberg model on a periodic chain, and second-order Trotter supercircuits with depths Mtrot = 8, 16, 32, 64, which together with the denoiser is affected by a two-qubit depolarizing noise with p = 0.01.The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively.
FIG. 7. The sampling overhead γ of the optimized denoisers from Fig. 2 of the main text, with denoiser depths M = 1, 2, 4, 6, 8 and Trotter depths Mtrot = 8, 16, 32, 64 at times t = 0.5, 1, ..., 5, for the Heisenberg model on a chain with PBC affected by two-qubit depolarizing noise with p = 0.01.The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively.
FIG.8.The domain wall magnetization Z dw after evolving a periodic density wall |dw |dw * with the denoised second-order Trotter supercircuits D C from Fig.2of the main text.These supercircuits have various Trotter depths Mtrot = 8, 16, 32, 64, denoiser depths M = 1, 2, 4, 6, 8, and evolution times t = 0.5, 1, ..., 5, for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarizing noise of strength p = 0.01.The denoiser is affected by the same noise.The non-denoised results are labelled with M = 0 and the noiseless results with p = 0.The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively.We see that the denoiser allows us to recover the noiseless behavior.

abstract

We introduce a quantum error mitigation technique based on probabilistic error cancellation to eliminate errors which have accumulated during the application of a quantum circuit. Our approach is based on applying an optimal ""denoiser"" after the action of a noisy circuit and can be performed with an arbitrary number of extra gates.
The denoiser is given by an ensemble of circuits distributed with a quasiprobability distribution. For a simple noise model, we show that efficient, local denoisers can be found, and we demonstrate their effectiveness for the digital quantum simulation of the time evolution of simple spin chains. Introduction.
-Quantum information processing has been theoretically shown to hold great promises, and quantum algorithms were developed which can in principle achieve an exponential speed-up over their classical counterparts, both for general purpose computing and quantum simulation . However, present day quantum computing prototypes still suffer from significant noise processes which hinder the execution of many potentially groundbreaking quantum algorithms .
Nontrivial quantum algorithms typically require large sequences of quantum gates, each of which introduces dissipation and hence an overall loss of coherence, eventually rendering the results useless. Until quantum error correction becomes practical, quantum error mitigation seems to be more feasible to increase the accuracy of expectation values.
Here the goal is to induce the (partial) cancellation of errors that stem from noisy quantum gates by extending the circuit corresponding to the desired algorithm with an ensemble of gates , sampled from a quasiprobability distribution. The traditional way to accomplish this is with the gatewise method from , where noise is mitigated by inverting the noise channel of each gate separately, i.e. the cancellation of errors is performed for each gate on its own.
Here the local noise channel is approximated in a way such that it can be easily inverted analytically, e.g. using Pauli twirling . Gates are then sampled from the inverted noise channel by interpreting it as a quasiprobability distribution. Because in this gate-wise approach every noisy gate has to be modified separately, the sign problem is exponentially large in the number of gates, limiting the practicality of the mitigation.
The success of the gate-wise approach resulted in a large body of work concerning these methods , including extensions for simultaneous mitigation of multiple gates by Pauli-twirling entire layers or variationally constructing a mitigating matrix product operator . In principle, errors during the execution of a circuit can propagate and accumulate.
These propagated errors * david.luitz@uni-bonn.de ≈ C

C

FIG. 1. An example of the quantum error mitigation procedure used in this work for the time evolution of the wave function of a spin chain. The ideal second-order Trotter supercircuit C of depth Mtrot = 1 (light blue) is approximated by applying a denoiser D of depth M = 1 (red) to the noisy Trotter supercircuit C (dark blue).
Because the denoiser is applied after fully executing the noisy Trotter supercircuit, it represents an approximate inverse of the global noise channel with a precision tunable by the depth of the denoiser. can potentially blow up and lead to large errors for the circuit as a whole . Here we introduce a mitigation technique that takes into account the propagation of errors, can be performed with a tunable number of extra gates, and works for non-Clifford local noise channels since the inversion of the accumulated global noise channel is implicit.
We first execute the targeted noisy circuit completely, letting the noise propagate and accumulate, and only afterwards we apply an extra random circuit sampled from a quasiprobability distribution. We call the corresponding ensemble of random circuits a denoiser, and we construct it such that upon averaging the accumulated errors cancel.
Essentially, the denoiser inverts a global noise channel. Since we will construct it as a local brickwall circuit, following the classical preprocessing approach from , we call this compressed quantum error mitigation. Method. -Due to the inevitable coupling of a quantum processor to its environment, every qubit operation is affected by noise.
Therefore, the simplest technique to minimize the impact of the resulting noise is to minimize the number of operations when performing a quantum algorithm. In we showed that many-body time evolution operators can be efficiently compressed into brick-wall circuits with high fidelity per gate. In this Letter, we consider the noise explicitly by treating quantum operations as (generally non-unitary) quantum channels, corresponding to completely positive and trace preserving (CPTP) maps .
For example, instead of a noiseless two-qubit gate G, which acts on a quantum state |ρ in superoperator form as G|ρ = G⊗G * |ρ , we get the noisy channel G = N G, where the noise channel N implements the two-qubit noise . These channels are used to construct a ""supercircuit"" C = N G i=1 Gi , consisting of N G channels, which is affected by multi-qubit accumulated noise.
This supercircuit encodes an ensemble of circuits . For simplicity, we assume that the noisy channels Gi in each half brickwall layer are lattice inversion and translation invariant, such that we can construct a denoiser with these properties, limiting the number of variational parameters. The purpose of quantum error mitigation is to modify the ensemble of circuits described by C in a way that we can use it to obtain the noiseless expectation values.
In superoperator language, we do this by following the supercircuit C with a denoiser supercircuit D, such that D C is as close to the noiseless supercircuit C = C ⊗ C * as possible. Here C is the target unitary circuit. Because the noise channel N is non-unitary, hence making the supercircuit C non-unitary, we need to use a non-unitary denoiser to retrieve the unitary C.
We illustrate the mitigation procedure in Fig. , where a denoiser with one layer is used to mitigate errors for a second-order Trotter supercircuit with one layer. This circuit architecture is commonly used to simulate the time evolution of a quantum many-body system, until some time t, with controllable precision , and we will use it to benchmark the denoiser.
In practice, we cannot directly implement a supercircuit, and so we have to utilize its interpretation as an ensemble of circuits. Essentially, after executing a shot of the noisy circuit we sample the denoiser and apply it. The goal is to construct the denoiser in a way that averaging over many of its samples cancels the accumulated errors and gives us a good approximation of the noiseless expectation values.
It should be noted that our approach requires more gate applications on the quantum processor than with the gate-wise scheme, since there each sample from the mitigation quasiprobability distribution can be absorbed into the original circuit, whereas our approach increases the circuit depth. We take this into account by imposing the same noise on the denoiser.
Furthermore, within our scheme, the dimensionality of the quasiprobabilistic mitigating ensemble can be controlled, in contrast to the gate-wise approach where it is equal to the gate count. To facilitate the stochastic interpretation we parameterize each two-qubit denoiser channel G i as a sum of CPTP maps, such that we can sample the terms in this sum and execute the sampled gate on the quantum processor.
Concretely, we use a trace preserv-ing sum of a unitary and a non-unitary channel. For the unitary part we take a two-qubit unitary channel U( φ i ) = U ( φ i ) ⊗ U * ( φ i ), with U ( φ i ) a two-qubit unitary gate parameterized by φ i . For this we take the two-qubit ZZ rotation exp(−iα(σ z ⊗ σ z )) with angle α, which can be obtained from native gates on current hardware , and dress it with four general one-qubit unitaries, only two of which are independent if we want a circuit that is space inversion symmetric around every bond.
The resulting gate has 7 real parameters φ i . For the non-unitary part, which is essential because D has to cancel the non-unitary accumulated noise to obtain the noiseless unitary circuit, we use a general onequbit measurement followed by conditional preparation channel M( , with V a general one-qubit unitary and each κ i a 3-dimensional vector, resulting in a real 9-dimensional ζ i .
This yields the two-qubit correlated measurement M( With these parts we construct the parameterization with coefficients η i ∈ R that satisfy η 0 + η 1 = 1 because G i is trace preserving. Note that here the tensor product symbol corresponds to combining two one-qubit channels to make a two-qubit channel, whereas in most of the paper it is used to link the column and row indices of a density matrix.
We construct the denoiser from the noisy channels Gi = N G i . With this parameterization one denoiser channel has 17 independent real parameters, such that a denoiser of depth M , i.e. consisting of M brickwall layers, has 34M real parameters (we use one unique channel per half brickwall layer). For reference, a general channel has 544M parameters.
To determine the mitigated expectation values we use the full expression where |ρ 0 is the initial state and |1 is the vectorized identity operator on the full Hilbert space. To evaluate this on a quantum processor, we use the stochastic interpretation of (1) to resample . In particular, from each channel (1) we get a unitary with probability p 0 = |η 0 |/γ and a measurement followed by conditional preparation with probability p 1 = |η 1 |/γ.
Here γ = |η 0 | + |η 1 | is the sampling overhead, which characterizes the magnitude of the sign problem from negative η i . For quasiprobability distributions, i.e. with γ > 1, every denoiser sample has an extra sign sgn(η) = N G g=1 sgn(η g ), 2. The normalized distance between the denoised Trotter supercircuit D C and the noiseless Trotter supercircuit C (top panels), at evolution times t = 0.5, 1, ..., 5, and the twopoint z-spin correlator C zz i=L/2,j=L/2 (t) of a spin on the middle site at times 0 and t (bottom panels), for the infinite temperature initial state.
We consider denoisers with depths M = 1, 2, 4, 6, 8 and second-order Trotter circuits with depths Mtrot = 16, 32, 64. In the top panels we use a Heisenberg chain with L = 8, and in the bottom panels with L = 14, both with periodic boundary conditions. All gates are affected by two-qubit depolarizing noise with p = 0.01.
The non-denoised results are labelled with M = 0, and the noiseless values with p = 0. where sgn(η g ) is the sign of the sampled coefficient of the gth channel. γ = 1 means that all signs are positive. Observables Ô p=0 for the noiseless circuit are then approximated by resampling the observables from the denoiser ensemble
where γ = N G g=1 γ g is the overall sampling overhead, with γ g the overhead of the gth gate. Clearly, a large γ implies a large variance of Ô p=0 for a given number of samples, with accurate estimation requiring the cancellation of large signed terms. The number of samples required to resolve this cancellation of signs is bounded by Hoeffding's inequality, which states that a sufficient number of samples to estimate Ô p=0 with error δ at probability 1 − ω is bounded by (2γ 2 /δ 2 ) ln(2/ω) .
Since γ scales exponentially in γ g , it is clear that a denoiser with large M and γ 1 will require many samples. We observed that decompositions with γ > 1 are crucial for an accurate denoiser. Restricting to γ = 1 leads to large infidelity and no improvement upon increasing the number of terms in or the depth M of the denoiser.
Simply put, probabilistic error cancellation of gate noise introduces a sign problem and it is crucial to find optimal parameterizations (1) which minimize γ to make the approach scalable. This issue arises in all high performance error mitigation schemes , because the inverse of a physical noise channel is unphysical and cannot be represented as a positive sum over CPTP maps.
This is clearly visible in the spectra of the denoiser, which lies outside the unit circle (cf. Fig. ). This makes the tunability of the number of gates in each denoiser sample a crucial ingredient, which allows control over the sign problem, because we can freely choose the η i in . For the parametrization (1) of denoiser channels, we try to find a set of parameters for error mitigation by minimizing the normalized Frobenius distance between the noiseless and denoised supercircuits
which bounds the distance of output density matrices and becomes zero for perfect denoising. We carry out the minimization of on a classical processor, using gradient descent with the differential programming algorithm from . Instead of explicitly calculating the accumulated global noise channel and subsequently inverting it, we approximate the noiseless supercircuit C with the denoised supercircuit D C, effectively yielding a circuit representation D of the inverse noise channel.
Results. -To benchmark the denoiser we apply it to the second-order Trotter circuits of the spin-1/2 Heisenberg chain with periodic boundary conditions (PBC) where is the Pauli algebra acting on the local Hilbert space of site i. A second-order Trotter circuit for evolution time t with depth M trot consists of M trot − 1 half brickwall layers with time step t/M trot and two layers with half time step .
We consider circuits that are affected by uniform depolarizing noise with probability p for simplicity, but our approach can be used for any non-Clifford noise. The two-qubit noise channel is which acts on neighboring qubits i and i + 1 and is applied to each Trotter and denoiser gate, and p = 0.01 unless stated otherwise.
We study circuits with depths M trot = 16, 32, 64 for evolution times t = 0.5, 1, ..., 5, and denoisers D with depths M = 1, 2, 4, 6, 8. In the top panels of Fig. we show (4) for a chain of size L = 8 as a function of time t. Here it can be seen that even for M trot = 32 a denoiser with M = 1 already improves by roughly an order of magnitude at all considered t.
Depending on M trot and t, further increasing M lowers , with the biggest improvements occurring for high precision Trotter circuits with large depth M trot = 64 and short time t = 0.5, where the Trotter gates are closer to the identity than in the other cases. At the other extreme, for M trot = 16 the improvements are relatively small upon increasing M > 2. In all cases the denoiser works better at early times than at late times, again indicating that it is easier to denoise Trotter gates that are relatively close to the identity.
To probe the accuracy of the denoiser on quantities that do not enter the optimization, as a first test we consider the two-point correlator between spins at different times where we have chosen the infinite temperature initial state, and C(t) is the Trotter supercircuit for time t. In the bottom panels of Fig. we show C zz i=L/2,j=L/2 (t) for the supercircuits from the upper panels, now for a L = 14 chain.
Here we see that at M trot = 16 we can retrieve the noiseless values already with M = 1, but that increasing M trot makes this more difficult. At M trot = 64 we see larger deviations, and improvement upon increasing M is less stable, but nonetheless we are able to mitigate errors to a large extent. As a further test, we compute the out-of-time-ordered correlator (OTOC) ]
In Fig. we show the results for i = L/2, for a Trotter circuit with depth M trot = 32 and a denoiser with depth M = 2. Here we see that a denoiser with M M trot is able to recover the light-cone of correlations, which are otherwise buried by the noise. In the Supplementary Material we consider how the denoiser performs at different noise levels p, and how the denoised supercircuits perform under stacking.
There we also calculate domain wall magnetization dynamics, and show the distribution of the optimized denoiser parameters and the sampling overhead associated to the denoiser as a whole. In Fig. we show the eigenvalues of the noisy supercircuits for a noisy second-order Trotter supercircuit with M trot = 16 at t = 1 (left), the corresponding optimized denoiser with M = 4 (center), and the denoised supercircuit (right).
The eigenvalues λ of a unitary supercircuit lie on the unit circle, and in the presence of dissipation they are pushed to the center. We see that the spectrum of the denoiser lies outside the unit circle, making it an unphysical channel which cures the effect of the noise on the circuit, such that the spectrum of the denoised circuit is pushed back to the unit circle.
The noiseless eigenvalues are shown as blue bars, making it clear that the denoiser is able to recover the noiseless eigenvalues from the noisy circuit. In the Supplementary Material we show the spectra for a p = 0.036 denoiser, where we observe a clustering of eigenvalues reminiscent of Refs. . There we also investigate the channel entropy of the various supercircuits .
Conclusion. -We have introduced a probabilistic error cancellation scheme, where a classically determined denoiser mitigates the accumulated noise of a (generally non-Clifford) local noise channel. The required number of mitigation gates, i.e. the dimensionality of the corresponding quasiprobability distribution, is tunable and the parameterization of the corresponding channels provides control over the sign problem that is inherent to probabilistic error cancellation.
We have shown that a denoiser with one layer can already significantly mitigate errors for second-order Trotter circuits with up to 64 layers. This effectiveness of low-depth compressed circuits for denoising, in contrast with the noiseless time evolution operator compression from , can be understood from the non-unitarity of the denoiser channels.
In particu-lar, measurements can have non-local effects, since the measurement of a single qubit can reduce some highly entangled state (e.g. a GHZ state) to a product state, whereas in unitary circuits the spreading of correlations forms a light-cone. To optimize a denoiser with convenience at L > 8, the optimization can be formulated in terms of matrix product operators or channels , which is convenient because the circuit calculations leading to the normalized distance and its gradient are easily formulated in terms of tensor contractions and singular value decompositions .
This provides one route to a practical denoiser, which is relevant because the targeted noiseless circuit and the accompanying noisy variant in (4) need to be simulated classically, confining the optimization procedure to limited system sizes with an exact treatment or limited entanglement with tensor networks.
Nonetheless, we can use e.g. matrix product operators to calculate (4) for some relatively small t, such that the noiseless and denoised supercircuits in (4) have relatively small entanglement, and then stack the final denoised supercircuit on a quantum processor to generate classically intractable states.
Analogously, we can optimize the channels exactly at some classically tractable size and then execute them on a quantum processor with larger size. Both approaches are limited by the light-cone of many-body correlations, as visualized in Fig. , because finite-size effects appear when the light-cone width becomes comparable with system size.
1. The normalized distance (left) and z spin correlator C zz i=L/2,j=L/2 (right), for a second-order Trotter supercircuit of depth Mtrot = 16 for time t = 1, affected by various twoqubit depolarizing errors p. We compare the values obtained with and without a denoiser, i.e. M > 0 and M = 0, to the noiseless values (p = 0).
The denoiser is affected by the same noise as the Trotter circuit. We consider denoisers with depths M = 1, 2, 4, 6, 8, and we use a L = 8 Heisenberg chain with PBC for the normalized distance, while for the correlator we use L = 14. * david.luitz@uni-bonn.de to observe that even for larger noise strength p, the local observable C zz improves significantly even with denoisers of depth M = 1.
For large noise strengths, we generally see that the optimization of the denoiser becomes difficult, leading to nonmonotonic behavior as a function of p, presumably because we do not find the global optimum of the denoiser. It is interesting to analyze the spectra of the supercircuits considered in this work.
As mentioned in the main text, the spectrum of the ideal, unitary supercircuit C lies on the unit circle. The comparison to this case is therefore instructive. In the main text, we showed an example of the spectra in Fig. for moderate noise strength. Here, we show additional data for stronger noise p = 0.036 in Fig. for a denoiser with M = 4 layers, optimized to mitigate errors for a second-order Trotter supercircuit with M trot = 16 layers at time t = 1.
The eigenvalues λ of the noisy supercircuit C are clustered close to zero, far away from the unit circle (except for λ = 1), showing that the circuit is strongly affected by the noise. To mitigate the impact of the noise, the denoiser consequently has to renormalize the spectrum strongly. If it accurately represents the inverse of the global noise channel, its spectrum has to lie far outside the unit circle, which is the case.
Interestingly, we observe a clustering of eigenvalues which is reminiscent to the spectra found in . By comparison to these works, we suspect that this is due to the local nature of the denoiser, and warrants further investigation. The right panel of Fig. shows the result of the denoiser, pushing the eigenvalues back to the unit circle, nearly with the exact same distribution along the circle as the noiseless eigenvalues (blue bars).
Due to the strong noise, this is not achieved perfectly, and it is clear that this cannot work in principle if the global noise channel has a zero eigenvalue. The complexity of an operator can be quantified by its operator entanglement entropy . Here we calculate the half-chain channel entanglement entropy S of the noiseless C, noisy C, denoiser D, and denoised D C supercircuits.
We define S as the entanglement entropy of the state that is related to a supercircuit C via the Choi-Jamio lkowski isomorphism, i.e. ψ C = χ C /N , where the process matrix χ ab,cd C = C ac,bd is simply a reshaped supercircuit and N ensures normalization. Then we have S = −Tr [ψ C ln ψ C ]. This entropy measure is a particular instance of the ""exchange entropy"", which characterizes the information exchange between a quantum system and its environment .
In Fig. we plot the various S for a second-order Trotter circuit with M trot = 16 at t = 2, for a denoiser with M = 4, both affected by two-qubit depolarizing noise with p ∈ [10 −3 , 10 −1 ]. The Trotter circuit is for a Heisenberg model with L = 6 and PBC. We see that at large p, the noise destroys entanglement in the noisy supercircuit, and that the denoiser S increases to correct for this, such that the denoised supercircuit recovers the noiseless S.
Here we investigate how denoised supercircuits perform upon repeated application. We optimize the denoiser for a Trotter supercircuit for a fixed evolution time t. Then, to reach later times, we stack the denoised supercircuit n times to approximate the evolution up to time nt: In Fig. we stack a denoised t = 1 supercircuit up to n = 20 times and calculate the correlation function, defined in the main text, for the middle site.
We consider Trotter depths M trot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8, for a L = 14 Heisenberg chain with p = 0.01 depolarizing two-qubit noise. The noisy results correspond to M = 0 and the noiseless results to p = 0. In Fig. we calculate the OTOC, defined in the main text, with stacked time evolution for a denoised t = 2 supercircuit with M trot = 32 and M = 2, stacked up to ten times.
We see that the stacked supercircuit performs very well, and the additional precision obtained by using deep denoisers (M = 8) pays off for long evolution times, where we see convergence to the exact result (black dashed lines in Fig. ) as a function of M . FIG. . The two-point z-spin correlator C zz i=L/2,j=L/2 (t) of a spin on the middle site at times 0 and t, for the infinite temperature initial state, for denoised second-order Trotter supercircuits that are optimized at evolution time t = 1 and then stacked up to twenty times.
We use Trotter depths Mtrot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8. The calculations were performed for a periodic Heisenberg model with L = 14 and PBC, affected by two-qubit depolarizing noise with strength p = 0.01, which also affects the denoiser. The non-denoised results are labelled with M = 0, and the noiseless results with p = 0.
The panels are arranged as Mtrot = 8, 16, 32, 64 for top left, top right, bottom left, bottom right, respectively. The costliest and most noise-susceptible operation is the two-qubit ZZ rotation with angle α, which is the foundation of the unitary piece in our channel parameterization, defined in the main text.
For completeness, we here present the α angles of the optimized denoisers. The results are shown in Fig. , which contains histograms for the channel count N G versus α. The histograms are stacked, with the lightest color corresponding to the angles of the denoiser at t = 0.5 and the darkest at t = 5. The top four panels are for a denoiser with M = 2 and the bottom four with M = 8.
We consider M trot = 8, 16, 32, 64. We see that in both cases the distribution widens upon increasing M trot , indicating that the unitary channels start deviating more from the identity. Moreover, while the M = 2 denoisers in all cases except M trot = 64 have ZZ contributions close to the identity, this is clearly not the case for M = 8.
For simplicity, we did not focus on obtaining denoisers with the smallest sampling overhead γ, which is required to minimize the sign problem and hence ease the sampling of mitigated quantities. Instead, we let the optimization freely choose the η i in the denoiser parameterization, as defined in the main text.
In Fig. we show the sampling overhead of the denoisers from Fig. of the main text. We see that for M = 1 and M = 2 the sampling overhead is relatively small and uniform across the different t, whereas for M > 2 the optimization sometimes yields a denoiser with large γ and other times with small γ. This could be related to the difference in α distributions from Fig. .
The large fluctuations of γ appears to stem from the difficulty in finding optimal deep denoisers, and our optimization procedure likely only finds a local minimum in these cases. Here C(t) is the Trotter supercircuit for time t. In Fig. we show Z dw for the circuits from Fig.",['L = 8 and L = 14.'],5385,multifieldqa_en,en,,e568cc6d77a0a433937ab4bcf62e49b36a5cf7b3faa0d3ab," We consider evolution times t = 0.5, 1, ..., 5, for the periodic L = 14 Heisenberg chain that is affected by two-qubit depolarizing noise with p =0.01. The denoiser recovers with high accuracy the noiseless eigenvalues from the noisy circuit. The different curves correspond to the different supercircuits, i.e. the noisy supercircuit, the denoisers, the corresponding denoised super Circuit, and thenoiseless variant. We conclude that compressed quantum error mitigation can be used to improve the performance of quantum computers. The results are published in the Journal of Quantum Theory and Theory of Relativity (J.C.A.R.I.S.P. 2014) and are available for download from the J. C. A. R. S.E. Discussion and Discussion (http://j.c.a.r.i.s.p.org/2014/05/07/j.html#jc-a-j-a.j.s-s-e.J.S-e-j.J-S-S.J.-SJ.html. The. paper is also available in the online version with the. title Compressed Quantum Error Mitigation for Reliable Computing (JSCM 2014) ( http://www.jscm.org/. The paper is available in two versions: one with the title Compression quantum error Mitigation (JCSM 2014), and the other with the subtitle Compressed quantum error mitigation forreliable computing (J SCM 2014). For the full text of the paper, visit the JSCM website (http: www.jc.org.uk/. For the second version of the article, see the JCSM.org website ( http:www. jc.gov.uk/2014-05/06/jcsm-2014-06/06.html). The. final version of this article includes the text of a paper by Maurits Tepaske (from Physikalisches Institut, Universität Bonn) and David Luitz (from the University of Bonn), with the titles: CompressedQuantumErrorMitigation.org and CompressionQuantumErgium.org (http): http: //www.scribd.com/. The article also includes the abstract, which includes the. text, the. abstract, the main text, and two appendices. For the. second-order Trotter super Circuits with depths Mtrot = 8, 16, 32, 64, and 64 at top left, right, and bottom left, respectively, the panels are arranged as Mtrot, Mtrot and Mtrot. depth M = 2. The panels are for a L = 6 Heisenburg model with PBC, and all twoqubit channels are affected by depolarization with p= 0.0046. For a second order Trotters with Mtrot & M = 4, the ZZ angle is α, with the darkest color with t = 5. For. the second order. Trottster superCircuit with Mt rot = 16 and t = 2, the distribution of the Zz angle is shown in the bottom panels. The figure also shows the overhead sampling of the optimized denoisers from Fig. 2 of the Fig. 3 of the Figs 1 and 2 of this paper, which is optimized at t = 1 and stacked up to ten times. The figures are for the. noisy second-orders Trottes with Mt Rot = 16 at time t. In principle, errors during the execution of a circuit can propagate and accumulate. We introduce a quantum error mitigation technique based on probabilistic error cancellation to eliminate errors which have accumulated during the application of a quantum circuit. Our approach is based on applying an optimal ""denoiser"" after the action of a noisy circuit and can be performed with an arbitrary number of extra gates. For a simple noise model, we show that efficient, local denoisers can be found, and we demonstrate their effectiveness for the digital quantum simulation of the time evolution of simple spin chains. We show that the denoiser allows us to recover the noiseless behavior of a Heisenberg chain that is affected by two-qubit depolarizing noise of strength p = 0.01. We conclude that quantum error correction seems to be more feasible to increase the accuracy of expectation values than the traditional gatewise method from , where noise is mitigated by inverting the noise channel of each gate separately, i.e. the cancellation of errors is performed for each gate on its own. In this gate-wise approach every noisy gate has to be modified separately, the sign problem is exponentially large in the number of gates, limiting the practicality of the mitigation. We present a mitigation technique that takes into account the propagation of errors, can be. performed with a tunable nu nu, and that can be applied to a range of quantum algorithms. The results are published in the open-source Proceedings of the European Association for Quantum Theory and Science (EPQS) (http://www.epqs.org/content/early/2013/10/14/07/08/06/07373737/EPQs-Quantum-Error-Mitigation-Technique-and-Sustaining-Algorithms.html#storylink=cpy). The author would like to make clear that this article is not intended to be an endorsement of any of the methods used in the EPQS-series, but rather to provide a framework for the development of quantum error-cancellation techniques. For more information, please visit: www.epQS.org.uk/news/article/073837/073637/063738/071737/083737%20Quantum%20Error%20Mitigation%20Technique%20of%20Algorithm%20and%20Sustained Algorithm%,%20also%20includes%20quantum errors%20that%20can potentially blow up and lead to large errors for the circuit as a whole. The article is also available in the online version of the journal Quantum Theory & Science (http:www. epqs-journal.com/news/. For more details, please go to: http: //www.eepq.com/. In this Letter, we consider the noise explicitly by treating quantum operations as (generally non-unitary) quantum channels. The noise channel N implements the two-qubit noise . These channels are used to construct a ""supercircuit"" C = N G i=1 Gi , consisting of N G channels. This supercircuit encodes an ensemble of circuits. We call the corresponding ensemble of random circuits a denoiser, and we construct it such that upon averaging the accumulated errors cancel. We illustrate the mitigation procedure in Fig. , where adenoiser with one layer is used to mitigate errors for a second-order Trotter supercircuits with oneLayer. This circuit architecture is commonly used to simulate the time evolution of a quantum many-body system, until some time t, with controllable precision , and we will use it to benchmark the denoisers. We take this into account by imposing the same noise on the denoser as on the noisy circuit. The goal is to construct theDenoiser in a way that averaging over many of its samples cancels the accumulatederrors and gives us a good approximation of the noiseless expectation values. It should be noted that our approach requires more gate applications on the quantum processor than with the gate-wise scheme, since there each sample from the mitigation quasiprobability distribution can be absorbed into the original circuit, whereas our approach increases the circuit depth. To facilitate the stochastic interpretation we parameterize each two- qubit denoising channel G i as a sum of CPTP maps, such that for each sample th we can get a sample th of extra gates, and works for non-Clifford local noise channels since the inversion of the accumulated global noise channel is implicit. In this Letter we show how this can be done using the classical preprocessing approach from , we call this compressed quantum error mitigation. We also show how we can control the dimensionality of the mitigating ensemble within our scheme, in contrast to theGate-wise approach where it is equal to the gate count. We will use the benchmarking procedure to show how to use the Denoiser to create a quantum supercirc circuit with high fidelity per gate, and how to control the number of gates per circuit. We hope to show that this approach can be used to create quantum superconductors with high accuracy per gate in the future, and that it can be applied to other quantum systems as well as to quantum computers and other quantum processors. We conclude by showing that this technique can be employed to simulate time evolution in a quantum computer with high precision, such as in the proof of concept of quantum entanglement. The paper is open-source and can be downloaded from the arXiv.org/2013-0715. We are happy to share our findings with the open-access version of the book, ""Quantum Entanglement in a Quantum Computer"", which is available for free on the Apple website. We consider denoisers with depths M = 1, 2, 4, 6, 8 and second-order Trotter circuits with depths Mtrot = 16, 32, 64. All gates are affected by two-qubit depolarizing noise with p = 0.01. We use a trace preserv-ing sum of a unitary and a non-unitary channel. The number of samples required to resolve this cancellation of signs is bounded by Hoeffding's inequality, which is that a sufficient number of states is sufficient to estimate the error at 1.0. The normalized distance between the denoised Trotner supercircuit D C and the noiseless Trotters C (top panels), at evolution times t =0.5, 1, ..., 5, and the twopoint z-spin correlator C zz i=L/2,j=l/2 (t) of a spin on the middle site at times 0 and t (bottom panels), for the infinite temperature initial state. The non-denoised results are labelled with M = 0 and the noniseless values with p= 0, where sgn(η) is the sampled coefficient of the gth channel. Observables are then approximated by resampling the observables from the ensemble ensemble, where N g is the overall sampling overhead with gth g=1. Clearly, a large overhead implies a large variance of Ô É, with accurate estimation requiring the cancellation of large signed terms at p=0.0 with ω=0, with large probability at 1 at 0.0, and with large variance at 0 at 0, 0, and 0.1. We conclude that all signs are positive, that all all signs of all gates are positive and that all positive signs are 0. All signs are negative, that is, all signs have no signs that are positive at p = 1 and 0, except for the positive sign of the gate that is positive at 0,.0, 0,.1, 0.2, and 1, where the sign of this gate is negative and the negative sign is 0.5. We also show that all the signs of the gates are negative and that this is true for all gates that are space inversion symmetric around every bond. We end up with a quantum processor that can be used to make quantum computers. We call this the ""quantum computer of the future"" and call it the ""Quantum Computer of the 21st century"" (QC of the 20th century). We use the term "" quantum processor"" to refer to the type of quantum computers that will be used in the future. We show that the quantum computer will be much more powerful than the one currently in use in the field of quantum computing in the next few years. We will also show how this technology can be applied to quantum computers in the 2030s and 2050s. We hope to use this technology in the development of the next generation of superconductors, such as superconducting quantum computers and quantum nanowaves. We'll also discuss how to use quantum computers to predict the future state of the universe in the 2070s and 2080s, and how they will play a key role in our understanding of the Earth's climate and environment. We plan to show how we can use quantum computing to predict future events in the environment of the moon, Mars, Venus, Mars and Mars, and other planets in the solar system, and even the moon's atmosphere. We find that decompositions with γ > 1 are crucial for an accurate denoiser. Restricting to γ = 1 leads to large infidelity and no improvement upon increasing the number of terms in or the depth M of the denoisers. We carry out the minimization of on a classical processor, using gradient descent with the differential programming algorithm from . Instead of explicitly calculating the accumulated global noise channel, we approximate the noiseless supercircuit C with the Denoised supercircuits D C. We consider circuits that are affected by uniform depolarizing noise with probability p for simplicity, but our approach can be used for any non-Clifford noise. The two-qubit noise channel is which acts on neighboring qubits i and i + 1 and is applied to each Trotter and Denoiser gate, and p = 0.01 unless stated otherwise. We study circuits with depths M trot = 16, 32, 64 for evolution times t.5, 1, ..., 5, and denoisers D with depth M = 1, 2, 4, 6, 8. We see that at M Trot = 64 we can retrieve the no iseless gates already, but that increasing Mtrot = 14 makes this more difficult. At M troT = 64, we are able to mitigate errors to a large extent, but nonetheless we see larger and larger improvements upon increasing MTrot = 2. In all cases theDenoiser works better at early times than at late times, again indicating that it is easier to denoise TroTter gates that are relatively close to the identity. In the bottom panels of Fig. 4 we show C zz i=L/2,j=l/2 (t) for the super Circuits from the upper panels, now for a L = 8 chain, and now for the L = 14 chain. We show that for a chain of size L= 8 as a function of time t. Here it can be seen that even for M tro t = 32 a denoising with M =1 already improves by roughly an order of magnitude at all considered t. The biggest improvements occur for high precision Trotner circuits with large depth M tron = 64 and short time t = 0,5. At the other extreme, for M Tron = 16 the improvements are relatively small upon increasing  M > 2. We conclude that theDenoise works better on spins at different times where we have chosen the infinite temperature initial state, and C(t) is the Trotters' spin state for time t(t). We show the two-point correlator between spins at the spin-1/2 Heisenberg chain with periodic boundary conditions (PBC) where is the Pauli algebra acting on the local Hilbert space of site i in Fig. (4) in the top panels of the Fig. 3rd panel of the fig. 2nd section of the article. We compute the out-of-the-box test for the Denoise on spins of different spins and show the results in the bottom Panel of thefig. (3) and Fig. 2. The results are shown in the section ‘Results’. We have shown that a denoiser with one layer can significantly mitigate errors for second-order Trotter circuits with up to 64 layers. The effectiveness of low-depth compressed circuits for denoising, in contrast with the noiseless time evolution operator compression from Refs. , can be understood from the non-unitarity of the denoisers channels. The required number of mitigation gates, i.e. the dimensionality of the corresponding quasiprobability distribution, is tunable and provides control over the sign problem that is inherent to probabilistic error cancellation. We can use e.g. matrix product operators to calculate (4) for some relatively small t, and then stack the final denoised supercircuit on a quantum processor to generate classically intractable states. The normalized distance and its gradient are easily formulated in terms of tensor contractions and singular value decompositions. This provides one route to a practicalDenoiser, which is relevant because the targeted noisless circuit and the accompanying noisy variant need to be simulated classically, confining the optimization procedure to limited system sizes with an exact treatment or limited entanglement with tensor networks. In Fig. 1 we show the eigenvalues of the noisy supercircuits for a noisy second- order Trotters with depth M trot = 16 at t = 1. We also investigate the channel entropy of the various super Circuits. We show the spectra for a p = 0.036 denoizer, where we observe a clustering of eigen Values reminiscent of Refs 1.1.1 and Refs 2.2. We see that the spectrum of the Denoiser lies outside the unit circle, making it an unphysical channel which cures the effect of the noise on the circuit, such that the Spectrum of thedenoised circuit is pushed back to theUnit circle. The eigen values are shown as blue bars, and it is clear that the deneiser is able to recover the noi-less eigenvalue from the noisy circuit. The spectrum of a unitary superCircuit lie on the unitcircle, and in the presence of dissipation they are pushed to the center. The denoise supercirc circuit is shown as a light-cone, which can be seen in Fig. 2. The light- cone of many-body correlations can have non-local effects, since the measurement of a single qubit can reduce some highly entangled state to a product state, whereas in unitary circuits the spreading of correlations forms aLight-cone. The Normalized Distance of a Denoised Supercircuit (L/2) can be found at the bottom of the page in the Supplementary Material, and the normalized distance is shown at the top of Fig. 3. We have shown the results for i = L/2, for a TroTter circuit with depth. M Trott = 32 and a deneising circuit with Depth M = 2. There we also calculate domain wall magnetization dynamics, and show the distribution of the optimizeddenoiser parameters. istance (left) and z spin correlator C zz i=L/2,j=L-2 (right), for a second-order Trotter supercircuit of depth Mtrot = 16 for time t = 1, affected by various twoqubit depolarizing errors p. We compare the values obtained with and without a denoiser, i.e. M > 0 and M = 0, to the noiseless values (p = 0). We observe that even for larger noise strength p, the local observable C Zz improves significantly even with denoisers of depth M = 1. For large noise strengths, we generally see that the optimization of the denoisers becomes difficult, leading to nonmonotonic behavior as a function of p. The complexity of an operator can be quantified by its operator entanglement entropy . Here we calculate the half-chain channel entangler entropy S of the noisy C, noisy C, denoising D, and denoised D C supercircuits. We find that the noise destroys the noisy supercircumuit, and that theDenoised S increases for this, such that the denOised S recovers the noise. We optimize the Denoising S to approximate the evolution up to nt = 20 times and calculate t = nt. Then, to reach later times, we stack up to 20 times, and calculate T = 1 nt for a fixed-size Trottersuit. We see at large entanglements in p, we see that at large p, such as in p = 10, we can find the correct denoises for this case. We conclude by saying that we have found a new way to make quantum computing more efficient and efficient. We hope that this will lead to a new generation of quantum computers that can be used to solve problems in the field of quantum computing. Back to Mail Online home. back to the page you came from.. The original version of this article stated that the study was based on the work of David Luitz@uni-bonn.de. We are happy to clarify that this was not the case. The study was in fact based on work by Luitz and his colleagues at the University of Bonn, Germany, and on a Heisenberg model with L = 6 and PBC. The results were published in the Journal of Quantum Theory and Theory of Quantum Entanglement (J.C.I.R.P.S. 1). Back to the site you come from. The article was originally published on November 14, 2013. It has been updated on November 16, 2013, to reflect that the work was published in a number of journals, including the prestigious journal, The journal of quantum theory and the Proceedings of the Royal Society of the United States of America (R.A.U.S., and the journal of the American Physical Society (P.A., U.S). The article has also been updated to reflect the fact that the research was conducted on a second order Trottster circuit with M trot = 16 at t = 2, for adenoiser with M = 4, both affected by two-qubit Depolarizing noise with p ∈ [10 −3 , 10 −1 ]. The study has also now been published on the journal’s website, which has been amended to include the results of the study. The full article can be found at: http://www.j.cnn.com/2013/11/16/science/science-topics-quantum-science-research-and-topical-news/article.html. We consider Trotter depths M trot = 8, 16, 32, 64 and denoiser depths M = 1, 2, 4, 6, 8 for a L = 14 Heisenberg chain with p = 0.01 depolarizing two-qubit noise. In Fig. we calculate the OTOC, defined in the main text, with stacked time evolution for a denoised t = 2 supercircuit with M Trot = 32 and M = 2, stacked up to ten times. The noisy results correspond to M = 0 and the noiseless results to p =0. The costliest and most noise-susceptible operation is the two- qubit ZZ rotation with angle α, which is the foundation of the unitary piece in our channel parameterization. The results are shown in Fig. , which contains histograms for the channel count N G versus α. The histograms are stacked, with the lightest color corresponding to the angles of theDenoiser at t = 0,5 and the darkest at t.5. We see that in both cases the distribution widens upon increasing M troT , indicating that the unitARY channels start deviating more from the identity. The large fluctuations of γ appears to stem from the difficulty in finding optimal deep denoisers, and our optimization procedure likely only finds a local minimum in these cases. For M > 2 the optimization sometimes yields a Denoiser with large γ and other times with small γ. This could be related to the difference in α distributions from Fig. .. Here C(t) is the TroTter supercircuits for time t. We show Z dw for the infinite temperature initial state, for the Z.e correlation function for the middle site, and we show Z.Dw for the final temperature state. The non-denoised results are labelled with M =0, and thenoise-free results with p=0.01. The top four panels are for adenoiser and the bottom four are for M = 8. We conclude that the additional precision obtained by using deep denoers (M = 8) pays off for long evolution times, where we see convergence to the exact result as a function of M . FIG. . Here we show the sampling overhead of the denoiser and the distribution of the ZZ contributions close to the identity in all cases except MTrot = 64. The panels are arranged as Mtrot = 8,. top left, top right, bottom left, bottom right, respectively, and MTroT = 64 for top left,. top right,. bottom left,. bottom right,. and bottom right. respectively, respectively. We also show the distribution for the z-spin correlator C zz i=L/2,j=L/.2 (t) of a spin on the middle sites at times 0 and t, for t = 1 and t."
How many experiments were demonstrated to test the capabilities of the controller?,"Paper Info

Title: Force Feedback Control For Dexterous Robotic Hands Using Conditional Postural Synergies
Publish Date: Unkown
Author List: Dimitrios Dimou, José Santos-Victor, Plinio Moreno

Figure

Fig. 1.Example of modeling the contacts and friction during manipulation.
Fig. 2. Schematic representation of the proposed force controller.The input is the state (GRASP or RELEASE) and the force readings.Based on that the grasp size is adjusted by a value C and is given to the posture mapping function along with the desired grasp type.A finger configuration is then generated and commanded to the robot.
Fig. 3. Our control algorithm in Python-like pseudocode.
Fig. 4. Our first experiment.The robot picks up a bottle, transports it, and places down on the desk.In the bottom part of the figure, you can see the control signals during this task.
Fig. 5.The household objects used in our experiments.
Under the pictures of the execution you can see the signals recorded by the controller: the average normal force applied by all fingers (blue line), the thresholds f threshold high n .(purple dashed line) and f threshold low n.(yellow dashed line), the average tangential force (green), and the grasp size used in each time-step (red).The task is divided four stages: 1) (red part) the initial grasp of the object, in this stage the force controller closes the grasp until the applied normal
Fig.6.In the upper row of images, you can see our second experiment.The robot picks up the chips can, rotates it 90 degrees, and places back down.In the middle row, for our third experiment, the robot picks up the chips can, rotates it 90 degrees, and hands it over to a person.In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp.

abstract

We present a force feedback controller for a dexterous robotic hand equipped with force sensors on its fingertips. Our controller uses the conditional postural synergies framework to generate the grasp postures, i.e. the finger configuration of the robot, at each time step based on forces measured on the robot's fingertips.
Using this framework we are able to control the hand during different grasp types using only one variable, the grasp size, which we define as the distance between the tip of the thumb and the index finger. Instead of controlling the finger limbs independently, our controller generates control signals for all the hand joints in a (lowdimensional) shared space (i.e.
synergy space). In addition, our approach is modular, which allows to execute various types of precision grips, by changing the synergy space according to the type of grasp. We show that our controller is able to lift objects of various weights and materials, adjust the grasp configuration during changes in the object's weight, and perform object placements and object handovers.

INTRODUCTION

To perform complex manipulation tasks in unstructured environments, humans use tactile feedback from their fingers. This feedback is provided by tactile afferents located in the skin of the hand. Particularly, for handling small objects with precise movements, the afferents located in the fingertips are used, which have high density and adapt fast to pressure changes .
These afferents provide information about the characteristics of the exerted contact forces, such as the magnitude and the direction. For anthropomorphic robots to be able to perform dexterous tasks similar force feedback signals must be used to alleviate problems arising from uncertainty in measurements, and handle external perturbations.
For example, using open-loop position control to lift a heavy object may fail due to slip without any feedback mechanism to provide tactile information. Previous works have used tactile sensors to design force controllers that use slip prediction to update the desired normal forces applied by the fingertips.
The slip predictors are based on machine learning models such as neural networks and random forests to classify multi-modal signals from a tactile sensor. In all previous works, each finger was separately controlled by an independent force controller. In addition, they required labeled data to train the slip predictors and because each finger is controlled independently is not obvious how to implement different anthropomorphic grasp types.
In this work we develop a force controller that takes as input the force readings of the fingertips and computes the grasp size which is then used along with a grasp type label to generate a grasp posture with the desired characteristics. To avoid slippage the desired normal contact force is calculated to be proportional to the tangential contact forces.
The applied normal force is then controlled using the size of the grasp as a control variable. Larger grasp sizes mean less force is applied to the object. So the grasp size is calculated from the error between the desired normal force and the actual measured normal force. The grasp size is then given to the posture sampler that generates a grasp posture, i.e. the finger joint angles.
The posture sampler is modeled with a conditional Variational Auto-Encoder (cVAE) based on the framework proposed in . With this framework we abstract away the low-level control of the fingers and generate hand postures based on high-level properties such as the type and the size of the grasp. So it works as a mapping function that takes as input a low-dimensional vector and the grasp type and size as conditional variables and maps them to a set of joint angles.
We show that with our controller we can control a dexterous robotic hand to lift objects of different weights using three precision grasps. Our controller is also able to compensate and retain a stable grasp during changes in the objects' weight, for example when filling up a cup or emptying it. In addition we show how with the addition of the hand pose information we can use the controller to calculate if the tangential force is due to gravity or due to a support surface and use this information to perform handovers and place down objects on surfaces.
We perform several real-world experiments with a dexterous robotic hand to showcase the capabilities of our controller and support our design choices. To sum up our main contributions are • We develop a controller for a dexterous robotic hand that uses force feedback and the conditional synergies framework to perform dexterous manipulation tasks.
• We show that with our controller we can easily use different precision grasp types, by changing only the grasp type variable which is given to the grasp posture mapping function. • We demonstrate by incorporating information about the world pose of the hand we can use our controller to perform additional tasks such as placing down and handing over objects.
Roboticists have looked for inspiration in humans for developing methods for complex object manipulation . Neuroscientists have studied for a long time the processes that allow humans to use tactile feedback to perform complex manipulation tasks. Humans tend to adjust the grip force according to the object's weight, its friction and they use a safety margin to account for uncertainties .
To gather information about the tactile states they use multiple afferents that are located in the skin of the fingers . There are different afferents in different parts of the hand depending on their usage, e.g. fast adapting afferents in the fingertips for precise manipulation. Based on signals from these afferents, humans encode simple contact events into action phases, such as grasping, lifting or releasing, which they combine in order to perform more complex and long-horizon manipulation tasks .
In robotics tactile sensors have been used for object stabilization and slip prediction in a variety of settings. For example, in , a compliant anthropomorphic prosthetic hand was controlled using force sensing to maintain object stability and avoid slip. In , they develop a control approach that uses integrated force and spatial tactile signals to avoid slip with unknown objects in real world settings.
In , , grasp quality metrics are computed based on the tactile feedback from the robots fingertips. In these works, simple two or three fingered grippers were considered for simple grasping tasks. Force control with anthropomorphic robotic hands has also been explored in more recent works. In , they employ three slip prediction methods to estimate when slip starts and based on the force signals at that moment they calculate the friction coefficient value.
Based on the calculated friction coefficient, they design a force controller that independently controls each finger to achieve a desired normal force. The desired normal contact force is set to be proportional to the tangential contact force and a safety margin based on the evidence found in . In , they train a random forest to classify the contact states into the classes: no contact, contact, slip.
Based on this classification signal, when slip is detected they increase the desired normal contact force to avoid it. In they train a recurrent neural network to estimate slip and the object material from the readings of a Biotac sensor. The force controller is increasing the desired normal contact force when slip is detected.
All these works , , use tactile feedback sensors to predict slip. They collect labeled data, on which they train their models. This approach is based on complex and expensive tactile sensors, and the process of collecting data is cumbersome. In addition, the data do not cover all possible hand poses, which would be impractical.
In contrast, in our work we do not rely on slip prediction, we avoid slip by defining a tangential force gain and a safety margin that work for a large number of objects. Furthermore, instead of independently controlling each finger we use a synergistic framework to generate grasp postures, that is conditioned on two variables: the grasp type and the grasp size.
This way, instead of controlling the values of each joint of each finger, we control only the two conditional variables greatly simplifying the control pipeline. This also, gives us the ability to use different grasp types in our manipulation tasks by changing only the grasp type variable. In also a synergistic framework was used to prevent an object from slipping from a humanoid hand, but they modeled only one synergy for a tripod grasp and they used the forces on the robotic arm as feedback, while we use force feedback from the fingertips.
Our control algorithm could also be applied to different hands as it does not depend on the hands configuration. Finally, in previous approaches only lifting tasks had been considered. In our work we demonstrate that our approach can be used to perform more complex tasks, such as placing objects on surfaces and performing handovers, which was not done in previous works.
Our goal in this work is to design a control algorithm for an anthropomorphic robotic hand to perform dexterous manipulation skills such as lifting and placing down objects. Our control algorithm will use tactile feedback from the force sensors on the fingertips of the hand to decide the forces that need to be applied to the object in each step of the task.
Given the desired forces to be applied, the size of the grasp will be computed. Given the grasp size and a desired grasp type, the posture generator will generate a grasp posture, i.e. the hand configuration, such that the force constraints are satisfied. To model the contacts and friction we use Coulombs' law, which states that in order to avoid slip, the normal contact force f n to the contact surface of an object, times the fiction coefficient µ, has to be larger than the tangential force f t :
µf n ≥ f t You can see an example in Figure , where an object is pressed against a wall by an applied normal force f n , and we have the tangential force f t = mg due to gravity. In order for the object to remain stable we need to apply a normal force: where µ is the friction coefficient between the object and the wall.
In the case of a dexterous hand manipulating an object, we want the normal forces applied by all fingers to be greater than the tangential force divided by the friction coefficient of the materials of the object and the fingertip. Since it is hard to accurately compute the friction coefficient between all possible object materials previous works have used multi-modal tactile sensors like the BioTac sensor, which provides information about the pressure, skin deformation, and temperature, to predict slip and based on that signal to increase the applied normal force.
In our work we use the FTS3 sensors which is a low-cost sensor that measures the 3D force applied in each fingertip. In addition, previous works gathered labeled datasets in order to train their slip prediction models which is time-consuming and limits the possible orientations of the hand, because gathering labeled data for all possible orientations is impractical.
To overcome this we experimentally selected the parameters that determine the value of the applied normal force such that we avoid slip for all objects in our dataset, from the lightest to the heaviest. In order to guarantee contact between the fingertip and the object, in the beginning of the grasping phase, we use an offset f of f set n as the minimum normal force applied by each finger.
In they also suggest that humans use an additional safety margin which is proportional to the tangential force, f margin n ∝ f t . So the final desired normal contact force becomes: where G is the gain that includes the friction coefficient and the additional safety margin. To alleviate the effects of noise in the sensors, the running average of the measured normal force f n and tangential force f t is used, as a low pass filter.
So for each force measurement we have the following relation: where α ∈ (0, 1) is a parameter that determines how much new measurements affect the value, and is experimentally selected. Given the measured normal force f n from the fingertip sensors we can compute the error f err n = f des n − f n . We use this error signal to control the grasp size variable g size , that we use as a conditional variable in our posture mapping function.
The grasp size represents the distance between the thumb and the index finger in a grasp posture. So a smaller grasp size will result in a tighter grasp and greater normal force applied to the surface of the object. We use a linear controller for the grasp size variable that is implemented as follows: where K is a parameter that controls the rate of decrease of the grasp size, and is experimentally selected.
So when the error between the desired normal force and the actual normal force is large the grasp size decreases so tighter grasp postures are generated in order to apply more normal force. In practice, in order to avoid oscillations in the grasp size we use the desired normal force as a high threshold that we want the measured normal force to be below:
If the normal force is below that threshold the grasp size does not change even if there are small oscillations in the measured tangential and normal forces. Also, in order to avoid the hand applying too much force that damages the hardware or the object we use a low threshold, that is: where w threshold is the width of the threshold in mN .
If the measured normal force is below the grasp size increases in order to apply less force. So the final grasp size variable for grasping is calculated as follows: where This is similar to the deadband control method , where instead of having a fixed reference point, an operating range is set. If the response is in this range, the controller does not exert any correction.
In our case, the operating range changes according to the force signals from the robot's fingertips. The grasp posture mapping function is based on the conditional postural synergies model presented in . It uses a conditional Variational Auto-Encoder model to generate grasps postures conditioned on additional variables such as the grasp size.
In this work we augment this model to also generate grasp postures conditioned on the grasp type. The model is trained on a set of labeled grasp samples acquired by teleoperating a robotic hand using a data-glove. Using this model we are able to abstract away the low-level control of each joint of each finger and generate grasps based on more general characteristics such as the type and the size of the grasp.
In this way we can control all the fingers jointly by a single value, the grasp size, thus greatly reducing the control parameters. In addition we are able to use the same control algorithm for different precision grasp types, by changing the grasp type conditional variable. Finally, we can modify our controller to release objects instead of grasping them.
Given the pose of the hand in the world coordinate frame, which we can acquire from the robotic arm that is attached to, we can use the forward kinematics of the hand to compute the poses of each fingertip. Then using the force readings of each fingertip we can calculate the global direction of the net tangential force.
If the angle between the direction of the net tangential force and the direction of gravity is less than 90 degrees, i.e. the net tangential force's direction is towards the ground, we assume that the tangential force is due to gravity pulling the object, so the force controller tries to grasp it. If the angle is more than 90 degrees, i.e. the net tangential force's direction is upward, it means that something is pushing (or pulling) the object upward, in which case we assume that the object is touching on a support surface or someone is pulling the object so the controller increases the grasp size given to the posture mapping function proportionally to the normal force measured thus slowly releasing the object.
Opening the grasp is done by controlling the grasp size variable as follows: That way we can place objects on surfaces but also perform robot to human handovers, where the robot holds the object and the human grasps the object and slightly pushes or pulls it up, signaling to the robot that there is a support surface.
The robot then slowly releases the object by opening its grasp. We showcase these scenarios in the experiments' section. Based on these observations, we present our force controller in Figure . The hand starts in an open pre-grasp position, a latent point is sampled from the prior distribution of the posture mapping function, and given the desired grasp type and the grasp size a grasp posture, i.e. the joint angles of the fingers, is sampled.
The initial grasp size is set to the maximum value, and when the force controller comes into effect and depending on the state of the system and the forces on the fingertips grasp size changes by some value C, according to equations 1,2, until the desired normal force is achieved. To choose between grasping or releasing an object we use a finite state machine formulation.
When the hand reaches the desired grasp pose, which we assume is provided, the GRASP state is activated, in which the controller tries to grasp the object. When the controller detects that the tangential force applied to the object is coming from a support surface the state changes to the RELEASE state, in which the controller releases the object by opening the grasp.
You can see the full algorithm in Python-like pseudocode in Figure . To summarize, the advantages of our controller compared with previous approaches are threefold: 1) instead of controlling each joint of each finger of the hand we use only two variables, the grasp size and the grasp type, which allows us to perform multiple grasp types by changing only one variable while the grasp size variable is common among all grasp types, that greatly reduces the complexity of the control process compared to independently controlling a 21 DoF hand to perform different grasp types, 2) we do not rely on slip prediction for controlling the desired normal force, which involves gathering labeled data and works only for the hand poses in the training dataset, and 3) we can use our controller to also release objects instead of only grasping them.

Experimental Set-up.

For our experiments we used the Seed Robotics RH8D Hand , which is a robotic hand with 7 DoFs. The hand is equipped with the FTS-3 force sensors in each fingertip, which are high resolution tactile sensors that provide the 3D force applied in each fingertip. The sensor provides data at a rate of 50Hz. For the experiments the hand was mounted on a Kinova Gen3 7DoF robot.
To train the posture mapping function we used the CyberGlove to teleoperate the hand and collect 468 grasps belonging to three precision grasp  types: tripod, pinch, lateral tripod. The architecture of the cVAE model was the same as in , with the addition of the grasp type as a conditional variable, which was one-hot encoded.
We used 10 household objects shown in Figure . With the heaviest object weighing 380g and the lightest 1g. During the experiments the trajectories of the arm were prerecorded, while the hand was controlled online by our control algorithm.

Parameter tuning.

To select the values of the parameters in our controllers we conducted preliminary experiments where we tested lifting and releasing several objects, with different physical properties. To select the value of the normal offset force f of f set n , we used an empty plastic cup as our test object, and we choose a value such that the fingers do not deform the cup.
The final value of the parameter was set to -50 mN. To select the values of the gain G and the rate of decrease K, of the grasp size, we experimented with the heaviest object in our dataset, which is the mustard bottle and weighs 380g. The gain G was set to 2.0 such that the desired normal force would be enough to hold the object.
The rate of change of the grasp size was set to 100.0, based on the operating frequency of the force sensor and the range of values of the tangential force. For the tangential force averaging process we used a parameter value of α t = 0.7, because we want the controller to be sensitive to fast changes in its value, that can arise for example during lifting an object.
For the normal force averaging process we used a parameter value of α n = 0.5, as we do not want it to be affected by noise that could make the controller overconfident.

Experiments.

To explore the capabilities of our controller, we demonstrate five experiments of increasing complexity: 1) we picked and placed a bottle using a tripod grasp, 2) we picked, rotated and placed a chips can on a box using a tripod grasp, 3) we picked, rotated and handed over the chips can to a person using a tripod grasp, 4) we picked, rotated and handed over a brown foam brick to a person using a pinch grasp, 5) a person handed over a plastic cup to the robot, filled it with coins to increase its weight, and the robot then handed it back to the person using a tripod grasp.
You can see the execution of the first experiment in  In the middle row, for our third experiment, the robot picks up the chips can, rotates it 90 degrees, and hands it over to a person. In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp.
Fig. . In our fifth experiment, a person hands over an empty plastic cup to the robot, throws coins in it to increase its weight while the robot adjusts its grip to stabilize the object, and then hand overs the cup back to the person. force is below the offset f of f set n , 2) (green part) the robot lifts the object, as it tries to lift the tangential force increases, increasing the threshold, so the grasp size decreases to apply more normal force, 3) (orange part) the robot transports the object, you can see, in point A in the Figure, a perturbation in the tangential force when the robot begins to move, the controller responds by decreasing the grasp thus stabilizing the object, and 4) (blue part) the robot enters the releasing phase, where it lowers the arm until it detects that the tangential force is due to a support surface, then it stops lowering the arm and increases the grasp size slowly releasing the object.
In point B in the Figure, you can see that there is noise in the tangential force, due to the arm moving to place the object on the table, that is also reflected in the desired normal force. Because we use the desired normal force as a threshold and not as a reference signal this noise is not manifested in the control of the grasp size.
You can see the execution of the second experiment in the upper part of Figure . This experiment demonstrates the ability of the controller to handle arbitrary hand poses. The experiment is divided in four parts: 1) the robot enters the GRASP phase and the force controller generates grasps to achieve a normal contact force below the f of f set n threshold, 2) the robot lifts the object and adjusts the grasp size to avoid the object falling, 3) the hand rotates to place the chips can on the horizontal position, and 4) the robot enters the RELEASE phase, and the arm lowers until the object touches the box, when the hand detects the supporting surface, it starts to slowly release the object.
You can see the execution of the third experiment in the middle part of Figure . This experiment demonstrates the ability of the controller to perform robot to human handovers. The experiment is divided in four parts: 1) the robot enters the GRASP phase and the force controller generates grasps to achieve a normal contact force below the f of f set n threshold, 2) the robot lifts the object and adjusts the grasp size to avoid the object falling, 3) the hand rotates to place the chips can on the vertical position, and 4) the robot enters the RELEASE phase, the arm stays still, the human grasps the object from the bottom and slightly pushes it up, the hand then detects that there is a supporting surface and starts to slowly release the object.
You can see the execution of the fourth experiment in the bottom part of Figure . This experiment is similar to previous one, but the grasp type that the robot uses is a pinch grasp, that involves only the thumb and the index finger. To perform this we only had to alter the grasp type conditional variable that was given to the posture mapping function.
You can see the execution of the fifth experiment in the bottom part of Figure . In the first part (blue) of the experiment the robot closes its grasp, by reducing the grasp size, until the normal force is below the force offset. In the next three parts (pink, green, red) the person throws coins in the cup to increase its weight.
You can see in the signal plots that each time coins are added the tangential force decreases so the normal force threshold decreases too. The grasp sizes then decreases as well in order to apply more normal force. This experiment demonstrates the ability of the controller to handle perturbations in the weight of the object during grasping.

CONCLUSION

In summary, we presented a controller that uses force feedback integrated with conditional synergies to control a dexterous robotic hand to grasp and release objects. We demonstrated that our controller can lift objects of different weights and materials while avoiding slip, react online when the weight of the object changes, place them down on surfaces, and hand them over to humans.
In addition, the control architecture is modular, so the synergy grasp mapping component can be easily changed in order to control several precision grasp types. However, our experiments also revealed various limitations of our controller. For example our method fails to stabilize the object when rotational slip occurs.
In addition hardware limitations such as, slow update rates and noise in the force measurements can create problems that result in the object falling. In future work we plan to incorporate additional sensing modalities, such as vision to alleviate some of these issues.",['5.'],4837,multifieldqa_en,en,,daa4eb9d8b28a987b1c2c049200634cdc510636b19a64ccd," Title: Force Feedback Control For Dexterous Robotic Hands Using Conditional Postural SynergiesPublish Date: Unkown.Author List: Dimitrios Dimou, José Santos-Victor, Plinio Moreno. Figure 1: Example of modeling the contacts and friction during manipulation. Figure 2: Schematic representation of the proposed force controller. Figure 3: Our control algorithm in Python-like pseudocode. Figure 4: Our first experiment. Figure 5: The household objects used in our experiments. Figure 6: The average normal force applied by all fingers (blue line), the thresholds f threshold high n (purple dashed line) and f threshold low n (yellow dashed line), and the grasp size used in each time-step (red). The task is divided four stages: 1) (red part) the initial grasp of the object, in this stage the force controller closes the grasp until the applied normal force is 0.5. In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pinch grasp. Figure 7: The second experiment. The robot picks. up the chips can,rotates it 90 degrees and places back down. Figure 8: The third experiment. the robot. picks up the chip can, rotating it 90°, and places it down on a desk. Figure 9: The fourth experiment. in the bottom part of the figure, you can see the control signals during this task. The fifth experiment. is the final experiment, in which the robot lifts a bottle, transports it and places down on the desk. It is shown that our controller is able to lift objects of various weights and materials, adjust the grasp configuration during changes in the object’s weight, and perform object placements and object handovers. Figure 10: The final experiment is the first experiment, where the controller generates control signals for all the hand joints in a (lowdimensional) shared space (i.e. e. the conditional postural synergies framework to generate the grasp postures, i.e the finger configuration of the robot, at each time step based on forces measured on the robot's fingertips. This feedback is provided by tactile afferents located in the skin of the hand. For example, using open-loop position control to lift a heavy object may fail due to slip without any feedback mechanism to provide tactile information. For anthropomorphic robots to be able to perform dexterous tasks similar force feedback signals must be used to alleviate problems arising from uncertainty in measurements, and handle external perturbations. Figure 11: The last experiment. shows that our controllers are able to. operate various types of precision grips, by changing the synergy space according to the type of grasp. We show that our. controllers can lift objects and adjust the. grasp configuration. during changes. The force controller takes as input the force readings of the fingertips and computes the grasp size. To avoid slippage the desired normal contact force is calculated to be proportional to the tangential contact forces. The applied normal force is then controlled using the size of the grasp as a control variable. Larger grasp sizes mean less force is applied to the object. The grasp size is then given to the posture sampler that generates a grasp posture, i.e. the finger joint angles. We show that with our controller we can control a dexterous robotic hand to lift objects of different weights using three precision grasps. Our controller is also able to compensate and retain a stable grasp during changes in the objects' weight, for example when filling up a cup or emptying it. In addition we show how with the addition of the hand pose information we can use the controller to calculate if the Tangential force is due to gravity or due to a support surface. We use this information to perform handovers and place down objects on surfaces. We demonstrate by incorporating information about the world pose of the hands we can using our controller to perform additional tasks such as placing down and handing over objects. We develop a controller for a dexterOUS robotic hand that uses force feedback and the conditional synergies framework to perform dexterous manipulation tasks. We also show how we can easily use different precision grasp types, by changing only the grasp type variable which isgiven to the grasp posture mapping function. For example, in , a compliant anthropomorphic prosthetic hand was controlled using force sensing to maintain obj in a variety of settings. For. example, we show that we can. use force feedback to maintainobj in a range of settings, such as when filling and emptying a cup, or when placing down an object on a table. We can also use the conditional synergy framework to control a robotic arm that can lift and place objects on a surface with different weights. We conclude by showing that our controller can be used to perform complex manipulations of objects such as lifting and placing them on a desk or a table with different weight levels. We are the first to show that a force controller can control an anthropomorphic robotic hand using force feedback, and that it can perform complex manipulation tasks with different precision grip types. We have published our results in the Journal of Computer Vision and Pattern Science, which is published by the Proceedings of the National Academy of Sciences of the U.S. (PAS) and the American Chemical Society (ACS) (http://www.acsp.org/content/early/2013/09/07/08/14/13/acsp-v2.html) In previous works only lifting tasks had been considered. In our work we demonstrate that our approach can be used to perform more complex tasks, such as placing objects on surfaces and performing handovers, which was not done in previous works. Our control algorithm will use tactile feedback from the force sensors on the fingertips of the hand to decide the forces that need to be applied to the object in each step of the task. We avoid slip by defining a tangential force gain and a safety margin that work for a large number of objects. We use a synergistic framework to generate grasp postures, that is conditioned on two variables: the grasp type and the grasp size. We control only the two conditional variables greatly simplifying the control pipeline.Our control algorithm could also be applications to different hands as it does not depend on the hands configuration. It could be used for dexterous manipulation skills such as lifting and placing down objects. For more information on the project visit: http://www.cnn.com/2013/01/29/science/science-technology/features/force-control-with-anthropomorphic-robot-hand.html#storylink=cpy.ct.ct stability and avoid slip. The study was published in the open-access journal, Theoretical and Applied Physics (TCAP) – The journal is published by the Association for Computational Physics (ACP) and the corresponding journal is: Computer Vision and Pattern Recognition (CPR) – the journal of the American Association for Precision Engineering (AAP). For more details on the study, visit: www.cpr.org/2013-01-29/the-science-and-planning-of-robotic-hands-in-the-real-world-by-cpa.html. The paper is also available in the online edition of the journal Computer Vision & Pattern Analysis (CPA) – a peer-reviewed, open-source, peer-to-peer, online-only, open source, open access journal. It is available on www.theacp.org/. For more info on the book, visit the website: http:www.acp-journal.com/. The book is available in hardback and hardback, as well as on the online version of the hard-copy version, which is available for free on Amazon.com and the Kindle version for a limited time only. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here for details. The book can also be downloaded from: http:\/www.samaritans.org – the online version of this article includes a free download. Force f t = mg due to gravity. In order for the object to remain stable we need to apply a normal force. Previous works have used multi-modal tactile sensors like the BioTac sensor, which provides information about the pressure, skin deformation, and temperature, to predict slip and based on that signal to increase the appliednormal force. In our work we use the FTS3 sensors which is a low-cost sensor that measures the 3D force applied in each fingertip. We use this error signal to control the grasp size variable g size, that we use as a conditional variable in our posture mapping function. The grasp posture mapping func func is implemented as follows: where K is a parameter that controls the rate of decrease of the grasp sizes, and is experimentally selected. If the measured normal force is below the grasped size increases in order to apply less force. This is similar to the deadband control method, where instead of having a fixed reference point, an operating range is set. If a response is in this range, the controller does not exert any correction. The operating range changes according to the force signals from the robot's fingertips. The final grasp size for grasping is calculated as following: where G is the gain that includes the friction coefficient and the additional safety margin. So a smaller grasp size will result in a tighter grasp and greater normal force applied to the surface of the object. In practice, we use a high threshold that we want the measurednormal force to be below: where w threshold is the width of the threshold in mN . We use a low threshold to avoid the hand applying too much force that damages the hardware or the object we use, that is: where f set n = f des n − f n . The grasp size does not change even if there are small oscillations in the measured tangential and normal force f t is used, as a low pass filter. So when the error between the desired normal force and the actualnormal force is large the graspSize decreases so tighter grasp postures are generated inorder to apply more force. So the final graspSize variable is calculated: where g size is the distance between the thumb and the index finger in a grasp posture. In. practice, in. order to avoid oscillations, we used the desirednormal force as a. high threshold. That is: Where w threshold was set at mN (0, 1) so that the response is not in the range of 0, 1, or 0, 2, or 1, depending on the position of the robot. So if the response was in this. range, it would not be necessary to change the grasping posture. So we used a lower threshold of w threshold of mN of 0.5 (0,.5, 2) to avoid this. ion is based on the conditional postural synergies model presented in . It uses a conditional Variational Auto-Encoder model to generate grasps postures conditioned on additional variables such as the grasp size. The model is trained on a set of labeled grasp samples acquired by teleoperating a robotic hand using a data-glove. Using this model we are able to abstract away the low-level control of each joint of each finger and generate gr asps based on more general characteristics such as type and the size of the grasp. Finally, we can modify our controller to release objects instead of grasping them. The hand starts in an open pre-grasp position, a latent point is sampled from the prior distribution of the posture mapping function, and given the desired grasp type and grasp size a grasp posture is sampled. The initial grasp size is set to the maximum value, and when the force controller comes into effect and depending on the state of the system and the forces on the fingertips grasp size changes by some value C, according to equations 1,2, until the desired normal force is achieved. To choose between grasping or releasing an object we use a finite state machine formulation. We showcase these scenarios in the experiments' section of the paper. Back to Mail Online home. back to the page you came from. Back To the pageYou can see the full algorithm in Python-like pseudocode in Figure . Back to the pages you came From. The page you were from, the page that you were on, the one that you left, and the one you left behind. You can see it all in the appendix, the full version of which can be downloaded for free on the Google Play store or the Amazon.com store. You will need an internet connection to see the rest of the article and the link will be available in the next few days. You'll need an Internet connection to read the rest. The link will take you to the site that the article was originally published on, which is located at: http://www.jupiter.com/news/features/2013/01/29/the-future-of-robotic-hand.html#storylink=cpy. It will also provide a link to the website that explains how to download the software that allows you to control the robot's hand. It is also available for download on the Apple App Store and the Android App Store. The code is available on the company's website and the Apple Play Store page. It can also be downloaded from the Apple.com site for the same price as the original version of this article, but with the option to pay for the software through a subscription. The price of the software is $99.99 per month. It has been released by Apple in a partnership with Google. plexity of the control process compared to independently controlling a 21 DoF hand to perform different grasp types. We do not rely on slip prediction for controlling the desired normal force, which involves gathering labeled data and works only for the hand poses in the training dataset. We can use our controller to also release objects instead of only grasping them. We used 10 household objects shown in Figure . With the heaviest object weighing 380g and the lightest 1g. During the experiments the trajectories of the arm were prerecorded, while the hand was controlled online by our control algorithm. The architecture of the cVAE model was the same as in , with the addition of the grasp type as a conditional variable, which was one-hot encoded. The final value of the parameter was set to -50 mN. To select the values of the parameters in our controllers we conducted preliminary experiments where we tested lifting and releasing several objects, with different physical properties. For the experiments we used the Seed Robotics RH8D Hand, which is a robotic hand with 7 DoFs. The hand is equipped with the FTS-3 force sensors in each fingertip, which are high resolution tactile sensors that provide the 3D force applied inEach fingertip. The sensor provides data at a rate of 50Hz. To train the posture mapping functionWe used the CyberGlove to teleoperate the hand and collect 468 grasps belonging to three precision grasp  types: tripod, pinch, lateral tripod. The gain G and the rate of decrease K were set to 2.0 and 100.0, based on the operating frequency of the force sensor and the range ofvalues of the tangential force. The rate of change of the grasping size was set. to 100. 0.0. For our experiments we experimented with the heaviestobject in our dataset, which. is the mustard bottle and weighs 380g. The gains G and K were chosen so that the desirednormal force would be enough to hold the object. We. used an empty plastic cup as our test object, and we choose a value such that the fingers do not deform the cup. We wanted the controller to be sensitive to fast changes in its value, that can arise for example during lifting an object. To explore the capabilities of our controller, we demonstrate five experiments of increasing complexity: 1) we picked and placed a bottle using a tripod grasp, 2) we. picked, rotated and handed over the chips can on a box using a Tripod grasp, and 3) a person handed over a brown foam brick to the robot using a pinch grasp. In the bottom row, for our forth experiment, the robot picks up a foam brick, rotates it 180 degrees, and hands it over to a person, using a pin. In our fifth experiment, a person. throws coins in it to increase its weight while the robot adjusts its grip to stabilize the object and then hand overs the cup back to the person. The robot then picks up the chip can, rotating it 90 degrees, to increase the weight, and then hands it back to a. person. The experiment is divided in four parts: 1) the robot enters the GRASP phase and the force controller generates grasps to achieve a normal contact force below the f of f set n threshold. 2) The robot lifts the object and adjusts the grasp size to avoid the object falling, 3) the hand rotates to place the chips can on the horizontal position, and 4) the arm lowers until the object touches the box, when the hand detects the supporting surface, it starts to slowly release the object. The controller can also react online when the weight of the object changes, place them down on surfaces, and hand them over to humans. The control architecture is modular, so the synergy grasp mapping component can be easily changed in order to control several precision grasp types. However, our experiments also revealed various limitations of our controller. For example our method fails to stabilize the object when rotational slip  For example, the controller fails to handle objects of different weights and materials while avoiding slip. We presented a controller that uses force feedback integrated with conditional synergies to control a dexterous robotic hand to grasp and release objects. We demonstrated that our controller can lift objects of. different weights. While avoiding slip, we demonstrated that the controller can carry out robot to human handovers while avoiding. slip, and react online to changes in the object’s weight. In the next three parts (pink, green, red) the person throws coins in the cup to increase its weight. You can see in the signal plots that each time coins are added the tangential force decreases so the normal force threshold decreases too. The grasp sizes then decreases as well to apply more normal force. The final part of the experiment demonstrates the ability of the controller to handle perturbations in the weight. during grasping. To perform this we only had to alter the grasp type conditional variable that was given to the posture mapping function. To do this, we only needed to change the grasptype conditional variable to the position of the thumb and the index finger in the robot. The robot then uses a pinch grasp, that involves only the thumb. The hand then detects that there is a supporting surface and starts to Slowly release the. the object from the bottom and slightly pushes it up, the hand then presses the object up to the top of the box. The arm stays still, the arm stays. until it detects that the object is on a support surface, then it stops lowering the arm and increases the. grasp size slowly releasing the object, until it reaches the top position. This is the RELEASE phase, where it lowers the arm until the arm reaches the box and the hand stops lowering. The object is placed on the table and the robot releases it. The human then pushes the object to the bottom of the cup and pushes it. until the hand reaches the vertical position. In addition hardware limitations such as, slow update rates and noise in the force measurements can create problems that result in the object falling. In future work we plan to incorporate additional sensing modalities, such as vision to alleviate some of these issues."
What are the three teams that used conflict optimization in the challenge?,"Paper Info

Title: Conflict Optimization for Binary CSP Applied to Minimum Partition into Plane Subgraphs and Graph Coloring
Publish Date: 25 Mar 2023
Author List: Loïc Crombez (from LIMOS, Université Clermont Auvergne), Guilherme Da Fonseca (from LIS, Aix-Marseille Université), Florian Fontan (from Independent Researcher), Yan Gerard (from LIMOS, Université Clermont Auvergne), Aldo Gonzalez-Lorenzo (from LIS, Aix-Marseille Université), Pascal Lafourcade (from LIMOS, Université Clermont Auvergne), Luc Libralesso (from LIMOS, Université Clermont Auvergne), Benjamin Momège (from Independent Researcher), Jack Spalding-Jamieson (from David R. Cheriton School of Computer Science, University of Waterloo), Brandon Zhang (from Independent Researcher), Da Zheng (from Department of Computer Science, University of Illinois at Urbana-Champaign)

Figure

Figure 1: A partition of the input graph of the CG:SHOP2022 instance vispecn2518 into 57 plane graphs.It is the smallest instance of the challenge with 2518 segments.On top left, you see all 57 colors together.On top right, you see a clique of size 57, hence the solution is optimal.Each of the 57 colors is then presented in small figures.
Figure 2: Number of colors over time for the instance vispecn13806 using different values p.The algorithm uses σ = 0.15, easy vertices, q max = 59022, but does not use the BDFS nor any clique.
Figure 3: Number of colors over time with different values of q max obtained on the instance vispecn13806.Parameters are σ = 0.15, p = 1.2, no clique knowledge, and no BDFS.
Figure 4: Number of colors over time with and without clique knowledge and BDFS obtained on the instance vispecn13806.Parameters are σ = 0.15, p = 1.2, and q max = 1500000.
Figure 5: Number of colors over time for the instance vispecn13806 for different values of σ.In both figures the algorithm uses p = 1.2, easy vertices, q max = 59022, but does not use the BDFS nor any clique.For σ ≥ 0.25, no solution better than 248 colors is found.
Figure 6: Number of colors over time (in hours) for the instance vispecn13806.
Several CG:SHOP 2022 results.We compare the size of the largest known clique to the smallest coloring found by each team on a selection of 14 CG:SHOP 2022 instances.
[20][21][22][23][24][25] with state-of-the-art graph coloring algorithms.The conflict optimizer underperforms except on the geometric graphs r* and dsjr*.CE39-0007), SEVERITAS (ANR-20-CE39-0005) and by the French government IDEX-ISITE initiative 16-IDEX-0001 (CAP[20][21][22][23][24][25].The work of Luc Libralesso is supported by the French ANR PRC grant DECRYPT (ANR-18-CE39-0007).

abstract

CG:SHOP is an annual geometric optimization challenge and the 2022 edition proposed the problem of coloring a certain geometric graph defined by line segments. Surprisingly, the top three teams used the same technique, called conflict optimization. This technique has been introduced in the 2021 edition of the challenge, to solve a coordinated motion planning problem.
In this paper, we present the technique in the more general framework of binary constraint satisfaction problems (binary CSP). Then, the top three teams describe their different implementations of the same underlying strategy. We evaluate the performance of those implementations to vertex color not only geometric graphs, but also other types of graphs.

Introduction

The CG:SHOP challenge (Computational Geometry: Solving Hard Optimization Problems) is an annual geometric optimization competition, whose first edition took place in 2019. The 2022 edition proposed a problem called minimum partition into plane subgraphs. The input is a graph G embedded in the plane with edges drawn as straight line segments, and the goal is to partition the set of edges into a small number of plane graphs (Fig. ) .
This goal can be formulated as a vertex coloring problem on a graph G defined as follows. The vertices of G are the segments defining the edges of G, and the edges of G correspond to pairs of crossing segments (segments that intersect only at a common endpoint are not considered crossing). The three top-ranking teams (Lasa, Gitastrophe, and Shadoks) on the CG:SHOP 2022 challenge all used a common approach called conflict optimization while the fourth team used a SAT-Boosted Tabu Search .
Conflict optimization is a technique used by Shadoks to obtain the first place in the CG:SHOP 2021 challenge for low-makespan coordinated motion planning , and the main ideas of the technique lent themselves well to the 2022 challenge. Next, we describe the conflict optimizer as a metaheuristic to solve constraint satisfaction problems (CSP) .
We start by describing a CSP. A CSP is a triple of • variables X = (x 1 , . . . , x n ), Each of the 57 colors is then presented in small figures. • domains D = (D 1 , . . . , D n ), and • constraints R. Each variable x i must be assigned a value in the corresponding domain D i such that all constraints are satisfied.
In general, the constraints may forbid arbitrary subsets of values. We restrict our attention to a particular type of constraints (binary CSP ), which only involve pairs of assignments. A partial evaluation is an assignment of a subset of the variables, called evaluated, with the remaining variables called non-evaluated.
All constraints involving a non-evaluated variable are satisfied by default. We only consider assignments and partial assignments that satisfy all constraints. The conflict optimizer iteratively modifies a partial evaluation with the goal of emptying the set S of non-evaluated variables, at which point it stops.
At each step, a variable x i is removed from S. If there exists a value x ∈ D i that satisfies all constraints, then we assign the value x to the variable x i . Otherwise, we proceed as follows. For each possible value x ∈ D i , we consider the set K(i, x) of variables (other than x i ) that are part of constraints violated by the assignment x i = x.
We assign to x i the value x that minimizes where w(j) is a weight function to be described later. The variables x j ∈ K(i, x) become non-evaluated and added to S. The weight function should be such that w(j) increases each time x j is added to S, in order to avoid loops that keep moving the same variables back and forth from S. Let q(j) be the number of times x j became non-evaluated.
A possible weight function is w(j) = q(j). More generally, we can have w(j) = q(j) p for some exponent p (typically between 1 and 2). Of course, several details of the conflict optimizer are left open. For example, which element to choose from S, whether some random noise should be added to w, and the decision to restart the procedure from scratch after a certain time.
The CSP as is, does not apply to optimization problems. However, we can, impose a maximum value k of the objective function in order to obtain a CSP. The conflict optimizer was introduced in a low makespan coordinated motion planning setting. In that setting, the variables are the robots, the domains are their paths (of length at most k) and the constraints forbid collisions between two paths.
In the graph coloring setting, the domains are the k colors of the vertices and the constraints forbid adjacent vertices from having the same color. The conflict optimizer can be adapted to non-binary CSP, but in that case multiple variables may be unassigned for a single violated constraint. The strategy has some resemblance to the similarly named min-conflicts algorithm , but notable differences are that a partial evaluation is kept instead of an invalid evaluation and the weight function that changes over time.
While the conflict optimization strategy is simple, there are different ways to apply it to the graph coloring problem. The goal of the paper is to present how the top three teams applied it or complemented it with additional strategies. We compare the relative benefits of each variant on the instances given in the CG:SHOP 2022 challenge.
We also compare them to baselines on some instances issued from graph coloring benchmarks. The paper is organized as follows. Section 2 presents the details of the conflict optimization strategy applied to graph coloring. In the three sections that follow, the three teams Lasa, Gitastrophe, and Shadoks present the different parameters and modified strategies that they used to make the algorithm more efficient for the CG:SHOP 2022 challenge.
The last section is devoted to the experimental results.

Literature Review

The study of graph coloring goes back to the 4-color problem (1852) and it has been intensively studied since the 1970s (see for surveys). Many heuristics have been proposed , as well as exact algorithms . We briefly present two classes of algorithms: greedy algorithms and exact algorithms. Greedy algorithms.
These algorithms are used to find good quality initial solutions in a short amount of time. The classic greedy heuristic considers the vertices in arbitrary order and colors each vertex with the smallest non-conflicting color. The two most famous modern greedy heuristics are DSATUR and Recursive Largest First (RLF ) .
At each step (until all vertices are colored), DSATUR selects the vertex v that has the largest number of different colors in its neighbourhood. Ties are broken by selecting a vertex with maximum degree. The vertex v is colored with the smallest non-conflicting color. RLF searches for a large independent set I, assigns the vertices I the same color, removes I from G , and repeats until all vertices are colored.
Exact algorithms. Some exact methods use a branch-and-bound strategy, for example extending the DSATUR heuristic by allowing it to backtrack . Another type of exact method (branch-and-cut-and-price) decomposes the vertex coloring problem into an iterative resolution of two sub-problems . The ""master problem"" maintains a small set of valid colors using a set-covering formulation.
The ""pricing problem"" finds a new valid coloring that is promising by solving a maximum weight independent set problem. Exact algorithms are usually able to find the optimal coloring for graphs with a few hundred vertices. However, even the smallest CG:SHOP 2022 competition instances involve at least a few thousands vertices.

Conflict Optimization for Graph Coloring

Henceforth, we will only refer to the intersection conflict graph G induced by the instance. Vertices will refer to the vertices V (G ), and edges will refer to the edges E(G ). Our goal is to partition the vertices using a minimum set of k color classes C = {C 1 , . . . , C k }, where no two vertices in the same color class C i are incident to a common edge.

Conflict Optimization

TABUCOL inspired neighbourhood One classical approach for the vertex coloring involves allowing solutions with conflicting vertices (two adjacent vertices with the same color). It was introduced in 1987 and called TABUCOL. It starts with an initial solution, removes a color (usually the one with the least number of vertices), and assigns uncolored vertices with a new color among the remaining ones.
This is likely to lead to some conflicts (i.e. two adjacent vertices sharing a same color). The local search scheme selects a conflicting vertex, and tries to swap its color, choosing the new coloring that minimises the number of conflicts. If it reaches a state with no conflict, it provides a solution with one color less than the initial solution.
The process is repeated until the stopping criterion is met. While the original TABUCOL algorithm includes a ""tabu-list"" mechanism to avoid cycling, it is not always sufficient, and requires some hyper-parameter tuning in order to obtain a good performance on a large variety of instances. To overcome this issue, we use a neighbourhood, but replace the ""tabu-list"" by the conflict optimizer scheme presented above.
PARTIALCOL inspired neighbourhood PARTIALCOL another local search algorithm solving the vertex coloring problem was introduced in 2008. This algorithm proposes a new local search scheme that allows partial coloring (thus allowing uncolored vertices). The goal is to minimize the number of uncolored vertices.
Similarly to TABUCOL, PARTIALCOL starts with an initial solution, removes one color (unassigning its vertices), and performs local search iterations until no vertex is left uncolored. When coloring a vertex, the adjacent conflicting vertices are uncolored. Then, the algorithm repeats the process until all vertices are colored, or the stopping criterion is met.
This neighbourhood was also introduced alongside a tabu-search procedure. The tabu-search scheme is also replaced by a conflict-optimization scheme. Note that this neighbourhood was predominantly used by the other teams.

Finding Initial Solutions

Lasa team used two approaches to find initial solutions: 1. DSATUR is the classical graph coloring algorithm presented in Section 1. 2. Orientation greedy is almost the only algorithm where the geometry of the segments is used. If segments are almost parallel, it is likely that they do not intersect (thus forming an independent set).
This greedy algorithm first sorts the segments by orientation, ranging from − π 2 to π 2 . For each segment in this order, the algorithm tries to color it using the first available color. If no color has been found, a new color is created for coloring the considered segment. This algorithm is efficient, produces interesting initial solutions and takes into account the specificities of the competition.

Solution Initialization

The gitastrophe team uses the traditional greedy algorithm of Welsh and Powell to obtain initial solutions: order the vertices in decreasing order of degree, and assign each vertex the minimum-label color not used by its neighbors. During the challenge Gitastrophe attempted to use different orderings for the greedy algorithm, such as sorting by the slope of the line segment associated with each vertex (as the orientation greedy initialization presented in Section 3), and also tried numerous other strategies.
Ultimately, after running the solution optimizer for approximately the same amount of time, all initializations resulted in an equal number of colors.

Modifications to the Conflict Optimizer

Taking inspiration from memetic algorithms, which alternate between an intensification and a diversification stage, the algorithm continually switched between a phase using the above conflict score, and one minimizing only the number of conflicts. Thus during the conflict-minimization phase, the random variables f (C j ) and w(u) are both fixed equal to 1 leading to a conflict score
Each phase lasted for 10 5 iterations. Adding the conflict-minimization phase gave minor improvements to some of the challenge instances.

Shadoks

In this section, we describe the choices used by the Shadoks team for the options described in Section 2.1. The Shadoks generally chose to eliminate the color with the smallest number of elements. However, if the multistart option is toggled on, then a random color is used each time. The conflict set S is stored in a queue.
The Shadoks tried other strategies, but found that the queue gives the best results. The weight function used is w(u) = 1 + q(u) p , mostly with p = 1.2. The effect of the parameter p is shown in Fig. . Notice that in all figures, the number of colors shown is the average of ten executions of the code using different random seeds.
The algorithm uses σ = 0.15, easy vertices, q max = 59022, but does not use the BDFS nor any clique. If q(u) is larger than a threshold q max , the Shadoks set w(u) = ∞ so that the vertex u never reenters S. If at some point an uncolored vertex v is adjacent to some vertex u of infinite weight in every color class, then the conflict optimizer is restarted.
When restarting, the initial coloring is shuffled by moving some vertices from their initial color class to a new one. Looking at Fig. , the value of q max does not seem to have much influence as long as it is not too small. Throughout the challenge the Shadoks almost exclusively used q max = 2000 • (75000/m) 2 , where m is the number of vertices.
This value roughly ensures a restart every few hours. q max =0.5k q max =5k q max =50k q max =100k q max =250k The Shadoks use the function f as a Gaussian random variable of mean 1 and variance σ. A good default value is σ = 0.15. The effect of the variance is shown in Fig. . Notice that setting σ = 0 gives much worse results.
Option (e) The goal of BDFS is to further optimize very good solutions that the conflict optimizer is not able to improve otherwise. Fig. shows the influence of BDFS. While on this figure, the advantages of BDFS cannot be noticed, its use near the end of the challenge improved about 30 solutions. The bounded depth-first search (BDFS) algorithm tries to improve the dequeuing process.
The goal is to prevent a vertex in conflict with some adjacent colored vertices from entering in the conflict set. At the first level, the algorithm searches for a recoloring of some adjacent vertices which allows us to directly recolor the conflict vertex. If no solution is found, the algorithm  In both figures the algorithm uses p = 1.2, easy vertices, q max = 59022, but does not use the BDFS nor any clique.
For σ ≥ 0.25, no solution better than 248 colors is found. could recolor some vertices at larger distances from the conflict vertex. To do so, a local search is performed by trying to recolor vertices at a bounded distance from the conflict vertex in the current partial solution. The BDFS algorithm has two parameters: adjacency bound a max and depth d.
In order to recolor a vertex v, BDFS gets the set C of color classes with at most a max neighbors of v. If a class in C has no neighbor of v, v is assigned to C. Otherwise, for each class C ∈ C, BDFS tries to recolor the vertices in C which are adjacent to v by recursively calling itself with depth d − 1.
At depth d = 0 the algorithm stops trying to color the vertices. During the challenge the Shadoks used BDFS with parameters a max = 3 and d = 3. The depth was increased to 5 (resp. 7) when the number of vertices in the queue was 2 (resp. 1). Degeneracy order Given a target number of colors k, we call easy vertices a set of vertices Y such that, if the remainder of the vertices of G are colored using k colors, then we are guaranteed to be able to color all vertices of G with k colors.
This is obtained using the degeneracy order Y . To obtain Y we iteratively remove from the graph a vertex v that has at most k − 1 neighbors, appending v to the end of Y . We repeat until no other vertex can be added to Y . Notice that, once we color the remainder of the graph with at least k colors, we can use a greedy coloring for Y in order from last to first without increasing the number of colors used.
Removing the easy vertices reduces the total number of vertices, making the conflict optimizer more effective. The Shadoks always toggle this option on (the challenge instances contain from 0 to 23% easy vertices).

Results

We provide the results of the experiments performed with the code from the three teams on two classes of instances. First, we present the results on some selected CG:SHOP 2022 instances. These instances are intersection graphs of line segments. Second, we execute the code on graphs that are not intersection graphs, namely the classic DIMACS graphs , comparing the results of our conflict optimizer implementations to previous solutions.
The source code for the three teams is available at: • Lasa: https://github.com/librallu/dogs-color • Gitastrophe: https://github.com/jacketsj/cgshop2022-gitastrophe • Shadoks: https://github.com/gfonsecabr/shadoks-CGSHOP2022

CG:SHOP 2022 Instances

We selected 14 instances (out of 225) covering the different types of instances given in the CG:SHOP 2022 challenge. The results are presented in Table . For comparison, we executed the HEAD code on some instances using the default parameters. The table shows the smallest number of colors for which HEAD found a solution.
We ran HEAD for 1 hour of repetitions for each target number of colors on a single CPU core (the HEAD solver takes the target number of colors as a parameter and we increased this parameter one by one). At the end of the challenge, 8 colorings computed by Lasa, 11 colorings computed by Gitastrophe, and 23 colorings computed by Shadoks over 225 instances have been proved optimal (their number of colors is equal to the size of a clique).
In order to compare the efficiency of the algorithms, we executed the different implementations on the CG:SHOP instance vispecn13806. The edge density of this graph is 19%, the largest clique that we found has 177 vertices and the best coloring found during the challenge uses 218 colors. Notice that vispecn13806 is the same instance used in other Shadoks experiments in Section 5. Notice also that HEAD algorithm provides 283 colors after one hour compared to less than 240 colors for the conflict optimizers.
We ran the three implementations on three different servers and compared the results shown in Figure . For each implementation, the x coordinate is the running time in hours, while the y coordinate is the smallest number of colors found at that time.

Results on DIMACS Graphs

We tested the implementation of each team on the DIMACS instances to gauge the performance of the conflict optimizer on other classes of graphs. We compared our results to the best known bounds and to the state of the art coloring algorithms HEAD and QACOL . The time limit for Lasa's algorithms is 1 hour.
CWLS is Lasa's conflict optimizer with the neighbourhood presented in TABUCOL , while PWLS is the optimizer with the neighbourhood presented in PARTIALCOL . Gitastrophe algorithm ran 10 minutes after which the number of colors no longer decreases. Shadoks algorithm ran for 1 hour without the BDFS option (results with BDFS are worse).
Results are presented in Table . We only kept the difficult DIMACS instances. For the other instances, all the results match the best known bounds. The DIMACS instances had comparatively few edges (on the order of thousands or millions); the largest intersection graphs considered in the CG:SHOP challenge had over 1.5 billion edges.
We notice that the conflict optimizer works extremely poorly on random graphs, but it is fast and appears to perform well on geometric graphs (r250.5, r1000.1c, r1000.5, dsjr500.1c and dsjr500.5), matching the best-known results . Interestingly, these geometric graphs are not intersection graphs as in the CG:SHOP challenge, but are generated based on a distance threshold.
On the DIMACS graphs, Lasa implementation shows better performance than the other implementations.","['Lasa, Gitastrophe, and Shadoks.']",3791,multifieldqa_en,en,,191c14b84f0c8cdad3297f2bee552fb089178995208d7185," The conflict optimizer underperforms except on the geometric graphs r* and dsjr*.CE39-0007. The algorithm uses σ = 0.15, easy vertices, q max = 59022, but does not use the BDFS nor any clique. For σ ≥ 0.25, no solution better than 248 colors is found. This technique has been introduced in the 2021 edition of the challenge, to solve a coordinated motion planning problem. Surprisingly, the top three teams used the same technique, called conflict optimization, which was first used in the CG:SHOP challenge in 2019. We compare the size of the largest known clique to the smallest coloring found by each team on a selection of 14 CG: SHOP 2022 instances. We also evaluate the performance of those vertex color implementations not only for geometric graphs, but also for other types of graphs. We present the technique in the more general framework of binary constraint satisfaction problems (binary CSP) and show how it can be applied to a number of different types of geometric problems. The paper is published in the open-source journal, Theoretical and Applied Computer Science (TCS). It was written by Loïc Crombez, Guilherme Da Fonseca, Florian Fontan, Yan Gerard, Aldo Gonzalez-Lorenzo, Pascal Lafourcade, Luc Libralesso, Jack Spalding-Jamieson, Da Zheng, and Jack R. Cheriton School of Computer Science, University of Waterloo. It was published on 25 Mar 2023 by the French journal, LIS, Université Clermont Auvergne, and the University of Illinois at Urbana-Champaign. It is the first paper to use the Conflict Optimization for Binary CSP (COCS) technique, which has been used in CG:Shop challenges since the first edition in 2011. It has been published on the CCS website and is available for download from the LIS and LIS search engines. For more information, visit LIS’s website or the CCSA’S search engine, which is available from the link at the bottom of the page. For confidential support, call the National Institute of Standards and Technology (NIST) on 1-800-273-8255 or visit the NIST website at http://www.nist.org/. For confidential. support, contact the National Institutes of Standards (NIS) at 1-844-788-9090 or the NIS on the same link at http:www.NIS.org/NIS/NISS/NSC/NCS/NPS/NMS/NNS/NSS/NSP/NTS/N/S/N2/N3/N0/N1/N4/N5/N6/N7/N9/N8/N10/N12/N11/N15/N20/N18/N21/N19/N22/N26/N16/N27/N25/N28/N23/N24/N30/N32/N29/N31/N36/NEN/NAN/NU/N+1/0/1/2/0, and for more details on how to use this paper, visit http: www.lis.com/lis/ lis/lcos/lcshop.html. The three top-ranking teams (Lasa, Gitastrophe, and Shadoks) on the CG:SHOP 2022 challenge all used a common approach called conflict optimization. The strategy has some resemblance to the similarly named min-conflicts algorithm, but notable differences are that a partial evaluation is kept instead of an invalid evaluation and the weight function that changes over time. The conflict optimizer was introduced in a low makespan coordinated motion planning setting. In that setting, the variables are the robots, the domains are their paths (of length at most k) and the constraints forbid collisions between two paths. The goal of the paper is to present how the top three teams applied it and compare the relative benefits of each variant. We also compare the strategies with additional challenges or with additional information about CG: SHOP 2022. The paper is published in the online edition of the book, ""Conflict Optimization: A Metaheuristic to Solve Constraint Satisfaction Problems"" (Simon & Schuster, 2013). The book is also available in the hard-to-read online edition, which includes an abridged version of this article. The hard- to-read version of the article is available online at http://www.simonandschuster.com/conflict-optimization-a-metaheuristic-solve-constraint-satisfaction-problems-in-the-hardest-case-and-in the hard-to to read online version, as well as the online version with the additional information on how to use the SAT-Boosted Tabu Search. The online version also includes the full text of this paper, which is published online at the same time as the paperback version, and can be downloaded for free by clicking on the following link: http://bit.ly/ConflictOptimization-A-Paper-And-The-SAT-boosted-Tabu-Search-for-Hard-To-Read-For-Hardly-Huge-Comes-Included-CSPs-2-1-1. The paperback version also has the option of reading the entire paper in one go, which will take you to the end of the chapter. The book also includes a free download of the full version, which can be used to help students with reading comprehension and vocabulary. The full version is available on Amazon.com for $1.99, or $2.99 for $3.99 with the option to pay by the month, or by the day, depending on how much you want to spend on the book. The Kindle version has a price tag of $1, and the PDF version is $2, or £1, or €2, for the same price as the print-on-demand version. The PDF version also comes with a free print-out of the entire book, which you can download for free at the Amazon Kindle version, or you can buy on the Kindle version for $4, $5, $6, $7, $8, $9, and $10. Team Lasa, Gitastrophe, and Shadoks present the different parameters and modified strategies that they used to make the algorithm more efficient for the CG:SHOP 2022 challenge. The paper is organized as follows. Section 2 presents the details of the conflict optimization strategy applied to graph coloring. In the three sections that follow, the three teams present the various strategies they used. The last section is devoted to the experimental results for the conflict optimizer scheme. The study of graph coloring goes back to the 4-color problem (1852) and it has been intensively studied since the 1970s (see for surveys). Many heuristics have been proposed, as well as exact algorithms . We briefly present two classes of algorithms: greedy algorithms and exact algorithms. The classic greedy heuristic considers the vertices in arbitrary order and colors each vertices with the smallest non-conflicting color. The two most famous modern exact methods use a branch-and-bound strategy, for example extending the DSATUR heuristic by allowing it to backtrack. The ""pricing problem"" finds a new valid coloring that is promising by solving a maximum weight independent set problem. Exact algorithms are usually able to find the optimal coloring for graphs with a few hundred vertices. However, even the smallest CG: SHOP 2022 competition instances involve at least a few thousands vertices, so we use a ""tabu-list"" mechanism to avoid cycling. The original TABUCOL algorithm includes a "" tabu- list"" mechanism, but it is not always sufficient, and requires some hyper-parameter tuning in order to obtain a good performance on a large variety of instances. This is likely to lead to some conflicts (i.e. two adjacent vertices sharing a same color). We use a local search scheme to overcome this issue, but replace the ""Tabu- List"" by the conflict Optimizer scheme presented above. We will only refer to the intersection conflict graph G induced by the instance, and edges will referring to the edges E(G ). Our goal is to partition the vertice using a minimum set of k color classes C = {C 1 , . . . , C k } , C i where no two vertice in the same color class C i are incident to a common edge edge. It is possible to find a solution with one color less than the initial solution. If it reaches a state with no conflict, the process is repeated until the stopping criterion is met. The result is a solution that is more efficient than the previous one. The algorithm is called Conflict Optimization for Graph Coloring. It has been published in the Review of Graph Algorithms (Review of Graph Colored Algorithm) and is available for download from the arXiv.org website. The book also includes a free version of the code that was used to develop the algorithm for the paper, which can be downloaded from the ArXiv site. The code can also be downloaded as a PDF. algorithm proposes a new local search scheme that allows partial coloring. The goal is to minimize the number of uncolored vertices. The Shadoks generally chose to eliminate the color with the smallest number of elements. If the multistart option is toggled on, then a random color is used each time. The conflict set S is stored in a queue. The weight function used is w(u) = 1 + q(u), mostly with p = 1.2. The algorithm uses σ = 0.15, easy vertices, q max = 59022, but does not use the BDFS nor any clique. Looking at Fig. , the value of q max does not seem to have much much much influence a much much more powerful algorithm than the one used by the Shadoks team in this study. The results of this study are published in the open-source version of the code. The code is available for download from the GitHub repository here: http://www.jupiter.com/jupiter-project/gitastrophe/gitsastrophe-gives-a-report-on-the-results-of-this-challenge-and-how-we-decided-to-compete-against-the Shadoks-team-in-this challenge in the next-round. The full report is available in the next issue of the GITASTrophe journal, which is also available on the Google Play store. The GITastrophe team used the traditional greedy algorithm of Welsh and Powell to obtain initial solutions: order the vertices in decreasing order of degree, and assign each vertices the minimum-label color not used by its neighbors. For each segment in this order, the algorithm tries to color it using the first available color. If no color has been found, a new color is created for coloring the considered segment. After running the solution optimizer for approximately the same amount of time, all initializations resulted in an equal number of colors. This algorithm is efficient, produces interesting initial solutions and takes into account the specificities of the competition. It is also the only algorithm where the geometry of the segments is used. If at some point an uncolored vertex v is adjacent to somevertex u of infinite weight in every color class, then the conflict optimizer is restarted. This neighbourhood was also introduced alongside a tabu-search procedure. Note that this neighbourhood was predominantly used to find initial solutions by the other teams. Adding the conflict-minimization phase gave minor improvements to some of the challenge instances. For the full report, see http://jupiter.-project.org/gitzastrophe gitzostrophe/2014/01/26/report-gitzstrophe-2014-01-26.html. The report also includes the results of the second challenge, which was held in the U.S. on January 26. B bounded depth-first search (BDFS) algorithm tries to improve the dequeuing process. BDFS is to further optimize very good solutions that the conflict optimizer is not able to improve otherwise. Shadoks almost exclusively used q max = 2000 • (75000/m) 2 , where m is the number of vertices. The Shadoks use the function f as a Gaussian random variable of mean 1 and variance σ. For σ ≥ 0.25, no solution better than 248 colors is found. The source code for the three teams is available at: Lasa: https://www.cgshop.com/dogs-color • Gitastrophe:https://github.j/202222-librets- CG:SHOP 2022 Instances (out of 225) The results are presented in Table . For comparison, we executed the HEAD code on some instances using the default parameame parame. The results of the experiments performed with the code from three teams on two classes of instances. These instances are intersection graphs of line segments. Second, we execute the code on graphs that are not intersection graphs, namely the classic DIMACS graphs , comparing the results of our conflict Optimizer implementations to previous solutions. We present the results on some selected CG: SHOP 2022 instances. We selected 14 instances for the different types of instances given in the CG:ShOP 2022 challenge. We hope that this will help you understand the challenges and learn more about how to solve them. We are happy to help you with any questions you have about the challenge. Back to the page you came from. The challenge is open-source and can be downloaded from: http://bit.ly/Shadoks202020 CG:HOP 2022. The code is available for download from the GitHub repository at http://gcode.org/shadoks-2020:2022:20CG:HG:HC:H CG:SH:HSC:HCT:HTC:HST:HGT:HCC:HGC:HCL:HAC:HOC:HRC:HPC:HAS:HCS:HASH:HASC:HOST:HACK:HARCH:HACH:HALT:HATCH:HISTORIC.HAS.HASH.HAC.HACK.HSC.HCS.HASC.HCT.HOST.HST.HRC.HARCH.HATCH.HACH.HES.HEC.H ASC.HECT.HAST.HARC.H SC.HEST.HARAC. HSC. HAC.HC.HATT.HACT.HAT.HERT.HART.HGT.HALT.HISTORY. HARCH. HASH. HST. HATCH. HCT. HARC. HASC. HATT. HERT. HART. HAT. HALT. HHC. HTH. HECT. HACT. HGT. HOST. HACH. HATS. HTC. HATH. HAST. HEST. HHT. HCAR. H CT. HCHR. HCH. HTA. HH. HAUT. HT. H ACT. HAS. HACK. We ran HEAD for 1 hour of repetitions for each target number of colors on a single CPU core. At the end of the challenge, 8 colorings. computed by Lasa, 11 colorings computed by Gitastrophe, and 23. coloring computed by Shadoks over 225 instances have been proved optimal. The largest clique that we found has 177 vertices and the best coloring found during the challenge uses 218 colors. The conflict optimizer works extremely poorly on random graphs, but it is fast and appears to perform well on geometric graphs (r250, r1000.5, dsjr500.1c and dsJr 500.5), matching the best-known results. The DIMACS instances had comparatively few edges (on the order of thousands or millions) The largest intersection graphs considered in the CG:SHOP challenge had over 1.5 billion edges. We compared our results to the best known bounds and to the state of the art coloring algorithms HEAD and QACOL . The time limit for Lasa's algorithms is 1 hour. The timelimit for Gitstrophe algorithm is 10 minutes after which the number of Colors no longer decreases. The table shows the smallest number of colorings for which HEAD found a solution. The results are presented in Table . For the other instances, all the results match the bestknown bounds. For the DIMS graphs, Lasa implementation shows better performance than the other implementations. The result is shown in Figure . For each implementation, the x coordinate is the running time in hours, while the ycoord is the smallestNumber of colors found. at that time. For other graphs, the results are shown in the Table . We only kept the difficult DIMacS instances. We tested the implementation of each team to gauge the performance of the conflict Optimizer on other classes of graphs. For example, we found that on the largest intersection graph, the largest edge density was 19%. The results for the other graphs were similar to those shown in Section 5.4 of the study. For more details on the results, see the Discussion."
What did the court in In re Ferguson conclude about the transformation prong of the Bilski test?,"Xpp-pdf support utility
Xpp-pdf support utility
PATENT, TRADEMARK
& COPYRIGHT !
Reproduced with permission from BNA’s Patent,Trademark 11/20/09, 11/20/2009. Copyright ஽ 2009 by The Bu-reau of National Affairs, Inc. (800-372-1033) http://www.bna.com As the patent community anticipates a decision by the U.S. Supreme Court on subject matter patentability, recent rulings by the Federal Circuit and the Board of Patent Appeals and Interferences suggest strategies for preparing method patent applications that will sur- vive the Federal Circuit’s ‘‘machine-or-transformation’’ test.
The Changing Landscape of Method Claims in the Wake of In re Bilski:What We Can Learn from Recent Decisions of Federal Courts and the Board ofPatent Appeals rulings on software-based and other business methodpatent applications.
On review before the high court is the en banc ruling ‘‘Pure’’ business methods are out. Algorithms by the U.S. Court of Appeals for the Federal Circuit1 are out. Machines and data transformations that, in order to be eligible for patent protection, an in- ventive method must either be tied to a machine or re- While the patent community waits for the Supreme cite a transformation of an article.2 This ‘‘machine-or- Court’s decision in Bilski v. Kappos, No. 08-964 (U.S.
transformation’’ test replaced the Freeman-Walter- argued Nov. 9, 2009) (79 PTCJ 33, 11/13/09), patent ap- Abele3 test and the ‘‘useful, concrete and tangible plicants seeking to write patentable claims are stuckwith trying to conform to the lower courts’ most recent 1 In re Bilski, 545 F.3d 943, 88 USPQ2d 1385 (Fed. Cir.
2008) (en banc) (77 PTCJ 4, 11/7/08).
2 ‘‘The machine-or-transformation test is a two-branched Adriana Suringa Luedke and Bridget M. Hay- inquiry; an applicant may show that a process claim satisfies den are lawyers at Dorsey & Whitney, Min- § 101 either by showing that his claim is tied to a particular neapolis. Luedke can be reached at machine, or by showing that his claim transforms an article.’’ leudke.adriana@dorsey.com. Hayden can be reached at hayden.bridget@dorsey.com. 3 In re Freeman, 573 F.2d 1237, 197 USPQ 464 (C.C.P.A.
1978); In re Walter, 618 F.2d 758, 205 USPQ 397 (C.C.P.A.
COPYRIGHT ஽ 2009 BY THE BUREAU OF NATIONAL AFFAIRS, INC.
result’’ inquiry advocated in State Street,4 each of gregating, and selling real estate property and claims which had been applied by the Federal Circuit and its reciting a method of performing tax-deferred real estate predecessor court in various cases, and both of which property exchanges were not statutory under Section 101. Since no machine was recited, the only issue be- In this article, we examine the 2008 decision of the fore the court was whether the claims met the ‘‘trans- Federal Circuit, federal district court decisions, and de- formation’’ prong of the Bilski test.13 The court held cisions of Patent and Trademark Office’s Board of that the claims ‘‘involve[d] only the transformation or Patent Appeals and Interferences. Based upon the out- manipulation of legal obligations and relationships’’ comes in these cases, we offer guidance as to what is that did not qualify under Bilski.14 patent-eligible under 35 U.S.C. § 101, strategies for pre- Concerning the recitation of the ‘‘creation of deed- senting methods in patent applications and claiming shares’’ in some of the claims, the court found that the these methods, and possible ‘‘fixes’’ for applications deedshares themselves were not physical objects, but drafted pre-Bilski that must now withstand scrutiny un- only represented intangible legal ownership interests in der the new machine-or-transformation test.
property.15 Therefore, the creation of deedshares wasnot sufficient to establish patent eligibility under Bil- A number of recent federal court and board decisions have applied the patent eligibility test set forth in Bilski, implemented step to an otherwise obvious method was not sufficient to avoid invalidity of the claim. In KingPharmeuticals Inc. v. Eon Labs Inc.,17 the district court held invalid claims to a method of increasing the oral Several cases have addressed (and rejected) claims bioavailability of metaxalone because the claims were obvious over the prior art asserted by the accused in- In In re Ferguson,6 the Federal Circuit reviewed the board’s rejection of claims directed to a method of mar- Two dependent claims added a step of informing the keting a product and a ‘‘paradigm’’ for marketing soft- patient of certain results, which the patentee argued ware as nonstatutory subject matter under Section was not obvious. The court rejected this argument, con- 101.7 The appellate court affirmed the board’s rejection, cluding that ‘‘[b]ecause the food effect is an inherent concluding that the method claims were neither tied to property of the prior art and, therefore, unpatentable, a particular machine or apparatus nor did they trans- then informing a patient of that inherent property is form a particular article into a different state or thing.8 The court defined a machine broadly as ‘‘a concrete The court also commented that the added step of in- thing, consisting of parts, or of certain devices or com- forming the patient did not meet the patent eligibility binations of devices,’’ which did not include the ‘‘shared standard set forth in Bilski because the step did not re- marketing force’’ to which the method claims were quire use of a machine or transform the metaxalone into a different state or thing.19 Notably, this conclusion The claims directed to a ‘‘paradigm’’ were non- runs counter to the Supreme Court’s instruction that statutory because the claims did not fall within any of claims are to be examined ‘‘as a whole’’ and not dis- the four statutory categories (machines, manufactures, sected into old and new elements and that are evaluated compositions of matter and processes). Concerning the two closest possible categories, the court concluded Recent board decisions have been consistent with the that the claimed paradigm was not a process, because holdings of the federal courts. For example, in Ex parte no act or series of acts was required, and was not a Roberts,21 the board found ineligible under Section 101 manufacture, because it was not a tangible article re- a ‘‘method of creating a real estate investment instru- sulting from a process of manufacture.10 Concerning ment adapted for performing tax-deferred exchanges’’ the recitation of a ‘‘marketing company’’ in the para- because the claim did not satisfy either the machine or digm claims, the court concluded that the patent appli- cants did ‘‘no more than provide an abstract idea—a Similarly, in Ex parte Haworth,23 a method for ‘‘at- business model for an intangible marketing com- tempting to collect payments from customers having delinquent accounts concurrently with a partner that In Fort Properties Inc. v. American Master Lease owns the delinquent accounts’’ was found to be patent LLC,12 the California district court held that claims re- ineligible because the claim wording was ‘‘broad in that citing a series of transactions involving acquiring, ag- 1980); In re Abele, 684 F.2d 902, 214 USPQ 682 (C.C.P.A.
4 State Street Bank & Trust Co. v. Signature Financial 16 See Ex parte Roberts., 2009-004444 at 4-5 (B.P.A.I. June Group, 149 F.3d 1368, 1370, 47 USPQ2d 1596 (Fed. Cir. 1998) 19, 2009) (holding a ‘‘method of creating a real estate invest- ment instrument adapted for performing tax-deferred ex- changes’’ patent ineligible as not passing the machine-or- 7 The court accepted the board’s definition of ‘‘paradigm’’ 17 593 F. Supp.2d 501 (E.D.N.Y. 2009).
to mean ‘‘a pattern, example or model.’’ Id. at 1362.
20 See Diamond v. Diehr, 450 U.S. 175, 188 (1981).
21 No. 2009-004444 (B.P.A.I. June 19, 2009).
12 2009 WL 249205, *5 (C.D. Cal. Jan. 22, 2009).
23 No. 2009-000350 (B.P.A.I. July 30, 2009).
it refers generally to extending an offer, receiving an machine. Accordingly, the process claims . . . are not acceptance, and paying a commission’’ and did not in- voke, recite or limit the method of implementation us-ing any particular machine or apparatus.24 The court also evaluated similar claims that recited the use of a ‘‘comparator’’ to perform the recited pixel- B. Software Claims Not Expressly Tied to a ‘Particular by-pixel comparison and held that this recitation also did not mandate a machine.29 While the court acknowl-edged that software was offered as one ‘‘option,’’ the Other cases have addressed software methods where court concluded that the claimed function of the com- the claim language was either not expressly tied to com- parator could also be performed in one’s mind or on pa- puter hardware components or the ties to computer per such that a machine was not required. The court components were somewhat ambiguous. In several further noted that, even though the ‘‘comparator’’ was cases, courts have rejected the recitation of generic defined as a ‘‘device,’’ ‘‘the use of the term ‘device’ is computer components as sufficient to satisfy the ‘‘ma- not synonymous with machine.’’30 As a result, none of chine’’ prong of the Bilski test. A number of these deci- the claims at issue met the ‘‘machine’’ prong of the Bil- sions also addressed the ‘‘transformation’’ prong of the Concerning the ‘‘transformation’’ prong, the court re- In Research Corporation Technology Inc. v. Mi- lied in particular upon the Abele decision in expanding crosoft Corp.,25 the district court considered the patent the requirements of this test by requiring that the eligibility of method claims in six patents directed to claimed transformation process be both ‘‘(1) limited to methods of halftoning of gray scale images by using a transformation of specific data, and 2) limited to a vi- pixel-by-pixel comparison of the image against a blue sual depiction representing specific objects or sub- noise mask. Relying on the Federal Circuit’s Bilski stances.’’31 It then concluded that a number of the analysis as well as a decision of its predecessor court, patent claims did not meet the second prong of this ex- In re Abele,26 the judge concluded that a number of the panded test because the claims did not ‘‘require any vi- sual depiction or subsequent display’’ even though the transformation test set forth in Bilski.27 claimed method did transform specific image data.32 Concerning the ‘‘machine’’ prong, the district court The district court also found other claims patent- found that the pixel-by-pixel comparison recited in the eligible under Section 101 because these claims recited claims did not require the use of a machine, but could the use of the comparison data ‘‘to produce a halftoned ‘‘dictate[d] a transformation of specific data, and [were] be done on a sheet of paper using a pen. The com- further limited to a visual depiction which represents parison uses formulas and numbers to generate a bi- specific objects.’’33 Thus, the patent eligibility of the nary value to determine the placement of a dot at a claims turned on whether the claims recited the use of location. Formulas and numbers not tied to a particu- the transformed data to generate a display.
lar machine cannot be patented, under the machine In DealerTrack Inc. v. Huber,34 the district court prong, even with a field-of-use limitation because granted a summary judgment of invalidity under § 101 they represent fundamental principles, and to do so of patent claims directed to ‘‘a computer aided method’’ would preempt the entire field. The patent claims . . .
of managing a credit application reciting the following do not mandate the use of a machine to achieve their algorithmic and algebraic ends. Simply because adigital apparatus such as a computer, calculator, or [A] receiving credit application data from a remote the like could assist with this comparison does not render it patent eligible material. RCT’s argument [B] selectively forwarding the credit application data that a pixel by its nature is electronic and therefore to remote funding source terminal devices; necessitates a machine is a post solution argumentand the Court rejects it. The claim construction specifies that the comparison is of a value to a mask 29 The term ‘‘comparator’’ was construed by the court to be (or set of values) to determine whether the dot is a ‘‘device (or collection of operations, as in software) that com- turned on at a specific location. This process does pares an input number (called the operand) to a number pre- not require a particular machine. The Bilski test is stored in the comparator (called the threshold) and produces clear: the process claims must be tied to a particular as output a binary value (such as ‘‘0,’’ zero) if the input is alge-braically less than the threshold [the result of comparing anoperand against a fixed threshold and setting an operand less 24 Id. at 9-10. See also, e.g., Ex parte Farnes, 2009-002770 than the threshold to one value and an operand greater than (B.P.A.I. June 2, 2009) (rejecting a method claim for develop- or equal to the threshold to another value], and produces the ing a solution to a customer experience issue including steps opposite binary value (such as ‘‘1,’’ one) if the input is algebra- of: ‘‘identifying a target customer,’’ ‘‘defining a current cus- ically greater than or equal to the threshold.’’ Id. at *17 (em- tomer experience,’’ ‘‘summarizing values and benefits’’ to pro- vide to the customer, and ‘‘identifying metrics for measuring success’’); Ex parte Salinkas, 2009-002768 (B.P.A.I. May 18, 31 Id. at *9. Notably, Bilski concluded that the Abele visual 2009) (finding patent ineligible a method of launching a depiction was ‘‘sufficient’’ to establish transformation (545 knowledge network involving ‘‘selecting an executive spon- F.3d at 963), while the Research Corporation court went fur- sor,’’ ‘‘forming a core team of experts,’’ and ‘‘providing pre- ther by making visual depiction ‘‘required’’ to establish trans- 25 2009 WL 2413623 (D. Ariz. July 28, 2009) (78 PTCJ 432, 26 684 F.2d 902, 214 USPQ 682 (C.C.P.A. 1982).
34 2009 WL 2020761 (C.D. Cal. July 7, 2009) (78 PTCJ 341, PATENT, TRADEMARK & COPYRIGHT JOURNAL [C] forwarding funding decision data from at least tation of ‘over the Internet’ suffices to tie a process one of the remote funding source terminal de- claim to a particular machine’’ and concluded that it vices to the remote application entry and display The internet continues to exist despite the addition [D] wherein the selectively forwarding the credit ap- or subtraction of any particular piece of hardware. It may be supposed that the internet itself, rather than [E] sending at least a portion of a credit application any underlying computer or set of computers, is the to more than one of said remote funding sources ‘‘machine’’ to which plaintiff refers. Yet the internet is an abstraction. If every computer user in the world [F] sending at least a portion of a credit application unplugged from the internet, the internet would to more than one of said remote funding sources cease to exist, although every molecule of every ma- sequentially until a finding [sic ] source returns a chine remained in place. One can touch a computer or a network cable, but one cannot touch ‘‘the inter- [G] sending . . . a credit application . . . after a prede- Additionally, the court found that the recitation of the [H] sending the credit application from a first remote internet in this case merely constituted ‘‘insignificant funding source to a second remote funding extra-solution activity’’ and therefore did not qualify as a ‘‘particular machine’’ under Bilski.41 ‘‘[T]ossing in In concluding that the claim did not satisfy the Bilski references to internet commerce’’ was not sufficient to machine-or-transformation test, the court held that the render ‘‘a mental process for collecting data and weigh- claimed central processor, remote application and dis- ing values’’ patent-eligible.42 Additionally, ‘‘limiting’’ play device, and remote funding source terminal device the claim to use over the Internet was not a meaningful could be ‘‘any device’’ and did not constitute a ‘‘’par- limitation, such that the claims ‘‘broadly preempt the ticular machine’ within the meaning of Bilski.’’35 The fundamental mental process of fraud detection using court relied upon several board decisions to support its associations between credit cards.’’43 premise that ‘‘claims reciting the use of general purpose processors or computers do not satisfy the test.’’36 claim,44 notwithstanding the Federal Circuit’s holding In Cybersource Corp. v. Retail Decisions Inc.,37 the in In re Beauregard,45 the district court concluded that district court held claims for ‘‘a method for verifying the ‘‘there is at present no legal doctrine creating a special validity of a credit card transaction over the Internet’’ ‘‘Beauregard claim’’ that would exempt the claim from and ‘‘a computer readable medium containing program the analysis of Bilski.’’ Moreover, ‘‘[s]imply appending instructions for detecting fraud in a credit card transac- ‘A computer readable media including program instruc- tion . . . over the Internet’’ invalid under § 101 based tions’ to an otherwise non-statutory process claim is in- upon the court’s interpretation of Bilski.
sufficient to make it statutory.’’46 Consequently, this Concerning the method claim, the court considered claim also failed the Bilski test.
both the ‘‘transformation’’ and ‘‘machine’’ prongs of the In at least one instance, the U.S. International Trade Bilski test. In concluding that there was no transforma- Commission has interpreted the ‘‘machine’’ prong of tion, the court focused on the intangibility of the ma- Bilski less stringently than did the district courts in the nipulated data. According to the court, transformation cases discussed above. In In the Matter of Certain Video is limited to transformation of a physical article or sub- Game Machines and Related Three-Dimensional Point- stance. Accordingly, the method claim did not qualify ing Devices,47 the accused infringer filed a motion for because the data representing credit cards did not rep- summary judgment alleging that the asserted claims resent tangible articles but instead an intangible series impermissibly sought to patent a mathematical algo- of rights and obligations existing between the account rithm. According to the movant, the recitations of a ‘‘3D pointing device,’’ ‘‘handheld device,’’ or ‘‘free space Concerning whether the claimed method was tied to pointing device’’ were not sufficient to tie the claims to a particular machine, the court assessed whether ‘‘reci- a particular machine, but served ‘‘only to limit the field-of-use of the claimed mathematical algorithm and [did] not otherwise impart patentability on the claimed math- Id. at *3. The court relied upon the holdings in Ex parte Gutta, No. 2008-3000 at 5-6 (B.P.A.I. Jan. 15, 2009) (stating In denying the motion for summary judgment, the ‘‘[t]he recitation in the preamble of ‘[a] computerized method ITC first noted that, ‘‘[w]hile the ultimate determination performed by a data processor’ adds nothing more than a gen- of whether the asserted claims are patentable under eral purpose computer that is associated with the steps of the § 101 is a question of law, the Federal Circuit has ac- process in an unspecified manner.’’); Ex parte Nawathe, No.
2007-3360, 2009 WL 327520, *4 (B.P.A.I. Feb. 9, 2009) (finding‘‘the computerized recitation purports to a general purpose processor [], as opposed to a particular computer specifically programmed for executing the steps of the claimed method.’’); and Ex parte Cornea-Hasegan, No. 2008-4742 at 9-10 (B.P.A.I.
Jan. 13, 2009) (indicating the appellant does not dispute ‘‘the recitation of a processor does not limit the process steps to any 44 Claims having this format are called ‘‘Beauregard’’ specific machine or apparatus.’’). The court also cited Cyber- claims and were found to not be barred by the traditional source Corp. v. Retail Decisions Inc., (discussed below), in sup- printed matter rule in In re Beauregard, 53 F.3d 1583, 1584, 35 port of its interpretation of the required ‘‘particular machine.’’ 37 620 F. Supp. 2d 1068, 92 USPQ2d 1011 (N.D. Cal. 2009) 47 2009 WL 1070801 (U.S.I.T.C. 2009).
knowledged that ‘there may be cases in which the legal given a dataset of feature vectors associated with the question as to patentable subject matter may turn on subsidiary factual issues’ ’’ (citation omitted). In con- for each binary partition under consideration, rank- struing the claims, the tribunal found that there was a ing features using two-category feature ranking; and genuine dispute as to whether the claimed ‘‘devices’’represented a ‘‘particular machine’’ under the Bilski while the predetermined number of features has not test and whether the claimed ‘‘two-dimensional rota- yet been selected: picking a binary partition p; tional transform’’ was merely a mathematical calcula- selecting a feature based on the ranking for binary tion or instead meant ‘‘changing the mathematical rep- resentation of a two-dimensional quantity from oneframe of reference to a differently-oriented frame of ref- adding the selected feature to an output list if not al- erence’’ as asserted by the patentee. Additionally, the ready present in the output list and removing the se- dispute over the meaning of the claimed ‘‘two- lected feature from further consideration for the bi- dimensional rotational transform’’ also raised a dis- puted issue as to whether this element recited a trans- Notably, while the independent claim failed the formation that would qualify under the ‘‘transforma- machine-or-transformation test, its dependent claim tion’’ prong of Bilski. Given these disputed issues, the was eligible because it recited, ‘‘further comprising us- ITC concluded that it was inappropriate to grant sum- ing the selected features in training a classifier for clas- mary judgment as to the patent eligibility of the claims.
sifying data into categories.’’ In view of the specifica- A similar conclusion was reached in Versata Soft- tion, the board indicated that the ‘‘classifier’’ was a par- ware Inc. v. Sun Microsystems Inc.,48 in which the dis- ticular machine ‘‘in that it performs a particular data trict court denied the defendant’s motion for summary classification function that is beyond mere general pur- judgment of invalidity under Section 101 based upon pose computing.’’53 The board also concluded that the the Bilski court’s refusal ‘‘to adopt a broad exclusion claim ‘‘transforms a particular article into a different over software or any other such category of subject state or thing, namely by transforming an untrained matter beyond the exclusion of claims drawn to funda- classifier into a trained classifier.’’54 In Ex parte Casati,55 the board reversed the examin- Less stringent ‘‘machine’’ prong analyses are also er’s Section 101 rejection of a method claim reciting: found at the board level. For example, in Ex parteSchrader,50 the board held patent-eligible under Bilski A method of analyzing data and making predictions, reading process execution data from logs for a busi- A method for obtaining feedback from consumers re- ceiving an advertisement from an ad provided by anad provider through an interactive channel, the collecting the process execution data and storing the process execution data in a memory defining a ware-house; creating a feedback panel including at least one feed-back response concerning said advertisement; and analyzing the process execution data; generatingprediction models in response to the analyzing; and providing said feedback panel to said consumers, using the prediction models to predict an occurrence said feedback panel being activated by a consumer to of an exception in the business process.
provide said feedback response concerning said ad-vertisement to said ad provider through said interac- In this case, giving consideration to the specification, which ‘‘unequivocally describes the data warehouse aspart of the overall system apparatus, and subsequent Here, the board found ‘‘interactive channel’’ to be descriptions describe the memory/warehouse device in part of an ‘‘overall patent eligible system of appara- terms of machine executable functions,’’ the board con- tuses’’ when viewed in the context of the specification, cluded that ‘‘one of ordinary skill in the art would un- which included ‘‘the Internet and World Wide Web, In- derstand that the claimed storing of process execution teractive Television, and self service devices, such as In- data in a memory defining a warehouse constitutes formation Kiosks and Automated Teller Machines.’’51 patent-eligible subject matter under § 101 because the In another recent decision, Ex parte Forman,52 the memory/warehouse element ties the claims to a particu- board found a ‘‘computer-implemented feature selec- tion method’’ including a ‘‘classifier’’ eligible under Other recent board decisions have reached the oppo- Section 101 because it satisfied both the machine and transformation prong. Here, the ‘‘classifier’’ was recitedin a dependent claim, in which its independent claim re-cited: 53 Id. at 13.
54 Id. See also Ex parte Busche, No. 2008-004750 (B.P.A.I.
A computer-implemented feature selection method May 28, 2009) (holding a process claim and a computer pro- for selecting a predetermined number of features for gram product claim, each reciting training a machine, ‘‘are di- a set of binary partitions over a set of categories rected to machines that have such structure as may be adaptedby training.’’) 55 No. 2009-005786 (B.P.A.I. July 31, 2009).
48 2009 WL 1084412, *1 (E.D. Tex. March 31, 2009).
56 Id. at 7. See also Ex parte Dickerson, No. 2009-001172 at 49 Citing Bilski, 545 F.3d at 959 n. 23.
16 (B.P.A.I. July 9, 2009) (holding claims that ‘‘recite a comput- 50 No. 2009-009098 (B.P.A.I. Aug. 31, 2009).
erized method which includes a step of outputting information from a computer . . . are tied to a particular machine or appa- 52 No. 2008-005348 (B.P.A.I. Aug. 17, 2009).
PATENT, TRADEMARK & COPYRIGHT JOURNAL implemented methods ineligible under the Bilski test transformation test applied to this type of claim.63 because the claims failed to tie the method steps to any Then, applying the Bilski test, the board concluded that concrete parts, devices, or combinations of devices. For the claim did not qualify. According to the board, the example, in Ex parte Holtz,57 the board found ineligible under Section 101 a ‘‘method for comparing file tree de-scriptions’’ because the claim ‘‘obtains data (a file struc- does not transform physical subject matter and is not ture), compares data (file structures), generates a tied to a particular machine. . . . Limiting the claims change log, and optimizes the change log without tying to computer readable media does not add any practi- these steps to any concrete parts, devices, or combina- cal limitation to the scope of the claim. Such a field- tions of devices’’ and the ‘‘file structures’’ did not repre- of-use limitation is insufficient to render an other- Similarly, in Ex parte Gutta,58 the board held ineli- gible under § 101 a ‘‘method for identifying one or moremean items for a plurality of items . . . having a sym- II. The Current Scope of Patent Eligibility bolic value of a symbolic attribute,’’ concluding that the These recent cases establish that some types of meth- claim ‘‘computes a variance and selects a mean item ods are clearly patent-eligible under Section 101, others without tying these steps to any concrete parts, devices, clearly are not eligible, and yet others may be depend- or combinations of devices’’ and ‘‘symbolic values are ing on how they are described and claimed.
neither physical objects nor do they represent physicalobjects.’’ First, the eligibility of system and apparatus claims is largely unaffected by the Bilski decision, with the ca- In contrast to the district court’s decision in Cyber- veat that such claims may be more closely scrutinized source Corp., discussed supra, in a recent board deci- for compliance with Diamond v. Diehr and Gottschalk sion, Ex parte Bodin,59 ‘‘a computer program product’’ v. Benson, which prohibit patenting of a claim directed was found to be patent-eligible subject matter as being to ‘‘laws of nature, natural phenomena, [or] abstract embodied in a ‘‘computer readable medium.’’ Here, the board considered whether the phrase ‘‘recorded on the Also, methods that are performed at least in part by a recording medium’’ as it is recited in the body of the machine qualify for patent eligibility under Section 101.
claims was the same as ‘‘recorded on a computer- Thus, for example, some computer-implemented and readable medium.’’ Acknowledging the differences be- software-related inventions remain patentable as long tween a statutory claim to a data structure stored on a as they are properly described and claimed as being computer readable medium compared to a nonstatutory performed by a computer or computer components.
claim to a data structure that referred to ideas reflected The tie to a machine, however, cannot merely be im- in nonstatutory processes, the board stated: ‘‘[w]hen plicit based upon the description and context of the ap- functional descriptive material is recorded on some plication or general language in the preamble of the computer-readable medium, it becomes structurally claim. Instead, the use of a machine to perform one or and functionally interrelated to the medium and will be more of the claimed functions must be expressly de- statutory in most cases since use of technology permits scribed in the body of the claim so as to be a meaning- the function of the descriptive material to be real- ful limitation on the claim. If a method claim can be read in such a way that all functions can be performed Similarly, in Ex parte Azuma,61 a claim to a ‘‘com- by a human, it will likely not pass the machine prong of puter program product . . . comprising: a computer us- able medium’’ was found to be directed to statutory The ‘‘Interim Examination Instructions for Evaluat- subject matter under § 101 because the language ‘‘com- ing Subject Matter Eligibility Under 35 U.S.C. § 101’’ re- puter usable medium’’ referred to tangible storage me- cently issued by the Patent and Trademark Office con- dia, such as a server, floppy drive, main memory and firm that the recitation of a general purpose computer hard disk as disclosed by appellant’s specification, and is sufficient to satisfy Section 101 where the general did not ‘‘implicate the use of a carrier wave.’’ purpose computer is ‘‘programmed to perform the pro- In an older decision, Ex parte Cornea-Hasegan,62 cess steps, . . . in effect, becom[ing] a special purpose however, the Board seemingly came to the opposite conclusion, holding that a claim reciting ‘‘a computer Concerning data transformation, there seems to be readable media including program instructions which agreement of the Federal Circuit and at least one dis- when executed by a processor cause the processor to trict court that a method that is both limited to transfor- perform’’ a series of steps was not patent-eligible under mation of specific data and limited to a visual depiction Bilski. The board first determined that ‘‘analysis of a representing specific objects or substances qualifies un- ‘manufacture’ claim and a ‘process’ claim is the sameunder 63 Id. at 11.
57 No. 2008-004440 at 12-13 (B.P.A.I. Aug. 24, 2009).
65 Diamond v. Diehr, 450 U.S. 175, 185, 205 USPQ 488 58 No. 2008-004366 at 10-11 (B.P.A.I. Aug. 10, 2009).
(1980); Gottschalk v. Benson, 409 U.S. 63, 67, 175 USPQ 673 59 No. 2009-002913 (B.P.A.I. Aug. 5, 2009).
60 Id. at 10 (comparing In re Lowry, 32 F.3d 1579, 1583-84, 66 ‘‘Interim Examination Instructions for Evaluating Sub- 32 USPQ2d 1031 (Fed. Cir. 1994) to In re Warmerdam, 33 F.3d ject Matter Eligibility Under 35 U.S.C. § 101,’’ U.S. Patent and 1354, 1361-62, 31 USPQ2d 1754 (Fed. Cir. 1994)).
Trademark Office, Aug. 24, 2009, at 6 (78 PTCJ 530, 8/28/09).
61 No. 2009-003902 at 10 (B.P.A.I. Sept. 14, 2009).
The authors’ recent experiences with examiners suggest that 62 No. 2008-004742 (B.P.A.I. Jan. 13, 2009).
the examiners are following these instructions.
der Section 101.67 Thus, claims analogous to those in In Concerning claims directed to computer program re Abele68 in which ‘‘data clearly represented physical products, one district court has held that appending ‘‘A and tangible objects, namely the structure of bones, or- computer readable media including program instruc- gans, and other body tissues [so as to recite] the trans- tions’’ to an otherwise non-statutory process claim is in- formation of that raw data into a particular visual depic- sufficient to make it statutory.72 The board has also tion of a physical object on a display’’ are patent- held ineligible claims to ‘‘a computer readable me- dia.’’73 The board has, however, also upheld the eligibil-ity of ‘‘a computer program product’’ as being embod- ied in a computer readable medium.74 Given these in- Bilski has had a significant impact in eliminating consistent decisions, the patent eligibility of claims in patent protection for inventions that are performed en- tirely by humans or can be interpreted as such if read Concerning claims directed to generalized computer broadly. This includes claims that describe processes processing functions, several Board decisions suggest for creating or manipulating legal and financial docu- that, absent a tie to a concrete real-world application, ments and relationships. In this area in particular, many such claims are likely to be deemed an ‘‘algorithm’’ un- pending applications filed prior to Bilski are no longer der Benson and therefore held to be non-statutory. 75 patent-eligible, and many issued patents are no longer Any recitation of a specific field of use for the claimed valid. This retroactive impact of the Bilski decision is process or use of the outcome of such processes are troubling, given the investment in these patents and ap- also more likely to be found ‘‘field-of-use’’ or ‘‘post- plications, which have now been rendered essentially solution activity’’ limitations insufficient to render the worthless despite the suggestion in the Federal Circuit’s claim patent-eligible. Thus, the more tied a claimed pro- earlier State Street decision, now overruled, that such cess is to tangible results or particular applications (not claims qualified for patent protection.
just fields of use), the more likely it is to qualify under Inventions that do not fit within the four statutory categories are also not patent-eligible. The Federal Cir-cuit and the board have rejected claims directed to ‘‘a III. Presenting and Claiming Methods in Patent signal,’’ ‘‘a paradigm,’’ ‘‘a user interface’’ and ‘‘a corr-elator’’ on the basis that these items did not qualify as a ‘‘machine, manufacture, composition of matter or pro- Several strategies for describing and claiming meth- cess’’ under § 101. 70 There is also an increasing focus ods or processes in patent applications may avoid or on the tangibility of the claimed invention in that, to minimize potential Section 101 problems.
qualify as a ‘‘machine’’ or ‘‘manufacture’’ under Section First, the description provided in a patent application should include well-defined steps or functions associ-ated with method or process. For example, when the claims include ‘‘initiating’’ method steps, a description Remaining areas of uncertainty concerning the scope of well-defined physical steps or functions for initiating of Section 101 include (1) what qualifies under Bilski as should be provided, and a concrete item, machine, de- a ‘‘transformation of an article or data,’’ (2) whether vice, or component that is responsible for the initiating claims to computer programs (Beauregard claims) function should be identified. For claiming ‘‘identify- qualify, and (3) whether internal computer processing ing’’ method steps, provide specific parameters for functionality not tied to a specific application or tan- making the identification, such as according to a speci- fied measurement.76 Where data is involved, the source Concerning data transformation, other than Abele- and type of data should be specified.
style claims discussed above, what qualifies as a data or Also, drawings should be provided that depict the article transformation remains unclear. Claims that concrete item, device, component or combination have been held not to meet the transformation prong in- thereof, and each method or process step or function clude claims directed to the creation or manipulation of should be linked expressly to at least one item, device data representing an intangible series of rights and ob- or component in the drawings that performs the step or ligations (e.g., credit card data) and claims directed to function. Broadening language indicating that other the transformation or manipulation of legal obligations components may also be used to perform the function and relationships. Beyond these specific examples, it is may also be included to avoid an unduly narrow inter- difficult to predict what will or will not qualify as a data or article transformation under Bilski.
The claims should affirmatively claim the device, ma- chine or component performing each step or function.
67 In re Bilski, 545 F.3d at 963; Research Corporation Tech- For computer or software-related inventions, the de- nologies, 2009 WL 2413623 at *9.
scription should specify that the software functionality 68 The claimed process involved graphically displaying vari- ances of data from average values wherein the data was X-rayattenuation data produced in a two dimensional field by a com- 72 Cybersource Corp., 620 F. Supp. 2d at 1080.
puted tomography scanner. See In re Bilski, 545 F.3d at 962- 73 Cornea-Hasegan, No. 2008-004742.
74 Ex parte Bodin, No. 2009-002913 (B.P.A.I. Aug. 5, 2009).
69 In re Bilski, 545 F.3d at 963.
75 E.g., Ex parte Greene, No. 2008-004073 (B.P.A.I. Apr. 24, 70 In re Nuijten 500 F.3d 1346, 1357, 84 USPQ2d 1495 (Fed.
2009); Daughtrey, No. 2008-000202; Ex parte Arning, No.
Cir. 2007) (74 PTCJ 631, 9/28/07) (signal); In re Ferguson, 558 2008-003008 (B.P.A.I. Mar. 30, 2009); Cybersource Corp., 620 F.3d 1359, 1366, 90 USPQ2d 1035 (Fed. Cir. 2009) (77 PTCJ F. Supp.2d at 1080 (concerning claim 2).
489, 3/13/09) (paradigm); Ex parte Daughtrey, No. 2008- 76 See Brief of American Bar Association as Amicus Curiae 000202 (B.P.A.I. Apr. 8, 2009) (user interface); Ex parte Laba- Supporting Respondent, Bilski v. Kappos, No. 08-964, ABA die, No. 2008-004310 (B.P.A.I. May 6, 2009) (correlator).
Amicus Br. at 12-13 (U.S. amicus brief filed Oct. 2, 2009) (78 71 E.g., Nuijten, 500 F.3d at 1356-7.
PATENT, TRADEMARK & COPYRIGHT JOURNAL is performed by a computer or computer components.
patent or published application, the option of importing Specificity as to the type of computer component per- subject matter into the specification is limited to ‘‘non- forming each function may be helpful in establishing essential’’ subject matter. In other words, the specifica- eligibility under the Bilski test.
tion can only be amended to disclose a machine for per-forming process steps as long as one skilled in the art IV. Fixing Pre-Bilski Applications to Meet the New would recognize from the original disclosure that the process is implemented by a machine. The key in mak- For patent applications filed prior to the Bilski deci- ing this type of amendment is avoiding (or overcoming) sion, it can be challenging to meet the new require- a rejection under 35 U.S.C. § 112, para. 1, for lack of ments for patent eligibility, particularly when no ma- chine or transformations were expressly described in If incorporation by reference is not an option, a patent applicant may submit evidence, such as a decla- In some cases, there may be sufficient explicit de- ration by the inventor or a duly qualified technical ex- scription of a machine, e.g., a computer, such that the pert, demonstrating that one skilled in the art would un- machine can be added into the body of the claims. For derstand the disclosed method to be one performed by example, patent applications for computer-related in- a machine. Unlike attorney argument, which can be dis- ventions sometimes contain a generic description of regarded, such evidence must be considered by the ex- computers that are used to perform the claimed method, and such a generic description may be suffi- One other option is to reformat the claims. Since Bil- cient to impart patent eligibility to the claims when the ski ostensibly does not apply to system and apparatus general-purpose computer is programmed to become a claims, in some instances it may be possible for an ap- plicant to convert his method claims into system claims For patent applications lacking in an explicit descrip- to avoid application of the Bilski test. This strategy, tion of any machine, however, the application may in- however, is unlikely to succeed where the patent speci- corporate by reference patents or publications that can fication does not describe such a system for implement- be used to bolster the specification and provide support ing the method and therefore does not provide the req- for the requisite claim amendments. When an applica- uisite disclosure of the claimed invention under Section tion incorporates by reference a U.S. patent or pub- lished U.S. patent application, any description from the incorporated references, whether or not the subject The future of the Bilski machine-or-transformation matter is ‘‘essential’’ to support the claims, may be im- test now rests with the Supreme Court. Regardless of ported into the specification. This option may enable the outcome of the appeal, however, it is clear that the importation of the requisite description of a machine, scope of statutory subject matter under Section 101 has which can then also be recited in the claims.77 When been narrowed. The Supreme Court now has a chance the document incorporated by reference is not a U.S.
to clarify what has been excluded; it may even reject ormodify the Bilski machine-or-transformation test. How 77 Manual of Patent Examining Procedure, Eighth Ed., Rev.
this will affect the development and protection of cur- 7/2008, at § 608.01(P); see also 37 C.F.R. § 1.57.
rent and future technologies remains to be seen.
Source: http://www.dorsey.com/files/upload/luedke_bna_patent_journal_nov09.pdf
(resolução 404.2012 retificação 19062012)
RESOLUÇÃO Nº 404 , DE 12 DE JUNHO DE 2012 Dispõe sobre padronização dos procedimentos administrativos na lavratura de Auto de Infração, na expedição de notificação de autuação e de notificação de penalidade de multa e de advertência, por infração de responsabilidade de proprietário e de condutor de veículo e da identificação de condutor infrator, e dá outras providências.
Cheloidi e cicatrici ipertrofiche in dermatologia
a cura del dr. Antonio Del Sorbo - Specialista in Dermatologia e Venereologia antoniodelsorbo@libero.it I Cheloidi di Alibert A volte una ferita anche apparentemente banale, guarisce lasciando una cicatrice voluminosa, rossastra e soprattutto antiestetica. I cheloidi sono cicatrici abnormi che possono far seguito a intervento chirurgico (es: tiroide, mammella, etc) e questo u",['It required the transformation to be limited to specific data and a visual depiction representing specific objects or substances.'],6925,multifieldqa_en,en,,9831469b98405cd1fc1bee7de6f9630ae1cbb5946fd7ca0b," The patent community anticipates a decision by the U.S. Supreme Court on subject matter patentability. Recent rulings suggest strategies for preparing method patent applications that will sur- vive the Federal Circuit’s ‘‘machine-or-transformation’’ test. In this article, we examine the 2008 decision of the fore the court was whether the claims met the ‘trans- Federal Circuit, federal district court decisions, and de- formation’ prong of the Bilski test. We also examine the strategies for pre-cerning the recitation of the deed-shares in patent applications sent in the wake of the new machine-or.transformation test. And we conclude that the patent community waits for the Supreme Court to make a decision on the subject matter of this article. Back to Mail Online home. Back To the page you came from. The Bu-reau of National Affairs, Inc. (800-372-1033) http://www.bna.com/news/features/patent-trademark- 2009-11-20-20.html#storylink=cpy. The article was originally published on 11/20/09, and has been republished with permission from BNA’S Patent,Trademark 11/21/09 and 11/22/09. The original version of this story stated that ‘Pure’ business methods are out. On review before the high court is the en banc ruling ‘'Pure' business methods.’ The article has been updated to reflect that the court found that these methods, and possible ‘physical objects’ for applications’ were not physical objects, but for deedshares themselves were. The court also found that the claims ‘involve[d] only the transformation or Patent Appeals and Interferences. based upon the out of legal obligations and relationships’ comes in these cases, we that did not offer as to qualify under 35 U.C.P.A. § 101.14. We are happy to make clear that this is not the case. The. court held cisions of the Patent and Trademark Office’'s Board of that the claim is not physical, but that the. claim is tied to a particular neapolis. The ‘useful, concrete and tangible plicants seeking to write patentable claims are stuckwith trying to conform to the lower courts’ most recent ‘machine’- or ‘Transformation-based’ claims. This ‘ machine- or- transformation test is a two-branched Adriana Suringa Luedke and Bridget M. Hay- inquiry; an applicant may show that a process claim satisfies den are lawyers at Dorsey & Whitney, Min- § 101 either by showing that his claim is ties to a specific neapolis, or by showing his claim transforms an article. The test replaced the Freeman-Walter- argued Nov. 9, 2009) (79 PTCJ 33, 11/13/09), patent ap- Abele3 test and the ’‘’ inquiry advocated in State Street,4 each of gregating, and selling real estate property and claims which had been applied by the Federal. Circuit and its reciting a method of performing tax-deferred real estate predecessor court in various cases, and both of which property exchanges were not statutory under Section 101. Since no machine was recited, the only issue be- The Federal Circuit reviewed the board’s rejection of claims directed to a method of mar- Two dependent claims added a step of informing the keting a product and a ‘paradigm’’ for marketing soft- patient of certain results. The court rejected this argument, con- 101. Recent board decisions have been consistent with the that the claimed paradigm was not a process, because holdings of the federal courts. See Ex parte Roberts., 2009-004444 at 4-5 (B.P.A.I.I) (Fed. Cir. 1998) 19, 2009) (holding a. ‘method of creating a real estate invest- ment instrument adapted for performing tax-deferred ex- changes’ patent ineligible as not passing the machine-or-thing test. See Diamond v. Diehr, 2009-2062 at 13-20 (P.C.R. No. 22, 2009). Accordingly, the claims are generally to extend an offer, not an offer to extend a process. See In re Abele, 684 F.2d 902, 214 USPQ 682 (C.P.-A.A.) 19,2009 (P-L 249, 5205, *C12, W.L.205) (F.C.-R.C.) 593 F.N.Y.c. v. Eon Labs Inc.,17 the district court held invalid claims to a. method of increasing the oral Several cases have addressed (and rejected) claims bioavailability of metaxalone because the claims were obvious over the prior art asserted by the accused in- In In re Ferguson,6 the Federal Circuit review the board's rejection ofClaims directed to an ‘in- thing’ did not meet the patent eligibility binations of devices,’ which did not include the ‘shared standard set forth in Bilski because the step did not re- marketing force’ The court also commented that the added step of in- thing, consisting of parts, or of certain devices or com- forming the patient did not. meet the Patent eligibility bination of devices. The claim did not satisfy either the machine or digm claims, and the court concluded that the patent appli- cants did ‘no more than provide an abstract idea—a. Similarly, in Ex. parte Haworth,23 a method for ‘‘at- business model for an intangible marketing com- tempting to collect payments from customers having delinquent accounts concurrently with a partner that. American Master Lease owns the delinquent accounts’ was found to be patent ineligible.’ In Fort Properties Inc., v. AmericanMaster Lease,12 the California district court. held that claims re- ineligible because the claim wording was ‘broad in that citing a series of transactions involving acquiring, ag- 1980; In. In re. Haworth,. 23, the court found ineligible under Section 101 manufacture, because it was. not a tangible article re-a ‘ method of creating an investment in real estate investment instru- sulting from a process of manufacture. The claims did not fall within any of the four statutory categories (machines, manufactures, sected into old and new elements and that are evaluated compositions of matter and processes) and not dis- the four. categories (‘’as a whole’ and ‘to mean ‘a pattern, or model’ or ‘an idea’). The court also evaluated similar claims that recited the use of a ‘comparator’’ to perform the recited pixel- B. The court also found other claims patent- found that the pixel-by-pixel comparison recited in the eligible under Section 101 because these claims recited claims did not require a machine. In DealerTrack Inc. v. Huber, the district court prong, even with a field-of-use limitation because granted a summary judgment of invalidity under § 101 they represent fundamental principles, and to do so of patent claims directed to ‘a computer aided method’ would ‘preempt the entire field of patent claim claims . . .of managing a credit application reciting the following: ‘I do not use a machine to achieve their algorithmic ends. Simply because adigital apparatus such as a computer, calculator, or [A] remote the application data from a computer could assist with this patent argument does not render it eligible for patent. RCT’s argument [B] selectively forwarding the application that a pixel by its nature does not rendering it eligible to be patented.’ ‘‘The use of the term ‘device’ is computer components as sufficient to satisfy the ‘ma- not synonymous with machine’ prong of the Bilski test,’ the court said. ‘The com- further limited to a visual depiction which represents parison uses formulas and numbers to generate a bi- specific objects.�’ The patent eligibility of the nary value to determine the placement of a dot at a claims turned on whether the claims recite the use. of location. Formulas and numbers not tied to a particu- the transformed data to generated a display. In re Abele,26 the judge concluded that a number of the panded test because the claims didn’t ‘require any vi- sual depiction or subsequent display’ even though the transformation test set forth in Bilsk’27 claimed method did transform specific image data. In Research Corporation Technology Inc.v. Mi- lied in particular upon the Abele decision in expanding crosoft Corp.,25 the court considered the patent the requirements of this test by requiring that the eligibility of method claims in six patents directed to claimed transformation process be both ‘(1) limited to methods of halftoning of gray scale images by using a transformation of specific data, and 2)limited to a vi- pixel- by-pixel. comparison of the image against a blue depiction representing specific objects or sub- noise mask.” The court said the claims could also be done on a sheet of paper using a pen. The district court also concluded that some of the analysis as well as a decision of its predecessor court, patent claims did Not meet the second prong. of this ex- In re Able, 26 the court concluded that many of the claims were not eligible for a patent because they did not ‘in- voke, recite or limit the method of implementation us-ing any particular machine or apparatus’ and did not in- vokes the method to produce the image. In. Able v Mi- lies, the court also said that the claim language was either not expressly tied to com- parator or could also. be performed in one‘s mind or on pa- puter hardware components. Claim construction specifies that the comparison is of a value to a mask 29 The term ‘comparator’’ was construed by the court to be (or set of values) to determine whether the dot is a ‘‘device (or collection of operations, as in software) that com- turned on at a specific location. This process does pares an input number (called the operand) to a number pre- not require a particular machine. The Bilski test is stored in the comparator and produces clear: the process claims must be tied to a particular as output a binary value. The internet continues to exist despite the addition [D] wherein the selectively forwarding the credit ap- or subtraction of any particular piece of hardware. It may be supposed that the internet itself, rather than [E] sending at least a portion of a credit application any underlying computer or set of computers, is the to more than one of said remote funding sources ‘machine’ to which plaintiff refers. Yet the internet is an abstraction. One can touch a computer or a network, but one cannot touch . . . a cable but one can’t touch the inter- cable. In this case, the court found that the claim did not satisfy the references to the ‘Tossing in the Internet’ that the court held to be ‘insignificant’ and therefore did not qualify as a � ‘particular machine’ The court also concluded that the patent claim was not sufficient to hold the internet was not ‘a part of commerce’ in the first place. The court held that the first recitation of the credit application from a remote funding source in this case merely constituted ‘an extra’solution to a second funding extra-solution activity.’ In other words, it was not enough to claim that the credit was sent from a second remote source to a first remote source, such as the internet, and therefore the patent was not valid. The Court concluded that it vices to the remote application entry and display and therefore held that it did not hold the claim to be valid. In. this case the court also held that. the claim ‘did not satisfy’ the references. to the internet as a part of the commerce in this. case, and that the. court held. that the Internet was not a part. of commerce in that it was merely a. ‘transformation to the second remote funding extrasolution,’ therefore. the patent did not. not hold. the internet to be a. part of ‘ commerce. in that case. The claim was. not sufficient. to hold that the online credit application in this first case was merely an extra-transformation of a second. remote funding activity, and. therefore did. not qualify. as ‘ particular machine,�’. The patent was. found ineligible a method of launching a depiction was ‘sufficient’' to establish transformation (545 knowledge network involving ‘selecting an executive spon- F.3d at 963), while the Research Corporation court. went fur- sor, ‘forming a core team of experts,‘’ ‘providing pre- ther by making visual depiction ‘required’, and ‘defining a current cus- ically greater than or equal to the threshold’) 25 2009 WL 2413623 (D.D.I. July 28, 2009) (78 PTCJ 432, 26 684 F.2d 902, 214 USPQ 682 (C.C.P.A. 1982). 34 2009 Wl 2020761 (CD.A.) July 7, 2009), (78PTCJ 341, PATENT, TRADEMARK & COPYRIGHT JOURNAL [C] forwarding funding decision data from at least tation of ’over the Internet' suffices to tie Claims ‘broadly preempt the ticular machine’ within the meaning of Bilski. The court relied upon the holdings in Ex parte Gutta, No. 2008-3000 at 5-6 (B.P.A.I. Jan. 15, 2009) (stating In denying the motion for summary judgment, the ‘[t]he recitation in the preamble of ‘ [a] computerized method’’36 claim,44 notwithstanding the Federal Circuit’s holding In Cybersource Corp. v. Retail Decisions Inc.,37 the in In re Beauregard,45 the district court concluded that district court held claims for ‘a method for verifying the. ‘‘There is at present no legal doctrine creating a special validity of a credit card transaction over the Internet.’ ‘The method claim did not qualify ing Devices,47 the accused infringer filed a motion for because the data representing credit cards did not rep- summary judgment alleging that the asserted claims resent tangible articles but instead an intangible series impermissibly sought to patent a mathematical algo- of rights and obligations existing between the account rithm and the credit card account.‘’ The court assessed whether ‘reci- a particular machine, but served ‘only to limit the field-of-use of the claimed mathematical algorithm and [did] not otherwise impart patentability on the claimed math- Id. at *3. Concerning the method claim, the court considered claim also failed the Bilki test. According to the court, transformation cases discussed above. is limited to transformation of a physical article or sub- Game Machines and Related Three-Dimensional Point- stance. Accordingly, the claimed method claim was not sufficient to tie the claims to a particularMachine. The claim was also invalid under § 101 based tions’ to an otherwise non-statutory process claim is in- upon the court’S interpretation of Bilkki. “The court focused on the intangibility of the ma- Bilskis less stringently than did the district courts in the nipulated data.” ‘A computer readable media including program instruc- tion . . . over the. Internet’ was not enough to tie claims to the particular machine. � ‘3D pointing device’ or ‘handheld device,’or ‘free space Concerning whether the claimed. method was tied to pointing device, the. court assessed if ‘the recitation of a ‘ 3D pointing. device, and remote funding source terminal device the claim to use over. the Internet was not a meaningful could be ‘any device�’ and did not constitute a ’par- limitation, such that the claims ‘could not be applied to that device. ’”“‘[s]imply appending instructions for detecting fraud in a. credit card transac- ‘ ‘ a computerreadable media’ over the internet’,” said the movant, ‘are invalid under. the Bilska test’.“The. court does not dispute the process of a processor does not limit the steps to having a particular computer specifically programmed to execute the steps of this format. ”, said the court. In In the Matter of Certain Video, the ITC first noted that, ’[w]hile the ultimate determination performed by a data processor’ adds nothing more than a gen- of whether the claims are patentable under eral purpose computer that is associated with the steps. of the § 101 is a question of law, the Circuit has acheived in an unspecified manner. Claims were not barred by the traditional source Corp. v. Retail Decisions Inc., (discussed below), in sup- printed matter rule in In re Beauregard, 53 F.3d 1583, 1584, 35 port of its interpretation of the required ‘particular machine.’ Claims were eligible because it recited, ‘further comprising us- ITC concluded that it was inappropriate to grant sum- ing the selected features in training a classifier for clas- mary judgment as to the patent eligibility of the claims’ In view of the specifica- A similar conclusion was reached in Versata Soft- tion, the board indicated that the ‘classifier’’ was a par- ware Inc. v Sun Microsystems Inc.,48 in which the dis- ticular machine ‘in that it performs a particular data trict. court denied the defendant’s motion for summary classification function that is beyond mere general pur- judgment of invalidity under Section 101 based upon pose computing’ The board also concluded that the the Bilski court‘transforms a particular article into a different over software or any other such category of subject state or thing, namely by transforming an untrained matter beyond the exclusion of claims drawn to funda- classifier into a trained classifier. In Ex parte Casati,55 the board reversed the examin- Less stringent ‘‘machine’' prong analyses are also er’S Section 101 rejection of a method claim reciting: found at the board level. The board held patent-eligible under Bilska A method of analyzing data and making predictions, reading process execution data from logs for a busi- A method for obtaining feedback from consumers re- ceiving an advertisement from an ad provided by anad provider through an interactive channel, the collecting the process executiondata and storing the processExecution data in a memory defining ware-house, and generatingprediction models in response to a feedback panel. For example, in Ex. parteSchrader,50 the board held that a method for analyzing data was patent- eligible under Bil-ski A. A. method for. analyzing data, including at least one feedback response concerning one said advertisement; generatingpredicted models and a response to the analyzing; providing feedback panel to consumers, using prediction models to predict an occurrence of a consumer to being activated by an ad p-provide an exception to said ad. 47 2009 WL 1070801 (U.S.I.T.C. 2009) 47 2009WL 1070001 ( U.S.-I.I-C.T.-C.2009) (citation omitted) The board found that there was a ing features using two-category feature ranking; and genuine dispute as to whether the claimed ‘devices’. represented a ‘ particular machine’ under the Bilki while the predetermined number of features has not test and whether the. ‘two-dimensional rota- yet been selected: picking a binary partition p; tional transform’ or instead meant ‘changing the mathematical rep- resentation of a two-dimensional quantity’ The board found ‘interactive channel’’ to be descriptions describe the memory/warehouse device in part of an ‘overall patent eligible system of appara- terms of machine executable functions’ When viewed in the context of the specification, cluded that ‘‘one of ordinary skill in the art would un- which included ‘the Internet and World Wide Web, and self service devices, such as In- data in a memory defining a warehouse constitutes formation Kiosks and Automated Teller Machines.’ The board concluded that concrete parts, devices, or combinations of devices. for the claim did not qualify. The board found ineligible under Section 101 a ‘method for comparing file tree de-scriptions’ because the claim ‘obtains data (a file struc- does not transform physical subject matter and is not ture), compares data (file structures), generates a tied to a particular machine or appa- 52 No. 2008-005348 (B.P.A.I. Aug. 17, 2009). The board also found methods ineligible under the Bilski test transformation test applied to this type of claim.63 because the claims failed to tie the method steps to any concrete part, device, or combina- cal limitation to the scope of the claim. In this case, giving consideration to the. specification, which ‘unequivocally describes the data warehouse aspart of the overall system apparatus, and subsequent. Here, the ‘classifier’' was recitedin a dependent claim, in which its independent claim re-cited: 53 Id. at 13. In another recent decision, Ex parte Forman,52 the memory-warehouse element ties the claims to a particu- board found a. ‘computer-implemented feature selec- tion method’ including a � ‘ classifier” eligible under Other recent board decisions have reached the oppo- Section 101 because it satisfied both the machine and transformation prong. The. board held ineli gible under § 101 a method for identifying one or more items having a sym-igolic value, yet others are not eligible, yet depend on ‘symbolic values of devices’ and others may be depend on other. types of devices that are clearly ing-olic values and yet are not entitled to a patent. The Current Scope of Patent II: Eligibility bolic value of a symbolic attribute,’ concluding that some types of types of claims are clearly metholic and ‘computes a variance and selects a mean item ods’ Other. cases that establish that the claim is not eligible under. Section 101, without tying these steps, or others without tying them to concrete parts or devices, are yet to be decided. The current scope of patent II: The current. scope of Patent I: ‘The current. Scope of patent I: The Current Eligible bolicvalue of a. symbolic attribute,.’ concludes that the. claim is ‘unquestionably’ patent-eligible under Section. 101, yet other types of. claims may not be eligible under that section, yet they are not depend on others. The claim is also eligible under Sections. 2 and 3 of the Patent Act of 1988. The patent is for ‘a system of software that allows users to create and test computer programs that can be used to test and test software for various types of applications’. The ‘system of software’ can be downloaded from the Internet. In Diamond v. Diehr and Gottschalk sion, Ex parte Bodin,59 ‘‘a computer program product’’ v. Benson, which prohibit patenting of a claim directed was found to be patent-eligible subject matter as being to ‘laws of nature, natural phenomena, [or] abstract embodied in a ‘computer readable medium’ The board considered whether the phrase. ‘recorded on the Also, methods that are performed at least in part by a recording. medium as it is recited in the body of the machine qualify for patent eligibility under. Section 101. The board first determined that ‘analysis of a. representing specific objects or substances qualifies un- ‘manufacture’ claim and a � ‘process’ claims is the sameunder 63 Id. at 11:57 No.57 (B.P.I. at 12-00) The board seemingly came to the opposite conclusion, holding that a claim reciting ‘a. computer Concerning data transformation, there seems to be readable media including program instructions. which agreement of the Federal Circuit and at least one dis- when executed by a processor cause the processor to trict court that a method that is both limited to transfor- perform’ a series of steps was not patent- eligible under mation of specific data and limited to a visual depiction Bilski. is largely unaffected by the Bilki decision, with the ca- In contrast to the district court’s decision in Cyber- veat that such claims may be more closely scrutinized source Corp., discussed supra, in a recent board deci- for compliance with Diamond. v. diehr and gottschalks sion. The court found that the recitation of a general purpose computer hard disk is sufficient to satisfy Section 101 where the general did not ‘implicate the use of a carrier wave. The tie to a machine, however, cannot merely be im- in nonstatutory processes, the board stated: ‘[w]hen plicit based upon the description and context of the ap- functional descriptive material is recorded on some plication or general language in the preamble of the computer-readable medium, it becomes structurally claim.’ In an older decision, Ex. parte Cornea-Hasegan,62 cess steps, . . . in effect, becom[ing] a special purpose however, the Board seemed to hold that a. claim recited by a human, it will likely not pass the machine prong of puter program product. The ‘Interim Examination Instructions for Evaluat- subject matter under 35 U.S.C. § 101’ referred to tangible storage me- cently issued by the Patent and Trademark Office con- dia, such as a server, floppy drive, main memory and firm that the Recitation of the hard disk as disclosed by appellant's specification, and is enough to satisfy the Section 101 claim. In re-comparing In Lowry v. Lowry, 32-d 1579, 1583, 66-84 ‘3-83, ‘2-83 ‘4-83 (B-P.A. No. 58-4-4) ‘ Interim Examination. Instructions for. Evaluating Submitting Submitting. Submitted. Claims for Patent-eligible Subject Matter under 35.U.S., No. 63, 67, 69, 69 ‘5-73 6-73 5-73’ Bilski has had a significant impact in eliminating consistent decisions, the patent eligibility of claims in patent protection for inventions that are performed en- tirely by humans or can be interpreted as such. This includes claims that describe processes processing functions, several Board decisions suggest for creating or manipulating legal and financial docu- that, absent a tie to a concrete real-world application, ments and relationships. Many such claims are likely to be deemed an ‘‘algorithm’’ un- pending applications filed prior to Bilski are no longer der Benson and therefore held to be non-statutory. 75 patent-eligible. The more tied a claimed pro- earlier State Street decision, now overruled, that such cess is to tangible results or particular applications (not claims qualified for patent protection), the more likely it is to qualify under Inventions that do not fit within the four statutory categories. The authors’ recent experiences with examiners suggest that 62 No. 2008-004742 (B.P.A.I. Jan. 13, 2009) is being followed by examiners. They say that examiners are following these instructions and that they are concerned about the impact of the Bilskis on the patent process. They also say that many patent applications are now being rejected because they do not have a ‘field-of-use’ or ‘post- plications,’ which have now been rendered essentially solution activity’ The authors say that patent applications should include well-defined steps or functions with a concrete method or process for ‘initiating the process’ Remaining areas of uncertainty concerning the scope of the claims include what qualifies under 35 U.S.C. § 101. 70 There is also an increasing focus ods or processes in patent applications may avoid or on the tangibility of the claimed invention in that, to minimize potential Section 101 problems, they say, the application should include a concrete item, such as a machine, de- facto item, or component that is responsible for the ‘transformation of an article or data’ (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 57,. 58, 59, 60, 58,. 59, 59,. 60, 61, 60,. 61, 62, 63, 63,. 62, 64, 63 and 63, 64,. 64, 65, 65,. 65, 67, 68, 69, 70, 68,. 69, 69,. 68, 70,. 70, 80, 71, 72, 73, 74, 79, 78, 79,. 80, 81, 82, 83, 84, 83,. 84, 85, 86, 87, 88, 89, 90, 88,. 89, 91, 92, 93, 94, 95, 96, 94,. 94, 96,. 95, 95,. 96, 97, 98, 97,. 98, 99, 100, 104, 101, 102, 103, 104,. 104, 105, 106, 107, 108, 109, 109,. 106, 108,. 109, 110, 111, 110,. 111, 114, 113, 111,. 116, 113,. 114, 115, 116, 115,. 111,. 118, 116,. 119, 116. 113, 116., 116, 117, 118, 119, 123, 120, 123,. 118,. 119,. 116,. 123, 118,. 116. 119, 119,. 118. 123, 123. 124, 123 The claims should affirmatively claim the device, ma- chine or component performing each step or function. Broadening language indicating that other the transformation or manipulation of legal obligations components may also be used to perform the function and relationships. Specific parameters for functionality not tied to a specific application or tan- making the identification, such as according to a speci- fied measurement. The key in disclosure that the process is implemented by a machine is a key to the Bilski amendment. The amendment can only be amended to disclose a machine per-forming process as long as one skilled person is involved in the process. For this type of amendment, avoiding it can be overcome by avoiding the sion sion (or sion-or) of challenging the type of claim under the amendment. Pre-Bilski Applications to Meet the New Fixing Precedent would recognize it as one of the original patent applications to meet the new fixing pre-fixing requirements of the new patent amendment. It would also recognize that the original application would recognize the new Fixing prefixing conditions as part of the patent application. The application would also require the defendant to disclose that the method of processing the data is a ‘machine-based’ process, not a “computer program” or “a computer program’ or ‘a program that is written by a computer.’ It would require that the defendant identify the source of the data transformation, other than Abele- and type of data, and what qualifies as a data or. Also, drawings should be provided that depict the article transformation remains unclear. It is possible to avoid an unduly narrow inter- difficult to predict what will or will not qualify as a. data or article transformation under Bil-i-ki. The claim must be linked expressly to at least one item, device data representing an intangible series of rights and ob- or component in the drawings that performs the step or ligations (e.g., credit card data) and claims directed to function. For claiming ‘‘identify- qualify, and (3) whether internal computer processing ing’’ method steps, provide specific parameters for. functionality. The claims should be linked to the. type of subject matter into which the technology is being used. For claims to be linked directly to computer programs (Beauregard claims) function should be identified. For. claiming claims to computer. programs (Abele-and-beuregard) function, the source should be specified. The method of. data transformation should be. specified, and where data is involved, the type. of data should also be specified, including the source and type. The. method of data transformation. should be described. For the claim to be eligible for patent protection, the claim should be listed as ‘non-specific’ and ‘not tied to any specific. application or computer component’ The claim should specify that the software functionality is performed by a. computer or computer. or computer components. is ‘ non-specific,’ not ‘the type of. subject matter’ that is being created by the computer or. computer components, or the type and method of importing. the data into the patent. If. the claim is. not specific to the type or subject matter, the. claim should. be limited to ‘Non-non- Specific’ In other words, the specific words, ‘ Non-Non-Specific’ may be helpful in establishing essential. terms for the claim. is not an option, a patent applicant may submit evidence, such as a decla- In some cases, there may be sufficient explicit de- ration by the inventor or a duly qualified technical ex- scription of a machine, e.g., a computer, such that the pert, demonstrating that one skilled in the art would un- machine can be added into the body of the claims. For patent applications lacking in an explicit descrip- to avoid application of the Bilski test, however, the application may in- however, is unlikely to succeed where the patent speci- corporate by reference patents or publications that can fication does not describe such a system for implement- be used to bolster the specification and provide support ing the method. The Supreme Court now has a chance the document incorporated by reference is not a U.S. patent or pub- lished patent application, whether or not the subject is ‘‘essential’’ to support the claims, may be im- test now rests with the Supreme Court. Regardless of the outcome of the appeal, it is clear that the importation of the requisite description of a. machine, scope of statutory subject matter under Section 101 has which can then also be recited in the claims is a key part of the patent application process. The future of the. Bilsky machine-or-transformation matter, and future technologies remains to be seen. The decision will affect the development and protection of cur- 7/2008, at § 608.01(P); see also 37 C.F.R. § 1.57.77 When been narrowed. It may be possible for an ap- plicant to convert his method claims into system claims, but it may even reject ormodify the Bil-ki machine- or- transformation test. It is not clear whether the decision will be appealed to the U.N. International Court of Justice or the European Court of Human Rights, which has the power to rule on the validity of patent claims in the European Patent Office’s jurisdiction. The case is currently before the European Courts of Justice, which is expected to make a decision in the next few months. It will be the first time that the European courts have dealt with the issue of whether a patent application can be made on a computer-related subject matter. The European Patent office has not ruled on the question of whether it is a ‘computer-related in- a machine’ or a “computer-based invention”. The issue is currently under consideration. The U.K. Patent Office has ruled that the term ‘ computer-based inventions’ does not apply to the ‘system and apparatus general-purpose computer is programmed to become a claims, and that the claim must be changed to ‘ system and apparatus computer.’ It is unclear whether the European court of human rights will rule in the case of the Microsoft Surface tablet, which was granted a patent in 2010, or whether it will rule that the technology is not ‘a computer- based invention’. It has not yet been decided whether the US patent will be allowed to use the Surface tablet."
How does the transition probability of the environment affect the learning rate in the static agent?,"Paper Info

Title: Environmental variability and network structure determine the optimal plasticity mechanisms in embodied agents
Publish Date: Unkown
Author List: Sina Khajehabdollahi (from Department of Computer Science, University of Tübingen)

Figure

Figure2: An outline of the network controlling the foraging agent.The sensor layer receives inputs at each time step (the ingredients of the nearest food), which are processed by the plastic layer in the same way as the static sensory network, Fig.1.The output of that network is given as input to the motor network, along with the distance d and angle α to the nearest food, the current velocity v, and energy E of the agent.These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent
Figure4: The evolved parameters θ = (θ 1 , . . ., θ 8 ) of the plasticity rule for the reward prediction (a.) and the decision (b.) tasks, for a variety of parameters (p tr = 0.01, d e ∈ 0, 0.1, . . ., 1, and σ ∈ 0, 0.1, . . ., 1 in all 100 combinations).Despite the relatively small difference between the tasks, the evolved learning rules differ considerably.For visual guidance, the lines connect θs from the same run.
Figure5: a.The trajectory of an agent (blue line) in the 2D environment.A well-trained agent will approach and consume food with positive values (green dots) and avoid negative food (red dots).b.The learning rate of the plastic sensory network eta p grows with the distance between environments d e c. and decreases with the frequency of environmental change.d.The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network.e.The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red).In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food.

abstract

The evolutionary balance between innate and learned behaviors is highly intricate, and different organisms have found different solutions to this problem. We hypothesize that the emergence and exact form of learning behaviors is naturally connected with the statistics of environmental fluctuations and tasks an organism needs to solve.
Here, we study how different aspects of simulated environments shape an evolved synaptic plasticity rule in static and moving artificial agents. We demonstrate that environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity. Interestingly, the form of the emerging plasticity rule is additionally determined by the details of the task the artificial organisms are aiming to solve.
Moreover, we show that coevolution between static connectivity and interacting plasticity mechanisms in distinct sub-networks changes the function and form of the emerging plasticity rules in embodied agents performing a foraging task. One of the defining features of living organisms is their ability to adapt to their environment and incorporate new information to modify their behavior.
It is unclear how the ability to learn first evolved , but its utility appears evident. Natural environments are too complex for all the necessary information to be hardcoded genetically and more importantly, they keep changing during an organism's lifetime in ways that cannot be anticipated ; . The link between learning and environmental uncertainty and fluctuation has been extensively demonstrated in both natural ; , and artificial environments .
Nevertheless, the ability to learn does not come without costs. For the capacity to learn to be beneficial in evolutionary terms, a costly nurturing period is often required, a phenomenon observed in both biological , and artificial organisms . Additionally, it has been shown that in some complex environments, hardcoded behaviors may be superior to learned ones given limits in the agent's lifetime and envi-ronmental uncertainty ; ; .
The theoretical investigation of the optimal balance between learned and innate behaviors in natural and artificial systems goes back several decades. However, it has recently found also a wide range of applications in applied AI systems ; . Most AI systems are trained for specific tasks, and have no need for modification after their training has been completed.
Still, technological advances and the necessity to solve broad families of tasks make discussions about life-like AI systems relevant to a wide range of potential application areas. Thus the idea of open-ended AI agents that can continually interact with and adapt to changing environments has become particularly appealing.
Many different approaches for introducing lifelong learning in artificial agents have been proposed. Some of them draw direct inspiration from actual biological systems ; . Among them, the most biologically plausible solution is to equip artificial neural networks with some local neural plasticity , similar to the large variety of synaptic plasticity mechanisms ; ; that performs the bulk of the learning in the brains of living organisms .
The artificial plasticity mechanisms can be optimized to modify the connectivity of the artificial neural networks toward solving a particular task. The optimization can use a variety of approaches, most commonly evolutionary computation. The idea of meta-learning or optimizing synaptic plasticity rules to perform specific functions has been recently established as an engineering tool that can compete with stateof-the-art machine learning algorithms on various complex tasks ; ; Pedersen and Risi (2021); .
Additionally, it can be used to reverse engineer actual plasticity mechanisms found in biological neural networks and uncover their functions ; . Here, we study the effect that different factors (environ-arXiv:2303.06734v1 [q-bio.NC] 12 Mar 2023 mental fluctuation and reliability, task complexity) have on the form of evolved functional reward-modulated plasticity rules.
We investigate the evolution of plasticity rules in static, single-layer simple networks. Then we increase the complexity by switching to moving agents performing a complex foraging task. In both cases, we study the impact of different environmental parameters on the form of the evolved plasticity mechanisms and the interaction of learned and static network connectivity.
Interestingly, we find that different environmental conditions and different combinations of static and plastic connectivity have a very large impact on the resulting plasticity rules. We imagine an agent who must forage to survive in an environment presenting various types of complex food particles. Each food particle is composed of various amounts and combinations of N ingredients that can have positive (food) or negative (poison) values.
The value of a food particle is a weighted sum of its ingredients. To predict the reward value of a given resource, the agent must learn the values of these ingredients by interacting with the environment. The priors could be generated by genetic memory, but the exact values are subject to change. To introduce environmental variability, we stochastically change the values of the ingredients.
More precisely, we define two ingredient-value distributions E 1 and E 2 and switch between them, with probability p tr for every time step. We control how (dis)similar the environments are by parametrically setting E 2 = (1 − 2d e )E 1 , with d e ∈ [0, 1] serving as a distance proxy for the environments; when d e = 0, the environment remains unchanged, and when d e = 1 the value of each ingredient fully reverses when the environmental transition happens.
For simplicity, we take values of the ingredients in E 1 equally spaced between -1 and 1 (for the visualization, see Fig. ). The static agent receives passively presented food as a vector of ingredients and can assess its compound value using the linear summation of its sensors with the (learned or evolved) weights, see Fig. .
The network consists of N sensory neurons that are projecting to a single post-synaptic neuron. At each time step, an input X t = (x 1 , . . . , x N ) is presented, were the value x i , i ∈ {1, . . . , N } represents the quantity of the ingredient i. We draw x i independently form a uniform distribution on the [0, 1] interval (x i ∼ U (0, 1)).
The value of each ingredient w c i is determined by the environment (E 1 or E 2 ). The postsynaptic neuron outputs a prediction of the food X t value as y t = g(W X T t ). Throughout the paper, g will be either the identity function, in which case the prediction neuron is linear, or a step-function; however, it could be any other nonlinearity, such as a sigmoid or ReLU.
After outputting the prediction, the neuron receives feedback in the form of the real value of the input R t . The real value is computed as R t = W c X T t + ξ, where W c = (w c 1 , . . . , w c N ) is the actual value of the ingredients, and ξ is a term summarizing the noise of reward and sensing system ξ ∼ N (0, σ).
Figure : An outline of the static agent's network. The sensor layer receives inputs representing the quantity of each ingredient of a given food at each time step. The agent computes the prediction of the food's value y t and is then given the true value R t ; it finally uses this information in the plasticity rule to update the weight matrix.
For the evolutionary adjustment of the agent's parameters, the loss of the static agent is the sum of the mean squared errors (MSE) between its prediction y t and the reward R t over the lifetime of the agent. The agent's initial weights are set to the average of the two ingredient value distributions, which is the optimal initial value for the case of symmetric switching of environments that we consider here.
As a next step, we incorporate the sensory network of static agents into embodied agents that can move around in an environment scattered with food. To this end, we merge the static agent's network with a second, non-plastic motor network that is responsible for controlling the motion of the agent in the environment.
Specifically, the original plastic network now provides the agent with information about the value of the nearest food. The embodied agent has additional sensors for the distance from the nearest food, the angle between the current velocity and the nearest food direction, its own velocity, and its own energy level (sum of consumed food values).
These inputs are processed by two hidden layers (of 30 and 15 neurons) with tanh activation. The network's outputs are angular and linear acceleration, Fig. . The embodied agents spawn in a 2D space with periodic boundary conditions along with a number of food particles that are selected such that the mean of the food value distribution is ∼ 0. An agent can eat food by approaching it sufficiently closely, and each time a food particle is eaten, it is The sensor layer receives inputs at each time step (the ingredients of the nearest food), which are processed by the plastic layer in the same way as the static sensory network, Fig. .
The output of that network is given as input to the motor network, along with the distance d and angle α to the nearest food, the current velocity v, and energy E of the agent. These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent re-spawned with the same value somewhere randomly on the grid (following the setup of ).
After 5000 time steps, the cumulative reward of the agent (the sum of the values of all the food it consumed) is taken as its fitness. During the evolutionary optimization, the parameters for both the motor network (connections) and plastic network (learning rule parameters) are co-evolved, and so agents must simultaneously learn to move and discriminate good/bad food.
Reward-modulated plasticity is one of the most promising explanations for biological credit assignment . In our network, the plasticity rule that updates the weights of the linear sensor network is a rewardmodulated rule which is parameterized as a linear combination of the input, the output, and the reward at each time step:
Additionally, after each plasticity step, the weights are normalized by mean subtraction, an important step for the stabilization of Hebbian-like plasticity rules . We use a genetic algorithm to optimize the learning rate η p and amplitudes of different terms θ = (θ 1 , . . . , θ 8 ). The successful plasticity rule after many food presentations must converge to a weight vector that predicts the correct food values (or allows the agent to correctly decide whether to eat a food or avoid it).
To have comparable results, we divide θ = (θ 1 , . . . , θ 8 ) by We then multiply the learning rate η p with θ max to maintain the rule's evolved form unchanged, η norm p = η p • θ max . In the following, we always use normalized η p and θ, omitting norm . To evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism .
The agents' parameters are initialized at random (drawn from a Gaussian distribution), then the sensory network is trained by the plasticity rule and finally, the agents are evaluated. After each generation, the bestperforming agents (top 10 % of the population size) are selected and copied into the next generation.
The remaining 90 % of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (σ = 0.1) to its parameters. To start with, we consider a static agent whose goal is to identify the value of presented food correctly. The static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task.
We first look at the evolved learning rate η p , which determines how fast (if at all) the network's weight vector is updated during the lifetime of the agents. We identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition.
The first natural factor is the distance d e between the two environments, with a larger distance requiring a higher learning rate, Fig. . This is an expected result since the convergence time to the ""correct"" weights is highly dependent on the initial conditions. If an agent is born at a point very close to optimality, which naturally happens if the environments are similar, the distance it needs to traverse on the fitness landscape is small.
Therefore it can afford to have a small learning rate, which leads to a more stable convergence and is not affected by noise. A second parameter that impacts the learning rate is the variance of the rewards. The reward an agent receives for the plasticity step contains a noise term ξ that is drawn from a zero mean Gaussian distribution with standard deviation σ.
This parameter controls the unreliability of the agent's sensory system, i.e., higher σ means that the information the agent gets about the value of the foods it consumes cannot be fully trusted to reflect the actual value of the foods. As σ increases, the learning rate η p decreases, which means that the more unreliable an environment becomes, the less an agent relies on plasticity to update its weights, Fig. .
Indeed for some combinations of relatively small distance d e and high reward variance σ, the EA converges to a learning rate of η p ≈ 0. This means that the agent opts to have no adaptation during its lifetime and remain at the mean of the two environments. It is an optimal solution when the expected loss due to ignoring the environmental transitions is, on average, lower than the loss the plastic network will incur by learning via the (often misleading because of the high σ) environmental cues.
A final factor that affects the learning rate the EA will converge to is the frequency of environmental change during an agent's lifetime. Since the environmental change is modeled as a simple, two-state Markov process (Fig. ), the control parameter is the transition probability p tr . When keeping everything else the same, the learning rate rapidly rises as we increase the transition probability from 0, and after reaching a peak, it begins to decline slowly, eventually reaching zero (Fig. ).
This means that when environmental transition is very rare, agents opt for a very low learning rate, allowing a slow and stable convergence to an environment-appropriate weight vector that leads to very low losses while the agent remains in that environment. As the rate of environmental transition increases, faster learning is required to speed up convergence in order to exploit the (comparatively shorter) stays in each environment.
Finally, as the environmental transition becomes too fast, the agents opt for slower or even no learning, which keeps them ) and the decision (b.) tasks, for a variety of parameters (p tr = 0.01, d e ∈ 0, 0.1, . . . , 1, and σ ∈ 0, 0.1, . . . , 1 in all 100 combinations). Despite the relatively small difference between the tasks, the evolved learning rules differ considerably.
For visual guidance, the lines connect θs from the same run. near the middle of the two environments, ensuring that the average loss of the two environments is minimal (Fig. ). The form of the evolved learning rule depends on the task: Decision vs. Prediction The plasticity parameters θ = (θ 1 , . . . , θ 8 ) for the rewardprediction task converge on approximately the same point, regardless of the environmental parameters (Fig. ).
In particular, θ 3 → 1, θ 5 → −1, θ i → 0 for all other i, and thus the learning rule converges to: Since by definition y t = g(W t X T t ) = W t X T t (g(x) = x in this experiment) and R t = W c X T t + ξ we get: Thus the distribution of ∆W t converges to a distribution with mean 0 and variance depending on η p and σ and W converges to W c .
So this learning rule will match the agent's weight vector with the vector of ingredient values in the environment. We examine the robustness of the learning rule the EA discovers by considering a slight modification of our task. Instead of predicting the expected food value, the agent now needs to decide whether to eat the presented food or not.
This is done by introducing a step-function nonlinearity (g(x) = 1 if x ≥ 1 and 0 otherwise). Then the output y(t) is computed as: Instead of the MSE loss between prediction and actual value, the fitness of the agent is now defined as the sum of the food values it chose to consume (by giving y t = 1). Besides these two changes, the setup of the experiments remains exactly the same.
The qualitative relation between η p and parameters of environment d e , σ and p tr is preserved in the changed experiment. However, the resulting learning rule is significantly different (Fig. ). The evolution converges to the following learning rule: In both cases, the rule has the form ∆W t = η p X t [α y R t + β y ].
Thus, the ∆W t is positive or negative depending on whether the reward R t is above or below a threshold (γ = −β y /α y ) that depends on the output decision of the network (y t = 0 or 1). Both learning rules (for the reward-prediction and decision tasks) have a clear Hebbian form (coordination of preand post-synaptic activity) and use the incoming reward signal as a threshold.
These similarities indicate some common organizing principles of reward-modulated learning rules, but their significant differences highlight the sensitivity of the optimization process to task details. We now turn to the moving embodied agents in the 2D environment. To optimize these agents, both the motor network's connections and the sensory network's plasticity parameters evolve simultaneously.
Since the motor network is initially random and the agent has to move to find food, the number of interactions an agent experiences in its lifetime can be small, slowing down the learning. However, having the larger motor network also has benefits for evolution because it allows the output of the plastic network to be read out and transformed in different ways, resulting in a broad set of solutions.
The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. e. The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red).
In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food. The agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights (Fig. ).
After ∼ 100 evolutionary steps (Fig. ), the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values. We compare the dependence of the moving and the static agents on the parameters of the environment: d e and the state transition probability p tr .
At first, in order to simplify the experiment, we set the transition probability to 0, but fixed the initial weights to be the average of E 1 and E 2 , while the real state is E 2 . In this experiment, the distance between states d e indicates twice the distance between the agent's initial weights and the optimal weights (the environment's ingredient values) since the agent is initialized at the mean of the two environment distributions.
Same as for the static agent, the learning rate increases with the distance d e (Fig. ). Then, we examine the effect of the environmental transition probability p tr on the evolved learning rate η p . In order for an agent to get sufficient exposure to each environment, we scale down the probability p tr from the equivalent experiment for the static agents.
We find that as the probability of transition increases, the evolved learning rate η p decreases (Fig. ). This fits with the larger trend for the static agent, although there is a clear difference when it comes to the increase for very small transition probabil-ities that were clearly identifiable in the static but not the moving agents.
This could be due to much sparser data and possibly the insufficiently long lifetime of the moving agent (the necessity of scaling makes direct comparisons difficult). Nevertheless, overall we see that the associations observed in the static agents between environmental distance d e and transition probability p tr and the evolved learning rate η p are largely maintained in the moving agents.
Still, more data would be needed to make any conclusive assertions about the exact effect of these environmental parameters on the emerging plasticity mechanisms. A crucial difference between the static and the moving agents is the function the plasticity has to perform. While in the static agents, the plasticity has to effectively identify the exact value distribution of the environment in order to produce accurate predictions, in the embodied agents, the plasticity has to merely produce a representation of the environment that the motor network can evolve to interpret adequately enough to make decisions about which food to consume.
To illustrate the difference, we plot the Pearson correlation coefficient between an agent's weights and the ingredient values of the environment it is moving in (Fig. ). We use the correlation instead of the MSE loss (which we used for the static agents in Fig. ) because the amplitude of the vector varies a lot for different agents and meaningful The evolved parameters of moving agents' plasticity rule for the g(s) = x, identity (a.) and the step function (Eq.
4) (b.) sensory networks (the environmental parameters here are d e ∈ [0, 1], σ = 0 and p tr = 0.001). The step function (binary output) network evolved a more structured plasticity rule (e.g., θ 3 > 0 for all realizations) than the linear network. Moreover, the learned weights for the identity network (c.) have higher variance and correlate significantly less with the environment's ingredient distribution compared to the learned weights for the thresholded network (d.)
conclusions cannot be drawn from the MSE loss. For many agents, the learned weights are consistently anti-correlated with the actual ingredient values (an example of such an agent is shown in Fig. ). This means that the output of the sensory network will have the opposite sign from the actual food value.
While in the static network, this would lead to very bad predictions and high loss, in the foraging task, these agents perform exactly as well as the ones where the weights and ingredients values are positively correlated, since the motor network can simply learn to move towards food for which it gets a negative instead of a positive sensory input.
This additional step of the output of the plastic network going through the motor network before producing any behavior has a strong effect on the plasticity rules that the embodied agents evolve. Specifically, if we look at the emerging rules the top performing agents have evolved (Fig. ), it becomes clear that, unlike the very well-structured rules of the static agents (Fig. ), there is now virtually no discernible pattern or structure.
The difference becomes even clearer if we look at the learned weights (at the end of a simulation) of the best-performing agents (Fig. ). While there is some correlation with the environment's ingredient value distribution, the variance is very large, and they do not seem to converge on the ""correct"" values in any way.
This is to some extent expected since, unlike the static agents where the network's output has to be exactly correct, driving the evolution of rules that converge to the precise environmental distribution, in the embodied networks, the bulk of the processing is done by the motor network which can evolve to interpret the scalar value of the sensory network's output in a variety of ways.
Thus, as long as the sensory network's plasticity rule co-evolves with the motor network, any plasticity rule that learns to produce consistent information about the value of encountered food can potentially be selected. To further test this assumption, we introduce a bottleneck of information propagation between the sensory and motor networks by using a step-function nonlinearity on the output of the sensory network (Eq.
4). Similarly to the decision task of the static network, the output of the sensory network now becomes binary. This effectively reduces the flow of information from the sensory to the motor network, forcing the sensory network to consistently decide whether food should be consumed (with the caveat that the motor network can still interpret the binary sign in either of two ways, either consuming food marked with 1 or the ones marked with 0 by the sensory network).
The agents perform equally well in this variation of the task as before (Fig. ), but now, the evolved plasticity rules seem to be more structured (Fig. ). Moreover, the variance of the learned weights in the bestperforming agents is significantly reduced (Fig. ), which indicates that the bottleneck in the sensory network is in-creasing selection pressure for rules that learn the environment's food distribution accurately.
We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental change that is neither too large nor too small are necessary conditions for an agent to be able to effectively adapt via synaptic plasticity.
Additionally, we find that minor variations of the task an agent has to solve or the parametrization of the network can give rise to significantly different plasticity rules. Our results partially extend to embodied artificial agents performing a foraging task. We show that environmental variability also pushes the development of plasticity in such agents.
Still, in contrast to the static agents, we find that the interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. We propose a potential cause of this degeneracy; as the relatively complex motor network is allowed to read out and process the outputs from the plastic network, any consistent information coming out of these outputs can be potentially interpreted in a behaviorally useful way.
Reducing the information the motor network can extract from the sensory system significantly limits learning rule variability. Our findings on the effect of environmental variability concur with the findings of previous studies that have identified the constraints that environmental variability places on the evolutionary viability of learning behaviors.
We extend these findings in a mechanistic model which uses a biologically plausible learning mechanism (synaptic plasticity). We show how a simple evolutionary algorithm can optimize the different parameters of a simple reward-modulated plasticity rule for solving simple prediction and decision tasks.
Reward-modulated plasticity has been extensively studied as a plausible mechanism for credit assignment in the brain ; ; and has found several applications in artificial intelligence and robotics tasks ; . Here, we demonstrate how such rules can be very well-tuned to take into account different environmental parameters and produce optimal behavior in simple systems.
Additionally, we demonstrate how the co-evolution of plasticity and static functional connectivity in different subnetworks fundamentally changes the evolutionary pressures on the resulting plasticity rules, allowing for greater diversity in the form of the learning rule and the resulting learned connectivity.
Several studies have demonstrated how, in biological networks, synaptic plasticity heavily interacts with and is driven by network topology . Moreover, it has been recently demonstrated that biological plasticity mechanisms are highly redundant in the sense that any observed neural connectivity or recorded activity can be achieved with a variety of distinct, unrelated learning rules .
This observed redundancy of learning rules in biological settings complements our results and suggests that the function of plasticity rules cannot be studied independently of the connectivity and topology of the networks they are acting on. The optimization of functional plasticity in neural networks is a promising research direction both as a means to understand biological learning processes and as a tool for building more autonomous artificial systems.
Our results suggest that reward-modulated plasticity is highly adaptable to different environments and can be incorporated into larger systems that solve complex tasks. This work studies a simplified toy model of neural network learning in stochastic environments. Future work could be built on this basic framework to examine more complex reward distributions and sources of environmental variability.
Moreover, a greater degree of biological realism could be added by studying more plausible network architectures (multiple plastic layers, recurrent and feedback connections) and more sophisticated plasticity rule parametrizations. Additionally, our foraging simulations were constrained by limited computational resources and were far from exhaustive.
Further experiments can investigate environments with different constraints, food distributions, multiple seasons, more complex motor control systems and interactions of those systems with different sensory networks as well as the inclusion of plasticity on the motor parts of the artificial organisms.","['As the transition probability increases, the learning rate initially rises and then declines.']",5346,multifieldqa_en,en,,7ae0ad8d4ded2dee79251ff4f951ecfcabad31d8b4f896ae," Environmental variability and network structure determine the optimal plasticity mechanisms in embodied agents. A well-trained agent will approach and consume food with positive values (green dots) and avoid negative food (red dots) The learning rate of the plastic sensory network eta p grows with the distance between environments d e c and decreases with the frequency of environmental change. The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red) is not an issue for performance since the motor network can interpret the inverted signs of food. For the capacity to learn to be beneficial in evolutionary terms, a costly nurturing period is often required, a phenomenon observed in both biological , and artificial organisms. It is unclear how the ability to learn first evolved, but its utility appears evident. The evolutionary balance between innate and learned behaviors is highly intricate, and different organisms have found different solutions to this problem. We hypothesize that the emergence and exact form of learning behaviors is naturally connected with the statistics of environmental fluctuations and tasks an organism needs to solve. We demonstrate that environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity. The link between learning and environmental uncertainty and fluctuation has been extensively demonstrated in both natural ;, and artificial environments. We show that coevolution between static connectivity and interacting plasticity mechanism in distinct sub-networks changes the function and form of the emerging plasticity rules in embodied agent performing a foraging task. The study was conducted by Sina Khajehabdollahi (from Department of Computer Science, University of Tübingen) and his co-authors. The paper was published in the journal Computer Science and Engineering (CSE). It was published by the German Academic Press (CAS) and the German Society for Computer Science (SAS) on November 14, 2013. It can be ordered by clicking here for a copy of the paper and its contents, or clicking here to see the online version. It includes the abstract, the first page, the second page, and the third page. It was written by the authors, Sina Khajiabdollahis and their co-researchers, and it is published by CSE on November 13, 2013, at the request of the CSE. For more information about the study, visit the CSA website. It also includes the second part of the article, the Introduction, the Third Page, the Methods, the Discussion, the Results and the Discussion. It has been translated from the German into English and the fourth page, as well as the English version. In some complex environments, hardcoded behaviors may be superior to learned ones given limits in the agent's lifetime and envi-ronmental uncertainty. The idea of meta-learning or optimizing synaptic plasticity rules to perform specific functions has been recently established as an engineering tool that can compete with state-of-the-art machine learning algorithms on various complex tasks. The plasticity mechanisms can be optimized to modify the connectivity of the artificial neural networks toward solving a particular task. We find that different environmental conditions and different combinations of static and plastic connectivity have a very large impact on the resulting plasticityRules. We imagine an agent who must forage to survive in an environment presenting various types of complex food particles. Each food particle is composed of various amounts and combinations of N ingredients that can have positive (food) or negative (poison) values. The priors could be generated by genetic memory, but the exact values are subject to change. To introduce environmental variability, we stochastically change the values of the ingredients. The value of a food particles is a weighted sum of its ingredients. To predict the rewardvalue of a given resource, the agent must learn the value of these ingredients by interacting with the environment. We control how (dis)similar the environments are by parametrically setting E 2 = (1 − 2d e )E 1 , with d e serving as a distance proxy for the environments. When d e = 0, the environment remains unchanged, and when D e = 1 the value. of each ingredient fully reverses when the environmental transition happens. The static agents perform a complex foraging task. Then we increase the complexity by switching to moving agents performing acomplex foragingtask. In both cases, we study the impact of different environmental parameters on the form of the evolved plasticity mechanism and the interaction of learned and static network connectivity. We study the effect that different factors (environ-arXiv:2303.06734v1 [q-bio.NC] 12 Mar 2023 mental fluctuation and reliability, task complexity) have on the. form of evolved functional reward-modulated plasticity Rules. We conclude by showing that the static agents are able to adapt to changing environments more easily than the moving ones, and that this can be used to create life-like AI systems that can continually interact with and adapt to new environments. The. idea of open-ended AI agents has become particularly appealing to a wide range of potential application areas, such as medicine, finance, medicine, and other fields. It can also be used as a tool to reverse engineer actual Plasticity mechanisms found in biological neural networks and uncover their functions. gent receives passively presented food as a vector of ingredients and can assess its compound value using the linear summation of its sensors with the (learned or evolved) weights. The network consists of N sensory neurons that are projecting to a single post-synaptic neuron. The agent computes the prediction of the food's value y t and is then given the true value R t. An agent can eat food by approaching it sufficiently closely, and each time a food particle is eaten, it is given a reward. The embodied agent has additional sensors for the distance from the nearest food, the angle between the current velocity and the nearby food direction, its own velocity, and its own energy level. These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent re-spawned with the same value somewhere randomly on the grid (following the setup of).After 5000 time steps, the cumulative reward of theAgent (the sum of the values of all the food it consumed) is taken as its fitness. During the evolutionary optimization, the parameters for both network (connections) and plastic network (learning rule parameters) are co-evolved, a-olved, the motor network and the plastic network are coevolved. The motor network's outputs are angular and linear acceleration, Fig. . The embodied agents spawn in a 2D space with periodic boundary conditions along with a number of food particles that are selected such that the mean of theFood value distribution is ∼ 0. The sensor layer receives inputs at each time step, which are processed by the plastic layer in the same way as the static sensory network. The output of that network is given as input to the motorNetwork, along with the distance d and angle α to the nearestFood, the currentVelocity v, and energy E of the Agent. These inputs areprocessed by twohidden layers (of 30 and 15 neurons) with tanh activation. The neural network's output is the speed and direction of the movement of an embodied agent in an environment scattered with food. The sensory network is then merged with a non-plastic motor network that is responsible for controlling the motion of the acting agent in the environment. The resulting motor network is called the embodied motor network, and it is used to control the embodied agent's movement in a variety of ways, such as the direction and speed of movement in which it is moving in a given environment, for example, the direction of travel in a straight line or the speed in a circle, or the angle in which the agent's velocity is in relation to the food direction. It is also used to train the embodied agents to move in a particular direction in which they can eat the same amount of food at a given time step. For example, to eat a piece of bread, the agent would move in such a way that the distance between the food and the next food direction is the same as the distance in which its velocity is at the start of a straightline. This is known as a ‘straightline’ movement. Reward-modulated plasticity is one of the most promising explanations for biological credit assignment. To evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism. We identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition. After each generation, the bestperforming agents (top 10 % of the population size) are selected and copied into the next generation. The remaining 90% of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (σ = 0.1) to its parameters. The reward an agent receives contains a noise term ξ that is drawn from a zero mean Gaussian distribution with standard deviation σ. This parameter controls the unreliability of the agent's sensory system, i.e., higher σ means that the information the agent gets about the value of the foods it consumes cannot be fully trusted to reflect the actual value. The more unreliable an environment becomes, the less an agent relies on plasticity to update its weights, Fig. . The EA converged to a learning rate of η p ≈ 0.d so agents must simultaneously learn to move and discriminate good/bad food. It is an optimal solution when the expected loss due to ignoring the environmental transitions is, on average, lower than the loss th th th for the agent to ignore the environment. The static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task of eating or avoiding a food. To start with, we consider a static agent whose goal is to identify thevalue of presented food correctly. We use a genetic algorithm to optimize thelearning rate of the EA to converge to a weight vector that predicts the correct food values. This is an expected result since the convergence time to the ""correct"" weights is highly dependent on the initial conditions. The distance d e between the two environments, with a larger distance requiring a higher learning rate, is expected to lead to a faster convergence time than the distance d a between the 2 environments. The agent opts to have no adaptation during its lifetime and remain at the mean of the two environment. This means that it can afford to have a small learning rate which leads to a more stable convergence and is not affected by noise. We then multiply the learned rate with θ max to maintain the rule's evolved form unchanged, η norm p = (θ 1 , . . . , θ 8 ) and omitting norm . In the following, we always use normalizedη p and θ, omittingnorm . The learning rate is the rate at which the network's weight vector is updated during the lifetime of the agents. As σ increases, the learningRate p decreases, which means the more unreliable the environment becomes. A final factor that affects the learning rate the EA will converge to is the frequency of environmental change during an agent's lifetime. When environmental transition is very rare, agents opt for a very low learning rate, allowing a slow and stable convergence to an environment-appropriate weight vector. As the rate of environmental transition increases, faster learning is required to speed up convergence in order to exploit the (comparatively shorter) stays in each environment. The form of the evolved learning rule depends on the task: Decision vs. Prediction. We examine the robustness of the learning rule the EA discovers by considering a slight modification of our task. Instead of predicting the expected food value, the agent now needs to decide whether to eat the presented food or not. The evolution converges to the following learning rule: In both cases, the rule has the form ∆W t = η p X t [α y R t + β y ]. The similarities indicate some common organizing principles of reward-modulated learning, but their significant differences highlight the sensitivity of the optimization process to task details. We turn to the moving agents in the 2D environment. To optimize these, both the motor network's connections and the sensory network's p. plastic network will incur by learning via the (often misleading because of the high σ) environmental cues. For visual guidance, the lines connect θs from the same run. near the middle of the two environments, ensuring that the average loss of theTwo environments is minimal (Fig. 1). Besides these two changes, the setup of the experiments remains exactly the same. The plasticity parameters. for the rewardprediction task converge on approximately the same point, regardless of the environmental parameters. The resulting learning rule is significantly different ( Fig. 2D). However, the resulting learning rules differ considerably. The rules have a clear Hebbian form (coordination of preand post-synaptic activity) and use the incoming reward signal as a threshold. For example, the output y(t) is computed as: Instead of the MSE loss between prediction and actual value,. the fitness of the agent is now defined as the sum of the food values it chose to consume (by giving y t = 1). The rules converge to a distribution with mean 0 and variance depending on the environment. So this learning rule will match the agent's weight vector with the vector of ingredient values in the environment and thus converge to W c X T t (g(x) = x in this experiment) and R t = W c x T t + ξ we get: Thus the distribution of w t is positive or negative depending on whether the reward R t is above or below a threshold (γ = −β y /α y ) that depends onthe output decision of the network (y t = 0 or 1). In this experiment, the rules converged to a positive distribution, which is the same as the one in the previous section of the study. We show that the learning rules (for the reward-prediction and decision tasks) have a similar form to those in the earlier section of this study. The results are shown in the next section. asticity parameters evolve simultaneously. The agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights. After 100 evolutionary steps, the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values. We compare the dependence of the moving and the static agents on the parameters of the environment: d e and the state transition probability p tr . We find that as the probability of transition increases, the evolved learning rate η p decreases in the moving agents. However, there is a clear difference when it comes to the increase for very small transition probabil-ities that were clearly identifiable in the static but not the moving agent. More data would be needed to make any conclusive assertions about the exact effect of these environmental parameters on the emerging plasticity mechanisms, we say. We conclude that the associations observed in thestatic agents between environmental distance d e. and transition probabilities p tr are largely maintained in theMoving agents. The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. The agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food. We use the correlation instead of the MSE loss because the amplitude of the vector varies a lot for different agents and   for different agent and  for different agents. We find the association between environmental distances and the agent's learning rate increases with the distance d E (Fig. 1). The agent is initialized at the mean of the two environment distributions. In order for an agent to get sufficient exposure to each environment, we scale down the probability p Tr from the equivalent experiment for the static agent. We see that the associated learning rate decreases as the distance between the agent and the environment increases. The association between the environmental distance and the evolving learning rate p tr increases as the agent gets more exposure to the environment. We show that this is the case for moving agents as well as static agents in Fig. 1. We also show that the correlation coefficient of an evolved agent’s weights with the ingredientvalue vector of the current environment (E 1 -blue, E 2 -red) is higher in moving agents than it is for static agents. In moving agents, the plasticity has to effectively identify the exact value distribution of the environments in order to produce accurate predictions. In the embodied agents, it has to merely produce a representation of the. environment that the motorNetwork can evolve to interpret adequately enough to make decisions about which food to consume. The step function (binary output) network evolved a more structured plasticity rule than the linear network. For many agents, the learned weights are consistently anti-correlated with the actual ingredient values. This effectively reduces the flow of information from the sensory to the motor network, forcing the sensory network to consistently decide whether food should be consumed. We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental changings are key to the evolution of plasticity rules in the embodied agents. We conclude by showing that the best-performing agents are able to learn the environment's food distribution more accurately than the static agents, but that this is not a fully-fledged plasticity process. The results are published in the open-source version of the book, ""Evolutionary Plasticity in the Embodied Brain"" (Simon & Schuster, 2013). For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. For confidential. support, contact the National suicide Prevention Helpline at 1-877-788-7255 or go to http://www.suicidespreventionhelpline.org/. For confidential help in the UK, call 08457 909090 or visit http:// www.sophistication.org/suicide-prevention-helplines/ or  the Samaritans in the UK on 0800-909090. For the UK version of this article, see http://sophistories.co.uk/sophism/features/features-of-your-life-in-the-trends-and-researchers-that-can-help-you-be-more-vulnerable-to-suicide.html. For more information, visit the Samaritans  website or the Sophisticated Psychology and Behavior and Cognition Online (http://www sophistics.com/blog/2014/01/09/sop/features of your-behavior and-behaviours-and behaviour in the-stark-structure-of evolutionary-plasticity-rules-that can-be taken-on-a-sensory-networks and/or from a trend to-a sensational-netway. The interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. The optimization of functional plasticity in neural networks is a promising research direction both as a means to understand biological learning processes and as a tool for building more autonomous artificial systems. The results suggest that reward-modulated plasticity is highly adaptable to different environments and can be incorporated into larger systems that solve complex tasks. Future work could be built on this basic framework to examine more complex reward distributions and sources of environmental variability. Further experiments can investigate environments with different constraints, food diaspora, and other factors that can affect learning in stochastic environments. The study was published in the journal Proceedings of the National Academy of Sciences of the United States of America (PNAS) (http://www.pnas.org/content/early/2014/09/29/151515/npsa.full) It is the first time the authors have used a mechanistic model which uses a biologically plausible learning mechanism (synaptic plasticity) to study the development of plasticity rules in the brain. The findings suggest that biological plasticity mechanisms are highly redundant in the sense that any observed neural connectivity or recorded activity can be achieved with a variety of distinct, unrelated learning rules . This observed redundancy of learning rules in biological settings complements our results and suggests that the function of Plasticity rules cannot be studied independently of the connectivity and topology of the networks they are acting on. It is hoped that the study will help develop a more realistic model of neural network learning in a more complex and realistic way than the one used in this study. It has been previously shown that plasticity can be very well-tuned to take into account different environmental parameters and produce optimal behavior in simple systems. This work studies a simplified toy model of Neural Network learning in the form of a simple reward-Modulated Plasticity rule for solving simple prediction and decision tasks. It also shows that the co-evolution of plasticITY and static functional connectivity in different subnetworks fundamentally changes the evolutionary pressures on the resulting plasticity Rules. The work could also be used as a basis for developing more sophisticated network architectures (multiple plastic layers, recurrent and feedback connections) and more sophisticated plasticity rule parametrizations. It could also lead to the creation of more realistic models of neural networks. tributions, multiple seasons, more complex motor control systems and interactions of those systems with different sensory networks as well as the inclusion of plasticity on the motor parts."
What kind of ultracold neutral plasmas does this study focus on?,"\section{Introduction}

Ultracold neutral plasmas studied in the laboratory offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems, including strongly coupled astrophysical plasmas \cite{VanHorn,Burrows}, as well as terrestrial sources of neutrons \cite{Hinton,Ichimaru_fusion,Atzeni,Boozer} and x-ray radiation \cite{Rousse,Esarey}.  Yet, under certain conditions, low-temperature laboratory plasmas evolve with dynamics that are governed by the quantum mechanical properties of their constituent particles, and in some cases by coherence with an external electromagnetic field.   

The relevance of ultracold plasmas to such a broad scope of problems in classical and quantum many-body physics has given rise to a great deal of experimental and theoretical research on these systems since their discovery in the late 90s.  A series of reviews affords a good overview of progress in the last twenty years \cite{Gallagher,Killian_Science,PhysRept,Lyon}.  Here, we focus on the subset of ultracold neutral plasmas that form via kinetic rate processes from state-selected Rydberg gases, and emphasize in particular the distinctive dynamics found in the evolution of molecular ultracold plasmas.  

While molecular beam investigations of threshold photoionization spectroscopy had uncovered relevant effects a few years earlier \cite{Scherzer,Alt}, the field of ultracold plasma physics began in earnest with the 1999 experiment of Rolston and coworkers on metastable xenon atoms cooled in a magneto optical trap (MOT) \cite{Killian}.  

This work and many subsequent efforts tuned the photoionization energy as a means to form a plasma of very low electron temperature built on a strongly coupled cloud of ultracold ions.  Experiment and theory soon established that fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes act to elevate the ion temperatures to around one degree Kelvin, and constrain the effective initial electron temperature to a range above 30 K \cite{Kuzmin,Hanson,Laha}.  

This apparent limit on the thermal energy of the electrons can be more universally expressed for an expanding plasma by saying that the electron correlation parameter, $\Gamma_e$, does not exceed 0.25, where, 
\begin{equation}
\Gamma_e = \frac{e^2}{4\pi \epsilon_0 a_{ws}}\frac{1}{k_B T_e}
\label{eqn:gamma_e}
\end{equation}
defines the ratio of the average unscreened electron-electron potential energy to the electron kinetic energy.  $a_{ws}$ is the Wigner-Seitz radius, related to the electron density by, $\rho_e = 1/(\frac{4}{3} \pi a_{ws}^3)$.  These plasmas of weakly coupled electrons and strongly coupled ions have provided an important testing ground for ion transport theory and the study of electron-ion collision physics \cite{Strickler}.

Soon after the initial reports of ultracold plasmas formed by direct photoionization, a parallel effort began with emphasis on the plasma that forms spontaneously by Penning ionization and electron-impact avalanche in a dense ultracold Rydberg gas \cite{Mourachko}.  This process affords less apparent control of the initial electron temperature.  But, pulsed field-ionization measurements soon established that the photoionized plasma and that formed by the avalanche of a Rydberg gas both evolve to quasi-equilibria of electrons, ions and high-Rydberg neutrals \cite{Rolston_expand,Gallagher}.  

Early efforts to understand plasmas formed by Rydberg gas avalanche paid particular attention to the process of initiation.  Evolution to plasma in effusive atomic beams was long known for high-Rydberg gases of caesium and well explained by coupled rate equations \cite{Vitrant}.  But, low densities and ultracold velocity distributions were thought to exclude Rydberg-Rydberg collisional mechanisms in a MOT.  

In work on ultracold Rydberg gases of Rb and Cs, Gallagher, Pillet and coworkers describe the initial growth of electron signal by a model that includes ionization by blackbody radiation and collisions with a background of uncooled Rydberg atoms \cite{Mourachko,Gallagher,Li,Comparat,Tanner}. This picture was subsequently refined to include many-body excitation and autoionization, as well as attractive dipole-dipole interactions \cite{Viteau,Pillet}, later confirmed by experiments at Rice \cite{Mcquillen}.  

The Orsay group also studied the effect of adding Rydberg atoms to an established ultracold plasma.  They found that electron collisions in this environment completely ionize added atoms, even when selected to have deep binding energies \cite{Vanhaecke}.  They concluded from estimates of electron trapping efficiency that the addition of Rydberg atoms does not significantly alter the electron temperature of the plasma.  

Tuning pair distributions by varying the wavelength of the excitation laser, Weidem\""uller and coworkers confirmed the mechanical effects of van der Waals interactions on the rates of Penning ionization in ultracold $^{87}$Rb Rydberg gases \cite{Amthor_mech}.  They recognized blackbody radiation as a possible means of final-state redistribution, and extended this mechanical picture to include long-range repulsive interactions \cite{Amthor_model}.  This group later studied the effects of spatial correlations in the spontaneous avalanche of Rydberg gases in a regime of strong blockade, suggesting a persistence of initial spatial correlations \cite{RobertdeSaintVincent}.  

Robicheaux and coworkers have recently investigated the question of prompt many-body ionization from the point of view of Monte Carlo classical trajectory calculations \cite{Goforth}.  For atoms on a regular or random grid driven classically by an electromagnetic field, they find that many-body excitation enhances prompt ionization by about twenty percent for densities greater than $5.6 \times 10^{-3}/(n_0^2 a_0)^3$, where $n_0$ is the principal quantum number of the Rydberg gas and $a_0$ is the Bohr radius.  They observed that density fluctuations (sampled from the distribution of nearest neighbour distances) have a greater effect, and point to the possible additional influence of secondary electron-Rydberg collisions and the Penning production of fast atoms not considered by the model, but already observed by Raithel and coworkers \cite{Knuffman}.  

The Raithel group also found direct evidence for electron collisional $\ell$-mixing in a Rb MOT \cite{Dutta}, and used selective field ionization to monitor evolution to plasma on a microsecond timescale in ultracold $^{85}$Rb $65d$ Rydberg gases with densities as low as $10^8$ cm$^{-3}$ \cite{WalzFlannigan}.  Research by our group at UBC has observed very much the same dynamics in the relaxation of Xe Rydberg gases of similar density prepared in a molecular beam \cite{Hung2014}.  In both cases, the time evolution to avalanche is well-described by coupled rate equations (see below), assuming an initializing density of Penning electrons determined by Robicheaux's criterion \cite{Robicheaux05}, applied to an Erlang distribution of Rydberg-Rydberg nearest neighbours.  

Theoretical investigations of ultracold plasma physics have focused for the most part on the long- and short-time dynamics of plasmas formed by direct photoionization \cite{PhysRept,Lyon}.  In addition to studies mentioned above, key insights on the evolution dynamics of Rydberg gases have been provided by studies of Pohl and coworkers exploring the effects of ion correlations and recombination-reionization on the hydrodynamics of plasma expansion \cite{Pohl:2003,PPR}.  Further research has drawn upon molecular dynamics (MD) simulations to reformulate rate coefficients for the transitions driven by electron impact between highly excited Rydberg states \cite{PVS}, and describe an effect of strong coupling as it suppresses three-body recombination \cite{Bannasch:2011}.  MD simulations confirm the accuracy of coupled rate equation descriptions for systems with $\Gamma$ as large as 0.3.  Newer calculations suggest a strong connection between the order created by dipole blockade in Rydberg gases and the most favourable correlated distribution of ions in a corresponding strongly coupled ultracold plasma \cite{Bannasch:2013}.  

Tate and coworkers have studied ultracold plasma avalanche and expansion theoretically as well as experimentally.  Modelling observed expansion rates, they recently found that $^{85}$Rb atoms in a MOT form plasmas with effective initial electron temperatures determined by initial Rydberg density and the selected initial binding energy, to the extent that these parameters determine the fraction of the excited atoms that ionize by electron impact in the avalanche to plasma \cite{Forest}.  This group also returned to the question of added Rydberg atoms, and managed to identify a crossover in $n_0$, depending on the initial electron temperature, that determines whether added Rydberg atoms of a particular initial binding energy act to heat or cool the electron temperature \cite{Crockett}.   

Our group has focused on the plasma that evolves from a Rydberg gas under the low-temperature conditions of a skimmed, seeded supersonic molecular beam.  In work on nitric oxide starting in 2008 \cite{Morrison2008,Plasma_expan,Morrison_shock,PCCP}, we established an initial kinetics of electron impact avalanche ionization that conforms with coupled rate equation models \cite{Saquet2011,Saquet2012,Scaling,haenelCP} and agrees at early times with the properties of ultracold plasmas that evolve from ultracold atoms in a MOT.  We have also observed unique properties of the NO ultracold plasma owing to the fact that its Rydberg states dissociate \cite{Haenel2017}, and identified relaxation pathways that may give rise to quantum effects \cite{SousMBL,SousNJP}.  The remainder of this review focuses on the nitric oxide ultracold plasma and the unique characteristics conferred by its evolution from a Rydberg gas in a laser-crossed molecular beam.  


\section{Avalanche to strong coupling in a molecular Rydberg gas}

\subsection{The molecular beam ultracold plasma compared with a MOT}

When formed with sufficient density, a Rydberg gas of principal quantum number $n_0>30$ undergoes a spontaneous avalanche to form an ultracold plasma \cite{Li,Morrison2008,RobertdeSaintVincent}.  Collisional rate processes combine with ambipolar hydrodynamics to govern the properties of the evolving plasma.  For a molecular Rydberg gas, neutral fragmentation, occurs in concert with electron-impact ionization, three-body recombination and electron-Rydberg inelastic scattering.  Neutral dissociation combined with radial expansion in a shaped distribution of charged particles, can give rise to striking effects of self-assembly and spatial correlation \cite{Schulz-Weiling2016,Haenel2017}.   

The formation of a molecular ultracold plasma requires the conditions of local temperature and density afforded by a high mach-number skimmed supersonic molecular beam.  Such a beam propagates at high velocity in the laboratory, with exceedingly well-defined hydrodynamic properties, including a propagation-distance-dependent density and sub-Kelvin temperature in the moving frame \cite{MSW_tutorial}.  The low-temperature gas in a supersonic molecular beam differs in three important ways from the atomic gas laser-cooled in a magneto-optical trap (MOT).

The milli-Kelvin temperature of the gas of ground-state NO molecules entrained in a beam substantially exceeds the sub-100 micro-Kelvin temperature of laser-cooled atoms in a MOT.  However, the evolution to plasma tends to erase this distinction, and the two further characteristics that distinguish a beam offer important advantages for ultracold plasma physics:  Charged-particle densities in a molecular beam can exceed those attainable in a MOT by orders of magnitude.  A great many different chemical substances can be seeded in a free-jet expansion, and the possibility this affords to form other molecular ultracold plasmas, introduces interesting and potentially important new degrees of freedom governing the dynamics of their evolution.


\subsection{Supersonic molecular beam temperature and particle density}

Seeded in a skimmed supersonic molecular beam, nitric oxide forms different phase-space distributions in the longitudinal (propagation) and transverse coordinate dimensions.  As it propagates in $z$, the NO molecules reach a terminal laboratory velocity, $u_{\parallel}$, of about 1400 ${\rm ms^{-1}}$, which varies with the precise seeding ratio.  

The distribution of $v_{\parallel}$, narrows to define a local temperature, $T_{\parallel}$, of approximately 0.5 K.  The beam forms a Gaussian spatial distribution in the transverse coordinates, $x$ and $y$.  In this plane, the local velocity, $v_{\perp}(r)$ is defined for any radial distance almost entirely by the divergence velocity of the beam, $u_{\perp}(r)$.  Phase-space sorting cools the temperature in the transverse coordinates, $T_{\perp}$ to a value as low as $\sim 5$ mK \cite{MSW_tutorial}.  

The stagnation pressure and seeding ratio determine the local density distribution as a function of $z$.  For example, expanding from a stagnation pressure of 500 kPa with a 1:10 seeding ratio, a molecular beam propagates 2.5 cm to a skimmer and then 7.5 cm to a point of laser interaction, where it contains NO at a peak density of $1.6 \times 10^{14}$ cm$^{-3}$.  

Here, crossing the molecular beam with a laser beam tuned to the transition sequence, ${\rm X} ~^2 \Pi_{1/2} ~N'' = 1 \xrightarrow{\omega_1} {\rm A} ~^2\Sigma^+ ~N'=0  \xrightarrow{\omega_2} n_0 f(2)$ forms a Gaussian ellipsoidal volume of Rydberg gas in a single selected principal quantum number, $n_0$, orbital angular momentum, $\ell = 3$, NO$^+$ core rotational quantum number, $N^+ = 2$ and total angular momentum neglecting spin, $N=1$.  

A typical $\omega_1$ pulse energy of 2 $\mu$J and a Gaussian width of 0.2 mm serves to drive the first step of this sequence in a regime of linear absorption.  Overlapping this volume by an $\omega_2$ pulse with sufficient fluence to saturate the second step forms a Rydberg gas ellipsoid with a nominal peak density of $5 \times 10^{12}$ cm$^{-3}$  \cite{Morrison2008,MSW_tutorial}.  Fluctuations in the pulse energy and longitudinal mode of $\omega_1$ cause the real density to vary.  For certain experiments, we find it convenient to saturate the $\omega_1$ transition, and vary the density of Rydberg gas by delaying $\omega_2$.  An $\omega_1$-$\omega_2$ delay, $\Delta t$, reduces the Rydberg gas density by a precise factor, $e^{-\Delta t/\tau}$, where $\tau$ is the 200 ns radiative lifetime of NO ${\rm A} ~^2\Sigma^+ ~N'=0$ \cite{Carter,Hancock}.


\subsection{Penning ionization}

The density distribution of a Rydberg gas defines a local mean nearest neighbour distance, or Wigner-Seitz radius of $ a_{ws} =  \left(3/4 \pi \rho \right)^{1/3} $, where $\rho$ refers to the local Rydberg gas density.  For example, a Rydberg gas with a density of $ \rho_0=0.5 \times 10^{12}$ cm$^{-3} $ forms an Erlang distribution \cite{Torquato.1990} of nearest neighbour separations with a mean value of $ 2 a_{ws}=1.6$  $\mu$m.  

A semi-classical model \cite{Robicheaux05} suggests that 90 percent of Rydberg molecule pairs separated by a critical distance, $ r_c = 1.8 \cdot 2 n_0^2 a_0 $ or less undergo Penning ionization within 800 Rydberg periods.  We can integrate the Erlang distribution from $ r=0 $ to the critical distance $r = r_c$ for a Rydberg gas of given $n_0$, to define the local density of Penning electrons ($ \rho_e$ at $t=0$) produced by this prompt interaction, for any given initial local density, $\rho_0$ by the expression:
\begin{equation}
\rho_e(\rho_0,n_0) = \frac{0.9}{2} \cdot 4 \pi \rho_0 ^2\int_0^{r_{c}} r^2 \mathrm{e}^{-\frac{4\pi}{3}\rho_0 r^3}\mathrm{d}r \quad.
\label{eqn:Erlang}
\end{equation}

Evaluating this definite integral yields an equation in closed form that predicts the Penning electron density for any particular initial Rydberg density and principal quantum number.
\begin{equation}
\rho_e(\rho_0,n_0) =\frac{0.9 \rho_0}{2}(1-\mathrm{e}^{-\frac{4\pi}{3}\rho_0 r_c^3}) \quad.
\label{Eq:PenDens}
\end{equation}
\begin{figure}[h!]
\centering
\includegraphics[scale=0.33]{Penning_Latice.pdf}
\caption{Distributions of ion-ion nearest neighbours following Penning ionization and electron-impact avalanche simulated for a predissociating molecular Rydberg gas of initial principal quantum number, $n_0$, from 30 to 80, and density of 10$^{12}$ cm$^{-3}$.  Dashed lines mark corresponding values of $a_{ws}$. Calculated by counting ion distances after relaxation to plasma in 10$^6$-particle stochastic simulations. Integrated areas proportional to populations surviving neutral dissociation.}
\label{fig:PL}
\end{figure}

Prompt Penning ionization acts on the portion of the initial nearest-neighbour distribution in the Rydberg gas that lies within $r_c$.  When a molecule ionizes, its collision partner relaxes to a lower principal quantum number, $n'<n_0/\sqrt{2}$.  This close-coupled interaction disrupts the separability of Rydberg orbital configurations in the Penning partner.  This causes mixing with core penetrating states that are strongly dissociative.  Penning partners are thus very likely to dissociate, leaving a spatially isolated distribution of ions.  We refer to the spatial correlation that results as a Penning lattice \cite{Sadeghi:2014}.  The extent of this effect varies depending on the local density and the selected initial principal quantum number.  Figure \ref{fig:PL} shows the degree to which Rydberg gases with initial principal quantum numbers from 30 to 80 form a Penning lattice for an initial density of $1 \times 10^{12} ~{\rm cm}^{-3}$.  

\subsection{Spontaneous electron-impact avalanche}

The electrons produced by prompt Penning ionization start an electron impact avalanche.  The kinetics of this process are well described by a set of coupled rate equations that account for state-to-state electron-Rydberg inelastic scattering, electron-impact ionization and three-body ion-electron recombination \cite{PPR,Saquet2011,Saquet2012,Scaling} using detailed rate coefficients,  $k_{ij}$, $k_{i,ion}$ and $k_{i,tbr}$ validated by MD simulations \cite{PVS}.  
\begin{eqnarray}
-\frac{d\rho_i}{dt}&=&\sum_{j}{k_{ij}\rho_e\rho_i}-\sum_{j}{k_{ji}\rho_e\rho_j} \nonumber\\
&& +k_{i,ion}\rho_e\rho_i-k_{i,tbr}\rho^3_e 
  \label{level_i}
\end{eqnarray}
\noindent and,
\begin{equation}
\frac{d\rho_e}{dt}=\sum_{i}{k_{i,ion}\rho_e^2}-\sum_{i}{k_{i,tbr}\rho^3_e}
  \label{electron}
\end{equation}

The relaxation of Rydberg molecules balances with collisional ionization to determine an evolving temperature of avalanche electrons to conserve total energy per unit volume. 
\begin{equation}
E_{tot}=\frac{3}{2}k_BT_e(t)\rho_e(t)-R\sum_i{\frac{\rho_i(t)}{n_i^2}},
  \label{energy}
\end{equation}
Here, for simplicity, we neglect the longer-time effects of Rydberg predissociation and electron-ion dissociative recombination \cite{Saquet2012}.

Such calculations show that the conversion from Rydberg gas to plasma occurs on a timescale determined largely by the local Penning electron density, or Penning fraction, $P_f = \rho_e/\rho_0$, which depends on the local density of Rydberg molecules and their initial principal quantum number.  

Avalanche times predicted by coupled rate equation calculations range widely.  For example, in a model developed for experiments on xenon, simulations predict that a Rydberg gas with $n_0 = 42$ at a density of $8.8 \times 10^8 ~{\rm cm}^{-3}$ ($P_f = 6 \times 10^{-5}$) avalanches with a half time of  40 $\mu$s \cite{Hung2014}.  At an opposite extreme, rate equations estimate that a Rydberg gas of NO with $n_0=60$ at a density of $1 \times 10^{12} ~{\rm cm}^{-3}$ ($P_f = 0.3$) rises to plasma in about 2 ns \cite{Saquet2012}.  

\begin{figure}[h!]
\centering
\includegraphics[width= .49 \textwidth]{SFI_n=49.pdf}
   \caption{Contour plots showing SFI signal as a function the applied field for an $nf(2)$ Rydberg gas with an initial principal quantum number, $n_0=49$.  Each frame represents 4,000 SFI traces, sorted by initial Rydberg gas density.  Ramp field beginning at 0 and 150 ns (top, left to right), and  300 and 450 ns (bottom) after the $\omega_2$ laser pulse.  The two bars of signal most evident at early ramp field delay times represent the field ionization of the $49f(2)$ Rydberg state respectively to NO$^+$ X $^1\Sigma^+$ cation rotational states, $N^+=0$ and 2.  The signal waveform extracted near zero applied field represents the growing population of plasma electrons.  
   }
\label{fig:SFI}
\end{figure}

Selective field ionization (SFI) probes the spectrum of binding energies in a Rydberg gas.  Applied as a function of time after photoexcitation, SFI maps the evolution from a state of selected initial principal quantum number, $n_0$, to plasma \cite{Haenel2017}.  Figure \ref{fig:SFI} shows SFI spectra taken at a sequence of delays after the formation of $49f(2)$ Rydberg gases of varying density.    

Here, we can see that a $49f(2)$ Rydberg gas with an estimated initial density $\rho_0 = 3 \times 10^{11} ~{\rm cm}^{-3}$ relaxes to plasma on a timescale of about 500 ns.  Observations such as these agree well with the predictions of coupled rate-equation calculations.  We can understand this variation in relaxation dynamics with $\rho_0$ and $n_0$ quite simply in terms of the corresponding density of prompt Penning electrons these conditions afford to initiate the avalanche to plasma.  

Figure \ref{fig:scaled_rise} illustrates this, showing how rise times predicted by coupled rate-equation simulations for a large range of initial densities and principal quantum number match when plotted as a function of time scaled by the ultimate plasma frequency and fraction of prompt Penning electrons.  The dashed line gives an approximate account of the scaled rate of avalanche under all conditions of Rydberg gas density and initial principal quantum number in terms of the simple sigmoidal function:

\begin{equation}
\frac{\rho_e}{\rho_0} = \frac{a}{b+e^{-c\tau}},
  \label{scaledEq1}
\end{equation}
where,
\begin{equation}
\tau = t \omega_e P_f^{3/4},
  \label{scaledEq2}
\end{equation}
in which $\omega_e$ is the plasma frequency after avalanche, $P_f$ is the fraction of prompt Penning electrons, and $a = 0.00062$,  $b =   0.00082$ and $c =     0.075$ are empirical coefficients.  

\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{sim_analytical_density.pdf}
   \caption{Rise in fractional electron density as a function of time scaled by the plasma frequency, $\omega_e$ and fraction, $\rho_e(t=0)/\rho_0 = P_f$, of prompt Penning electrons.  Simulation results shown for $n_0 = 30$, 50 and 70 with initial densities, $\rho_0 = 10^9,~10^{10},~10^{11},~{\rm and}~10^{12}~{\rm cm}^{-3}$.  
   }
\label{fig:scaled_rise}
\end{figure}


\subsection{Evolution to plasma in a Rydberg gas Gaussian ellipsoid}

As outlined above, the local density and principal quantum number together determine the rate at which a Rydberg gas avalanches to plasma.  Our experiment crosses a 2 mm wide cylindrically Gaussian molecular beam with a 1 mm diameter TEM$_{00}$ $\omega_1$ laser beam to produce a Gaussian ellipsoidal distribution of molecules excited to the A $^2\Sigma^+$ $v=0, ~N'=0$ intermediate state.  A larger diameter $\omega_2$ pulse then drives a second step that forms a Rydberg gas in a single $n_0f(2)$ state with the spatial distribution of the intermediate state.  

We model this shaped Rydberg gas as a system of 100 concentric ellipsoidal shells of varying density \cite{haenelCP}.  Coupled rate equations within each shell describe the avalanche to plasma.  This rate process proceeds from shell to shell with successively longer induction periods, determined by the local density as detailed above.  The rising conversion of Rydberg molecules to ions plus neutral dissociation products conserves the particle number in each shell.  We assume that local space charge confines electrons to shells, conserving quasi-neutrality.  Electrons exchange kinetic energy at the boundaries of each shell, which determines a single plasma electron temperature.  

\begin{figure}[h!]
\centering
\includegraphics[width= .5 \textwidth]{shell_model_100}
   \caption{(top frame) Cross-sectional contour diagram in the $x,y$ plane for $z=0$ describing the distribution of ion plus electron density over 100 shells of Gaussian ellipsoid with initial dimensions, $\sigma_x= 0.75$ mm and $\sigma_y= \sigma_z = 0.42$ mm and an initial $n_0 = 50$ Rydberg gas density, $\rho_0 = 2 \times 10^{11}$ cm$^{3}$ after an evolution time of 100 ns.  (bottom frame) Curves describing the (dashed) ascending ion and (solid) descending Rydberg gas densities of each shell as functions of evolution time, for $t=20$, 40, 60, 80 and 100 ns.  
   }
\label{fig:shell}
\end{figure}

The upper frame of Figure \ref{fig:shell} shows contours of NO$^+$ ion density after 100 ns obtained from a shell-model coupled rate-equation simulation of the avalanche of a Gaussian ellipsoidal Rydberg gas of nitric oxide with a selected initial state, $50f(2)$ and a density of $2 \times 10^{11}$ cm$^{-3}$.  Here, we simulate a relaxation that includes channels of predissociation at every Rydberg level and redistributes the energy released to electrons, which determines a uniform rising electron temperature for all shells.  

For comparison, the lower frame plots curves describing the ion density of each shell as a function of time from 20 to 100 ns, as determined by applying Eq \ref{scaledEq1} for the local conditions of initial Rydberg gas density.  This numerical approximation contains no provision for predissociation.  Coupled rate-equation simulations for uniform volumes show that predissociation depresses yield to some degree, but has less effect on the avalanche kinetics \cite{Saquet2012}.  Therefore, we can expect sets of numerically estimated shell densities, scaled to agree with the simulated ion density at the elapsed time of 100 ns to provide a reasonable account of the earlier NO$^+$ density profiles as a function of time.   


\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{Shell_pop}
   \caption{Global population fractions of particles as they evolve in the avalanche of a shell-model ellipsoidal Rydberg gas with the initial principal quantum number and density distribution of Figure \ref{fig:shell}
   }
\label{fig:shell_yields}
\end{figure}

For each time step, the difference, $\rho_0 - \rho_e$ defines the neutral population of each shell.  We assign a fraction of this population to surviving Rydberg molecules, such that the total population of NO$^*$ as a function of time agrees with the prediction of the shell-model simulation, as shown in Figure \ref{fig:shell_yields}.  We consider the balance of this neutral population to reflect NO$^*$ molecules that have dissociated to form N($^4$S) $+$ O($^3$P).  Figure \ref{fig:shell} plots these surviving Rydberg densities as a function of radial distance for each evolution time.  At the initial density of this simulation, note at each time step that a higher density of Rydberg molecules encloses the tail of the ion density distribution in $x$.   

\subsection{Plasma expansion and NO$^+$ - NO$^*$ charge exchange as an avenue of quench}

We regard the ions as initially stationary.  The release of electrons creates a radial electric potential gradient, which gives rise to a force, $-e\nabla \phi_{k,j}(t)$, that accelerates the ions in shell $j$ in direction $k$ according to \cite{Sadeghi.2012}:

\begin{align}
\frac{-e}{m'}\nabla \phi_{k,j}(t) = & \frac{\partial u_{k,j}(t)}{\partial t} \notag  \\
= & \frac{k_BT_e(t)}{m'\rho_j(t)} \frac{\rho_{j+1}(t) - \rho_j(t)}{r_{k,j+1}(t) - r_{k,j}(t)},
  \label{dr_dt}
\end{align}

\noindent where $\rho_j(t)$ represents the density of ions in shell $j$.  

The instantaneous velocity, $u_{k,j}(t)$ determines the change in the radial coordinates of each shell, $r_{k,j}(t)$, 
\begin{equation}
\frac{\partial r_{k,j}(t)}{\partial t}=u_{k,j}(t) = \gamma_{k,j}(t) r_{k,j}(t),
  \label{dr_dt}
\end{equation}
\noindent which in turn determines shell volume and thus its density, $ \rho_j(t)$.   
The electron temperature supplies the thermal energy that drives this ambipolar expansion.  Ions accelerate and $T_e$ falls according to: 

\begin{equation}
\frac{3k_B}{2}\frac{\partial T_e(t)}{\partial t}= -\frac{m'}{\sum_{j}{N_j}}\sum_{k,j}{N_j u_{k,j}(t)\frac{\partial u_{k,j}(t)}{\partial t}},
  \label{dr_dt}
\end{equation}
\noindent where we define an effective ion mass, $m'$, that recognizes the redistribution of the electron expansion force over all the NO$^+$ charge centres by resonant ion-Rydberg charge exchange, which occurs with a very large cross section \cite{PPR}.  
\begin{equation}
m' =\left (1+ \frac{\rho^*_{j}(t)}{ \rho_j(t)}\right) m ,
  \label{dr_dt}
\end{equation}
\noindent in which $\rho^*_{j}(t)$ represents the instantaneous Rydberg density in shell $j$.

The initial avalanche in the high-density core of the ellipsoid leaves few Rydberg molecules, so this term has little initial effect.  Rydberg molecules predominate in the lower-density wings.  There, momentum sharing by charge exchange assumes a greater importance.  

We see this most directly in the $\omega_2$ absorption spectrum of transitions to states in the $n_0 f(2)$ Rydberg series, detected as the long-lived signal that survives a flight time of 400 $\mu$s to reach the imaging detector.  The balance between the rising density of ions and the falling density of Rydberg molecules depends on the initial density of electrons produced by prompt Penning ionization.  As clear from Eq \ref{Eq:PenDens}, this Penning fraction depends sensitively on the principal quantum number, and for all principal quantum numbers, on the initial Rydberg gas density.  

\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{w2_spectra}
   \caption{Double-resonant spectra of nitric oxide Rydberg states in the $nf$ series converging to NO$^+$ $v=0$, $N^+=2$ (designated, $nf(2)$), derived from the late-peak signal obtained after a flight time of 400 $\mu$s by scanning $\omega_2$ for $\omega_1$ tuned to NO A $^2\Sigma^+$ $v=0$, $N'=0$ for initial $nf(2)$ densities from top to bottom of 0.07, 0.10, 0.13, 0.19, 0.27, 0.30, 0.32 and $3 \times 10^{12}$ cm$^{3}$.  
  }
\label{fig:w2_spectra}
\end{figure}

Figure \ref{fig:w2_spectra} shows a series of $\omega_2$ late-signal excitation spectra for a set of initial densities.  Here, we see a clear consequence of the higher-order dependence of Penning fraction - and thus the NO$^+$ ion - NO$^*$ Rydberg molecule balance - on $n_0$, the $\omega_2$-selected Rydberg gas initial principal quantum number.  This Penning-regulated NO$^+$ ion - NO$^*$ Rydberg molecule balance appears necessary as a critical factor in achieving the long ultracold plasma lifetime required to produce this signal.  We are progressing in theoretical work that explains the stability apparently conferred by this balance.  


\subsection{Bifurcation and arrested relaxation}

Ambipolar expansion quenches electron kinetic energy as the initially formed plasma expands.  Core ions follow electrons into the wings of the Rydberg gas.  There, recurring charge exchange between NO$^+$ ions and NO$^*$ Rydberg molecules redistributes the ambipolar force of the expanding electron gas, equalizing ion and Rydberg velocities.  This momentum matching effectively channels electron energy through ion motion into the overall $\pm x$ motion of gas volumes in the laboratory.  The internal kinetic energy of the plasma, which at this point is defined almost entirely by the ion-Rydberg relative motion, falls.  Spatial correlation develops, and over a period of 500 ns, the system forms the plasma/high-Rydberg quasi-equilibrium dramatically evidenced by the SFI results in Figure \ref{fig:SFI}.  

\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{Bifurcation.pdf}
   \caption{$x,y$ detector images of ultracold plasma volumes produced by 2:1 aspect ratio ellipsoidal Rydberg gases with selected initial state, $40f(2)$ after a flight time of 402 $\mu$s over a distance of 575 mm.  Lower frame displays the distribution in $x$ of the charge integrated in $y$ and $z$.  Both images represent the unadjusted raw signal acquired in each case after 250 shots.  
   }
\label{fig:bifurcation}
\end{figure}

In the wings, momentum redistribution owing to cycles of ion-Rydberg charge transfer retards radial expansion \cite{Pohl2003,PPR}.  By redirecting electron energy from ambipolar acceleration to $\pm x$ plasma motion, NO$^+$ to NO$^*$ charge exchange dissipates electron thermal energy.  This redistribution of energy released in the avalanche of the Rydberg gas to plasma, causes the ellipsoidal Rydberg gas to bifurcate \cite{Schulz-Weiling2016,Haenel2017}, forming very long-lived, separating charged-particle distributions.  We capture the electron signal from these recoiling volumes on an imaging detector as pictured in Figure \ref{fig:bifurcation}.  Here, momentum matching preserves density and enables ions and Rydberg molecules to relax to positions that minimize potential energy, building spatial correlation.  

The semi-classical description of avalanche and relaxation outlined above forms an important point of reference from which to interpret our experimental observations.  The laser crossed molecular beam illumination geometry creates a Rydberg gas with a distinctively shaped high-density spatial distribution.  This initial condition has an evident effect on the evolution dynamics.  We have developed semi-classical models that explicitly consider the coupled rate and hydrodynamic processes governing the evolution from Rydberg gas to plasma using a realistic, ellipsoidal representation of the ion/electron and Rydberg densities \cite{haenelCP}.  No combination of initial conditions can produce a simulation that conforms classically with the state of arrested relaxation we observe experimentally. 


\subsection{A molecular ultracold plasma state of arrested relaxation}

Thus, we find that spontaneous avalanche to plasma splits the core of an ellipsoidal Rydberg gas of nitric oxide. As ambipolar expansion quenches the electron temperature of this core plasma, long-range, resonant charge transfer from ballistic ions to frozen Rydberg molecules in the wings of the ellipsoid quenches the ion-Rydberg molecule relative velocity distribution. This sequence of steps gives rise to a remarkable mechanics of self-assembly, in which the kinetic energy of initially formed hot electrons and ions drives an observed separation of plasma volumes. These dynamics redistribute ion momentum, efficiently channeling electron energy into a reservoir of mass-transport. This starts a process that evidently anneals separating volumes to a state of cold, correlated ions, electrons and Rydberg molecules. 

We have devised a three-dimensional spin model to describe this arrested state of the ultracold plasma in terms of two, three and four-level dipole-dipole energy transfer interactions (spin flip-flops), together with Ising interactions that arise from the concerted pairwise coupling of resonant pairs of dipoles \cite{SousMBL,SousNJP}.  

The Hamiltonian includes the effects of onsite disorder owing to the broad spectrum of states populated in the ensemble and the unique electrostatic environment of every dipole.  Extending ideas developed for simpler systems \cite{Burin1,Sondhi}, one can make a case for slow dynamics, including an arrest in the relaxation of NO Rydberg molecules to predissociating states of lower principal quantum number.

Systems of higher dimension ought to thermalize by energy transfer that spreads from rare but inevitable ergodic volumes (Griffiths regions) \cite{Sarang2, Roeck_griffith, RareRegions_rev, Thermal_inclusions}.  However, a feature in the self-assembly of the molecular ultracold plasma may preclude destabilization by rare thermal domains:  Whenever the quenched plasma develops a delocalizing Griffiths region, the local predissociation of relaxing NO molecules promptly proceeds to deplete that region to a void of no consequence.

In summary, the classical dynamics of avalanche and bifurcation appear to create a quenched condition of low temperature and high disorder in which dipole-dipole interactions drive self-assembly to a localized state purified by the predissociation of thermal regions.  We suggest that this state of the quenched ultracold plasma offers an experimental platform for studying quantum many-body physics of disordered systems. \\

\section{Acknowledgments}
This work was supported by the US Air Force Office of Scientific Research (Grant No. FA9550-17-1-0343), together with the Natural Sciences and Engineering research Council of Canada (NSERC), the Canada Foundation for Innovation (CFI) and the British Columbia Knowledge Development Fund (BCKDF). 

",['A subset that form via kinetic rate processes from state-selected Rydberg gases.'],5088,multifieldqa_en,en,,91c198d5820bfba5caf396532a0b30a531e9d09a7a9005a4," Ultracold neutral plasmas offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems. They form via kinetic rate processes from state-selected Rydberg gases. They have provided an important testing ground for ion transport theory and the study of electron-ion collision physics. We focus on the subset of ultracoldneutral pl asmas that form viaKinetic rate processes. We also emphasize the distinctive dynamics found in the evolution of molecular ultracolds. We describe the initial growth in the growth of research in the field of Ultracold Plasmas. We conclude with an overview of the progress made in the last twenty years of research on the field. The book is published by Oxford University Press, London, priced £16.99, with a print run of 1,000 copies. For confidential support call the Samaritans on 08457 909090 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. In the UK call the heliocentric heliocentre on 0800-847-9255 or go to www.sophistication.org.uk for information on how to call a heliographic heliotic heliostat to help people in need of help in the UK. The heliogenic heliosphere is a type of heliometer that can be used to study the origin and evolution of the solar system. For more details, see http://sophicethelium.org/heliocostatlas.html/. For more information on the Heliocasts, visit the Helicothelium-based Heliocentrism website. For a full list of articles on the topic, see: http:// www.salon.com/helicotracestories/Helicotactic-heliostats/Heliocotracers.html. For the full text of the book, visit: http:/www.salotheliography.com/. For the entire book, please go to:http:/Users/salon/helics/helisocasts/ Heliotranscript.html%. For the video, please visit:http://salticotranetics.com%. For a list of videos on the subject, please click here.  For the rest of the article, click on the link to see http: /www.salta.com.uk/2013/09/07/seliocontacts/Helics-in-the-sky-and-plasmas-in the-sky/Halicotron.html#slv=1. The video will be shown at the bottom of the page.  It will also be available at the end of this article.  The video is available in the next issue of The Astrophysical Journal of Physics (ASP2013). For the next two issues of the ASP2013, the article will be published at the beginning of the month. The article will also appear in the September 2013 issue of the Astrophysics-Initiative (ASI2013) issue. of electron signal by a model that includes ionization by blackbody radiation and collisions with a background of uncooled Rydberg atoms. This picture was subsequently refined to include many-body excitation and autoionization, as well as attractive dipole-dipole interactions. Research by our group at UBC has observed very much the same dynamics in the relaxation of Xe Ryd Berg gases of similar density prepared in a molecular beam. In both cases, the time evolution to avalanche is well-described by coupled rate equations (see below), assuming an initializing density of Penning electrons determined by Robicheaux's criterion. This group later studied the effects of spatial correlations in the spontaneous avalanche of Rydburg gases in a regime of strong blockade, suggesting a persistence of initial spatial correlations. The Raithel group also found direct evidence for electron collisional $\ell$-mixing in a Rb MOT, and used selective field ionization to monitor evolution to plasma on a microsecond timescale in ultracold $^{85}$Rb $65d$ Rdberg gases with densities as low as $10^8$ cm$^{-3}$ \cite{WalzFlannigan}.  Further research has drawn upon molecular dynamics (MD) simulations to reformulate rate coefficients to explain the impact of strong coupling between highly excited Ryberg states and highly excited states, and describe an effect of strong recombination-reionization as it suppresses three-body recombination as it goes from one state to the next.                                Â      Â          Â       “ ”    “” ’’ ‘’.’ ‘”’  ’”. ’. ”.” ‘.  ’,’;  ”, ’, ‘;’,. ’;”,.  , ””; ,  ”,   ”,. ,.’ .’: ‚’ ;’., ’,.”., ‘,  ,. ”.,’ , ”; ’:’!’.;’}.’%.’ and so on. ’'’The Orsay group found that electron collisions in this environment completely ionize added atoms, even when selected to have deep binding energies’ - even when selecting to haveDeep binding energies. The Orsay Group also studied the effect of adding Rydberg atoms to an established ultracolds plasma.   They concluded from estimates of electron trapping efficiency that the addition of Ryberberg atoms does not significantly alter the electron temperature of the plasma.  The Rydberger atoms do not significantly altered the electron temperatures of the plasmas.  The Rygerberg atoms are the key to understanding the dynamics of the Ryderberg gas. The formation of a molecular ultracold plasma requires the conditions of local temperature and density afforded by a high mach-number skimmed supersonic molecular beam. Newer calculations suggest a strong connection between the order created by dipole blockade in Rydberg gases and the most favourable correlated distribution of ions in a corresponding strongly coupled ultracolds plasma. The low-temperature gas in a supersonIC molecular beam differs in three important ways from the atomic gas laser-cooled in a magneto-optical trap (MOT). The milli-Kelvin temperature of the gas of ground-state NO molecules entrained in a beam substantially exceeds the sub-100 micro-KELvin temperature in a MOT. Charged-particle densities in a molecular beam can exceed those attainable by a MOT by orders of magnitude. A great deal of chemical expansion can be seeded in a free-jet expansion of many different chemical can be achieved. The NO ultrac old plasma has unique properties owing to the fact that its Ryd Berg states dissociate. It also has relaxation pathways that may give rise to quantum effects. It is the first study of this type of ultra-old plasma to be published in a peer-reviewed journal. The journal is the Proceedings of the National Academy of Sciences of the United States of America (PNAS) (http://www.pnas.org/content/early/2013/09/29/14/nano-ultracold-plasmas-in-a-laser-crossed-molecular-beam-and-the-unique-properties-of-this-type-of no-ultra-old plasma-and the unique properties of this plasma of this type have been discovered in the past and discussed in the past by several research groups including SousMBL,SousNJP and the PCCP (http:// www.nasa.gov/science/technology/topics/plasma-expan and-topography/plasms/no ultra old polarities-in laser expan.html#plasm_expan_and topics included in this article. The article is also available in the online edition of the journal, The Journal of Plasma Physics, published by the American Chemical Society (www.acse.org) (available in the U.S. and Europe). The article also available in Europe is the Journal of Ultracold Plasmas, published in the UK is the journal of the Royal Astronomical Society (UK) (see www.cse.uk for details). The article also includes a back to the page you came from for more information on how to create a laser-crossing molecular beam and how to seed it in the lab. The full article is available online at: www.the journal of plasma physics and topography. Seeded in a skimmed supersonic molecular beam, nitric oxide forms different phase-space distributions in the longitudinal (propagation) and transverse coordinate dimensions. As it propagates in $z$, the NO molecules reach a terminal laboratory velocity of about 1400 mK. The density distribution of a Rydberg gas defines a local mean nearest neighbour distance, or Wigner-Seitz radius of a radius of $4/3/3. For certain experiments, we find it convenient to saturate the $z$ transition, and vary the density of Rydburg gas by delaying $1/2/2, $2/3, $3/4, $5/5, $6/6, $7/7, $8/8, $9/9, $10/10, $11/11, $12/12, $13/13, $14/14, $15/15, $16/16, $17/17, $18/18, $19/19, $20/20, $21/21, $22/22, $23/23, $24/24, $25/25, $26/26, $27/28, $28/29, $29/28,. $32/27, $33/29,. $34/34,. $35/36,. $36/37,. $37/38,. $38/39,. $39/40,. $40/41,. $41/42,. $42/43,. $43/44,. $44/45,. $45/46,. $46/47,. $47/48,. $48/49,. $49/50,. $51/52,. $52/53,. $53/54,. $54/55,. $55/56,. $56/57,. $57/58,. $58/59,. $59/60,. $60/61,. $63/64,. $64/65,. $65/66,. $67/68,. $68/69,. $69/70,. $70/74,. $74/75,. $75/76,. $78/78,. $79/78, $80/79, $81/82, $82/83, $83/84, $84/87, $86/88, $87/88,. $88/89, $89/90, $90/91, $91/92, $92/93, $93/94, $94/95, $96/96, $97/98, $98/99, $99/100, $100/102, $102/103, $103/104, $104/103,. $104,000, $105,000,. $106,000., $107,000$,. $108,000 , $109,000!, $110,000%, $111,500, $112,500,. $113,600, $111,. $111.000, £111,600,. $114,200, $115,000', $113.000,. . . . $114,. $115,. $116,000$, $113,. $118,000. $119,000 . $120,. $122,. $123,. $124,. $125, $122, $130, $143, $144, $145, $150, $153, $154, $157, $158, $160, $159, $162, $164, $165, $167, $168, $170, $173, $174, $178, $179, $180, $181, $182, $183, $184, $189, $190, $240, $230, $238, $239, $250, $242, $255, $260, $258, $259, $252, $263, Penning ionization acts on the portion of the initial nearest-neighbour distribution in the Rydberg gas that lies within $r_c$. When a molecule ionizes, its collision partner relaxes to a lower principal quantum number, $n'<n_0/\sqrt{2}$. This close-coupled interaction disrupts the separability of Ryd Berg orbital configurations in the Penning partner. Penning partners are thus very likely to dissociate, leaving a spatially isolated distribution of ions. The extent of this effect varies depending on the local density and the selected initial quantum number. The electrons produced by prompt Penning ionized start an electron impact avalanche. The energy per unit unit of avalanche determine the evolving temperature per unit of the avalanche. Such calculations show that the conversion from Rydburg gas to plasma occurs on a local timescale determined by the longer-time effects of the Rydenberg gas on a plasma. We neglect the longer time effects of a plasma on a longer timescale, for simplicity, we neglect the no-time effect of the plasma on the Ridenberg gas. We refer to the spatial correlation that results as a Penning lattice. We call it the ‘Penning lattices’ and it is based on a set of coupled rate equations that account for state-to-state electron-Rydberg inelastic scattering, electron-impact ionization and three-body ion-electron recombination using detailed rate coefficients, $k,i,ion, and $k_{i,tbr}$ using detailed rates coefficients,  $k.i,.i,i-‘’’ ‘“Penning” is a term used to refer to a type of quantum theory that describes the interaction between electrons and ions.” ““”  ’” means ‘the interaction of electrons and atoms’. “ ” ”’ means “the interaction between the electrons and the atoms”.   ”,   “The interaction of the ions and the electrons is known as the ’Penning interaction’,” says the author, “and it’s called the  Penning effect’ or the ”Penning equation.’ ”   The Penning equation is a closed form that predicts the electrons’ density for any particular initial Rydenberg density and principal quantumNumber. ,   '’: The Penning electron density for a particular initial density of $1’ is predicted by counting ion distances after relaxation to plasma in 10$^6’-particle stochastic simulations.‚”: The ‘penning’ equation predicts the electron density of a given initial density, $1, for a given density of 10$^{12’$ cm$^{-3’). : The equation for a Rydberg gas with initial principal quantum numbers from 30 to 80 is: $a_ws$ (1-’rho_e’) (2- Rho_0) (3-”Rho’e”) (4-“Rydberb’),  Rydgerb” (5-‚Rydderb) (6-–Rydrerb ) (7-3) (8-4) (9-7) (10-7). Selective field ionization (SFI) probes the spectrum of binding energies in a Rydberg gas. SFI maps the evolution from a state of selected initial principal quantum number, $n_0$, to plasma. A $49f(2)$ Ryd Berg gas with an estimated initial density relaxes to plasma on a timescale of about 500 ns. The rise times predicted by coupled rate-equation simulations for a large range of initial densities and principal quantum numbers match when plotted as a function of time scaled by the ultimate plasma frequency and fraction of prompt Penning electrons. The dashed line gives an approximate account of the scaled rate of avalanche under all conditions of Rydburg gas density and initial principal Quantum number in terms of the simple sigmoidal function: $\omega_e$ and fraction, $\rho_e(t=0) P_f$ are empirical coefficients. Our experiment crosses a 2 mm wide cylindrically Gaussian molecular beam with a 1 mm diameter laser beam to produce a Ryberg gas avalanches to a plasma. As outlined above, the local density and principalQuantum number together determine the rate at which a R yberg gas avalanche takes place. We can understand this variation in relaxation dynamics by looking at the corresponding density of prompt electrons these conditions afford to initiate the avalanche to plasma in the experiment. The experiment was carried out at the University of California, San Diego, and the results were published in the Proceedings of the National Academy of Sciences of the U.S. (PNAS) in December. The study was funded by the National Institute of Standards and Technology (NIST) in California. For more information on the SFI experiment, visit: http://www.nasa.gov/sFI/sfi/sf.html. For the full report on the experiment, see: http:www.pnas.org/sfa/sfei/safa/sfo/sfu.html#sfo. The article has been adapted from the original version of this article. It also includes a link to the ‘SFI’ version of the article, which was published at the same time as the “SFI for the first time in the online version.” The SFI version has been updated to reflect the changes in the number of electrons in the plasma. It has also been published as a result of a change in the size of the laser beam, which has been increased from 1,000 to 1,500. The author is happy to point out that this change in size is due to the fact that the laser pulse has a much smaller diameter than the original laser beam of 1,200 mm. The S FI version also includes the addition of a ‘‘’’ to the end of the ’SFI ’s ‘sfo’ section,’ which means that the original SFI “safa’s’ ‘f’ is no longer the same as the one used in this article, but a different type of SFI.’ The author has also added a “” at the bottom of the page to make it clear that this is not the only way to study the evolution of the field field in this experiment, but that it can be used as a basis for other experiments as well. The full article can be found at: www.nass.org.uk/sfsf/sfli.html/. For more details, see the ”SFI_n=49” section. We assume that local space charge confines electrons to shells, conserving quasi-neutrality. Electrons exchange kinetic energy at the boundaries of each shell, which determines a single plasma electron temperature. We simulate a relaxation that includes channels of predissociation at every Rydberg level and redistributes the energy released to electrons. This determines a uniform rising electron temperature for all shells. We consider the balance of this neutral population to reflect NO$^*$ molecules that have dissociated to form N($^4$S) $+$ O($^3$P) as detailed above. We assign a fraction of this population to surviving Ryd Berg molecules, such that the total population of NO$*$ as a function of time agrees with the prediction of the shell-model simulation. We conclude that NO$+$ is an avenue of exchange as an avenue for quench of quench charge as well as for the formation of superconductors. We propose a new theory of supernovae that could be used to explain the origin of the quench effect in supernovas. We call this theory the NO-supernovae theory and it is based on the theory of the superconducting superconductor, which has been used in a number of previous studies. We also propose a theory that could explain how superconductivity is created in a superconductive environment. We will use this theory to explain why superconductions are so much more powerful than previously thought. We hope that this will help us understand how superconditions form in the real world and how they can be harnessed for research purposes. We are also interested in the potential effects of NO-Superconductor on the environment, such as the formation and evolution of super-conductors, on the Earth's magnetic field, and in the environment of supermassive supernovases. We believe that this could be a useful tool for understanding how superstructures form and behave in the quantum realm. We can also use this to understand the nature of superlatives in the universe and the role they play in the evolution of the universe. We plan to use this information to develop a new model of superstructured superlative superconductives. We expect to use the shell model to help us explain the effects of Superconducting Superconductors in a variety of ways. We have already shown that superlates can be used as a tool to study superlators in the physical world. We want to use it to explain how Superlates work in a more general way, and this could help us to understand how they work in the atmosphere and in other areas of the galaxy. We'll also use theshell model to show how superlations can be applied to the environment in a range of environments, including the environment on Earth and in space and in outer space. We think this could provide a new way of understanding superlastic superlions. We're also looking at the role of the ion density in superluminaries in the creation of supercondensates. The ion density of a superluminal superlion can be calculated using the Eq-scaledEq1 equation. We use this equation to estimate the ion and electron density of the NO$#+$ ion profile in a given superlunar environment. For each time step, the difference, $\rho_0 - $z=0$ defines the neutral population of eachShell, and we use the $x,y$ plane for $z = 0.75$ mm to calculate the ion-electron density of each Shell. The balance between the rising density of ions and the falling density of Rydberg molecules depends on the initial density of electrons produced by prompt Penning ionization. We see this most directly in the absorption spectrum of transitions to states in the $n_0 f(2)$ Ryd Berg series, detected as the long-lived signal that survives a flight time of 400 $\mu$s to reach the imaging detector. This Penning fraction depends sensitively on the principal quantum number, and for all principal quantum numbers, on theinitial Rydburg gas density. We show a clear consequence of the higher-order dependence of the NO+$ ion fraction - and thus the NO$+ ion fraction of Penning ions - in the spectra of nitric oxide Rydberg states converging to NO$^+$ $v=0$ (designated, $nf( 2)$), derived from the late-peak signal obtained after a flighttime of 400 $mu#s. We also show that the balance between ions and RydBERG molecules is dependent on the ion-Rydberg balance on the Penning-regulated NO+ ion-regulated ion number. We conclude that Penning ionization is the key to understanding the structure of the Ryd berg system. We call this theory ‘the Penning theory’ and it is based on the ‘Penning theory of ionization’, which was first proposed in the 1970s. The ‘Ryd Berg’ model of ionisation has been the subject of a number of previous studies, including a recent paper in the Journal of Quantum Theory and Theory of Ionization (J.C.A.R.I.S.E.). We will now focus on the theory of the “Penning ion mass’ (P.D.I) in the next section of this article. We will use the term ‘penning’ to refer to the ion mass that is formed by the redistribution of the electron expansion force over all the NO’s charge centres by resonant ion-rydberg charge exchange, which occurs with a very large cross section. We hope that this will help us understand how this redistribution occurs. We are also going to use the terms “penning mass” and “phenomenon” to describe the way this theory is applied to Ryd Berger molecules in the real world. This article will be amended to reflect that we are using the term Penning mass in the present study. We apologise for any confusion caused by the use of this terminology, and we are happy to clarify that this is not the same as ‘‘’ ’’””Penning” is the term used in the previous article.” ”’The Penning Ion Mass’ is a measure of the amount of electrons that have been ionized in a given environment, and it can be used to estimate the initial ion mass of a given RydBerger molecule. ”, “”. ” “Rydberger molecules predominate in the lower-density wings of the ellipsoid,” we say. “The initial avalanche in the high-density core of theEllipoid leaves few Rydberger molecules, so this term has little initial effect.“,  ’.   ”,  “We see that the initial avalanche leaves little Ryderberg molecules, and this term is called the ’nf’ term.‚’ The system forms the plasma/high-Rydberg quasi-equilibrium dramatically evidenced by the SFI results in Figure. The laser crossed molecular beam illumination geometry creates a Rydberg gas with a distinctively shaped high-density spatial distribution. We have developed semi-classical models that explicitly consider the coupled rate and hydrodynamic processes governing the evolution from Ryd Berg gas to plasma. The Hamiltonian includes the effects of onsite disorder owing to the broad spectrum of states populated in the ensemble and the unique electrostatic environment of every dipole. The model describes this arrested state of the ultracold plasma in terms of two, three and four-level dipole-dipole energy transfer interactions (spin flip-flops), together with Ising interactions that arise from the concerted pairwise coupling of resonant pairs of dipoles. We find that spontaneous avalanche to plasma splits the core of an ellipsoidal Rydburg gas of nitric oxide. As ambipolar expansion quenches the electron temperature of this core plasma, long-range, resonant charge transfer from ballistic ions to frozen Rydenberg molecules in the wings of theEllipsoid quenched the ion-Ryden molecule relative velocity distribution. This sequence of steps gives rise to a remarkable mechanics of self-assembly, in which the kinetic energy of initially formed hot electrons and ions drives an observed separation of plasma volumes. These dynamics redistribute ion momentum, efficiently channeling electron energy into a reservoir of mass-transport. This starts a process that evidently anneals separating volumes to a state of cold, correlated ions, electrons and Rydberg molecules. We capture the electron signal from these recoiling volumes on an imaging detector as pictured in Figure  in Figure \ref{fig:bifurcation}.  Here, momentum matching preserves density and enables ions and Rydenberg molecules to relax to positions that minimize potential energy, building spatial correlation.  No combination of initial conditions can produce a simulation that conforms classically with the state of arrested relaxation we observe experimentally.  We have devised a three-dimensional spin model to describe this arrestedState of the Ultracold Plasma.  One can make a case for including an arrestState in the theory of quantum dynamics, in the case of a slow relaxation of molecules in simpler systems. The system ought to predissociating lower principal numbers of lower principal number of lower energy transfer by energy transfer that ought to thermalize that lower number of thermal transfer by thermalize lower principal energy transfer.   The model is based on the ideas developed for ideas developed by Burin1,Sondhi, and SousNJP in the 1990s and 2000s. The study was published in the journal The Journal of Theoretical Physics and Astrophysics (J.R.A.P.S.I.C.A). The study is open-access and free-to-print. For confidential support, call the National Institute of Standards and Technology (NIST) on 1-800-273-8255 or visit http://www.nist.org/nist/nr/nr.html. A feature in the self-assembly of the molecular ultracold plasma may preclude destabilization by rare thermal domains. Whenever the quenched plasma develops a delocalizing Griffiths region, the local predissociation of relaxing NO molecules promptly proceeds to deplete that region to a void of no consequence. We suggest that this state offers an experimental platform for studying quantum many-body physics of disordered systems. This work was supported by the US Air Force Office of Scientific Research (Grant No. FA9550-17-1-0343), together with the Natural Sciences and Engineering research Council of Canada (NSERC), the Canada Foundation for Innovation (CFI) and the British Columbia Knowledge Development Fund (BCKDF).  The authors conclude that this work offers a new way of understanding the nature of quantum many body physics and the role of quantum mechanics in our understanding of the universe. For more information, visit: http://www.cnn.com/2013/01/29/science/science-topics/topics-in-quantum-physics-and-the-quenched-plasma/index.html."
What types of sensors are now capable of estimating physical activity levels and physiological outcomes of older adults?,"\section{Introduction}
Cognitive deficit of older adults is one of the biggest global public health challenges in elderly care. Approximately 5.2 million people of 65 and older are suffered with any form of cognitive impairments in United States in 2012 \cite{stat12}. Dementia is one of the major causes of the cognitive impairments which is more acute among 85 and older population (50\%) \cite{stat12}. However, the costs (financial and time) of health care and long-term care for individuals with Alzheimer's (special form of dementia) or other dementias are substantial. For example, during 2016, about 15.9 million family and friends in United States provided 18.2 billion hours of unpaid assistance to those with cognitive impairments which is a contribution to the nation valued at \$230.1 billion. One the other hand, total payments for all individuals with all form of cognitive impairments are estimated at \$259 billion. Total annual payments for health care, long-term care and hospice care for people with Alzheimer's or other dementias are projected to increase from \$259 billion in 2017 to more than \$1.1 trillion in 2050. Among the above costs, a significant amount are relevant to clinical and diagnostic tests \cite{stat17}. Although clinical and diagnostic tests have become more precise in identifying dementia, studies have shown that there is a high degree of underrecognition especially in early detection. However, there are many advantages to obtaining an early and accurate diagnosis when cognitive symptoms are first noticed as the root cause findings of impairment always lessen the progress of impairment status and sometimes symptoms can be reversible and cured.

With the proliferation of emerging ubiquitous computing technologies, many mobile and wearable devices have been available to capture continuous functional and physiological behavior of older adults. Wearable sensors are now capable of estimating number of steps being taken, physical activity levels, sleep patterns and physiological outcomes (heart rate, skin conductance) of older adults \cite{sano15}. Ambient sensors also help capture the movement patterns of objects and humans for activity and behavior recognition \cite{dawadi14,dawadi15}. Researchers also proved the existence of correlations between cognitive impairment and everyday task performance \cite{dawadi14, akl15,alam16} as well as physiological symptoms \cite{alam16,sano15}. Although current studies showed some successes in IoT-assisted cognitive health assessment in different domains individually, there are several existing challenges in developing and validating a fully automated multi-modal assessment model.

\begin{enumerate}
\item \emph{Real-time IoT System}: A real-time IoT system must include a continuous and fault tolerant data streaming capability among central hub, wearable sensors and ambient sensors regardless of network communication protocol (WiFi, Ethernet, Bluetooth etc.) which are not available in existing researches.
\item \emph{Multi-modal Context Fusion}: Though several offline clinically validated cognitive health assessment tools exist \cite{wai03, starling99, krapp07, yesavage82, zung71}, there is no universally accepted method for IoT-assisted automatic cognitive health assessment in smart home environment that can fuse multi-modal sensor contexts altogether. For example, some researchers showed ambient sensors based Activities of Daily Livigin (ADLs) sequence pattern can signify the cognitive health status of older adults \cite{akl15, dawadi15}. Researchers also showed wearable Electrodermal Activity pattern analysis may carry the significance of cognitive status \cite{sano15}. However, for validation of IoT based cognitive health assessment, self-reported surveys, clinical diagnosis and observation based tools are used individually by prior researchers \cite{akl15, dawadi15, sano15, alam16}.
\end{enumerate}

Regarding aforementioned challenges for the automation of cognitive health assessment, \emph{AutoCogniSys} considers (i) reproducibility of our model in any smart home system consists of ambient motion sensors, wearable accelerometer (ACC) sensors, wearable Electrodermal Activity (EDA) and Photoplethysmography (PPG) sensors individually or combined streams; (ii) context awareness based on ambient motion sensors and wearable ACC sensors in any types of activities such as hand gestural, postural and complex ADLs; and (iii) high accuracy, i.e., a recall rate of over 90\% with less than 5\% false positive rate. More specifically, \emph{AutoCogniSys} extends our existing work \cite{alam16} in three dimensions,

\emph{(1) True Automation:} We first investigate the correlations of cognitive impairment with human activities and stress where we manually labeled activities, extract the corresponding physiological sensor (EDA and PPG) features of each activity, and use statistical method to find correlations. Then, we propose automatic complex activity recognition based on a Hierarchical Dynamic Bayesian Network (HDBN) model, fine-grained extraction of physiological sensor features and finally machine learning classification of cognitive impairment.

\emph{(2) Noises Elimination:} We define different types of noises on ACC, EDA and PPG sensors, propose extensive signal processing techniques to remove noises and show significant improvement can be achieved in cognitive impairment classification.

\emph{(3) Implementation and Evaluation:} Finally, we design and implement IoT system and analytic methods and minimize the human involvement to automate our proposed cognitive health assessment approach by considering effective smart home sensor customization and deployment, data collection, screening, cleaning and filtering, feature computation, normalization and classification, and activity  model training.

\textbf{Research Questions:} \emph{AutoCogniSys} consequently tackles the following key research questions.

$\bullet$ Can we detect simultaneously the periodic rhythms of  both hand gestures and postural activities from wrist-worn ACC sensor signal for diverse population (population with same activity but diverse ways such as walking with walker, stretcher or normally)? If so, how can we incorporate the hand gesture, posture and ambient sensor data streams to help improve the ADLs recognition models?

$\bullet$ How can we exploit and relate the micro-activity features into noise free physiological sensor signals processing to automate cognitive health assessment process? What are the critical roles of clinical survey and technology guided assessment methodologies and their inter-relationships for automating the different intermediate steps of cognitive health assessment process?

To tackle these, we make the following \textbf{key contributions}:

$\bullet$ We employ an extensive signal deconvolution technique that in conjunction with machine learning technique helps facilitate a wrist-worn ACC-based multi-label (hand gestural and postural) activity recognition for diverse population. We then leverage multi-label context sets with ambient and object sensor signals for complex activity recognition based on HDBN model.

$\bullet$ We propose a novel collaborative filter for EDA signal processing by postulating signal as a mixture of three components: \emph{tonic phase, phasic phase} and \emph{motion artifacts}, and employ convex optimization technique for filtering out the motion artifacts. We also propose a novel PPG signal processing technique to filter out the inherent motion artifacts and noises using improved Periodic Moving Average Filtering (PMAF) technique.

$\bullet$  We design and prototype an IoT system consisting of multiple devices (wearable wrist band, IP camera, object and ambient sensors) connected with central hub via WiFi, Ethernet and Bluetooth communication protocols. We collected data from 22 older adults living in a continuing care retirement community center in a very natural setting (IRB \#HP-00064387).

$\bullet$ Finally, we employ statistical and machine learning techniques to jointly correlate the activity performance metrics and stress (EDA and PPG) features that helps achieve max. 93\% of cognitive impairment status detection accuracy. We evaluate \emph{AutoCogniSys} on 5 clinically validated offline assessment tools as ground truth.
\section{Related Works}
\emph{AutoCogniSys} builds on previous works on wearable devices based low-level (postural and hand gestural) activity recognition and their integration with ambient sensors to recognize complex ADLs, the underlying signal processing and applications on cognitive health assessment automation.
\subsection{Wearable Sensor Signal Processing}
Wearable sensors can be two types: physical and physiological. Physical sensors (accelerometer, gyroscope etc.) signal values change over the movements of the sensor devices. Physiological sensors change over physiological condition of body such as EDA changes over stress and PPG changes over heart rate. However, physical movements also impose noises on physiological sensor signals which is called \emph{motion artifacts}.
\subsubsection{Physiological Signal Processing}
A continuous and descrete decomposition of EDA, and time and frequency domain analytics of PPG signal have been investigated before to extract relevant physiological features which were contaminated with noises and motion artifacts \cite{alam16}. \cite{setz10} denoised and classified EDA from cognitive load and stress with accuracy higher than 80\%. Though motion artifacts removal techniques such as exponential smoothing \cite{hern11} and low-pass filters \cite{poh10, hernandez14} provide significant improvement in filtering EDA signals, wavelet transforms offer more sophisticated refinement for any kind of physiological sensors such as electroencephalogram \cite{krish06, zikov02}, electrocardiogram \cite{erc06,alfa08}, and PPG \cite{lee03}. \cite{chen15} proposed a stationary wavelet transform (SWT) based motion artifacts removal technique. `cvxEDA' proposed  a convex optimization technique considering EDA as a mixture of white gaussian noise, tonic and phasic components where white gaussian noise includes motion artifacts and external noises \cite{greco16}. \emph{AutoCogniSys} intelligently combines SWT and `cvxEDA' together to remove noises and motion artifacts from EDA signal. On the other hand, it is more difficult to remove motion artifacts from PPG signal due to its periodicity of nature \cite{wang13}. Researchers proposed different methods such as frequency analytics \cite{garde13,wang13}, statistical analytics \cite{peng14} and digital filter \cite{lee10} to reduce noises and motion artifacts from PPG. \emph{AutoCogniSys} used Periodic Moving Average Filter (PMAF) in this regard \cite{lee07}.
\subsubsection{Physical Sensor Signal Processing}
ACC based hand gesture recognition has been explored by several researchers in past such as discrete hidden markov model \cite{liu10}, artificial neural network \cite{arce11}, weighted naive bayes and dynamic time warping \cite{mace13}. Akl et. al. proposed 18 gesture dictionary based Support Vector Machine (SVM) classifier \cite{akl11}. Wrist-worn ACC based postural activity recognition approach has been proposed using Decision Tree, Random Forest, Support Vector Machines, K-Nearest Neighbors, Naive Bayes and deep neural networks \cite{gj14, wang16}, the accuracy stagnates at 85\% using SVM method \cite{martin16}. However, neither of past works proposed any technique that can provide single body worn ACC sensor-based multiple body contexts recognition nor works efficiently for diverse posture say walking normally, with walker, with double walker or wheel chair. Our proposed 8-hand gesture recognition technique assisted sparse-deconvolution method improves classification performances on both normal and diverse postures. However, we incorporated hand gestures and postures in conjunction with ambient sensors into single-inhabitant HDBN model \cite{alam16b} that provides significant improvement in complex activity recognition.
\subsection{Cognitive Health Assessment}
Smart home environment has been used for providing automated health monitoring and assessment in the ageing population before \cite{dawadi14, gong15, akl15, dawadi15}. `SmartFABER' proposed a non-intrusive sensor network based continuous smart home environmental sensor data acquisition and a novel hybrid statistical and knowledge-based technique to analyz the data to estimate behavioral anomalies for early detection of mild-cognitively impairment \cite{riboni16}. \cite{skubic15} presented an example of unobtrusive, continuous monitoring system for the purpose of assessing early health changes to alert caregivers about the potential signs of health hazards. Though, prior researches proposed a sequence of ambient motion sensor streams as complex activity components in activity based health assessment \cite{dawadi14, gong15, akl15, dawadi15}, we consider inclusion of an wearable wrist-band with in-built ACC sensor to detect hand gesture and posture, augmenting with the ambient sensor readings to help recognize complex activities as well as cognitive health assessment of older adults. Additionally, we propose intelligent use of physiological features of skin through different physiological sensor signals (EDA, PPG) processing in daily activity tasks and incorporate context-awareness for automation of cognitive health assessment that have not been explored before.
\begin{figure}[!htb]
\begin{center}
   \epsfig{file=flowchart.pdf,height=1.6in, width=3.5in}
\caption{Overall flow of \emph{AutoCogniSys} pipeline.}
   \label{fig:overview}
\end{center}
\end{figure}
\section{Overall Architecture}
We first investigate existing IoT-based cognitive health care frameworks that covers every aspects of wearable (physical, physiological) and ambient (passive infrared and object) sensor signals computing. \emph{AutoCogniSys} is comprised of three component modules: (i)~sensing, (ii)~processing, and (iii)~analysis. The `sensing' module consists of clinical assessment tools (surveys, observation and clinical backgrounds) and sensing signals (ambient and wearable sensors). `Sensor processing' module is comprised of three sub-modules: a)~clinical assessment feature extraction from assessment tools; b)~ambient sensor feature extraction; and c)~wearable sensor processing (noise removal, segmentation, feature extraction, classification etc.). `Analysis' module is comprised of machine learning and statistical analytics-based score prediction of cognitive impairment. Automation of each module's functionality and inter-intra modular transactions without human interference can be called {\it true automation} of cognitive health assessment. Fig.~\ref{fig:overview} shows the overall flow of \emph{AutoCogniSys} which is discussed in details in the following sections.
\subsection{Demographic Ground Truth Data Collection}
Currently in standard clinical practice and research, the most accurate evaluations of cognitive health assessment are one-to-one observation and supervision tasks/questionnaires for monitoring an individual's functional abilities and behavior \cite{resnick15}. In the first stage of this pilot study, we have investigated current literatures and carefully chosen the clinically proven functional and behavioral health assessment survey tools \cite{resnick15}. On the other hand, to cross check with the survey based evaluations, we have also chosen clinically justified observation based behavioral assessment methods. First, following the resident consent, our clinical research evaluator collects demographic and descriptive data (age, gender, race, ethnicity, marital status, education and medical commodities). She has performed two types of clinical assessments: (1) \emph{Observation based} where the resident's cognition is assessed using the Saint Louis University Mental Status (SLUMS) scale \cite{wai03}. (2) \emph{Survey based} where five widely used and clinically well validated surveys are taken into account: (a) \emph{Yale Physical Activity Survey} \cite{starling99}; (b) \emph{Lawton Instrumental Activities of Daily Living}; (c) \emph{Barthel Index of Activities of Daily Living} \cite{krapp07}; (d) \emph{Geriatric Depression Rating scale} \cite{yesavage82}; and (e) \emph{Zung Self-Rating Anxiety scale} \cite{zung71}.
\subsection{Smart Environment Creation}
For an ideal IoT-based system, instrumenting and deploying it at each participant's natural living environment warrants for assembling a flexible set of hardware and software interfaces to ease the system configuration, setup, and network discovery processes. The sensor system placed in the residences of volunteers needs to meet several specific physiological signals and activity monitoring needs. However, we must confirm that the devices are reliable with potential for re-deployment as well as appear unintimidating to the participants. Inspired by the above requirements, we developed a real testbed IoT system, {\it SenseBox}, by customizing Cloud Engine PogoPlug Mobile base station firmware to integrate with WiFi (connect ambient and object sensors) and Bluetooth (connect wristband) protocol. The smart home components are as follows: (i) PogoPlug base server with a continuous power supply, (ii) 3 binary Passive Infrared sensors in three different rooms (kitchen, livingroom and bedroom) to capture room level occupancy, (iii) 7 binary object sensors attached with closet door, entry door, telephone, broom, laundry basket, trash can and trash box, (iv) three IP cameras in the appropriate positions to collect the ground truth data and (v) an Empatica E4 \cite{empatica} wrist-band (integrated sensors: PPG at 64 Hz, EDA at 4 Hz, Body temperature at 1 Hz and a triaxial ACC at 32 Hz) on the participant's dominating hand.
\section{Activity Recognition}
We aim to detect single wrist-worn ACC sensor based hand gesture and postural activities and insert these into an HDBN graphical model in conjunction with ambient and object sensor values for complex activity recognition. We consider the recognition problem asan activity tupple of $\langle gesture,posture,ambient,object \rangle$. Though, Alam et. al. provides significant performance improvement for single wrist-worn ACC sensor aided 18-hand gesture based postural activity recognition in lab environment \cite{alam17}, it faces some practical challenges in real-time smart environment with older adults due to the diversity of their postures. For example, some older adults use walker, double walking sticks or wheel chair for walking in which cases collecting 18 hand gestures and corresponding postural activities for training requires endless efforts and carefulness. To reduce the complexity of ground truth labeling and later state space explosion for graphical model (HDBN), we propose to use rotational normalization method that can merge some hand-gestures subject to directional differences and forms an 8-hand gesture model. However, our proposed Feature Weight Naive Bayes (FWNB) classifier adds significant improvement on Alam et. al. proposed sparse-deconvolution method as well as recognition in diverse postural environment.
\begin{figure}[!htb]
\begin{center}
   \epsfig{file=hand_gestures.pdf,height=0.5in, width=3in}
   \vspace{-.2in}
\caption{8 hand gesture dictionary with direction}
   \label{fig:hand_gestures}
   \vspace{-.2in}
\end{center}
\end{figure}
\subsection{Hand Gesture Recognition}
\label{sec:hand_gesture}
\emph{AutoCogniSys} proposes an 8-gesture dictionary (as shown in Fig. \ref{fig:hand_gestures}) and a Feature Weighted Naive Bayesian (FWNB) framework for building, modeling and recognizing hand gestures. The method comprises of the following steps: (i) \emph{Preprocessing:} wrist-worn ACC sensor provided 3-axis data are passed through 0.4Hz low-pass filter to remove the data drift. (ii) \emph{Rotation normalization:} Normalizing the rotation of hand gestures provides greater accuracy and allows for more realistic, orientation-independent motion. At first, we find the best fit plane of the acceleration vectors thus if the motion lies in a single plane, then the acceleration vectors of a closed shape should on average lie in that main plane. Then, we take all acceleration segments between points of inflection to form one single vector called reference vector that provides us the general direction of user's motion. After that, each vector is normalized relative to the reference vector. This normalization helps remove a lot of hand gestures from prior considered 18 hand gestures resulting a reduced dictionary of 8 gestures. (iii) \emph{Feature Weighted Naive Bayesian model:} Naive Bayes classifier is light-weight and efficient technique for hand gesture recognition. We extract 12 ACC features \cite{alam17} and calculate weight for each feature type based on the similarity of feature measures of the trained gestures (0$<$weight$<$1). While recognizing gestures, the proximity of each feature measure to the average trained feature measure of each gesture type is calculated by a normal distribution. Then, the proximity value is multiplied by the feature weight that was calculated in the training phase.  All of these multiplied values are added together and the system predicts the gesture type with the greatest value as the user gesture. In the learning data points, there should be static postural activities (such as sitting, lying etc.) to avoid unexpected noises over wrist-worn ACC sensors. In the final hand gesture dictionary, we save the reference vector as our signal dictionary.
\subsection{Postural Activity Recognition}
In normal lab environment, wrist-worn ACC sensor signal is a mixture (convolution) of actual hand gesture and postural activity relevant signals \cite{alam17}. \emph{AutoCogniSys} improves the idea by reducing the number of hand gestures and postural activities to 8 (as shown in Fig.\ref{fig:hand_gestures}) using rotation normalization and 4 (walking, sitting, standing and lying). Then, we use sparse-deconvolution method (with 31\% signal reconstruction error) to get Approximately Sparse Factor. The summary of the entire process is stated bellow:

{\it Building Deconvolution Method:} We first consider the wrist-worn ACC sensor signals (3-axis values) as a convolution of hand gesture and postural activity effects and build a deconvolution framework. The deconvolution framework takes a known signal (hand gesture effects) and a equalizer parameter ($\lambda$) as input and provides an Approximately Sparse Factor signal (postural activity effects) as output. For 3-axis ACC signals, we need to learn associated 3 equalizer parameters for each hand gesture. Moreover, each equalizer parameter is involved with 4 postural activities that results a total 96 ($8\times 3\times 4$) equalizer parameters to learn. 

{\it Learning Classification Model:} We use the Approximately Sparse Factor signal to extract 12 statistical features and SVM with sequential machine optimization (SMO) \cite{cao06} for postural activity recognition.

{\it Prediction Model:} After recognizing the hand gestures following the method explained in Sec.~\ref{sec:hand_gesture}, we take the corresponding reference vector as known signal and extract the Approximately Sparse Factor signals incorporating corresponding 3 equalizer parameters ($\lambda$) for the sparse-deconvolution method. Then, we apply feature extraction and prior learned SMO based SVM classifier \cite{cao06} to classify final postural activity. Fig.~\ref{fig:deconvolution} illustrates a single axis example of the deconvolution.

\begin{figure}[!htb]
\begin{center}

   \epsfig{file=deconvolution.pdf,height=1.6in, width=3in}
   \vspace{-.15in}
\caption{Sample deconvolution example of X-axis. The raw x-axis of accelerometer signal, reference vector of the sample gesture and the extracted corresponding ASF signal of walking.}
   \label{fig:deconvolution}
\end{center}
\vspace{-.15in}
\end{figure}

\subsection{Complex Activity Recognition}
We build a HDBN based complex activity recognition framework for single inhabitant scenario smart home environment \cite{alam16b} taking the advantage of detected hand gestural and postural activities along with the ambient and object sensor streams. At first, we obtain instant hand gestural and postural activities from our above proposed models, and additionally motion sensor and object sensor readings from our IoT-system for every time instant generating a 4-hierarchy of HDBN model. Considering the context set $\langle gestural, postural, ambient,object\rangle$ as a hierarchical activity structure (extending two 2-hierarchical HDBN \cite{alam16b}), we build complex activity recognition model for single inhabitant scenario. Finally, we infer the most-likely sequence of complex activities (and their time boundaries), utilizing the well-known Expectation Maximization (EM) algorithm \cite{dempster77} for training and the Viterbi algorithm \cite{forney73} for run-time inference.
\section{Automatic Activity Features Estimation}
The effects of cognitive ability on daily activity performance have been studied before \cite{dawadi14,akl15}. They experimentally and clinically validated that cognitive impairment highly reduces the daily activity performances and this activity performance can be computed as an indicator of cognitive ability status of older adults. The standard activity features refer to completeness of task (TC), sequential task ability (SEQ), interruption avoidance capabilities (INT) etc. In current behavioral science literature, the above activity features carry specific definition based on the sub-tasks involved with a complex activity \cite{dawadi14,akl15}. Completeness of task refers to how many sub-tasks are missed by the participants. Sequential task ability refers to how many sequences of sub-tasks are missed referring the gerontologist defined standard sequences of the sub-task for the particular complex activity. Interruption avoidance capability refers to how many times the participants stop or interleave while doing any sub-task. The final goal of activity features estimation is to provide overall task score. The task score is proportional to the functional ability of participants in performance daily activities. Our behavioral scientist team, comprises with Nursing professor, gerontologist and retirement community caregivers, carefully discus, optimize and choose 87 sub-tasks in total for 13 complex activities.

Each of the sub-task comprises with sequential occurrences of hand gesture and postural activities. However, no researchers ever considered hand gesture for activity features estimation due to complexity of multi-modal wearable and ambient sensors synchronization and multi-label activity classification \cite{dawadi14,akl15}. \emph{AutoCogniSys} exploited single wrist-worn sensor based hand gesture and postural activity recognition, and proposed an activity features (TC, SEQ and INT) estimation method including these two parameters in conjunction with object and ambient sensor features that provide significant improvement of cognitive health assessment of older adults.
\subsection{Machine Learning Based Complex Activity Features Estimation}
In current cognitive health assessment literature, complex activity features can be defined as $\langle TC,SEQ,INT,TS\rangle$. We used supervised method to estimate TC, SEQ and INT, and unsupervised method to estimate TS. We first, formulate the automated scoring as a supervised  machine learning problem in which machine learning algorithms learn a function that maps $\langle${\it hand gesture, posture, object, ambient sensor}$\rangle$ feature set to the direct observation scores. We use bagging ensemble method to learn the mapping function and SMO based SVM \cite{cao06} as base classifier. The learner averages by boostrapping individual numeric predictions to combine the base classifier predictions and generates an output for each data point that corresponds to the highest-probability label. We train three classifiers considering observation as ground truth for TC, SEQ and INT scores and test on the testing dataset. We derive unsupervised scores using dimensionality reduction technique for each feature set. First, we take all features of each activity, apply optimal discriminant analysis technique as a dimensionality reduction process \cite{zhang09} and reduce the feature sets into single dimensional value which represents the automated task completeness scores of the particular user activity. A min-max normalization is applied that provides us a uniform range of the variables using $
z_i=\frac{x_i-min(x)}{max(x)-min(x)}$ equation where $x=\{x1,\ldots,x_n\}$ and $z_i$ is $i^{th}$ normalized data. The final single dimensional score represents machine learning based TS score.
\section{Physiological Sensor Signals Processing}
The autonomic nervous system (ANS) restrains the body's physiological activities including the heart rate, skin gland secretion, blood pressure, and respiration. The ANS is divided into sympathetic (SNS) and parasympathetic (PNS) branches. While SNS actuates the body's resources for action under arousal conditions, PNS attenuates the body to help regain the steady state. Mental arousal (say stress, anxiety etc.) activates the sweat gland causing the increment and reduction of  Skin Conductance on SNS and PNS physiological conditions respectively. However, Instant Heart Rate also has similar effect on SNS and PNS physiological condtions i.e., a higher value of heart rate is the effect of SNS and lower value is the outcome of PNS. EDA and PPG sensors are widely used to estimate the instant value of skin conductance and heart rate respectively \cite{alam16}.
\subsection{EDA Sensor Signal Processing}
EDA is the property of the human body that causes continuous variation in the electrical characteristics of the skin which varies with the state of sweat glands in the skin. There are three types of arousal: \emph{cognitive, affective and physical}. \emph{Cognitive} arousal occurs when a person tries to solve any problem using her cognitive ability. \emph{Affective} arousal occurs when a person is worried, frightened or angry either doing daily activities or in resting position. On the other hand, \emph{physical} arousal is related to the brain command to move bodily parts which is imposed on the total arousal as an artifact, called \emph{motion artifact}. However, there are always some noises due to the weather conditions (temperature, humidity etc.) and device motion. This \emph{motion artifact} can be the prime cause of signal contamination of physiological outcomes while performing daily activities which must be removed. \emph{AutoCogniSys} proposes an EDA sensor signal processing method consists of three steps: (i) noise and motion artifacts removal, (ii) separation of tonic component and phasic component (explained later) from contamination free EDA signal and (iii) feature extraction on the response window.
\subsubsection{Motion Artifacts Removal}
There are many types of motion artifacts but the unsual steep rise is the mostly occured ones associated with EDA signal while performing daily activities \cite{edel67}. We use well-known steep rising noises reduction technique, SWT \cite{chen15}. We first consider EDA signal as a mixture of a slow variant tonic and fast variant phasic component, i.e., SWT coefficient is modeled as a mixture of two Gaussian components, phasic (close to zero valued signal) and tonic (high rising signal). After expanding EDA signal into multiple levels of scaling and wavelet coefficients, we choose adaptively a threshold limit at each level based on the statistical estimation of the wavelet coefficients' distribution, and employ that on the wavelet coefficients of all levels. Finally, an inverse wavelet transform technique is applied to the thresholded wavelet coefficients to obtain the artifacts free EDA signal. Fig~.\ref{fig:eda_artifact_removal} shows a sample of raw and motion artifacts free EDA signal.

\begin{figure}[!htb]
\begin{center}
\vspace{-.1in}
   \epsfig{file=eda_signal_artifact.pdf,height=1.6in, width=3.5in}
\caption{Dashed line represents noisy EDA signal and solid red line represents \emph{AutoCogniSys} proposed motion artifact free EDA signal}
   \label{fig:eda_artifact_removal}
\end{center}
\end{figure}
\subsubsection{Convex Optimization Technique to EDA Deconvolution}
After the motion artifact removal, we consider EDA as the sum of three components for $N$ sample: a slow tonic driver ($t$), fast (compact, bursty) non-negative sparse phasic driver ($r$) and a reminder error term ($\epsilon_r$).
\begin{equation}
\label{eq:eda_signal}
y = t + r + \epsilon_r
\end{equation}
This additive error $\epsilon_r$ is a White Gaussian Noise. The central problem associated with the deconvolution method is to get tonic $t$ component from the above equation. \cite{greco16} showed that EDA signal deconvolution (separation of tonic, phasic and noise terms from EDA signal) is a quadratic optimization problem and defined tonic component as follows:
\begin{equation}
\label{eq:tonic}
t = Bl + Cd,
\end{equation}
where $B$ is a tall matrix whose columns are cubic $B$-spline basis functions, $l$ is the vector of spline coefficients, $C$ is a $N\times 2$ matrix, $d$ is a $2\times 1$ vector with the offset and slope coefficients for the linear trend. The above equation is subject to the following optimization problem,
\begin{eqnarray}
minimize \frac{1}{2} {||Mq + Bl + Cd- y||}^2_2 +\alpha {||Aq||}_1 + \frac{\lambda}{2} {||l||}^2_2\\
subject\;to\; Aq \geq 0\nonumber
\end{eqnarray}
where $M$ and $A$ are tridiagonal matrices and $q$ is an auxiliary variable. After solving the above equation, we can get the optimal values for $\{q,l,d\}$ that can be used to obtain tonic component from the equation~\ref{eq:tonic}. The reminder of the equation~\ref{eq:eda_signal} ($r+\epsilon_r$) is considered as a mixture of White Gaussian Noise ($\epsilon_r$) and a fast variant phasic component ($r$). We employ butterworth low-pass filter (5Hz) and hanning smoothing with window size 4 (optimal) to remove $\epsilon_r$ from phasic component ($r$).
\subsection{PPG Signal Processing}
PPG is used mainly for measuring the oxygen saturation in the blood and blood volume changes in skin. An ideal PPG signal processing must contain the following steps: noise and motion artifacts removal, heart rate detection, heart rate variability estimation and feature extraction.
\subsubsection{PPG Signal Noise and Motion Artifacts Removal}
Similar to EDA signal, PPG signal is also contaminated with motion artifacts and noises. However, unlike EDA signal, PPG produce quasiperiodicity in a time series spectrum \cite{mete30}. We use Periodic Moving Average Filter (PMAF) to remove motion artifacts and noises \cite{lee07}. We first segment the PPG signal on periodic boundaries and then average the $m^{th}$ samples of each period. After filtering the input PPG signal with a 5-Hz $8^{th}$-order Butterworth low-pass filter, we estimate the maximum and minimum value of each period. The mean of each period are obtained from the maximum and minimum values applying the zero crossing method. These points of the means help determine the boundaries of each period. Then, interpolation or decimation is performed to ensure that each period had the same number of samples \cite{lee07}. 
\subsubsection{Heart Rate and Heart Rate Variability Estimation}
We first apply PMAF on PPG signal to remove noises and motion artifacts, refine PPG by smoothing the signal using 1-dimensional Gaussian Filter and Convolution, calculate first derivative of the convoluted signal and finally find the differences between two consecutive peak values which is called HRV \cite{sel08}. The occurrences of total peak values (R-peak or beat) in each minute is called Heart Rate (HR) with an unit of Beat Per Minute. The signal value property of HRV and HR are inversely proportional which means the mental arousal that increases HR should decrease HRV in the time segment window. Fig~\ref{fig:ppg_artifact_removal} shows a sample of the noisy and filtered PPG signal and their corresponding Instant Heart Rate.
\begin{figure}[!htb]
\vspace{-.1in}
\begin{center}
   \epsfig{file=ppg_artifact_removal.pdf,height=1.4in, width=3.5in}
   \vspace{-.15in}
\caption{Top figure illustrates the noisy signal (dotted line) and filtered signal from PPG sensor based on our filtering method. Bottom figure illustrates instant heart rate calculated from noisy signal (dotted line) and filtered signal}
   \label{fig:ppg_artifact_removal}
\end{center}
\vspace{-.15in}
\end{figure}
\subsection{Physiological Sensor Signal Feature Extraction}
Using the above mentioned methods, we removed the noises and motion artifacts from EDA and PPG signals and generated two time series signal from EDA (tonic and phasic components) and one time series signal from PPG (HRV). Then, we segment each of the time series signal based on our prior detected complex activities such that each response window starts and ends with the starting and ending points of each complex activity. We extract 7 statistical time-series features for EDA (as shown in Table~\ref{tab:eda_features}) and 8 features for HRV (Table~\ref{tab:hrv_features}) within the response window).

\begin{table}[!t]
\begin{center}

\renewcommand{\arraystretch}{1}
\caption{EDA Features Within The Response Window}
\begin{scriptsize}


\label{tab:eda_features}
\begin{tabular}{|c|l|}
\hline
\bfseries Features& \bfseries Description\\
\hline
nSCR & Number of SCRs within response window (wrw)\\
\hline
Latency & Response latency of first significant SCR wrw\\
\hline
AmpSum & Sum of SCR-amplitudes of significant SCRs wrw\\
\hline
SCR & Average phasic driver wrw\\
\hline
ISCR & Area (i.e. time integral) of phasic driver wrw\\
\hline
PhasicMax & Maximum value of phasic activity wrw\\
\hline
Tonic & Mean tonic activity wrw\\
\hline
\end{tabular}
\end{scriptsize}
\end{center}
\end{table}



\begin{table}[!t]
  \begin{center}
\renewcommand{\arraystretch}{1}
\vspace{-.3in}
\caption{Heart Rate Variability features}
\label{tab:hrv_features}
\begin{scriptsize}
\begin{tabular}{|c|l|}

\hline
\bfseries Feature& \bfseries Description\\
\hline
$\overline{RR}$&Mean RR intervals\\
\hline
SDNN&Standard deviation of RR intervals\\
\hline
SDSD&Std of successive RR interval differences\\
\hline
RMSSD&Root mean square of successive differences\\
\hline
NN50&\#successive intervals differing more than 50 ms\\
\hline
pNN50&relative amount of NN50\\
\hline
HRVTI&Total number of RR intervals/height of the histogram\\
\hline
TINN&Width of RR histogram through triangular interpolation\\
\hline
\end{tabular}
\end{scriptsize}
  \end{center}
\end{table}
\section{Experimental Evaluation}
In this section, we explain our data collection, available benchmark dataset, baseline methods and evaluation.
\subsection{Datasets and Baseline Methods}
We validate and compare \emph{AutoCogniSys} with baseline methods on both publicly available and our collected datasets.
\subsubsection{RCC Dataset: Collection and Ground Truth Annotation}
For collecting Retirement Community Center Dataset (RCC Dataset), we recruited 22 participants (19 females and 3 males) with age range from 77-93 (mean 85.5, std 3.92) in a continuing care retirement community with the appropriate institutional IRB approval and signed consent. The gender diversity in the recruited participants reflects the gender distribution (85\% female and 15\% male) in the retirement community facility. A trained gerontology graduate student evaluator completes surveys with participants to fill out the surveys. Participants are given a wrist band to wear on their dominant hand, and concurrently another trained IT graduate student have the IoT system setup in participants' own living environment (setup time 15-30 minutes). The participants are instructed to perform 13 \emph{complex ADLs}. Another project member remotely monitors the sensor readings, videos and system failure status. The entire session lasts from 2-4 hours of time depending on participants' physical and cognitive ability.

We follow the standard protocol to annotate demographics and activities mentioned in the IRB. Two graduate students are engaged to annotate activities (postural, gestural and complex activity) whereas the observed activity performances are computed by the evaluator. Two more graduate students are engaged to validate the annotations on the videos. In overall, we are able to annotate 13 complex activities (total 291 samples) labeling for each participant; 8 hand gestures (total 43561 samples) and 4 postural activities (total 43561 samples) labeling. Annotation of postural and complex activities outcomes no difficulties from recorded videos. However, annotation of hand-gestures is extremely difficult in our scenario. We used video based hand tracker that can track and sketch wrist movements from a video episode \cite{hugo14}. This sketching can help us significantly to identify which particular hand gesture is being performed in the time segment.
\subsubsection{EES Datasets: EDA and PPG Sensor Datasets}
We used Eight-Emotion Sentics (EES) dataset to validate \emph{AutoCogniSys} proposed physiological signal processing approaches \cite{picard01}. The dataset consists of measurements of four physiological signals (PPG/Blood Volume Pulse, electromyogram, respiration and Skin Conductance/EDA) and eight affective states (neutral, anger, hate, grief, love, romantic love, joy, and reverence). The study was taken once a day in a session lasting around 25 minutes for 20 days of recordings from an individual participant. We consider only PPG and EDA for all of the affective states in our study.
\subsubsection{Baseline Methods}
Though no frameworks ever combined all modalities together into real-time automated cognitive health assessment, we evaluate \emph{AutoCogniSys} performance by comparing the performances of its components individually with upto date relevant works. For hand gesture and postural activity recognition, we consider \cite{alam17} proposed method as baseline. For complex activity recognition, we compare our hand gesture and postural activity classifiers aided HDBN model with three-level Dynamic Bayesian Network \cite{zhu12} framework. For activity performance estimation, activity performance based cognitive health assessment; and EDA and PPG based cognitive health assessment, we have considered \cite{alam16} proposed method as baseline.
\subsection{Activity Recognition Evaluation}
The standard definition for \emph{accuracy} in any classification problem is $\frac{TP+TN}{TP+TN+FP+FN}$ where $TP,TN,FP$ and $FN$ are defined as true positive, true negative, false positive and false negative. For complex activity recognition evaluation, we additionally consider \emph{start/end duration error} as performance metric that can be explained as follows: consider that the true duration of ``cooking'' is 30 minutes (10:05 AM - 10:35 AM) and our algorithm predicts 29 minutes (10.10 - to 10.39 AM). Then, the start/end duration error is 9 minutes ($|$5 minutes delayed start$|$ + $|$4 minutes hastened end$|$), in an overall error of e.g., 30\% (9/30=0.3). We measure cross-participant accuracy using leave-two-participants-out method for performance metrics, i.e., we take out two of the participants' data points from the entire dataset, train our proposed classification models, test the model accuracy on the two left-out participants relevant data points, and continue the process for entire dataset.

\begin{figure*}[!htb]
\begin{minipage}{0.45\textwidth}
\begin{center}
   \epsfig{file=hand_gesture_accuracy.pdf,height=1.6in, width=3in}
\caption{Feature Weighted Naive Bayes (FWNB) classification accuracy comparisons with baseline approaches (graphical signatures of all hand gestures are shown).}
   \label{fig:hand_gesture_accuracy}
\end{center}
\end{minipage}
\begin{minipage}{0.29\textwidth}
\begin{center}
\vspace{-.12in}
   \epsfig{file=posture_accuracy_normal.pdf,height=1.6in, width=2.1in}
\caption{4-class postural level activity recognition performance and comparisons with baseline method}
   \label{fig:posture_accuracy_normal}
\end{center}
\end{minipage}
\begin{minipage}{0.25\textwidth}
 \begin{center}
 \vspace{-.12in}
   \epsfig{file=posture_accuracy_extended.pdf,height=1.6in, width=2.1in}
\caption{6-class diverse postural activity recognition framework accuracy comparisons with the baseline approach.}
   \label{fig:posture_accuracy_extended}
\end{center}
 \end{minipage}
\end{figure*}

Fig~\ref{fig:hand_gesture_accuracy} displays Feature Weighted Naive Bayes (FWNB) based the 8-hand gestural activity recognition accuracies comparisons with the baseline methods which clearly depicts the outperformance of our method (5\% improvement) with an overall accuracy of 92\% (FP rate 6.7\%) in RCC dataset. For postural activity recognition, dataset achieving 91\% postural activity recognition accuracy (FP rate 9.5\%) which outperforms the baseline approach significantly (8\% improvement). Now, we expand the postural activities for RCC datasets into 3 diverse `walking' postures: `normal walking', `walking with walker', `walking with single stick' and the accuracy goes down to 88\% (FP 7.9\%). Fig.~\ref{fig:posture_accuracy_normal} and Fig.~\ref{fig:posture_accuracy_extended} illustrate 4-class postural and extended 6-class postural classifier accuracies respectively which clearly posit that \emph{AutoCogniSys} outperforms in each case of postural activities as well as overall performances (8\% and 7\% improvement respectively).

For complex activity classification, we choose RCC dataset to train our HDBN model. Our leave-two-participants out method results an accuracy of 85\% (FP Rate 3.6\%, precision 84.2\%, recall 84.5\%, ROC Area 98.2\%) with a start/end duration error of 9.7\%. We run the entire evaluation for baseline complex activity recognition algorithm too achieving an overall accuracy of 78\% (FP Rate 5.2\%, precision 79.6\%, recall 78.5\%, ROC Area 82.7\%) which is clearly lower performed method than our approach. Fig. \ref{fig:complex_activity_roc} and Fig~\ref{fig:complex_activity_accuracy} illustrate the ROC curve and each complex activity recognition accuracy comparisons with baseline method which depict the outperformance of our framework over baseline methods (7\% improvement). Fig~\ref{fig:complex_activity_accuracy} also shows that inclusion of postural activity improves the final complex activity recognition (4\% improvement).
 \begin{figure}  [!htb]
  \begin{minipage}{0.15\textwidth}
 \begin{center}
   \epsfig{file=complex_activity_roc.pdf,height=1.4in, width=1.1in}
\caption{ROC curve for complex activity recognition}
   \label{fig:complex_activity_roc}
\end{center}
\end{minipage}
\begin{minipage}{0.33\textwidth}
\begin{center}

   \epsfig{file=complex_activity_accuracy.pdf,height=1.4in, width=2.3in}
\caption{Complex ADLs recognition accuracy improvement and comparison with baseline \cite{zhu12} and HMM based method}
   \label{fig:complex_activity_accuracy}
\end{center}

\end{minipage}
\end{figure}

\subsection{Quantification of Performance Score}
To characterize both the qualitative and quantitative health assessment performance scores, we start with four different feature groups ranging from both functional and physiological health measures: (i) observation based activity features, (ii) automatic activity performance features, (iii) EDA features and (iv) PPG features.

In \emph{observation based activity features}, we design a complex activity set comprised of multiple subtasks which are involved with task {\it interruption, completion and sequencing}. Participants are instructed to perform the complex activities while the trained evaluator observed the aforementioned functional activity performance measures. Each incorrect attempt of performance measure will be assigned one point thus higher score reflects lower performance of functional activities \cite{dawadi14}. We first detect hand gesture and postural activities. Then, we feed the low-level activity contexts (gestural and postural) combined with ambient contexts (object and ambient motion sensor readings) into HDBN for single inhabitant model \cite{alam16b} to recognize complex activities. The complex activity recognition framework provides both activity labels and activity window (start-end points). Then, we extract features of object sensor, ambient sensor, gestural activity and postural activity events for each activity window. The features are number of occurrences, mean number of occurrences, consecutive 1, 2, 3, $\ldots$ 20 occurrences, top 10, 20, 30, $\ldots$, 90 percentile etc (29 features in total). In \emph{physiological features} we first detect 13 complex activities using HDBN algorithm which provides activity labels and activity window (start-end points), apply noise reduction, motion artifacts removal, extract 7 EDA features and 8 HRV features for each activity and take the mean of them over time (minutes) to get 15 (7+8) complex activity physiological features set for each participant. In summary, we extract 3 observation based activity features, 29 automatic activity performance features, 7 EDA features and 8 HRV features.
\subsection{Physiological Signal Processing Performance Evaluation}
Standard evaluation technique should use both experimental and publicly available datasets to confirm the outperformance of the novel approaches. We first evaluate our physiological signal processing techniques using a publicly available dataset (EES Dataset \cite{picard01}) to detect 8 human emotions. Then, in next section, we evaluate our methods in assessing cognitive health status of older adults using RCC dataset.

For EDA, we first apply SWT method to remove motion artifacts and noises. Then, we use cvxEDA method to separate tonic and phasic components of EDA. Then, we extract 7 EDA features on a sliding window of 4 seconds. Finally, we feed the 7 EDA features into a SMO based SVM algorithm \cite{cao06}. We use 10-fold cross validation to classify eight emotions achieving 87\% of overall accuracy (FP rate 6\%). For PPG, we first apply our proposed PMAF based noises and motion artifacts removal technique. Then, we calculate HRV and perform time-domain feature extraction to extract 8 HRV features on a sliding window of 4 seconds. We feed these features into a SMO based SVM algorithm \cite{cao06}. Our 10-fold cross validation shows accuracy of 79\% (FP rate 11.5\%) of detecting 8 emotions on EES Dataset. Fig. \ref{fig:ees_eda} and Fig. \ref{fig:ees_ppg} clearly depict that \emph{AutoCogniSys} proposed EDA and PPG signal processing techniques significantly improve the accuracy over the baseline \cite{alam16} method (10\% and 12\% improvement).

\begin{figure}[!htb]
\begin{minipage}{0.24\textwidth}
\begin{center}
   \epsfig{file=ees_eda.pdf,height=1.2in, width=1.8in}
\caption{(EES Databaset) EDA features based Eight Emotion classification accuracy comparisons with baseline method}
   \label{fig:ees_eda}
\end{center}
\end{minipage}
\begin{minipage}{0.23\textwidth}
\begin{center}
   \epsfig{file=ees_ppg.pdf,height=1.2in, width=1.7in}
\caption{(EES Dataset) PPG features based 8-Emotion classification accuracy comparisons with baseline method}
   \label{fig:ees_ppg}
\end{center}
\end{minipage}

\end{figure}
\subsection{Evaluation of Performance Scores}
The feature subsets used in the experimentation for observation and survey based clinical assessments and technology guided physiological and activity initiated health assessments are depicted in Table~\ref{tab:feature_subset}. From our 6 demographics surveys, we find significant distributions in terms of cognition only for SLUMS Score (S-Score). Based on that, we divide our participants pool into three groups: \emph{Not Cognitively Impaired (NCI), Mild Cognitively Impaired (MCI) and Cognitively Impaired (CI)} where the number of participants are $5$, $7$ and $10$ respectively.
\begin{table}[!t]
\begin{scriptsize}


{\centering 
\renewcommand{\arraystretch}{.6}
\caption{Feature Subsets}
\label{tab:feature_subset}
\begin{tabular}{|l|L{5.5cm}|}
\hline
\bfseries Feature& \bfseries Description\\
\hline
Observation & Task Completeness (TC), Sequencing (SEQ), Interruptions (INT)\\
\hline
Survey & SLUMS Score (S-Score), ZUNG Score (Z-Score), IADL Score (I-Score), Yale Score (YPAS), Barthel Score (B-Score), GDS Score (G-Score)\\
\hline
EDA  and HRV & 7 and 8 Features\\
\hline
Activity Performance& Supervised (TC, SEQ, INT), Unsupervised\\
\hline
Arousal& EDA and HRV features of each complex activity window\\
\hline

\end{tabular}
}
\end{scriptsize}
\end{table}


\begin{figure}[!htb]
\begin{center}
   \epsfig{file=group_correlation.pdf,height=1in, width=3.3in}
\caption{\emph{AutoCogniSys} Proposed Method Based Group Correlation analysis ( $r-value$) NCI, MCI and CI represent not cognitive, mild cognitive and cognitively impaired group of population. TC, INT, SEQ, EDA and HRV represent task completeness, interruption scores, sequencing scores, electrodermal activity features and heart rate variability features}
   \label{fig:group_correlation}
\end{center}
\vspace{-.2in}
\end{figure}
\begin{figure}[!htb]
\begin{center}
   \epsfig{file=group_correlation_baseline.pdf,height=1in, width=3.3in}
\caption{Baseline \cite{alam16} method based Group Correlation analysis ( $r-value$)}
   \label{fig:group_correlation_baseline}
   \vspace{-.25in}
\end{center}
\end{figure}

\subsection{Statistical Correlation Analysis of Cognitive Health}
We used Pearson correlation coefficients with significance on $p<0.05$* for individual feature and partial correlation coefficients with significance on $p<0.005$** for group of features correlation analysis. Fig. \ref{fig:group_correlation} and Fig. \ref{fig:group_correlation_baseline} show the group correlation analysis results based on \emph{AutoCogniSys} proposed framework and baseline \cite{alam16} framework respectively. It can be clearly depicted that our proposed framework improves the correlation with the ground truths.
\subsection{Machine Learning Classification of Cognitive Health}
We evaluate using machine learning classifiers to predict cognitive status of older adults using both individual modalities and combined features. We use leave-two-participants out method to train and test classification accuracy.

We first choose the individual activity features (machine learning method based interruption scores, sequencing scores, unsupervised scores) and their combined features to train and test cognitive impairment status classification for SMO based SVM algorithm \cite{cao06}. The classification accuracies are 72\%, 69\%, 76\% and 83\% respectively. Then we consider 7 EDA-activity features and 8 HRV-activity features individually in training and testing phase of SMO based SVM algorithm \cite{cao06} resulting 85\% and 80\% accuracy respectively.

\begin{figure}[!htb]
\begin{minipage}{0.24\textwidth}
\begin{center}
   \epsfig{file=combined_classification.pdf,height=1.2in, width=1.7in}
   \vspace{-.15in}
\caption{Individual and combined classification accuracies comparison with baseline method for cognitive impairment status detection}
   \label{fig:combined_classification}
\end{center}
\end{minipage}
\begin{minipage}{0.23\textwidth}
\begin{center}
   \epsfig{file=each_activity_cognitive_assessment.pdf,height=1.2in, width=1.7in}

\caption{Machine learning based cognitive health assessment accuracy for each complex activity in terms of activity, EDA and HRV features.}
   \label{fig:each_activity_cognitive_assessment}
\end{center}
\end{minipage}
\end{figure}

For combined classifier, we first applied sequential forward feature selection to find the best combinations of 1- 3 features for cognitive impairment classification group MCI, NCI and CI in terms of combined activity features (29 features), EDA-activity features (7 features) and HRV-activity features (8) features. Our final combined classifier (SMO based SVM algorithm \cite{cao06}) provides an accuracy of {\bf 93\%} in detecting the cognitive impairment status of older adults. Fig. \ref{fig:combined_classification} shows our proposed individual and combined methods outperform the baseline \cite{alam16} significantly (13\% improvement). Fig. \ref{fig:each_activity_cognitive_assessment} shows the cognitive impairment status prediction accuracy for each modality (activity feature, EDA and HRV) per individual complex activity.
\subsection{Discussion}
If we exclude the postural activities from automated activity performance scoring, we find reduced statistical correlation with original task completeness performance for \{NCI, MCI, CI\} participant group (INT 0.53*, SEQ 0.21' and unsupervised 0.49'). However, if we skip our proposed motion artifact removal stage, we find reduced statistical correlation with \{NCI, MCI\} and \{MCI, CI\} groups of participants (EDA and HRV correlations respectively \{0.51*, -0.51*\} and \{-0.53*,0.46\}). To test our proposed motion artifacts removal impact on EDA signals more rigorously, we choose 5 random participants, engage one expert motion artifact annotator to annotate motion artifacts segment on each participant's first 30 minutes of complex dataset using recorded video and apply both baseline and our methods to detect motion artifact segments. While baseline method achieves 75.5\% (FP rate 20.3\%) accuracy in detecting motion artifact segments, \emph{AutoCogniSys} outperforms achieving 89.9\% (FP rate 8.9\%) accuracy. In terms of experience, we have seen 100\% acceptance of wearing wrist-band,  71\% of acceptance for signing consent on using cameras and 0\% failure rate of collecting continuous data.
\section{Conclusion}
We propose, \emph{AutoCogniSys}, an IoT inspired design approach combining wearable and ambient sensors embedded smart home design, extensive signal processing, machine learning algorithms and statistical analytics to automate cognitive health assessment in terms of complex activity performances and physiological responses of daily events. Additionally, our postural activity detection approach in diverse population cum improved activity performance measurement and fundamental physiological sensor artifacts removal from physiological sensors help facilitate the automated cross-sectional cognitive health assessment of the older adults. Our efficient evaluation on each modality (physical, physiological and ambient) and each activity mode proves that any of the mode (say single activity and single sensor) also can provide significant improved cognitive health assessment measure.



",['Wearable sensors.'],7670,multifieldqa_en,en,,34f9b2c3d79ced580687f9653a6908215f79b1405a918cd0," Cognitive deficit of older adults is one of the biggest global public health challenges in elderly care. Approximately 5.2 million people of 65 and older are suffered with any form of cognitive impairments in United States in 2012. Total annual payments for health care, long-term care and hospice care for people with Alzheimer's or other dementias are projected to increase from \$259 billion in 2017 to more than \$1.1 trillion in 2050. There is no universally accepted method for IoT-assisted automatic cognitive health assessment in smart home environment that can fuse multi-modal sensor contexts altogether. However, there are many advantages to obtaining an early and accurate diagnosis when cognitive symptoms are first noticed as the root cause findings of impairment always lessen the progress of impairment status and sometimes symptoms can be reversible and cured. For validation of IoT based cognitive health Assessment, self-reported surveys, clinical diagnosis and observation based tools are used individually by prior researchers. For example, some researchers showed ambient sensors based Activities of Daily Livigin (ADLs) sequence pattern can signify the cognitive health status of older Adults. But there are several existing challenges in developing and validating a fully automated multi- modal assessment model. For more information, visit: http://www.cnn.com/2013/01/29/health/dementia/index.html#storylink=cpy. For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http:// www.suicidepreventionlifeline.org/. For confidential help, call  the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here. For information on suicide prevention in the UK, visit the Samaritan’s Samaritans’ website. For details on suicide support in the United Kingdom, visit http www.samaritans.org/ and or the Samaritans in the UK  “ ”“” ‘’’ “‘”  “The Samaritans are a charity that provides support to people in need.’,” ’. ’”. ‘ ’ ’  ’ ‚’,. ’.”, ‘ ‘. ”,’; ’;’ '’'’: ’, ”’ ’The Samaritans are not a charity, they’re a service provider. They’ve done a lot of work to help people in their lives. They provide a service that is vital to the community. .  ”,. ‘The  ‘Cognitive Health Index’ is a tool that can be used to monitor the health and well-being of people in the home. It can help people with dementia and other forms of cognitive impairment. ealth assessment, \emph{AutoCogniSys} considers reproducibility of our model in any smart home system. We propose automatic complex activity recognition based on a Hierarchical Dynamic Bayesian Network (HDBN) model, fine-grained extraction of physiological sensor features and finally machine learning classification of cognitive impairment. We design and prototype an IoT system consisting of multiple devices (wearable wrist band, IP camera, object and ambient sensors) connected with central hub via WiFi, Ethernet and Bluetooth communication protocols. We collected data from 22 older adults living in a continuing care retirement community center in a very natural setting (IRIR) We propose a novel collaborative filter for EDA signal processing by postulating signal as a mixture of three components:  tonic phase, phasic phase and motion artifacts. We use a novel PPG signal processing technique to filter out the inherent motion artifacts and noises using improved Periodic Moving Average Filtering (PMAF) technique. We then leverage multi-label context sets with ambient and object sensor signals for complex activity Recognition based on HDBN model. We consider effective smart home sensor customization and deployment, data collection, screening, cleaning and filtering, feature computation, normalization and classification, and activity model training. We employ an extensive signal deconvolution technique that in conjunction with machine learning technique helps facilitate a wrist-worn ACC-based multi- label (hand gestural and postural) activity recognition for diverse population. We show significant improvement can be achieved in cognitive impairment classification, i.e., a recall rate of over 90\% with less than 5\% false positive rate. We ask: How can we exploit and relate the micro-activity features into noise free physiological sensor signals processing to automate cognitive health assessment process? What are the critical roles of clinical survey and technology guided assessment methodologies and their inter-relationships for automating the different intermediate steps of cognitive health Assessment process? We make the following key contributions: key contributions, research questions, implementation and evaluation of our proposed cognitive healthessment approach. The study was published in the online edition of the journal “Cognitive Health Assessment’s’ “Journal of Cognition and Cognition”, published by the Proceedings of the Association for Computing Machinery (PCM) (http://www.pcm.org/journal/cognition/c Cognition/Cognition-Cognitive-Health-Assessment-Cognitivists/Journal/C Cognition.html). It is the first of its kind to be published in a peer-to-peer peer-reviewed journal and is available on the PCM website. It is available free of charge to all interested parties, with a limited number of embargoed copies of the book available for download. AutoCogniSys builds on previous works on wearable devices based low-level (postural and hand gestural) activity recognition and their integration with ambient sensors to recognize complex ADLs. We use statistical and machine learning techniques to jointly correlate the activity performance metrics and stress (EDA and PPG) features that helps achieve max. 93\% of cognitive impairment status detection accuracy. The underlying signal processing and applications on cognitive health assessment automation can be used in the future to improve the accuracy of cognitive health assessments. The study was published in the open-source journal Cogent Cognition (http://www.cogent-cognition.org/article.php?article=Cogent  Cognition and Cognitive Impairment Status Diagnosis &type=Cognition &ls=1&ls-key=1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,29-32,33,32-33,34,34-34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,59-60,61,62,63,64,63-64,65,66,67,68,69,70,72,73,74,75,76,78,77,78,.78,79,80,81,82,83,84,88,87,89,90,88,.88,89,.90,91,92,93,94,93,.88,.92,94,.94,95,96,97,98,99,100,102,103,104,103,.98,100,.99,101,102,.102,104,.103,103.94,104.99,103.,104,105,104.,105,106,107,108,107,.104,109,110,111,112,113,111,.111,113,.111,.112,112,.113,114,114,.113,.114,115,111.104,111.,111,114.103,112.111,115,.111.111,.110,113.114,113.,114,111;111,110.111.113,115.114,.111.,113,112.,111.109,114;111.114.113.110,114.,111,.113.111.,114.114.,115,114 111.110.113,.110.114 113,110,.114.115,115 111,.114 114.111 113.109,.111 112.113 111,111 114,109.110,.113 113,.115 114 115.111 .111.115.109.115,.115.112.110 113 114,.115,113 115,110 111 109.111:111.09.114:113.113.,115.115 110.112,114:115.113;111 115 113;110.115 111.11.114 113.09,.111:114. into single-inhabitant HDBN model that provides significant improvement in complex activity recognition. We consider inclusion of an wearable wrist-band with in-built ACC sensor to detect hand gesture and posture, augmenting with the ambient sensor readings. We propose intelligent use of physiological features of skin through different physiological sensor signals (EDA, PPG) processing in daily activity tasks. We incorporate context-awareness for automation of cognitive health assessment that have not been explored before. We first investigate existing IoT-based cognitive health care frameworks that covers every aspects of wearable (physical, physiological) and ambient (passive infrared and object) sensor signals computing. We have carefully chosen the clinically proven functional and behavioral health assessment tools for this pilot study. We also have chosen clinically justified observation based behavioral assessment methods for the residents. We conclude with a discussion of the benefits and risks of using this technology in the long-term health care system for older adults. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. In the U.S., call the National Suicide Prevention Line on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. In Europe, contact the National suicide Prevention Lifeline on 0800-825-7255 or click here for details on how to get in touch with Samaritans in the UK. For more information, visit the European Association for Suicide Prevention Lifelong Learning (ESL) website or the European Society for the Prevention of Suicide (ESSL) website. For information on the European Commission's Cognitive Health Assessment (ECHA) program, visit http://www.ecHA.org/ECHA/CognitiveHealthAssessment.html. For details on the ECHA program, see http:// www.ECHA.gov/CogniSys/ECHA/EHA/EHC/EHS/ECH/ECHS/EH/CGH/CH/EHK/ECH/EHL/EHE/EAC/EK/EEC/EHT/EJ/EHM/E/CG/EJC/ECHC/ECHT/ECHR/ECHL/ECHE/ECJ/ECMH/ECK/ECG/ECHK/ECHQ/ECHM/ECI/EHI/ECHI/EHO/ECM/ECS/EC/EH/EHR/EKA/ECNH/ECIL/ECHO/EHB/ECHP/ECU/ECL/EC0/EC09/EC9/EC1/EC8/EC10/ECE/EC00/EC3/EC7/EC5/EC2/EC4/ECC/EC6/EC20/EC13/EC14/EC15/EC17/EC18/EC19/EC21/EC05/EC12/EC08/EC11/EC22/EC24/EC26/EC28/EC29/EC30/EC01/EC23/EC27/EC25/EC38/EC16/EC31/EC39/EC99/EC07/EC100/EC50/EC110/EC,EC1,EC0,EC8,EC9,EC10,EC We aim to detect single wrist-worn ACC sensor based hand gesture and postural activities and insert these into an HDBN graphical model. We consider the recognition problem asan activity tupple of gesture,posture,ambient,object. We propose to use rotational normalization method that can merge some hand-gestures subject to directional differences and forms an 8-hand gesture model. Our proposed Feature Weight Naive Bayes (FWNB) classifier adds significant improvement on Alam et. al. proposed sparse-deconvolution method as well as recognition in diverse postural environment. For example, some older adults use walker, double walking sticks or wheel chair for walking in which cases collecting 18 hand gestures and corresponding postural activity for training requires endless efforts and carefulness. For more information, visit: http://www.sciencemag.org/content/early/2014/10/29/science-mag-science-hands-gesture-recognition-8.html#storylink=cpy. We are also happy to provide a link to our website, where you can see the full study and download a copy of the code for your own use. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. In the U.S., call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http:// www.suicidepreventionlifeline.org/. For confidential. support on suicide matters call theNational Suicide Prevention Line at 1-856-788-8888 or visit http://www-suiciderecovery.org. For information on suicide prevention in the UK, visit the Samaritan’s Samaritans’ website. For details on suicide support in the United Kingdom, visit the Samaritans on 08457 909090 or ‘click here’ for details on how to get in touch with a Samaritans Samaritans local branch or click here for information on the UK Samaritans. For. confidential help, contact the National suicide Prevention Line on 0800- Samaritans (8457 90 9090). For confidential help in the USA, call 1-877-909090. For the UK and Europe, visit www. Samaritans or the University of Liverpool. For further information, see the University of Liverpool’S Samaritans website or the University of London’’. For help in developing a smart home system, visit The Home Assistant’ (UK) or The Home Advisor (UK). For more details on the Home Assistant (UK), visit the Home Advisor’(UK) website or The Home Advisor(UK). for more information on a home automation system, click here. for the home automation tool, see The Home automation tool ( UK) or the Home Automation Tool (HOT) for the Home automation Tool (UK, UK, US, Canada, Ireland, Australia, or the UK). For. the Home Appliance Tool (HSOT) to monitor the home environment, see: The Home Automator (HAT) . For the Home Monitoring Tool (HMT) to measure the home's occupancy level, consider three different rooms (kitchen, livingroom and bedroom) to capture room level occupancy, (iii) 7 binary object sensors attached with closet door, entry door, telephone, broom, laundry basket, trash can and trash box, (iv) three IP cameras in the appropriate positions to collect the ground truth data and (v) an Empatica E4 wrist-band (integrated sensors: PPG at 64 Hz, EDA at 4 Hz, Body temperature at 1 Hz and a triaxial AutoCogniSys has developed a method to recognize hand gestures using rotation normalization and 4 (walking, sitting, standing and lying) postural activities. The deconvolution framework takes a known signal (hand gesture effects) and a equalizer parameter ($\lambda$) as input and provides an Approximately Sparse Factor signal (postural activity effects) as output. The system predicts the gesture type with the greatest value as the user gesture. For 3-axis ACC signals, we need to learn associated 3 equalizer parameters for each hand gesture. We then apply feature extraction and prior learned SMO based SVM classifier (SMO) to classify final postural activity. We used a light-weight and efficient technique for hand gesture recognition called Naive Bayes classifier. It is based on the similarity of feature measures of the trained gestures (0$<$weight$ <$1) and the proximity of each feature measure to the average trained feature measure of each gesture type is calculated by a normal distribution. It uses a sparse-deconvolution method (with 31\% signal reconstruction error) to get Approximately Spare Factor (SSPF) method to get SSPF method. It also uses an SVM with sequential machine optimization (S MO) for postural Activity Recognition (PRA) to identify postural movements in the postural space. It has a total of 96 ($8\times 3\times 4$) equalizerparameters to learn. It can be seen in Fig. 1 by taking the raw x-axis of accelerometer signal, the extracted X-axis vector and the extracted SSPf vector and comparing it to the reference vector to get the total number of data points to learn the hand gesture dictionary. It then uses this data to create the final 8 hand gestures to identify the user's postural movement. For more details on the method, see Sec. 1 of the AutoCognisys project, click here: http://www.autocognisys.com/news/2013/10/01/autocogni-sys-hand-gestures-recognition-with-rotation-normalization-and-normalizing-the-rotations-of-hand.html#storylink=cpy.html. For the rest of the article, please see: Sec. 2 of the project, where we show how we normalize the rotation of hand gestures for greater accuracy and allow for more realistic, orientation-independent motion. We also show how to use a feature-weighted NaiveBayesian model to classify hand gestures. We use the Approximately Sparser Factor signal to extract 12 statistical features and SMPV classifier to classify postural motion in postural areas. We have also shown a single axis example of the deconvolved gesture model in the Fig. 3 section of this article to show how the system can be used to predict postural positions. We hope this will be used in the future to help people in a variety of situations. We build a HDBN based complex activity recognition framework for single inhabitant scenario smart home environment. We take advantage of detected hand gestural and postural activities along with the ambient and object sensor streams. We infer the most-likely sequence of complex activities (and their time boundaries), utilizing the well-known Expectation Maximization (EM) algorithm for training and the Viterbi algorithm for run-time inference. Our behavioral scientist team, comprises with Nursing professor, gerontologist and retirement community caregivers, carefully discus, optimize and choose 87 sub-tasks in total for 13 complex activities. The final goal of activity features estimation is to provide overall task score. The task score is proportional to the functional ability of participants in performance daily activities. We first, formulate the automated scoring as a supervised  machine learning problem in which machine learning algorithms learn a function that maps the hand gesture, posture, object, ambient sensor feature set to the direct observation scores. We use bagging ensemble method to learn the mapping function and SMO based SVM as base classifier. The learner averages by boostrapping.ng ASF signal of walking to get the overall ASF score of walking. We used supervised method to estimate TC, SEQ and INT, and unsupervised method to estimates TS. The results are based on a single wrist-worn sensor based hand gesture andPostural activity recognition, and proposed an activity features (TC, SEq and INT) estimation method including these two parameters in conjunction with object and ambient sensor features that provide significant improvement of cognitive health assessment of older adults. The study was published in the online edition of the Journal of Alzheimer's Disease and Related Conditions (JAD&C) (http://www.jadedc.org/2013/01/08/11/jawadi-drc.html#storylink=cpy&title=Jawadi.DAC&link=JAW&storylink%20=cj%20and%20JAW%20&storyline%20title%20%20of%20the%20Research%20Report%20on%20Older%20Adults%20with%20Cognitive%20Alzheimer%20Dementia%20 and%20Other%20Behavior%20Issue%20We%20examined%20Hand%20Gestural%20Activities%20for%20Smart%20Home%20Sensors. We%20found%20hand-gestural-and-postural-activity-recognition%20to%20improve%20cognitive performance of older adults%20in%20smart%20home. The%20research%20author%20s%20also%20revealed%20that%20older%20adults with crippling dementia-related disease and-diseases%20may%20are%20more%20prone%20than%20themselves%20when%20they%20taken%20off%20by%20their%20self%20health%20treatment%20or%20hospital%20diseased%20alternative%20diagnosis%20factors%20such as psychology and medicine individual numeric predictions to combine the base classifier predictions and generates an output for each data point that corresponds to the highest-probability label. We train three classifiers considering observation as ground truth for TC, SEQ and INT scores and test on the testing dataset. We derive unsupervised scores using dimensionality reduction technique for each feature set. The final single dimensional score represents machine learning based TS score. EDA sensor signal processing method consists of three steps: (i) noise and motion artifacts removal, (ii) separation of tonic component and phasic component (explained later) from contamination free EDA signal and (iii) feature extraction on the response window. We use well-known steep rising noises reduction technique, SWT. We first consider EDA signals as a mixture of a slow variant tonic and fast variant ph asic component, i.e., SWT coefficient is modeled as a mix of two Gaussian components, phasIC (close to zero valued signal) and tonic (high rising signal). After expanding EDA. signal into multiple levels of scaling and wavelet coefficients, we choose adaptively a threshold limit at each level based on the statistical estimation of the wavelet. distribution, and inverse wavelet coefficient of all levels. Finally, we employ that tr' employ that employ that Tr' tr' technique that employs that tr’ tr' analysis technique to extract features from a set of. data points and create a single-dimensional score for each activity. We then use that score to calculate the automated task completeness scores of the particular user activity. The results are presented in the next section of the paper. It is based on a dataset of more than 100,000 user activities from the U.S. National Institute of Standards and Technology (NIST) and the University of California, Los Angeles (UCLA) The study was published in the journal Computer Vision and Pattern Recognition (CVPR) in October 2013 and is available for download on the NIST website. It was the first of its kind to address the issue of machine learning in the field of computer vision and pattern recognition. For more information on how the study was developed, please visit: http://www.cogniSys.org/research/v2.0/CVPR/v3.0-v3/v4/v5/v6/v7/v8/v9/v10/v11/v12/v13/v14/v15/v16/v17/v18/v19/v20/v21/v22/v23/v24/v26/v27/v28/v25/v29/v30/v31/v32/v33/v34/v38/v36/v37/v35/v39/v40/v1/v41/v42/v48/v47/v50/v52/v51/v53/v54/v55/v58/v56/v57/v61/v62/v59/v63/v60/v64/v65/v67/v68/v69/v70/v72/v71/v73/v74/v75/v77/v78/v80/v76/v79/v82/v81/v83/v84/v87/v88/ EDA signal deconvolution (separation of tonic, phasic and noise terms from EDA signal) is a quadratic optimization problem. We employ butterworth low-pass filter (5Hz) and hanning smoothing with window size 4 (optimal) to remove the tonic component from the noise. PPG Signal Processing is used mainly for measuring the oxygen saturation in the blood and blood volume changes in skin. An ideal PPG signal processing must contain the following steps: noise and motion artifacts removal, heart rate detection, variability estimation and feature extraction. We use the Periodic Moving Average Filter (PMAF) to first remove the first segment of the PPG signals. Then the interpolation is performed to ensure that the same number of samples had the same mean number of each period. After each period, we estimate the maximum and minimum value of each mean value of the input signal with a 5-Hz low-order filtering filter with a window size of 4. We then determine the maximum values and minimum values applying the zero crossing method and then the average samples of the first and second segments of PMAF are used to obtain the artifacts free EDA signals. For example, in the figure below, we use the thresholded wavelet coefficients to get the artifactsFreeEDA.nsform technique to remove motion artifacts from the raw EDA Signal. For more information, visit the EDA Deconvolution website or go to: http://www.eda-signal.org/deconvolve.php/. For more details on the PGP Signal Processing, visit: http:www.pgu.com/signal-processing/pg- Signal-processing-technique-and-training-pgr-solution-pg.html. For the full article, go to http: www.eda.gov/pgu/solution/solutions/pgen/sgn-sgn_sgn.html?sgn=sgn%20sgn+sgn sgn&sgn#sgn = ""sgn"", where sgn = ""Sgn"" and sgn is the name of the software used to extract PPG signal from the raw signal. For a more detailed explanation of the method, see:http:www.:www.edea.org/. For a full list of the techniques used, see http:://www:www:edea-signals.org.uk/. For the rest of the article, please visit:http://www.: www.edeasignal.gov.uk/news/article/features/features-features/pgn/features%20features%2C-pgn%2c-pgen.html%. For the remainder of this article, we will focus on the method for removing motion artifacts. We will also look at how to extract the motion artifact from the raw and motion signals from a PPG.ns Form. We hope that this article will help you understand how PPG can be used to detect heart rate variability and heart rate changes in the human body. In the next section, we'll look at a technique for extracting the heart rate variation in the skin. We'll also discuss a method for extracting PPG noise from a blood volume change in the body. For now, let's look at an example of how this can be done with the help of a blood pressure monitor. We're going to look at two different methods for removing PPG noises from the blood. R-peak or beat) in each minute is called Heart Rate (HR) with an unit of Beat Per Minute. The signal value property of HRV and HR are inversely proportional which means the mental arousal that increases HR should decrease HRV in the time segment window. We extract 7 statistical time-series features for EDA and 8 features for HRV within the response window. Then, we segment each of the time series signal based on our prior detected complex activities such that each response window starts and ends with the starting and ending points of each complex activity. We validate and compare and compare our methods with baseline methods on both publicly available and publicly available datasets. We conclude with an evaluation of our data collection, available benchmark dataset, and baseline evaluation methods. We hope that this will help you understand how to use the data to improve your retirement planning and decision-making. We are happy to help you with any questions you may have about the data collection and evaluation methods that we have used in this study. We would like to hear from you if you have any questions about this study or any other aspect of the study. Please contact us at [email protected] and we will try to answer them as soon as possible. Back to the page you came from. The study was first published in the Journal of the American Geriatric Society (JAGS) in January 2013. For more information on the JAGS, please visit: http://www.jags.org/2013/01/07/physiological-geriatric-sensors-in-the-workplace-and-rehabilitation.html#storylink=cpy. For the full study, please go to: http: //www.globally.gov/news/article/article-26/36/38/738/11/1238/13/1138/14/1/1136/1137/1236/1234/1235/1237/1134/1135/1038/1036/1037/1034/10/12/1215/1035/1133/1033/1113/1028/1027/1029/1026/1030/1023/1025/1020/1024/1022/1021/1031/1032/1039/10309/10307/10300/10305/10306/10303/10308/10313/10304/10310/10302/10299/10298/10296/10259/10312/1053/10281/10301/103000/10317/10273/10278/10263/10274/10283/10287/10270/10290/10272/10285/10282/10277/10275/10288/10280/10286/10269/10279/10271/10295/10258/10268/10207/10205/10229/10208/10209/10265/10297/10227/10233/10228/10203/10705/10255/10267/10223/10206/10239/10234/10222/10250/10240/10235/10253/10251/10254/10225/10257/10204/10261/10236/10237/10252/10262/10284/10294/10230/10276/10292/10238/10249/10226/10245/10231/10200/10264/10247/10201/10232/10244/10243/10241/10248/10242/10256/10343/10185/10260/10246/10215/1019/1052/1047/1049/1046/10508/1054/1048/10 We used Eight-Emotion Sentics (EES) dataset to validate physiological signal processing approaches. The study was taken once a day in a session lasting around 25 minutes for 20 days of recordings from an individual participant. We used video based hand tracker that can track and sketch wrist movements from a video episode. For complex activity recognition, we compare our hand gesture and postural activity classifiers aided HDBN model with three-level Dynamic Bayesian Network framework. We measure cross-participant accuracy using leave-two-participants-out method for performance metrics, i.e. we take out two of the participants' data points from the entire dataset. We train our proposed classification models, test the model accuracy on the two left-out participants relevant data points, and continue the process for entire dataset, which is called Bayes' Bayes method. We consider only PPG and EDA for all of the affective states in our study. We evaluate performance by comparing the performances of its components individually with upto date relevant works. For activity performance estimation, we have considered EDA and PPG based cognitive health assessment as baseline. We have considered hand gesture recognition as baseline for hand gestures. For postural and complex activities, we consider hand-gestures as the baseline for postural activities. We considered hand gestures as a baseline for complex activities. For hand gestures, we considered hand movements to be the most important part of the complex activity evaluation process. We are able to annotate 13 complex activities (total 291 samples) labeling for each participant; 8 hand gestures ( total 43561 samples) and 4 postural Activities (total 43561 sample) labeling. We also annotate postural, gestural andcomplex activity labeling. Annotation of postural or complex activities outcomes no difficulties from recorded videos. We use a Weighted Naïve Bayes (FWNB) classification (NBcaption) classification model to train the proposed models, which has a weighted Naive Bayed Weighted Bayed (NBBC) classification. We evaluated the model's accuracy using a leave- two-participation-out (L2) method forperformance metrics, which had an overall error of e.g., 30\% (9/30=0.3). We consider the start/end duration error as performance metric that can be explained as follows: consider that the true duration of ``cooking'' is 30 minutes (10:05 AM - 10:35 AM) and our algorithm predicts 29 minutes ( 10.10 - to 10.39 AM) Then, the start and end duration error is 9 minutes ($|$5 minutes delayed start$|$ + $|$4 minutes hastened end$ |$), in an overallerror of e-g., 10.40-10.45 AM). We considered the PPG/Blood Volume Pulse, electromyogram, respiration and Skin Conductance/EDA (PPG/BSP/BVP) as the basis for our analysis. The entire session lasts from 2-4 hours of time depending on participants' physical and cognitive ability. The participants are instructed to perform 13 \emph{complex ADLs}. Curacy comparisons with baseline approaches (graphical signatures of all hand gestures are shown) For postural activity recognition, dataset achieving 91\% posturalactivity recognition accuracy (FP rate 9.5\%) which outperforms the baseline approach significantly (8\% improvement) For complex activity classification, we choose RCC dataset to train our HDBN model. We run the entire evaluation for baseline complex activity recognition algorithm too achieving an overall accuracy of 78\% (FP Rate 5.2\%, precision 79.6\%, recall 78.5’%, ROC Area 82.7’%) which is clearly lower performed method than our approach. Inclusion of postural activities improves the final complexactivity recognition (4’% improvement), and the accuracy goes down to 88’s (FP 7.9’%), Fig.~\ref{fig:posture_accuracy_normal.pdf, height=1.6in, width=2.1in) and Fig. “AutoCogniSys” outperforms in each case of posturally activities as well as overall performances (8”% and 7 % improvement respectively) and the ROC curve and each complex activity. recognition accuracy comparisons with. baseline method which depict the outperformance of our framework over baseline methods (7 ’’) and (7)% improvement, respectively. For the quantitative and qualitative assessment of health, both quantitative and quantitative scores characterize both the qualitative and quantitative performance of the HMM method. The HMM based method is based on the Feature Weighted Naive Bayes (FWNB) and HMMM (HMM) based on a HMM-based ADL (HDBN) method with a start/end duration error of 9.7\’. The results show that the HDBN method is more accurate than the FWNB method with an accuracy of 85’ (FP 3.6’), 84’, 84.2’ and 84.3’%. The results also show that inclusion of posture activity improves the. final complex activityrecognition (4\‚% improvement and (4)’ ‘’ ‘“” “’ – the final score of the method is 4’(4) out of 5.’). For the qualitative assessment, the quantitative Score of Health (QH) based method was used to characterize the quantitative. and quantitative. performance of both the ADL and QH-based HMM methods. The QH method was based on both the quantitative quantitative and the qualitative. score of Health. based HMM. based ADL. method and the quantitative score of QH. based Zhu Zhu (Zhu) method. For more details on the quantitative assessment, see ‘Quantification of Health’ section of this article. See http://www.hhs.org.uk/content/2013/07/09/07.shtml.html#qh.qh-health-performance-assessment-quantification-quantitative-Quantification-QH-12-12.html. For a full version of the QH report, please see the ‘QH’ part of the online version of this report. See the “Quantification” section for more information on how to use QH to assess health and the QS. version of QS to assess the health of patients. We first evaluate our physiological signal processing techniques using a publicly available dataset (EES Dataset) to detect 8 human emotions. Then, in next section, we evaluate our methods in assessing cognitive health status of older adults using RCC dataset. In summary, we extract 3 observation based activity features, 29 automatic activity performance features, 7 EDA features and 8 HRV features. We use 10-fold cross validation to classify eight emotions achieving 87\% of overall accuracy (FP rate 6\%). For PPG, we first apply our proposed PMAF based noises and motion artifacts removal technique. We calculate HRV and perform time-domain feature extraction to extract 8 HRv features on a sliding window of 4 seconds. We feed these features into a SMO based SVM algorithm to classify 8 emotions on EES dataset. Our 10- fold cross validation shows accuracy of 79\% (FP rates 11.5\%) of detecting 8 emotions. We find significant significant distributions in terms of cognition only for S only S surveys. We also found significant distributions for S. only for cognition only S only for older adults. We used the feature subsets used in the observation and observation-based experiments for the clinical and clinical assessments and technology based observation and clinical assessment initiated by the trained evaluator. We have also used the features subsets for the observation based experiments and technology-based clinical assessments for the cognitive health assessments and guided clinical assessments. We will publish the results of our experiments in a forthcoming issue of the Journal of the American Medical Association (JAMA) (http://www.jamanet.org/jama/2013/01/07/physiological-signal-processing-technique-and-behavior-in-clinical-assessment-of-older-adults.html#storylink=cpy). We will also publish a version of this article in the next issue of JAMA, which will be published in the Spring 2014. We are happy to share our findings with the JAMA journal (available in the April 2014 issue of this issue). We are also happy to provide a copy of the JMA’s online version of the ‘Physiological Signal Processing Performance Evaluation’ (JAMPS) (available online in the June 2013 issue of The JAMA) for the purposes of this study. The JAMPS publication is also available in the September 2013 issue (available on the JAMPs website). For the full version, please visit the JAMS website (http: www.jammets.com/jamas/2013-07/09/physiology-signaling-performance-evaluation-in JAMPs.html). We have included the “physiological” and “emph{physiological features” sections in this article. The ‘physiological features’ section includes features of object sensor, ambient sensor, gestural activity and postural activity events for each activity window. The features are number of occurrences, mean number of occurrence, consecutive 1, 2, 3, consecutive 10, top 10, 20, 30, 90 percentile etc (29 features in total) and the mean of them over time (minutes) to get 15 (7+8) complex activity physiological features set for each participant. For EDA, we use SWT method to remove motion artifacts and noises. For the PPG technique we use cvxEDA method to separate tonic and phasic components of EDA. We extract 7 E DA features on an activity window of four seconds. UMS Score (S-Score) based on that, we divide our participants pool into three groups. NCI, MCI and CI represent not cognitive, mild cognitive and cognitively impaired group of population. TC, INT, SEQ, EDA and HRV represent task completeness, interruption scores, sequencing scores, electrodermal activity features and heart rate variability features. We used Pearson correlation coefficients with significance on $p<0.05$* for individual feature and partial correlation coefficients for group of features correlation analysis. We use leave-two-participants out method to train and test classification accuracy. Then we consider features individually and combined features in training and testing phase of SMO based SVM algorithm. The classification accuracy is 72%, 69%, 76% and 83% respectively. The resulting 85% and 80% accuracy respectively is compared with baseline method for cognitive impairment detection. We evaluate using machine learning classifiers to predict cognitive status of older adults using both individual modalities and combinedfeatures. We also evaluate using a machine learning method based on interruption scores and sequencing scores to test cognitive impairment status classification for SMO-based SVM algorithms. We conclude that our proposed framework improves the correlation with the ground truths. It can be clearly depicted that the proposed framework is more accurate than the baseline framework. We are happy to share the results of our study with the researchers at the University of California, San Diego, who are currently working on a project to develop a new machine learning algorithm for older adults with cognitive impairment. The project is called ‘Cognitive Impaired Older Adults’ and is funded by the National Institute of Mental Health (NIMH) and the National Institutes of Health (NIH). The cost of the project is estimated to be $1.2 billion. The study was published in the journal “Cognitive Impairment: A New Assessment of Cognitive Health’, published by the NIMH in May 2014. The cost to the NICE is $2.5 billion and the time to complete the study is $3.5 million. It is the first study of its kind to use machine learning to assess cognitive impairment in older adults. For more information, see ‘Machine Learning for Cognitive Impairments’ (NICE) and “Machine Learning For Older Adults (MTH) (http://www.nimh.org/CognitiveImpairment/Machine-Learning-For-Older-Adults-A-New-Researchers-Studies-and-Training-for-Cognitive-impairment-in- Older-Advisers-“.html”). For more details on how to use the MTH, see the ‘MTH’ section of this article. For the full report, see: ‘Computer-Aided Cognitive Impairsment (MHC) – A New Study of Older Adults with Cognitive Impaire (CACI) ( http:// www.ncah.org/. For the rest of the article, please see “Computer- Aims For Older Adults With Cognitive Impacts (MACI),” “MHC’s” section,   “Machine-Aims For Cognition,’   “Theory Of Cognition”, ‘Theory of Cognition and Cognition (COCA),’  ‘Psychology of Cognitive Impaired Adults (COGA), and ‘Research Methodology’. ion to find the best combinations of 1- 3 features for cognitive impairment classification group MCI, NCI and CI in terms of combined activity features. Our final combined classifier (SMO based SVM algorithm) provides an accuracy of 93% in detecting the cognitive impairment status of older adults. If we exclude the postural activities from automated activity performance scoring, we find reduced statistical correlation with original task completeness performance. We propose an IoT inspired design approach combining wearable and ambient sensors embedded smart home design, extensive signal processing, machine learning algorithms and statistical analytics to automate cognitive health assessment. We have seen 100% acceptance of wearing wrist-band,  71\% of acceptance for signing consent on using cameras and 0\% failure rate of collecting continuous data. We find that any of the mode (say single activity and single sensor) also can provide significant improved cognitive health Assessment measure. We found that our proposed individual and combined methods outperform the baseline significantly (13\% improvement). Fig. 1 shows the cognitive impairment status prediction accuracy for each modality (activity feature, EDA and HRV) per individual complex activity. The baseline method achieves 75.5\% (FP rate 20.3\%) accuracy in detecting motion artifact segments, while the AutoCogniSys method achieves 89.9\%  (FP rates 8.9aki%) accuracy. In terms of experience, we've seen 100\% acceptance  of wearing a wrist-bands, 71\%. of acceptance  for signature consent on cameras, and 0 % failure rates of collectingcontinuous data. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. In Europe, call the suicide prevention helpline on 0800-825-7255 or go to www.sophia-sophie.org to talk to a counsellor."
What did the decision to base the water rates on usage reflect?,"Time to clean house in Paso Robles Home
Front Page » Time to clean house in Paso Robles
September 5, 2010 Opinion By JIM REED
I’d like to give you an update on the issue of our civil servants cramming hundreds of millions of dollars in spending down our throats after the people of Paso Robles voted down the water rate increase last November. The rate increase is being hung up in the courts by the City Attorney. What was supposed to be a quick issue to get in front of a judge, has been drug out as long as possible by the City Attorney.
Even if the courts throw out the current rate increase, I expect that our civil servants will just change a couple of words in the rate increase notice and force the same old plan on us again.
There is a real problem with the people we have hired to work for us in Paso Robles. It seems that decisions are made based on some agenda, even if it is contrary to citizens’ wishes.
City Councilmen Ed Steinbeck, Nick Gilman and Mayor Duane Picanco, on August 19th, voted unanimously to hire the same law firm employed by the City of Bell. You may have heard the recent news story about the City of Bell’s corrupt city representatives.
This law firm allowed the elected officials and City employees to pillage the General Fund for their own benefit, contrary to the rights and interests of the citizens. We are already paying several City employees $12,000 per month with equally ridiculous benefits and pensions. What does this say about our elected representatives?
I believe most residents are like me. We elect people we believe have our best interest in mind. Over the last few years I have seen that nothing is farther from the truth. The people we have elected have lost track of the fact that “the City” exists to protect and deliver services to the citizens. To them it is some all-important ideal they strive to cultivate and improve according to their agenda. They have forgotten that they are elected to represent the citizens.
We have an election coming up in November. We have the opportunity to elect some responsible, principled people to represent us. If we elect more people from within this system, we will get more of the same type of government. We need to look at where the new candidates stand. Will they lawfully represent the citizens of the city? Or, are they happy with the way things are being run?
We have stood together in the past and have made real significant changes in important matters that are going to affect our lives for years to come. There are several thousand citizens that made their voice heard on the water issue, more than enough votes to make a change in our city government.
Please come out and vote for a democratic representative governing body for Paso Robles instead of the tyrannical leadership that exists now.
Jim Reed is a longtime resident of Paso Robles.
Subjects: Opinion Paso Robles Paso Robles City Council Vote	Related:
<- Previous Next ->	Endless Summer Nights at Edna Valley, event photos Trial postponed for Paso Robles woman accused of forgery The comments below represent the opinion of the writer and do not represent the views or policies of CalCoastNews.com. (moderator@calcoastnews.com Comment Guidelines )
2 whatisup says:	09/13/2010 at 9:27 pm
pasoobserver – Here is something to observe and get you going in the right direction:
California Government Code Section 65584
(a) (1) For the fourth and subsequent revisions of the
housing element pursuant to Section 65588, the department shall
determine the existing and projected need for housing for each region
pursuant to this article. For purposes of subdivision (a) of Section
65583, the share of a city or county of the regional housing need
shall include that share of the housing need of persons at all income
levels within the area significantly affected by the general plan of
(2) While it is the intent of the Legislature that cities,
counties, and cities and counties should undertake all necessary
actions to encourage, promote, and facilitate the development of
housing to accommodate the entire regional housing need, it is
recognized, however, that future housing production may not equal the
regional housing need established for planning purposes.
(b) The department, in consultation with each council of
governments, shall determine each region’s existing and projected
housing need pursuant to Section 65584.01 at least two years prior to
the scheduled revision required pursuant to Section 65588. The
appropriate council of governments, or for cities and counties
without a council of governments, the department, shall adopt a final
regional housing need plan that allocates a share of the regional
housing need to each city, county, or city and county at least one
year prior to the scheduled revision for the region required by
Section 65588. The allocation plan prepared by a council of
governments shall be prepared pursuant to Sections 65584.04 and
65584.05 with the advice of the department.
(c) Notwithstanding any other provision of law, the due dates for
the determinations of the department or for the council of
governments, respectively, regarding the regional housing need may be
extended by the department by not more than 60 days if the extension
will enable access to more recent critical population or housing
data from a pending or recent release of the United States Census
Bureau or the Department of Finance. If the due date for the
determination of the department or the council of governments is
extended for this reason, the department shall extend the
corresponding housing element revision deadline pursuant to Section
65588 by not more than 60 days.
(d) The regional housing needs allocation plan shall be consistent
with all of the following objectives:
(1) Increasing the housing supply and the mix of housing types,
tenure, and affordability in all cities and counties within the
region in an equitable manner, which shall result in each
jurisdiction receiving an allocation of units for low- and very low
(2) Promoting infill development and socioeconomic equity, the
protection of environmental and agricultural resources, and the
encouragement of efficient development patterns.
(3) Promoting an improved intraregional relationship between jobs
(4) Allocating a lower proportion of housing need to an income
category when a jurisdiction already has a disproportionately high
share of households in that income category, as compared to the
countywide distribution of households in that category from the most
recent decennial United States census.
(e) For purposes of this section, “household income levels” are as
determined by the department as of the most recent decennial census
pursuant to the following code sections:
(1) Very low incomes as defined by Section 50105 of the Health and
(2) Lower incomes, as defined by Section 50079.5 of the Health and
(3) Moderate incomes, as defined by Section 50093 of the Health
and Safety Code.
(4) Above moderate incomes are those exceeding the moderate-income
level of Section 50093 of the Health and Safety Code.
(f) Notwithstanding any other provision of law, determinations
made by the department, a council of governments, or a city or county
pursuant to this section or Section 65584.01, 65584.02, 65584.03,
65584.04, 65584.05, 65584.06, 65584.07, or 65584.08 are exempt from
the California Environmental Quality Act (Division 13 (commencing
with Section 21000) of the Public Resources Code).
pasoobserver says:	09/13/2010 at 6:52 pm
To whatisup —- First of all, I reviewed AB 602 Assembly Bill. Thanks. I am sorry to inform you but AB 602 is not the LAW as you so stated in your blog. I contacted the Deputy Chief Council’s office in Sacramento handling AB 602 to confirm your misstatement of facts. You know,in the English language, It shouldn’t be so difficult to answer some simple questions with a “YES” or “NO” answer. Yet, you are reluctant to do so, but you go on and on with a thesis along with some rhetoric. I never talked about a court suit over the “water issue”, I asked YOU, not about waiting for a court decision. Maybe, you did with some other people. Also, I was not ranting about the wineries usage of water. My response to you on your vague question about “there are people not paying their fair share for their use of water”. I related, are you talking about the wineries? I am well aware that most of the wineries are outside the city limits using the same aquifer. You took my question out of context., nice try! You are just being a popinjay and rhetorical. Also, you didn’t answer another question about “what is the unit cost of water” in Templeton? as compared to Paso Robles.
whatisup says:	09/13/2010 at 8:54 pm
I am on a well. I am sure you are capable of doing your own homework. I also am quite sure if you really contacted the Deputy Chief Counsel’s Office you have been set straight. What I gave you is a proposed small adjustment in the wide range of laws that make up the California Housing element. I assumed you could stumble onto the facts based on what I gave you. By the way, I believe you can review the Paso Robles Housing element plan on the City’s website or at the Library. The California Housing Element Laws that all cities and counties have to follow have been in place for almost 25 years. I realize you don’t actually have a clue how to look the laws up. Either educate yourself or keep making a fool of yourself, your choice. A simple Google search of California Housing Element Laws will get you going. Good Luck!
TO WHATISUP — I WOULD LIKE TO KNOW WHAT LAW YOU ARE REFERRING TO THAT SAYS “WE” THE PEOPLE HAVE TO SUBSIDIZE NEW DEVELOPMENT? AGAIN, FOR THE THIRD TIME, YOU FAILED TO ANSWER MY QUESTIONS POSED TO YOU IN MY PRIOR RESPONSES TO YOU ON SEPT.10TH &11TH. IS THERE A REASON WHY YOU DON’T WANT TO ANSWER THEM? YOU DO WHAT OUR ELECTED OFFICIALS DO SO WELL, AND THAT IS “IN ONE EAR AND OUT OF THE OTHER EAR” IT SEEMS TO ME THAT YOU ARE EITHER EMPLOYED BY THE CITY OR YOU HAVE OTHER DEALING WITH THE CITY, SO BE IT. IT APPEARS TO ME THAT YOU THINK THE CITY DOES EVERYTHING RIGHT. APPARENTLY, YOU PRESENT YOURSELF AS BEING VERY BIAS ON CITY DECISIONS. IT LIKE THEY CAN’T DO ANYTHING WRONG ACCORDING TO YOUR LOGIC. THEY KNOW WHAT IS BEST FOR THE CITIZENS OF PASO,THAT IS A GOOD EXAMPLE OF ARROGANCE ALONG WITH NARCISSISM.
WHAT PEOPLE ARE YOU TALKING ABOUT THAT DOESN’T PAY THEIR FAIR SHARE OF WATER? ARE YOU REFERRING TO THE WINERIES USING THE SAME AQUIFER?
I BELIEVE YOU RELATED THAT YOU RESIDE IN TEMPLETON, BUT YOU OWN PROPERTY IN PASO. BY THE WAY, WHAT IS THE COST PER UNIT OF WATER USAGE IN TEMPLETON COMPARED TO PASO? OF COURSE, TEMPLETON IS IN AN UNINCORPORATED AREA (COUNTY JURISDICTION).
WELL, I GAVE YOU SOME SUGGESTIONS ON HOW TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT. ALSO, REMEMBER IT’S THE CITIZENS’ MONEY THAT IS BEING SPENT. WHAT IS MOST IMPORTANT OF ALL, IS LET THE CITIZENS OF PASO DECIDE WITH THEIR VOTE ON HOW TO FINANCE THIS HUGE CAPITAL IMPROVEMENT PROJECT EXPENDITURE. JUST BE IN COMPLIANCE WITH STATE PROPOSITION 218 AND STOP CIRCUMVENTING THE LAW.
WOULD YOU OBJECT TO HAVING TO FINANCE SOME NEW BONDS ON YOUR PROPERTY TAX BILL AS A ” SPECIAL TAX” OR AN ASSESSMENT TAX” TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT? A PERCENTAGE OF PASO CITIZENS FINANCE LOCAL SCHOOL BONDS ON THEIR PROPERTY TAX BILL AND DON’T HAVE ANY KIDS GOING TO SCHOOL. HOW ABOUT THAT COMPARISON FOR YOU TO THINK ABOUT? WHAT SAY YOU?
I say less CapsLock, please.
whatisup says:	09/12/2010 at 11:41 pm
I have answered your questions. I have been quite detailed in my answers and I am sorry if you can’t deal with the detail. I guess it is your inconvenient truth. You do seem to like to deflect and go around in circles. Another example, now you are ranting about the wineries using the same aquaifier as the City. Let me be clear for you, I don’t like the amount of water the wineries are using. However, the wineries are in the County, not in the City and the City can’t do anything about it. They wineries are allowed to take the water they are taking even if it drops the City’s water levels in their wells. You need to complain to Sacramento. It sounds like you just don’t want to pay anything for the infrastructure because you really just don’t want it built.
Several of your observations of my opinions are bizarre considering I have stated several times I believe the Courts need to decide if Paso Robles has, or has not followed the rules as to funding the infrastucture. Obviously, as I have stated before, if the City loses the lawsuit the infrastructure will have to be paid out of the City’s General Fund until a new method of payment is voted on by the Citizens of Paso Robles. Pretty clear.
Your idea of charging based on a special assesment rather than the amount of water a property uses means that people who use little water, but live on a more expensive property will pay more than their share, based on their water usage. In addition, how do you deal with a rental unit where the renter is supposed to pay the water bill? Your idea is inherantly unfair, but my guess is it will favor you, so you don’t care if it is unfair and other people would pay part of your share. You also have decided that since I have alternative ideas to yours I must work for, or have business with the City of Paso Robles, another attempt to deflect from the issue. However, once again, I have never worked for the City or have ever done business with the City and don’t expect to ever do business with the City. I do own property in the City which is why I pay attention. Finally, it turns out there needs to be a fix to the housing element laws, the existance of which you are questioning. As I understand it the fix to the housing elemnt laws is because of some lawsuit. This should give you all the information you need to educate yourself on the California Housing Element laws that every city and county in California has to follow:
BILL ANALYSIS ————————————————————
|SENATE RULES COMMITTEE | AB 602|
|Office of Senate Floor Analyses | |
|1020 N Street, Suite 524 | |
|(916) 651-1520 Fax: (916) | |
|327-4478 | |
———————————————————— THIRD READING
Bill No: AB 602
Author: Feuer (D), et al
Amended: 8/20/10 in Senate
SENATE TRANSPORTATION & HOUSING COMM : 6-3, 6/29/10
AYES: Lowenthal, DeSaulnier, Kehoe, Pavley, Simitian, Wolk
NOES: Huff, Ashburn, Harman
ASSEMBLY FLOOR : Not relevant
SUBJECT : Statute of limitations on housing element
SOURCE : California Rural Legal Assistance Foundation
Housing California DIGEST : This bill states the intent of the Legislature
in enacting this bill to modify the courts opinion in Urban
Habitat Program v. City of Pleasanton (2008) 164
Cal.App.4th 1561, with respect to the interpretation of
Section 65009 of the Government Code, and revises and
clarifies statute of limitations and remedies for specified
housing related challenges.
Senate Floor Amendments of 8/20/10 revise the statute of
limitations and remedies for specified housing-related
ANALYSIS : The Planning and Zoning Law requires cities
and counties to prepare and adopt a general plan, including
a housing element, to guide the future growth of a
community. Following a staggered statutory schedule,
cities and counties located within the territory of a
metropolitan planning organization (MPO) must revise their
housing elements every eight years, and cities and counties
in rural non-MPO regions must revise their housing elements
every five years. These five- and eight-year periods are
known as the housing element planning period.
Before each revision, each community is assigned its fair
share of housing for each income category through the
regional housing needs assessment (RHNA) process. A
housing element must identify and analyze existing and
projected housing needs, identify adequate sites with
appropriate zoning to meet its share of the RHNA, and
ensure that regulatory systems provide opportunities for,
and do not unduly constrain, housing development. The
reviews both draft and adopted housing elements to
determine whether or not they are in substantial compliance
with the law. The Planning and Zoning Law and the Subdivision Map Act
also includes a number of sections governing zoning and
entitlements specifically related to housing, including:
? The Housing Accountability Act, which requires a city or
county to make one or more specified findings in order to
disapprove a particular housing development.
? A provision requiring cities and counties, when adopting
an ordinance which limits the number of housing units
which may be constructed on an annual basis, to make
findings as to the public health, safety, and welfare
benefits that justify reducing the housing opportunities
of the region. ? Density bonus law, which requires cities and counties to
grant a developer a density bonus, incentives, and
concessions when the developer proposes to include
specified percentages of affordable housing within a
development. ? The Least Cost Zoning Law, which requires cities and AB 602
counties to designate and zone sufficient vacant land for
residential use with appropriate standards to meet
housing needs for all income categories and to contribute
to producing housing at the lowest possible cost.
? A requirement that, when determining whether to approve a
tentative subdivision map, a city or county shall apply
only those ordinances, policies, and standards in effect
as of the date the developer’s application is deemed
Prior to a recent court decision, it was understood that
current law allowed a party to challenge the adequacy of a
city’s or county’s housing element at any time during a
planning period, provided that the challenger brought the
action “in support of or to encourage or facilitate the
development of housing that would increase the community’s
supply of [affordable] housing.” The challenging party was
required first to serve the city or county with a notice
identifying the deficiencies in the housing element. After
60 days or the date on which the city or county took final
action in response to the notice, whichever occurred first,
the challenging party had one year to file the action in
court. This process and statute of limitations also
applied to actions brought pursuant to the housing-related
statutes listed above. In 2006 Urban Habitat Program brought suit to challenge the
City of Pleasanton’s housing policies, including the city’s
annual cap on housing permits and the city’s cap on the
aggregate number of permissible housing units, both of
which Urban Habitat claimed were insufficient to allow the
city to meet its RHNA obligation. In 2008, the First
District California Court of Appeals issued an unpublished
decision in the case of Urban Habitat Program v. City of
Pleasanton allowing the case to proceed with respect to
some causes of action, but ruling that the challenge to the
housing element itself was time-barred. The court stated:
Although the statute does not specify the time within
which [a deficiency] notice must be given, it is our
conclusion that the statute must be interpreted as
containing a time limit within which this requirement
must be met? In sum, a party bringing a challenge AB 602
governed by section 65009, subdivision (d), has 90
days from the date a legislative action is taken or
approval is given to notify the local land use
authority of any claimed deficiencies in such an
action or approval. Its claim then accrues 60 days
after it gives this notice.
In other words, instead of being able to initiate a
challenge to a deficient housing element at any time during
the planning period, housing advocates and other interested
parties may now only initiate such a challenge by
submitting a deficiency notice within 90 days of the
housing element’s adoption.
1.Removes from the current list of city or county actions
which may be challenged pursuant to Government Code 65009
notice and accrual provisions those actions related to
the Housing Accountability Act, the Subdivision Map Act,
and the application of a Density Bonus ordinance to a
particular project, all of which are project-specific
actions. The bill maintains the ability to use these
notice and accrual provisions to challenge the adequacy
of a city’s or county’s density bonus ordinance
2.Extends lengthening the time in which a deficiency notice
may be served to cover all remaining city or county
actions described in this section of law, as opposed to
just housing element challenges. In other words, the
amendments apply the longer timeframe to serve the
deficiency notice to actions relating to the Least Cost
Zoning Law, annual limits on housing permits, and the
adequacy of a density bonus ordinance, in addition to
housing element law. 3.Provides that an entity challenging such an action in
support of affordable housing may serve the deficiency
notice up to five years after the city’s or county’s
action. After 60 days or the date on which the city or
county takes final action in response to the notice,
whichever occurs first, the challenging party has one
year to file an action in court, except that the lawsuit AB 602
may not be filed more than five years after the city’s or
county’s action. In other words, the entity must file
the lawsuit within one year of the expiration of the
deficiency notice or within five years of the city’s or
county’s action, whichever occurs first.
4.Provides that a housing element from a prior planning
period may not be challenged if the city or county has
adopted a revised housing element for the new planning
Government Code 65755 . Current law requires a court, if it
finds any portion of a general plan, including a housing
element, out of compliance with the law, to include within
its order or judgment one or more of the following remedies
for any or all types of developments or any or all
geographic segments of the city or county until the city or
county has complied with the law:
? Suspend the authority of the city or county to
issue building permits.
grant zoning changes and/or variances.
grant subdivision map approvals.
? Mandate the approval of building permits for
residential housing that meet specified criteria.
? Mandate the approval of final subdivision maps for
housing projects that meet specified criteria.
? Mandate the approval of tentative subdivision maps
for residential housing projects that meet specified
This bill clarifies that in any action or proceeding
brought pursuant to the notice and accrual provisions of
Government Code Section 65009 described above, neither the
court remedies described above nor any injunction against
the development of a housing project shall abrogate,
impair, or otherwise interfere with the full exercise of
the rights and protections granted to an applicant for a
tentative map or a vesting tentative map under specified
provisions of the Subdivision Map Act or to a developer
under a specified provision relating to development AB 602
Under current law, HCD operates a number of grant programs
to which cities and counties may apply. In many cases, the
law requires a city or county to have an HCD-approved
housing element in order to be eligible for funding. This bill provides that if a third-party challenges the
adequacy of a housing element in court and the court finds
that the housing element substantially complies with all of
the requirements of housing element law, the element shall
be deemed to be in compliance for purposes of state housing
The statutory language interpreted by the court and at
issue in this bill was added to statute by AB 998 (Waters),
Chapter 1138, Statutes of 1983, a bill sponsored by the
League of California Cities and the California Building
Industry Association. AB 998 created a short statute of
limitations period for land use decisions generally but
provided a specific exception to protect the ability to
challenge deficient housing elements. The Senate Housing
and Land Use Committee and the Senate Third Reading
analysis of the bill stated that the bill:
Specifies that for challenges in support of low- and
moderate-income housing requirements, the petitioner
shall notice local government 60 days prior to filing
action. The [one-year] statute of limitations then
begins on the first day the legislative body fails to
In the intervening 25 years prior to the Urban Habitat
ruling, housing advocates filed and successfully settled at
least ten cases in which the 60-day deficiency notice was
sent more than 90 days after adoption of the city’s or
county’s housing element. In none of these cases was the
timeliness on the advocates’ suit contested. Likewise, six
bills amended other portions of this statute during those
intervening years, and there was never any controversy
surrounding the lack of a deadline for housing advocates to
serve a deficiency notice nor any attempt to change the AB 602
statute in this regard. Current level of housing element compliance . According to
HCD’s website as of June 7, 2010, only 46 percent of cities
and counties have adopted an HCD-approved housing element
for the current planning period that began in 2005 for the
San Diego region, 2008 for the Southern California, Fresno,
Kern, and Sacramento regions, and the summer of 2009 for
the remaining areas of the state. Unlocking the private market . The purpose of housing
element law is to create opportunities for the private
housing market to function. Builders cannot build without
access to appropriately zoned land, and current land use
plans in many cities and counties in California fail to
provide sufficient opportunities to accommodate projected
population growth. The San Diego Association of
Governments’ Regional Comprehensive Plan describes this
typical California paradox in the following way:
Under current plans and policies, more than 90 percent
of [the San Diego region’s] remaining vacant land
designated for housing is planned for densities of
less than one home per acre, and most is in the rural
back country areas dependent upon scarce groundwater
supplies. And of the remaining vacant land planned for
housing in the 18 incorporated cities, only about
seven percent is planned for multifamily housing. When
taken together, the current land use plans of the 19
local jurisdictions do not accommodate the amount of
growth anticipated in our region. SANDAG’s population
forecast, which reflects the current adopted local
land use plans in the region, projects that while
population will increase by 37 percent by 2030,
housing will grow by just 30 percent. The forecast
shows that if local plans are not changed, demand for
housing will continue to outpace the supply, just as
Housing element law addresses this problem directly by
requiring cities and counties to zone land at appropriate
densities to accommodate the projected housing needs of all
income groups and to remove constraints that prevent such
sites from being developed at the allowed densities. AB 602
Cities and counties, however, are not required to build
housing because that is the role of private developers.
The law holds cities and counties accountable only for that
which they control: zoning and land use entitlements.
Without the ability to enforce housing element law, the
market’s ability to meet housing demand may well remain
locked up.
FISCAL EFFECT : Appropriation: No Fiscal Com.: No
SUPPORT : (Verified 8/23/10)
California Rural Legal Assistance Foundation (co-source)
Housing California (co-source)
Advocates for Affordable Homes in Fremont
California Coalition for Rural Housing
Community Housing Improvement Program
Community Housing Works
Eden Housing
Fair Housing of Marin
Grassroots Leadership Network of Marin
Kennedy Commission
Public Advocates, Inc
San Diego Housing Federation
Self-Help Enterprises
Sierra Club of California
American Planning Association, California Chapter
JA:nl 8/23/10 Senate Floor Analyses SUPPORT/OPPOSITION: SEE ABOVE
pasoobserver says:	09/11/2010 at 11:17 pm
To whatisup — Thank you for your response to my comments. However, you failed to answer some of my questions that I mentioned to you. It’s almost like dealing with some City officials. They just let the public vent at their bimonthly council meetings. In my opinion, it’s difficult to deal with narcissism and arrogance. Over the years, there has been some very good input to our elected officials on how to proceed on the Nacimiento water pipeline,but it fell on deaf ears. You wanted me to answer some of your questions,but you did not answer some of my questions. Again, are you willing to subsidize new development?,Yes?or No?, are you willing to pay for a commodity that you are not receiving? Yes?or No? and another question for you. Are you willing to pay over 300% on your water bills within the five (5) year plan that the City has proposed? Also, the water rates will be subject to later increases too. By the way, I do concur with the city’s plan of “you pay for the amount of water units you use”. (748 gal=one unit). However, the higher water rates are not good for our senior citizens on fixed incomes and other struggling families in our community. My first suggestion years ago was desalination. The response was it was too expensive. Of course, now it is more expensive. I would suggest that our elected officials recall the existing bonds (The bonds can be recalled early). The City council can explain to the citizens in detail with financing of new bonds at a lower interest rate as of now for the sewer plant and Nacimiento water pipeline and present their new proposal in compliance with Proposition 218. Let the citizens of Paso VOTE on the financing bonds for their approval. Most of the citizens,that I had spoken to were not happy with the way our City Council handled the Nacimiento water pipeline project. The citizens of Paso didn’t give our City Council a “BLANK CHECK” for $176 million to spend without voter approval. I would suggest that it be a “special tax” or “an assessment” be levied on our property taxes. A percentage of those bonds can be deducted on Federal Income taxes. As it is now, a” fee” on a capital funding project is not deductible. Of course, there are homeowners would not go for this suggestion due to our poor economy. My analogy mentioned above would be, you would get something back on a “special tax” or an “assessment” verses nothing on a “fee”. What say you?
whatisup says:	09/12/2010 at 9:02 am
Unfortunately the law says we have to subsidize new development in California. I don’t like it, but it is the law. I know paying using the property taxes was bandied about. The argument against it was it would mean some would be paying for water they aren’t using and others could be big water users, but pay a small special assessment on their property taxes. I think the decision that was made to base it on usage was out of fairness. It seems to me if people are using water and not paying their share of the costs it is not fair. The Senior issue is very difficult. If someone is retired for twenty years is it realistic to think prices don’t go up during the 20 years of retirement. Think what prices were in 1990 compared to today. Should Seniors never have to pay for capital improvements? Paso Robles also had very low water rates. Rates that are no longer possible given the circumstances. Desalination will happen eventually. California is out of water. If you want to pay $1,000,000 a gallon there is no more allotable water of any consequence in California. The expense will be tremendous — still have to build a desalination plant, still have to build a pipeline. I don’t know if the plant has to be built along the ocean or if the salt water could be piped over to Paso Robles. If it has to be built along the ocean, Paso Robles doesn’t own land on the ocean and, in any case, the environmentalists will keep it in courts for years as they have done so for other proposed desalination plants in Southern California. Eventually necessity will force desalination past the environmentalists, but not yet.
pasojim says:	09/13/2010 at 7:46 am
Whatisup – On one of your previous post you made the comment you haven’t heard any of the legal suggestions for the water issue, But you obviously have. That is a good thing. So we can move the discussion ahead.
Once, again this was handled incorrectly by our city custodians from the beginning. And now here we are. The public is not supporting this very expensive, very limited benefit project. As you said, until a plan is developed that the public can support, things don’t look good.
All this discussion about the water issue has only reinforced my opinion the issue hasn’t been about water, only how the plan should be paid for. Or more specifically, to what extent do we allow our elected custodians and our un-elected GOD tzar decide which laws they will follow and which laws they will ignore. When the City GOD tzar tell citizens at a council meeting if we don’t agree with the City’s plan, then we should just sue him, and when the City Attorney explains to a citizen at a City Council meeting that she does have to respond to their questions because she does NOT work for them. When the project is voted down by the citizens and the council brings it right back up, it is clear that our elected representatives are not doing their job providing direction to their employees and listening to and representing the CITIZENS.
The subject of the original post was the need to elect different representation. I think with all the conversation made on this post, as well as the post on Cal Coast about the hiring of the new legal firm you were involved in, Supports my original opinion.",['Fairness.'],5701,multifieldqa_en,en,,8fbf0a6531d9250e6bcda0c7ba456441f6d4073bf08de826," Time to clean house in Paso Robles Home. Jim Reed: It seems that decisions are made based on some agenda, even if it is contrary to citizens’ wishes. The rate increase is being hung up in the courts by the City Attorney. We are already paying several City employees $12,000 per month with equally ridiculous benefits and pensions. We have the opportunity to elect some responsible, principled people to represent us. If we elect more people from within this system, we will get more of the same type of government. We need to look at where the new candidates stand. Will they lawfully represent the citizens of the city? Or, are they happy with the way things are being run? We have stood together in the past and have made real significant changes in important matters that are going to affect our lives for years to come. There are several thousand citizens that made their voice heard on the water issue, more than enough votes to make a change in our city government. The comments below represent the opinion of the writer and do not represent the views or policies of CalCoastNews.com. (moderator@calcoastnews.com Comment Guidelines ) Back to Mail Online home. Back to the page you came from. Click here to read the rest of the article. For confidential support on suicide matters call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. For support in the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. For information on how to help a friend in need, go to http://www.suicidespreparation.org/. For confidential help in the United States, call the Salvation Army at 1-877-457-9090 or visit http:// www.sending.it/helping-a-friend-in-need or   http://www.-sending-it-back-to-us.org/news/article/stories/article-news/top-stories/top stories/topstories/stories-top- stories/story/stories%20stories/story%20on%20of%20the%20top%20tales%20in%20Paso%20Robles%20Home.html%20with%20Jim%20Reed%20and%20Opinion%,%20JIM%20Reed%20&story%2C%2A,%20#story%3A,%.%20jim%20reed%2B%,%2E%20previous%20post%20by%20him%,%3C%20James%20Rebel%20Eddie%20Mann%20Alden%20Stebeck, %20-%20Gilman,  &%2S%20Nick Gilman &%20Duane Picanco, on August 19th, voted unanimously to hire the same law firm employed by City of Bell. The department will determine each region’s existing and projectedhousing need pursuant to Section 65584.01 at least two years prior to the scheduled revision. The department will adopt a final regional housing need plan that allocates a share of the regionalhousing need to each city, county, or city and county. “Household income levels” are as of the most recent decennial census. AB 602 is not the LAW as you so stated in your blog. I contacted the Deputy Chief Council's office in Sacramento to confirm your misstatement of facts. You know,in the language, it’t so difficult to answer some simple questions with a “YES” or “NO” answer. Yet, you are reluctant to do so, but you go on and on with a thesis along with some rhetoric. I am sorry to inform you but AB 601 is the law, and you are wrong about it. You should have read the bill before you wrote your blog post. You are wrong, and I’m sorry to tell you that you’re wrong, but I am also sorry to say that AB 603 is the bill, and that you are incorrect about it as well. You’ve got a long way to go to get the bill you want, but at least you have your facts straight. The bill is the result of an effort by the California Environmental Quality Act (Division 13 (commencing with Section 21000) of the Public Resources Code) and the California Department of Health and Human Services (DHS) to protect the health and safety of Californians. The DHS is responsible for the DHS’ health and human rights, environment, and public safety programs. It is the responsibility of the DHHS to ensure that all residents of the state have access to the highest quality health care and safety services, including affordable housing, and to ensure the safety of children and the environment. The legislation is not meant to replace the DHR, but to provide a tool for the department to help it meet its responsibilities to the state and its citizens. It also provides a tool to help the department meet its obligations to the residents of California, including the need for affordable housing. The law is not intended to provide for the creation of new housing, but rather to provide an alternative to the current housing shortage. It does not provide for new housing that will be built to meet the needs of the region, and it does not allow for the construction of new homes that will meet those needs. It only provides for the allocation of housing that meets certain criteria, such as the “housing need” criteria. The goal of the bill is to increase the housing supply and the mix of housing types, tenure, and affordability in all cities and counties within the region in an equitable manner, which shall result in each city or county receiving an allocation of units for low- and very low income households. The plan must be consistent with all of the following objectives:. Promoting infill development and socioeconomic equity, theprotection of environmental and agricultural resources, and the. efficient development patterns. Allocating a lower proportion of housing need to an income category when a jurisdiction already has a disproportionately high share of households in that category, as compared to the countywide distribution of households. I am well aware that most of the wineries are outside the city limits using the same aquifer. What I gave you is a proposed small adjustment in the wide range of laws that make up the California Housing element. By the way, I believe you can review the Paso Robles Housing element plan on the City’s website or at the Library. The California Housing Element Laws that all cities and counties have to follow have been in place for almost 25 years. Either educate yourself or keep making a fool of yourself, your choice. Good Luck! To Whatisup — I WOULD LIKE TO KNOW WHAT LAW YOU ARE REFERRING to THAT SAYS “WE’S THE PEOPLE HAVE TO SUBSIDIZE NEW DEVELOPMENT? AGAIN, FOR the THIRD TIME, YOU FAILED TO ANSWER MY QUESTIONS POSED TO YOU IN MY PRIOR RESPONSES TO YOU ON SEPT.10TH &11TH. IS THERE A REASON WHY YOU DON’t WANT TO AnSWER THEM? YOU DO WHAT OUR ELECTED OFFICIALS DO SO WELL, AND THAT IS “IN ONE EAR AND OUT OF THE OTHER EAR” It SEEMS TO ME THAT YOU ARE EITHER EMPLOYED BY THE CITY OR YOU HAVE OTHER DEALING WITH THE CITY, SO BE IT. IT APPEARS TO ME that YOU THINK THE CITY DOES EVERYTHING RIGHT. YOU PRESENT YOURSELF AS BEING VERY BIAS ON CITY DECISIONS. IT LIKE THEY CAN’T DO ANYTHING WRONG ACCORDING TO YOUR LOGIC. THEY KNOW WHAT IS BEST FOR THE CITIZENS OF PASO,THAT IS A GOOD EXAMPLE OF ARROGANCE ALONG WITH NARCISSISM. What is the COST PER UNIT OF WATER USAGE IN TEMPLETON COMPARED TO PASo? OF COURSE, TEMPLON IS IN AN UNINCORPORATED AREA (COUNTY JURISDICTION). OF course, TENETTE is in an Unincorporated Area (Citizen JurisDICTion) I GAVE YOU SOME SUGGESTIONS ON HOW TO PAY FOR THE NACIMIENTO WATER PIPELINE AND SEWER TREATMENT PLANT. A PERCENTAGE PLANT PLANT? A PER CENTAGE PLant PLANT AND STOP THE PROPORTION WITH STATE PROPIANCE WITH CIRCUMVENTING CIRCumventING TAX ON YOUR PROPERTY TO FINANCE SOME NEW TAX TAX BILLS ON YOUR BILL. HOW ABOUT THAT YOU HAVE ANY KIDS TO GO TO SCHOOL? I’m quite sure if you really contacted the Deputy Chief Counsel”s Office you have been set straight. You are just being a popinjay and rhetorical. Also, I was not ranting about thewineries usage of water. I related to you on your vague question about “there are people not paying their fair share for their use of water” I related, are you talking about the WINERIES USING the SAME AQUIFER? I am sure you are capable of doing your own homework, I also am quite sure that you have a clue how to look the laws up. It appears to me that you think the City does EVERYTHING right. ALSO, REMEMBER it's the CitIZENS’ MONEY THAT IS BEING SPENT. he City and the City can't do anything about it. They wineries are allowed to take the water they are taking even if it drops the City’s water levels in their wells. It sounds like you just don’t want to pay anything for the infrastructure because you really just don't want it built. Your idea of charging based on a special assesment rather than the amount of water a property uses means that people who use little water, but live on a more expensive property will pay more than their share. In addition, how do you deal with a rental unit where the renter is supposed to pay the water bill? Your idea is inherantly unfair, but my guess is it will favor you, so you don't care if it is unfair and other people would pay part of your share. Finally, it turns out there needs to be a fix to the housing element laws, the existance of which you are questioning. This should give you all the information you need to educate yourself on the California Housing Element laws that every city and county in California has to follow. I have never worked for the City or have ever done business with the City and don't expect to ever do business withThe City. I do own property in the City which is why I pay attention. I don't know why you think Paso Robles is in need of a water treatment facility, but I do know that the City needs to find a way to make sure that the water supply is safe for the wineries and other businesses that use it. You need to complain to Sacramento. The City is in desperate need of the water. The wineries need to stop using the water so that they can continue to grow their crops. The city needs to make a decision on how to deal with the water crisis. The water crisis is a serious problem. The people of the City need to get involved and do something about it now. It is time to stop living in fear of water and start living in a safe, clean, and healthy community. The time to act is now, and the only way to do it is to get out of this mess is to live in a healthy, safe, and sustainable community. I hope you will join me in supporting this effort. I’m looking forward to seeing you in the next few weeks as we continue to work together to build a better, more sustainable, and more beautiful Paso Roble. I love the City of Paso RobLES. I will see you on the golf course next week. I can’d love to see you at the races. I want to meet with you again. I just want to thank you for all your hard work and support this project. I appreciate your support and all the hard work you’ve put into it. I am so grateful for your support. I wish you the best of luck in the future. I look forward to meeting with you in person next week to discuss the future of the city and the water system in California. I also hope that you will be able to help me in some way in the years to come. Thank you for your time and I appreciate all of your support for this project, and I hope to hear from you again in the coming weeks and months. I sincerely appreciate your continued support and support. Housing Accountability Act requires a city or county to make one or more specified findings in order todisapprove a particular housing development. Density bonus law requires cities and counties togrant a developer a density bonus, incentives, and concessions when the developer proposes to includespecified percentages of affordable housing within a development. Least Cost Zoning Law requires cities to designate and zone sufficient vacant land for vacant land with appropriate standards to meet housing needs for all income categories and to contribute to producing housing at the lowest possible cost. In 2006 Urban Habitat Program brought suit to challenge the city of Pleasanton’s housing policies. In 2008, the First.District California Court of Appeals issued an unpublished decision in the case of Urban. Habitat. Program v. City of. Pleasanton allowing the case to proceed with respect to some causes of action, but ruling that the challenge to the. housing element itself was time-barred. In sum, a party bringing a challenge AB 602Governed by section 65009, subdivision (d), has 90 days from the date a legislative action is taken or approval is given to notify the local land use authority of any claimed deficiencies in such an action or approval. Its claim then accrues 60 days after it gives this notice. The challenging party was first to serve the city or. county with a notice identifying the deficiencies in the housing element. After 60 days or the date on which the city. or county took final action in response to the notice, whichever occurred first, the challenging party had one year to file the action in. court. In other words, instead of being able to initiate a. deficient housing element at any time during the planning period, housing advocates and other interested parties may now only initiate such a challenge by submitting a deficiency notice within 90 days of the housing elements’ adoption. The current law allows a party to challenge. the adequacy of a city’“housing element” during a Planning and Zoning. period, provided that the challenger brought the. action “in support of or to encourage or facilitate the [affordable] development of housing that would increase the community’S supply of housing.” The challenge must be made within a year of the city/county”s approval of such a housing element, according to the law. The law also includes a number of sections governing zoning and.entitlements specifically related to housing, including the Housing Accountability Act and the Subdivision Map Act. It is unclear how the law will be applied to housing development in the future, but it is possible that the law could be amended to make it easier to apply to all housing developments in the state, not just certain types of development. The California Housing Authority Act could be used as a model for the state. The state could use the law to create a “housing development code” to guide local governments in the construction of new housing developments. It would be possible to apply the law in a way that makes it easier for developers to comply with the housing accountability Act. The bill extends the time in which a deficiency notice may be served to cover all remaining city or county actions described in this section of law. The bill maintains the ability to use these notices and accrual provisions to challenge the adequacy of a city’s or county’S density bonus ordinance. AB 602 provides that a housing element from a prior planning period may not be challenged if the city and county has adopted a revised housing element for the new planning code. It also clarifies that in any action or proceeding brought pursuant to the notice and acculal provisions of the bill, neither the court remedies described above nor any injunction against the development of a housing project shall abrogate, or otherwise interfere with the full exercise of the rights and protections granted to an applicant for a tentative map or a developer under specifiedprovisions of the Subdivision Map Act. The Senate Housing and Land Use Committee and the Sena Committee on Housing have voted in favor of this bill and it is expected to be signed into law by Gov. Jerry Brown in the coming weeks. The legislation was sponsored by the League of California Cities and the California BuildingIndustry Association. It was added to statute by AB 998 (Waters), Chapter 1138, Statutes of 1983, and passed by the Senate Housing Committee and Sena on November 6, 2013. It has been referred to as the Housing Accountability Act, or HAC Act, by the California House of Representatives and the Senate Land Use and Housing Committee, both of which have voted to approve the bill. It will go to the governor's desk for a hearing on December 11, 2013, and then to the Assembly's Housing Committee on December 13, 2013,. The bill is sponsored by Sen. Bob Hertzberg, D-Sacramento, and Sen. Mike McGuire, R-Concord, and Rep. Ted Lieu, D/Condo, of San Diego. It is expected that the Assembly will vote on the bill on December 14, 2013; it is scheduled to go to a vote on December 17, 2014, and it will be voted on by the Assembly on December 18, 2014. It's expected that it will pass by the end of the month. It would then go to Gov. Brown's desk on December 19, 2014; it would go into effect on January 1, 2015, and would be signed by the governor on January 2, 2015. It goes into effect the following day. It then goes on to take effect the next day, January 15, 2015; and it goes into force on January 17, 2015 (the first day of the second week of the third week of January 2015). The bill will take effect at the beginning of the fourth quarter of this year, and the fourth week of next year, if it is approved by the House and Senate. It takes effect the same way for the first and second weeks of each year. It applies to the Least Cost Zoning Law, annual limits on housing permits and the application of a Density Bonus ordinance to a particular project. Housing element law is to create opportunities for the private housing market to function. Builders cannot build without appropriately zoned land, and current land use plans in many cities and counties in California fail to provide sufficient opportunities to accommodate projected population growth. AB 602 would require housing advocates to serve a 60-day deficiency notice to local government before filing a lawsuit. Only 46 percent of cities                and counties have adopted an HCD-approved housing element for the current planning period that began in 2005 for the San Diego region, 2008 for the Southern California, Fresno, purposefullyKern, and Sacramento regions, and the summer of 2009 for the remaining areas of the state. The law holds cities  accountable only for that which they control: zoning and land use entitlements. Without the ability to enforce housing element law, the private market’s ability to meet housing demand may well remainlocked up. The bill has no Fiscal Com: No Fiscal Com.: No funding for the housing element program. It has no fiscal impact: No fiscal impact for the Housing Element Program. It will not have any impact on the state’S housing assistance programs. It would not have an impact on housing subsidies for low- and moderate-income families. It is not expected to have an effect on housing assistance for the elderly or the disabled. It does not have a financial impact on any of the housing programs that are funded by the state or the federal government. It could have a negative impact on affordable housing programs, however, as it does not provide funding for housing programs for the disabled or the elderly. It also does not address the issue of affordable housing for the chronically ill or those with disabilities. It doesn’t address the problem of homelessness, which is a major problem in the U.S. and in many European countries. It only addresses the problem by requiring the state and local governments to provide housing assistance to those who need it the most. It provides a way for the state to help those who are struggling to find housing. It gives the state a way to help the homeless and those who have been displaced by natural disasters. It helps the state meet its housing needs for the first time in a long time. It creates an opportunity for California to become a leader in the affordable housing movement in the United States. It makes it easier for people in need of housing to get the help they need. It protects the environment and provides a voice for the voiceless in a way that no other law has done in the past 25 years. It ensures that the state has a voice in the decision-making process on housing issues. It allows the state an opportunity to have a say in how much housing is built and how much it is built. And it provides a tool for the public to have its voice in deciding how much of it is available to be built and built and what type of housing it will be built. In the intervening 25 years prior to the Urban Habitat ruling, housing advocates filed and successfully settled atleast ten cases in which the 60- day deficiency notice was sent more than 90 days after adoption of the city’‘s housing element.’ I concur with the city’s plan of “you pay for the amount of water units you use” (748 gal=one unit). However, the higher water rates are not good for our senior citizens on fixed incomes and other struggling families in our community. My first suggestion years ago was desalination. The response was it was too expensive. Of course, now it is more expensive. I would suggest that our elected officials recall the existing bonds (The bonds can be recalled early). The City council can explain to the citizens in detail with financing of new bonds at a lower interest rate as of now for the sewer plant and Nacimiento water pipeline and present their new proposal in compliance with Proposition 218. Let the citizens of Paso VOTE on the financing bonds for their approval. I think the decision that was made to base it on usage was out of fairness. It seems to me if people are using water and not paying their share of the costs it is not fair. The Senior issue is very difficult. If someone is retired for twenty years is it realistic to think prices don’t go up during the 20 years of retirement. Think what prices were in 1990 compared to today. Should Seniors never have to pay for capital improvements? Paso Robles also had very low water rates. Rates that are no longer possible given the circumstances. Desalination will happen eventually. California is out of water. If you want to pay $1,000,000 a gallon there is no more allotable water of any consequence in California. If it has to be built along the ocean, Paso’les doesn�t own land on the ocean and water is overed over the ocean. Eventually they have done so for other proposed desalinated plants in Southern California. Eventually it will be necessitititly as they have. The expense will be tremendous — still have to build a desalinations plant, still haveto build a pipeline. I’m not sure if the plant has to been built along. the ocean or if the salt could be piped to Paso. Robles. If the ocean is not available, the environmentalists will keep it in the courts for years for so for. years. I don�’d like it, but it is the law. What say you? The law says we have to subsidize new development in California, and I don't like it. The argument against it was it would mean some would be paying for water they aren’T using and others could be big water users, but pay a small special assessment on their property taxes. What do you think? Do you agree with the plan? Are you willing to pay over 300% on your water bills within the five (5) year plan that the City has proposed? Also, the water rates will be subject to later increases too. You wanted me to answer some of your questions that I mentioned to you. You did not answers some of my questions. It's almost like dealing with some City officials. They just let the public vent at their bimonthly council meetings. In my opinion, it's difficult to deal with narcissism and arrogance. The issue hasn’t been about water, only how the plan should be paid for. When the project is voted down by the citizens and the council brings it right back up, it is clear that our elected representatives are not doing their job. The subject of the original post was the need to elect different representation. I think with all the conversation made on this post, as well as the post on Cal Coast about the hiring of the new legal firm you were involved in, Supports my original opinion. The public is not supporting this very expensive, very limited benefit project. Until a plan is developed that the public can support, things don't look good for the city of San Francisco. The city needs to find a way to pay for the water without having to rely on the federal government to do it. The state needs to work with the U.S. Army Corps of Engineers to develop a long-term vision for the future of the San Francisco Bay Area. The City needs to make a decision on how it wants to use the water. The current plan is not the right one. It would be better if the city worked with the state to develop an alternative plan that would be more cost-effective. It is time to find out what the state plans to do with the water, and how it will use it."
What did Mary tell the disciples?,"A Homily from Easter Sunday, 2017.
Early on the first day of the week, while it was still dark, Mary Magdalene came to the tomb and saw that the stone had been removed from the tomb. But Mary stood weeping outside the tomb. As she wept, she bent over to look[a] into the tomb; and she saw two angels in white, sitting where the body of Jesus had been lying, one at the head and the other at the feet. They said to her, “Woman, why are you weeping?” She said to them, “They have taken away my Lord, and I do not know where they have laid him.” When she had said this, she turned around and saw Jesus standing there, but she did not know that it was Jesus. Jesus said to her, “Woman, why are you weeping? Whom are you looking for?” Supposing him to be the gardener, she said to him, “Sir, if you have carried him away, tell me where you have laid him, and I will take him away.” Jesus said to her, “Mary!” She turned and said to him in Hebrew,[b] “Rabbouni!” (which means Teacher). Jesus said to her, “Do not hold on to me, because I have not yet ascended to the Father. But go to my brothers and say to them, ‘I am ascending to my Father and your Father, to my God and your God.’” Mary Magdalene went and announced to the disciples, “I have seen the Lord”; and she told them that he had said these things to her.
Early in the morning, while it was still dark, Mary wept in the throws of grief. Early in the morning, while it was still dark, Mary dragged herself out of bed after a sleepless night and walked to the tomb in a kind of trance. Early in the morning, while it was still dark, Mary cried—scared, confused, alone. Early in the morning, while it was still dark, Mary thought that the powers of death had the last Word. Early in the morning, while it was still dark, Mary heard a voice in the darkness calling her name—Mary.
Throughout this Lenten season, we’ve examined the ways that the powers and principalities hold us captive—how they push us towards securing our own survival, dominating others, using God for our own agenda. We’ve seen how in Jesus’ ministry, he’s constantly in resistance mode—exposing the powers for what they really are and envisioning an alternative way of living in the world. He describes this way as “the kingdom of God,” the living water we drink so we never thirst again, the light of the world. Jesus invites those who follow him into similar acts of resistance—to free us from the power money has on us by giving it away, to choose to see ourselves as Jesus sees us, resisting the shame that says I’m not enough, to practice Sabbath that contradicts productivity, to untie the grave clothes of someone who’s hands and feet are still tied in the trappings of death.
But all Jesus’ acts of resistance had a cost. All of the times he just wouldn’t shut up, all of the crowds he attracted because he actually noticed those who were normally ignored, the powers finally said enough is enough and put an end to his resistance the only way they could guarantee silence and division—by nailing him to a tree.
Just then, she turned and saw a man the shadow of a man behind her; a man she assumed was the gardener, his face unfamiliar in the darkness. He repeated the question—“Woman, why are you crying?” Thinking that perhaps he knew what happened or worse, that he was a culprit, she begged, “Sir if you have carried him away, tell me where you have put him and I will get him.” But Jesus interrupted her pleading, interrupted her desperation, and called her by name from the darkness, Mary.
Mary. He calls her name. Her name. The name that captures the particularity of her life. To the gardener, she would just be the crying woman. At other points in her life, she was the possessed woman, the woman who wasn’t enough, the woman on the outside of the group. Never nameless—but still unnamed. Never not Mary, but still, not known.
Early in the morning, while it was still dark, God defeated the powers and principalities in the ultimate act of resistance—resurrection. The grave could not contain the Lord. Even death wasn’t enough.
In the resurrection, God defeats the powers of death and shows that it’s God who has the final Word. Nothing, not even death, can keep us from being fully known by God. The powers try to have the final say on our names, our identities, the markers by which we measure ourselves, the systems that hold people captive or keep people in oppression. But Jesus calls us out of the darkness by name.
On this Easter Sunday, we hear our Risen Lord calling our names from the darkness—Jesus, the resurrected one, the name above all names, the great I am, the Prince of Peace, the alpha and omega, the light of the world. The risen Lord has spoken.
This is the name unto which you were baptized. As you come forward and mark the sign of the cross on your forehead today, hear Jesus speaking your name from the darkness and drawing you into the light.
From our worship service on the fifth Sunday of Lent, April 2, 2017.
“Is the Lord really with us or not?” “Is the Lord really with us or not?” Why did you bring us all the way from Egypt to let us die of thirst in this desert? At least in Egypt, we had water. At least in Egypt, we weren’t so thirsty. At least in Egypt, we knew what tomorrow would hold. At least in Egypt, we weren’t so thirsty.
But no, that’s not the story they give us. They are hard on their ancestors. They tell how it is. The elders who sat and wrote down these stories understood something about our bodies, who we are and how we work. After all the generations these stories passed through, they tell the truth about how quickly we forget, about how quickly we complain, about how quickly we grow thirsty, about how much we need water.
It doesn’t take long, does it. By the end of this sermon, I will no doubt feel thirsty, not from walking on hard dusty ground in the heat of the day, but just from speaking with you. Most of us wake up in the morning needing a drink. Our bodies depend on water. We cannot live without it. Thirst, then, doesn’t happen only one time. When the Israelites panicked that they had no water, they weren’t only thinking of the present moment. They knew what was coming! We need water to live! Without water, we will die! Even if we have water for today, we will need water again tomorrow! We can drink until we are satisfied, only to know that we will eventually be thirsty for more.
The gospel of John tells a story about a woman who gave up on this question all together. She moved beyond wondering if the Lord was really with her, so confident God had forgotten her that she gave up wondering at all. Born a Samaritan into a world that valued other bodies as better than her body: male bodies, Jewish bodies, even married bodies. Even after encountering Jesus, she still leaves their conversation without a name, numbered as one of many, simply called, “Samaritan woman.” She too, was thirsty. Most believe that her shame led her to drink water in the heat of the day, when no one else would be at the rocky well, when she could get a drink alone, without experiencing the stigma and stares of others. When she came to get a drink, Jesus was also at the well, thirsty himself and in need of rest and water from the long journey through Samaria.
The Israelites complaint for water sends Moses to the only one who can satisfy, the only one who can meet this need. Moses turns to God, “What should I do with these people? How can I satisfy their thirst? I’ve looked around, I’ve checked far and wide, turned the house upsidedown, looked under the seats of the car, at the bottle of every bottle, I’ve even looked for dew on the ground and under the lids of jars and there is no water to be found. Where do we go for water? Is the Lord really with us or not?
The Israelites who wrote down this story and allowed the ancestors to look like desperate complainers who doubted God and tested God, they were onto something. They knew that we are thirsty people. Jesus knew also. All who are thirsty, come! All who believe in me, drink this living water! We are desperate to feel God’s presence, to be bathed in the water of the Spirit, to know that this is not all there is, to feel a sense of belonging to the One who is greater than I. We can only make it so long in the desert, so long wandering from one trial to the next, without a drink.
And yet, Jesus also says, “Blessed are those who hunger and thirst for righteousness, for they will be filled. Notice with me: not blessed are those who are righteous, but those who thirst for righteousness. Not blessed are those who are righteous, but those who thirst for righteousness. Blessed are those who thirst for relationship with God, to know God, to see God.
I wonder, “Does Jesus want us to keep wanting?” Does Jesus want us to keep thirsting? Many faithful followers of Jesus throughout history have never claimed their thirst was quenched, never fully satisfied. You know that moment when you quench your thirst, when you sigh with relief when your throat is at ease once again, that’s the opposite of how many of God’s children have described the life of faith. They describe wanting more, being satisfied at times, while knowing they will be thirsty again.
What will be waiting for you at the rock? Will the water gush out, bursting forth, covering you from head to toe with God’s presence, drenching you in hope, cleansing you from the dust that’s caked to your feet and renewing you for a new day, a new hour, a new moment basking in the presence of God?
What will be waiting for you at the rock? Will the water drip slowly, quenching your thirst for but a moment, giving you just a glimpse of God’s spirit? Will it be so hard to get the water from the rock, that you’ll have to bend down, get underneath that dripping water to try and catch a drop? Will it be just enough for you to know, if only for a moment, that God is really with you? Will it be just enough to satisfy you for this hour, but keep you coming back for more?
What will be waiting for you at the rock? What if it seems like the water has run out, like there isn’t a drop left, the way that Mother Teresa described? What then? We follow her example. She still goes to the rock, over and over again, not to get water quench her own thirst, but to relieve the thirst of God’s other children.
This post was adapted from our sermon series on Interpreting Exodus. Pastor Megan preached this sermon at Butner Federal Prison complex on August 30, 2015.
On Father’s Day 2015, we gathered for worship at the labyrinth in front of UNC hospital, having devoted the month of June to exploring the question, “What happens after we die?” Many have watched their father’s die in this place or other similar spaces. We shared in a time of both remembrance and prayer/meditation, participating in the ancient spiritual practice of walking the labyrinth. A labyrinth is a kind of maze, laid out in a circle.
Tony graciously shared the following reflections from his experience at the labyrinth on the hot June day.
It’s smaller than I expected, stark and hard‐surfaced, with no landscaping for ornamentation or shade. I don’t know what to expect from it… or from myself. But that’s part of the appeal. I stand at the entrance, hesitating, trying to clear my mind. This doesn’t work very well, so I just start walking.
Almost immediately, the path presents itself as a linear and chronological symbol of my life’s journey. Like my physical lifetime, it has a beginning and an end, with an as‐yet undetermined amount between. This could be interesting. I like it so far… although I’m insecure about my style… and unsure about proper protocol. Is someone staring at me? Do I have to meditate? How slowly should I walk? Is it better to focus my thoughts… or to simply let them come? Will I control this thing, or allow it to control me?
I begin to see each step as an increment of elapsed time, an irretrievable expenditure of life energy. I equate my initial discomfort to the natural immaturity of my childhood years. I gradually move beyond it, into metaphorical adulthood. This is much better.
Most of the path is a series of gentle arcs. These are fairly easy to maneuver, like my comfortable life. But these segments are connected by intermittent sharp turns, mostly 180‐degree switchbacks. I see these as representing significant life changes or challenges, requiring more concentration and skill to negotiate. I notice that I am executing some of these turns mechanically, and some more gracefully. I begin to anticipate upcoming turns, and try to maintain good form around each one.
I can’t see much of the path ahead, nor the end. I spend a significant amount of mental energy dealing with this uncertainty, constantly wanting to know my real‐time ratio of “distance walked” to “distance remaining”. This is a recurring distraction.
Today is Father’s Day, and my Dad is on my mind. He recently completed his well‐walked journey, and is now watching me… even if as mere metaphor… or only as an element of my own (self‐) consciousness. I feel his presence embedded in his absence. I’m aware that it’s not only my turn to walk… it’s my only turn to walk.
I think about my children, grandson, soon‐to‐arrive granddaughter, and their descendants. The familiar succession of life, death and new life seems magical, divinely‐derived, and strangely better than living forever. My role is limited, but critical. I love the part, and embrace it.
I am acutely aware that others are journeying all around me. These are friends of mine. We meet, almost brushing, as we walk. The path seems purposefully narrow, perhaps perfectly so. I suddenly understand that it is impossible to walk this close to others without being affected by them. I affect them too… seen as small adjustments in their position or posture. As we meet, I try not to encroach too much, but making sure not to pull away. I put creative energy into maintaining the perfect degree of separation between our bodies. This feels like more art than science… each friend deserving a customized approach. This closeness seems good to me.
There is a much younger walker behind me, getting ever closer. I’m clearly holding her back. Maybe this means that the younger generation wants me to hurry up and get out of their way. I remind myself not to stretch the symbolism too far… as I pick up my pace.
I now see the end of the path ahead. I have been expecting this part to be emotionally complicated, but it is not. The final section is round… large and unrestrictive… a qualitative change from the narrow linear pathway. The circle opens up to welcome me. It is easy to step into, a perfectly natural thing to do at the end of my walk. Inside the circle, I am centered… comfortable… peaceful… thankful.
16 As Jesus passed alongside the Galilee Sea, he saw two brothers, Simon and Andrew, throwing fishing nets into the sea, for they were fishermen. 17 “Come, follow me,” he said, “and I’ll show you how to fish for people.” 18 Right away, they left their nets and followed him. 19 After going a little farther, he saw James and John, Zebedee’s sons, in their boat repairing the fishing nets. 20 At that very moment he called them. They followed him, leaving their father Zebedee in the boat with the hired workers.
This is a story about 4 fishermen, Simon, Andrew, James and John. It’s a normal morning at the docks. Each one of them is going about business as usual. They arrived at dawn, bundled up in the cool morning air and started work without much conversation. Simon and Andrew are working on one fishing boat and see the Teacher approaching. “Hey. There he is,” says Andrew. Jesus from Nazareth. You can’t go anywhere without hearing about him lately. What’s he doing down here?” They paddle back to shore, not wanting to miss any trouble this Jesus fellow might stir up. Simon and Andrew get the beach and Jesus comes over to talk to them. It’s like he had come there that morning just to find these two guys. Jesus didn’t say much, “Come and follow me.” Jesus invited these 2 fishermen to be his disciples, to follow after him, to walk behind him, tracing his every step.
Further down the beach, the same scene repeats. This time, Jesus walks directly up to James and John who are focused on repairing their fishing net. Jesus says the same thing to them and now all four fishermen walk behind their rabbi with no idea of what’s ahead of them.
It’s a big deal! The four normal guys, working a normal job, on a normal morning, decide to follow Jesus. Maybe you’ve wondered like I have, how is it that Simon, Andrew, James and John do it? How do they drop everything to follow Jesus? What were they thinking? How did they feel?
It’s interesting. The story doesn’t tell us. There’s nothing about how they felt. It doesn’t say they were excited, or moved, or scared, or joyful or resistant. This story about four fisherman gives us only verbs. Jesus passed alongside the Galilee Sea. He saw two brothers. He said, Come, follow. Then, Simon and Andrew left and followed. Jesus saw James and John. Jesus called them. They followed him.
This is a story about four fisherman who decided to follow Jesus.
This is also a story about fishing. I’ve been fishing been fishing three or four times. Once I realized that fishing was primarily a crack of dawn activity, I knew it wasn’t really for me. Jesus uses a kind of puzzling image about fishing. He says, “Come, follow me, and I’ll show you how to fish for people.” I don’t know about you, but this I find this to be very strange. I realized this week why his image is so confusing to me. What do you imagine when someone talks about fishing? What I imagine when I hear the word “fish” or “fishing” is a fishing pole, the rod, reel, bait, tackle box, worms, that kind of fishing. So I’ve always interpreted what Jesus said this way.
I will make you fishers of men, fishers of men, fishers of men. I will make you fishers of men, if you follow me.
The road Jesus invites these four fishermen to follow him on will mean casting a net of love and welcome to people that they do not anticipate. Jesus will cast his net into the sea of a broken world, filled with sinners, people who have messed up, people who are outsiders, who don’t belong. Jesus will stay in the homes of poor, be guilty of associating prostitutes and touching the hands of people with communicable diseases. Jesus will throw his net into the sea and invite everyone in. Jesus will eventually be arrested and executed because those in power decided his fishing net included a few too many of the wrong people. This is a story about fishing.
This isn’t only a story about four fisherman, or only a story about fishing. It’s also, and perhaps, most importantly, a story about God.
If this is only a story about four fisherman who decide to follow Jesus, the pressures on you and me! After all, aren’t we too called to follow Jesus? Called to be his disciples? Wasn’t that the invitation you first heard when you first heard about Jesus? God has called us and we must decide. Jesus wants us all to follow him, to be like him, to walk in his footsteps, to do what he does. Of course this story is about that! And they do it, don’t they? Simon, Andrew, James, John, they do it! They decide and they do follow Jesus, imperfectly at that. Still, it’s a lot of pressure, a lot of responsibility. If life becomes all about what we do for Jesus, something is missing.
If this is only a story about fishing, have some of us failed? Is it too late for us? Some of us might not be the best at fishing, not all the great about casting Jesus’ loving net to our brothers and sisters. His net is sometimes, or maybe more than sometimes, a bit more expansive than we might be comfortable with. He calls us to be like him and fish for people, and yet, sometimes we can barely get the net into the water. Perhaps for others, we aren’t even convinced that Jesus would include us in the net at all, no matter how deep into the water he goes. He can really mean me? Would his net really reach me? There’s still more to the story.
This is a story about God, who God is, how God acts, what God does. Before Andrew, Simon, James and John follow Jesus, Jesus finds them. Before they follow Jesus, Jesus comes to them! They don’t have to go searching, they have been found. Jesus saw. Jesus spoke. Jesus called. Jesus said, “Come.“ We don’t follow Jesus in order to find him, to prove our worthiness with what we do, or even by showing Jesus how big our nets are. We follow Jesus because he first came to us. He came down to the beach to meet these four fishermen. He came specifically for Simon and for Andrew, for James and for John, for you and me.
This blog post was adapted from Pastor Megan’s sermon at Butner Federal Prison on January 25, 2015.
The life of faith consists of seasons. One scholar suggests that we can categorize these seasons of life as seasons of being securely oriented, painfully disoriented, and surprisingly reoriented. These generalizations could apply to our self-acceptance, our relations to significant others, and our participation in public or private life. We might think about these seasons as passages of life, stages of growth, or even identity crises. Acknowledging where we find ourselves in a particular season can allow us to be honest about where we are at in our lives and where we are in relation to God.
The Psalms, a collection of prayers, songs, and poems addressed to God, correspond to these seasons of orientation, disorientation, and reorientation. As we read through the book, we find Psalms where the writer is full of thanksgiving to God, securely oriented in life. We also find Psalms that demonstrate disorientation, perhaps categorized by loss, transition, grief, suffering, or even anger. Finally, some Psalms are written from a perspective of reorientation, wherein the Psalmist transitions from a period of being disoriented to being reoriented in relation to God and others.
The Psalms can become our partner in prayer. Giving us words when we have none, we pray the Psalms joining with all those who have prayed them before us and all who will pray them after we are gone. As we pray the Psalms, we find permission to be utterly honest with God about our feelings and situation, free to speak openly and deeply to God about what we are experiencing. Praying the Psalms also helps us to envision God’s future when we can’t see it ourselves. Lastly, the Psalms guard us against religion or merely thinking about God. Using their words in prayer brings us into direct conversation with the living God, in language we may never have imagined would come from our lips.
-Pray the assigned Psalm from the daily lectionary, with set Scriptures to read each day. Click here to see today’s readings, subscribe to the daily readings by email, or download the app.
-Pray the Psalms using the practice of praying in color. Click here for an excerpt from Sybil MacBeth’s book that gives instructions for praying in color. I have the book available if anyone would like to borrow it. You can read more about praying in color on her website.
-Pray a Psalm, followed by journal writing. Consider these prompts: Where do I find myself in this Psalm? Where do I find my community? How am I being oriented to God in this prayer? What images or metaphors do I find striking? Explore the image more deeply.
-Pray through a list of Psalms, one per day or the same one each day for a week.
-Pray them as a family or with housemates at mealtime or bedtime.
-Pray abbreviated Psalms as breath prayers. A breath Ppayer rhythm is simple: Breathe in slow and deep as you whisper or think on a phrase… Hold your breath… Then exhale.
I will sing to my God as long as I am.
Psalm 8: Lord, our master, how great is your name in all the earth.
Psalm 104: Seek the Lord and his power; seek his face forever. Remember the wonders he has done.
-Pray the Psalms using lectio divina. For instructions on praying lectio divina individually or in groups, click here. There are also instructions for doing lectio divina in color from Sybil MacBeth’s book.
-Pray a Psalm from the category of life within which you find yourself—orientation, disorientation, or reorientation.
Psalms of Orientation: These Psalms reflect a confident belief that the world is well ordered, reliable, and life-giving to the person of faith.
Psalms of Disorientation: These Psalms reflect the brokenness of life, when it is no longer orderly but savage. Spoken out of the depths, they are still bold acts of faith.
Psalms of New Orientation: The pit is not the end of life; there is more. New orientation Psalms reflect the surprise of new possibilities that are experienced as pure gift from God. They are full of thanks.
Citations: The Message of the Psalms and Praying the Psalms by Walter Brueggemann and Getting Involved with God, by Ellen Davis.
Joseph’s story opens in Genesis 37 and it’s a long one. Joseph was one of 11 kids, the youngest son. In Genesis 37, the story says, “Now Jacob (Joseph’s dad) loved Joseph more than any of his other sons because he was born when Jacob was old. Jacob had made for him a long robe. When his brothers saw that their father loved him more than any of his brothers, they hated him and couldn’t even talk nicely to him.” Sibling rivalry, jealousy, family drama—maybe a little too familiar for some of us.
Just when you want to feel bad for Joseph, show him sympathy, “Poor kid—he can’t help that he’s the favorite,” Joseph makes himself quickly unlikeable. When Joseph’s head hits the pillow at night, he has vivid dreams about the future, dreams where he rules over his brothers. In one of these dreams, he’s in the field working with his brothers. They each tie a bundle of grain together…I imagine it like a hay bail. His bail rises up, towering and floating in the air above the others, while each of his ten older brother’s bails of hay, bows down to his bail, as if he’s ruling over them like a king. What’s worse—he didn’t keep his mouth shut about his dreams. Nope. He went ahead and announced them at the dinner table. When I imagine this scene, I’m reminded of the importance of friends. He seriously needed a friend to say, “Dude, listen, you have some dreams where you’re awesome and your brothers treat you like a king. They hate you, man. Keep your dreams to yourself.” Joseph lacked such a friend, so he bragged about his dreams—that combined with his fancy North Face jacket that Daddy bought for him only and the favoritism their dad showed him, brought his brothers to plot about how they might rid themselves of this pesky brat forever.
Joseph’s brothers considered killing Joseph, but they settled on kidnapping him and selling him into slavery instead. That way, they wouldn’t have his death on their foreheads, without having to put up with him anymore. They took Joseph’s fancy coat and destroyed it, making it look like a wild animal killed Joseph. This they showed to their father, so that he would assume that Joseph was dead; their dad would never suspect they had any part in his disappearance.
Meanwhile, Joseph was taken off to Egypt where he worked as a slave. Though he did well there and followed all the rules, he became a victim for a second time, when his master’s wife accused him of a crime he didn’t commit. Over a period of 13 years, Joseph worked as a slave and spent years locked up in prison. After a series of unlikely events, some terrible and some remarkable, Joseph rose to power and became the king’s right hand man, his adviser.
With the king’s blessing and support, Joseph led his country in preparing for a famine, putting food away on reserve during seven years of plenty. When a famine struck the land, Egypt was in a good position, able to lean on the reserved food that Joseph had put away. The surrounding lands, including Joseph’s homeland, had to lean on Egypt for food or else they would starve.
Joseph shows his brother’s enormous generosity. He has them go home, pack up and move their entire family, including their elderly father Jacob to Egypt to be near Joseph. Not long after making the trip to Egypt and being reunited with his father, their father, an elderly man at this point, Jacob dies.
And the final chapter of the story opens. Jacob is dead. Their father is gone. Now what?
Realizing that their father was dead, Joseph’s brothers said, “What if Joseph still bears a grudge against us and pays us back in full for all the wrong that we did to him?” 16 So they approached[b] Joseph, saying, “Your father gave this instruction before he died, 17 ‘Say to Joseph: I beg you, forgive the crime of your brothers and the wrong they did in harming you.’ Now therefore please forgive the crime of the servants of the God of your father.” Joseph wept when they spoke to him. 18 Then his brothers also wept,[c] fell down before him, and said, “We are here as your slaves.” 19 But Joseph said to them, “Do not be afraid! Am I in the place of God? 20 Even though you intended to do harm to me, God intended it for good, in order to preserve a numerous people, as he is doing today. 21 So have no fear; I myself will provide for you and your little ones.” In this way he reassured them, speaking kindly to them.
Fear is a powerful force. Fear motivates and fear paralyzes.
It’s a little funny how they phrase the words of their father. The brothers put a great deal of distance between themselves and Joseph. Instead of saying, “Our father told us to tell you…” they say, “Your father to us to tell you…” They distance themselves from Joseph and from the message that their dad supposedly gave them to pass along.
And then, they do it again. “Please forgive the crime of the servants of the God of your father,” his brothers say. They refer to themselves in third person—“the servants of the God of your father.” It’s not “our crime” that “we committed,” but the crime of these others.
Fear not only keeps them from confession, fear also keeps them from receiving forgiveness. They are scared for their lives the moment their father breathes his last, but haven’t they already been through this conversation with Joseph? At the dinner table, when Joseph revealed his identity to them, he tells them not to worry. “It’s ok. Yeah, it was awful, but look where I am! Look at how God has used me to help save those who would be starving now. I’m even saving you!” Joseph has already offered them forgiveness, but they haven’t fully received it. They haven’t believed what he’s said. Perhaps their views of themselves were so low that they didn’t see themselves worthy of forgiveness. Maybe they’ve carried the guilt for so long about what they’ve done, they fear what life will be like without it. It’s become so much an engrained part of their identity, they don’t know who they are apart from the guilt of what they’ve done. They fear receiving Joseph’s forgiveness. They fear forgiving themselves.
To the plea of the 10 brothers, to this made-up, manipulative, last cry for safety, Joseph has two responses. First, he weeps. His weeping—his display of vulnerability and emotion—causes his brothers to begin to weep also. There they are, 11 grown brothers, weeping on the floor of the house. Why did Joseph begin to weep? The story doesn’t say. Let’s notice, brothers and sisters—the road to releasing fear and offering and receiving forgiveness may not come without weeping.
Fear is a powerful force. Fear is an excellent motivator—moving us to do particular things and act in particular ways. But fear not only motivates, it can also paralyze, cause us to freeze right where we’re at, accept things for how they are. This final chapter begins with the brothers saying to one another, “What if…?” What if Joseph still bears a grudge against us…? Fear finishes the sentence, beginning in the words, “What if…?” Fear finishes the sentence. What if…he still bears a grudge against us? What if…we confess our evil to Joseph and he says that’s the end of us? What if…we ask for forgiveness and he denies it—if I say, will you forgive me, and he says, “no”?
“What if’s” sneak into our minds and hearts.
What if…I never get out of here?
What if I fail as a parent?
What if I don’t belong?
What if no one notices I’m gone?
What if I stand up for what I believe is right and it costs me my reputation?
What if I make a mistake at work and lose my job?
What if I risk opening myself up to someone and get hurt or betrayed again?
What if my body fails me?
What if I can never accept that the past can’t change?
What if I’m not worthy of God’s forgiveness or the forgiveness of those I’ve wronged?
What if I can never forgive myself?
We do not have to live in fear. We do not have to be motivated or paralyzed by it. Look at the God that we serve! Joseph explains how God has been with him. He says to his brothers, “Even though you intended to do harm to me, God intended it for good, in order to preserve a numerous people, as he is doing today.” Does this mean that God wanted or desired Joseph’s brothers to kidnap him, throw him into slavery and ruin his life to avenge their jealousy? No. God doesn’t desire that jealousy and revenge rule our lives. God doesn’t will for us to do evil or to harm other people. Rather, God is able to overcome evil and transform it. God can overcome evil! When Jesus was captured, tried as a criminal and sentenced to death, God overcame death, raising Jesus from the death.
This post was adapted from my sermon preached at Butner Federal Prison on September 14, 2014.
We were gathered at the plaza, right between the giant bull statue and the unattractive fences of a construction site. Luminary bags weighted with rice and lit candles marked the sacred space surrounding 30 of us, one to represent each person who died as a result of domestic violence the previous year in our state. The vigil began as planned, simple, but meaningful, to remember victims of this tragedy and raise awareness about the suffering that takes place behind closed doors. About halfway through the simple service, a woman stumbled into the vigil, interrupting the solemn mood without realizing that a group was gathered and someone was speaking. She stood silent for a few moments, listening to the speaker. When she realized that the speaker was talking about domestic violence, she began to interrupt, asking questions to the speaker, sharing details from her own experience with abuse. “What would you do…what would you do if…?” she cried. Then, as unexpectedly as she joined us and as abruptly as her interruption, she began to weep, uncontrollably crying for the rest of the vigil. A couple of women gathered around her and held her as she wept. Before long, it was my turn to pray. I barely got the words out…I could hardly project my shaking voice over her loud sobs.
“Blessed are those who mourn, for they will be comforted,” Jesus proclaims in the second line of the beatitudes. Blessed are those who mourn. How is this weeping woman, this victim of abuse, blessed? She mourns the injustices she’s experienced, her suffering, the ways her life has been shaped by pain and her inability to free herself from her oppression. Jesus says that this woman and all her sisters and brothers that mourn with her are blessed.
The Jewish culture that Jesus was born into has a rich history of mourning or practicing lament, stretching back hundreds of years before he was born. The prophets and the Psalms include poems, songs, and speeches, recounting the words of people gathered together for public mourning. This mourning wasn’t a kind of crying about having a bad day or because of a frustration at home or work. The mourning Jesus is referencing is the kind of mourning that is a response to injustice and oppression, those who mourn the impact of the powers, both material and spiritual, on the lives of the most vulnerable.
Blessed are those who mourn. Another beatitude and another paradox. Once again, Jesus’ words are outlandish and nonsensical. How is it that those who mourn are blessed? Aren’t those who are happy and fulfilled, aren’t they the ones that are blessed? Yet, in this beatitude, in this paradox, Jesus once again exposes the powers and envisions an alternative. Jesus exposes the powers that cause people to mourn in the first place, those who experience unjust suffering and loss, the same injustices that cause people to be poor in spirit. It’s these people, the mourners, that are blessed, Jesus says. These are the people that Jesus came for. In God’s empire, mourners are not written off or ignored as uncivilized, uneducated, or badly behaved. Instead, in God’s empire, they are the ones who receive God’s comfort and consolation; God’s hears their cries.
Our culture tends to restrict mourning or public displays of emotion to something appropriate for home life or private time. Further, spending time in mourning may be quickly relegated to a waste of time or an inactive posture. The expression, “Don’t just cry about it, do something,” illustrates this clearly. But mourning is not a useless waste of time or an inactive practice. Mourn is a verb. In fact, mourning elicits action and engagement. Mourning exposes the powers, shows their true colors. Seeing people in mourning is disorienting. It interrupts the lives we lead that are detached from suffering and injustice, forcing us to take another look, to pause, to listen, and to join.
The woman who interrupted our solemn vigil for victims of domestic violence exposed the powers with her loud wailing. She made me feel uncomfortable, like I wanted to look away and get away from her as quickly as possible. And yet, her cries made it impossible for me to forget her. The sound of her weeping echoed in my ears for weeks following and if I try, I can still hear them now, over nine months later. Her mourning moves me to engage in seeking justice for others who have suffered like she has.
1. James Howell, The Beatitudes for Today. Louisville: Westminster John Knox Press, 2005., 45.
2. James Howell, The Beatitudes for Today, 46.","['""I have seen the Lord."".']",6856,multifieldqa_en,en,,d1f31d2998513a4552d0419e16f8eba896e9ea1c8403605b," A Homily from Easter Sunday, 2017. Early on the first day of the week, while it was still dark, Mary Magdalene came to the tomb and saw that the stone had been removed from the tomb. As she wept, she bent over to look[a] into the tomb; and she saw two angels in white, sitting where the body of Jesus had been lying, one at the head and the other at the feet. She said to them, “They have taken away my Lord, and I do not know where they have laid him.” When she had said this, she turned around and saw Jesus standing there, but she did not know that it was Jesus. He interrupted her, pleading, and called her by name from the darkness, Mary. To the gardener, she would just be the possessed woman who wasn’t enough, the woman on the outside of the group. Never nameless—but still unnamed. Never not Mary, but still not known, but not known. God defeated the powers and principalities in the ultimate act of resistance—recturresurrecting the group in the name of Jesus. The name that captures the life of her life, she was just the woman who was possessed by the possessed life, and she was never nameless. She was never known as “Mary’s” name, but “Never not Mary” was the name that captured her life. Her name was never “not known,” but the name “never not known’ was the word that captures her life and the life that she was possessed of, and it was her name that God called her to be a woman. The word “Woman” means “woman” in Hebrew, which is the name Jesus calls her by. Jesus said to her in Hebrew “Rabbouni!” (which means Teacher) “Do not hold on to me, because I have not yet ascended to the Father” “I am ascending to my Father and your Father, to my God and your God’” Jesus invites those who follow him into similar acts of resistance, to free us from the power money has on us by giving it away, to choose to see ourselves as Jesus sees us, to practice Sabbath that contradicts productivity, to untie the grave clothes of someone whose hands and feet are still tied in the trappings of death. He describes this way as ‘the kingdom of God,’ the living water we drink so we never thirst again, the light of the world. The power of death had the last Word. Early in the morning, whileIt was stilldark, Mary wept in the throws of grief. Early. in the early morning, Mary dragged herself out of bed after a sleepless night and walked to the Tomb in a kind of trance. He repeated the question—“Woman, why are you crying?” Thinking that perhaps he knew what happened or worse, that he was a culprit, she begged, ‘Sir if you have carried him away, tell me where you have put him and I will get him’ “Just then, she turning and saw a man the shadow of a man behind her; a man she assumed was the Gardener, his face unfamiliar in the darkness. He told her that he had said these things to her. In the resurrection, God defeats the powers of death and shows that it’s God who has the final Word. Nothing, not even death, can keep us from being fully known by God. The powers try to have the final say on our names, our identities, the markers by which we measure ourselves. But Jesus calls us out of the darkness by name. As you come forward and mark the sign of the cross on your forehead today, hear Jesus speaking your name from the darkness and drawing you into the light. The grave could not contain the Lord. Even death wasn’t enough. On this Easter Sunday, we hear our Risen Lord calling our names from the dark. The risen Lord has spoken. The name above all names, the great I am, the Prince of Peace, the alpha and omega, the light of the world. We need water to live! Without water, we will die! Even if we have water for today,. we will need water again tomorrow! We can drink until we are satisfied, only to know that we will eventually be thirsty for more. We cannot live without it. Our bodies depend on water. Most of us wake up in the morning needing a drink. By the end of this sermon, I will no doubt feel thirsty, not from walking on hard dusty ground in the heat of the day, but just from speaking with you. The gospel of John tells a story about a woman who gave up on this question all together. She moved beyond wondering if the Lord was really with her, so confident God had forgotten her that she gave up wondering at all. When she came to get a drink, Jesus was also at the well, thirsty himself and in need of rest and water from the long journey through Samaria. The elders who sat and wrote down these stories understood something about our bodies, who we are and how we work. After all the generations these stories passed through, they tell the truth about how quickly we forget. They knew that we are thirsty. All who are also who are thirsty also knew that Jesus also knew also. They are hard on their ancestors. They tell how it is. The Israelites complaint for water sends Moses to the only one who can satisfy, the onlyOne who can meet this need. Moses turns to God, “What should I do with these people? How can I satisfy their thirst? I’ve checked far and wide, turned the house upsidedown, looked under the seats of the car, at the bottle of every bottle, I”ve even looked for de lids of jars and there is no water to be found. Where we we do go for water? Is the Lord really with us or not?” They were allowed to look like desperate complainers who doubted God, and tested God onto something that we knew we are. They were tested and tested and allowed the God to go for us or do we go for God? They knew they were desperate and that God knew we were also people who doubted him. The Bible tells us that Jesus was thirsty himself. He too, was thirsty. He also knew he was also a Samaritan, born into a world that valued other bodies as better than her body: male bodies, Jewish bodies, even married bodies. Pastor Megan preached this sermon at Butner Federal Prison complex on August 30, 2015. Tony graciously shared the following reflections from his experience at the labyrinth on the hot June day. Tony: “What will be waiting for you at the rock? Will the water drip slowly, quenching your thirst for but a moment, giving you just a glimpse of God’s spirit? Will it be just enough to satisfy you for this hour, but keep you coming back for more? What if it seems like the water has run out, like there isn’t a drop left, the way that Mother Teresa described? What then? We follow her example. She still goes to the rock, over and over again, not to get water quench her own thirst, but to relieve the thirst of God's other children’” “I see each step as an increment of elapsed time, an irretrievable expenditure of energy,” said Tony. “Like my physical lifetime, it has a beginning and an end, with an as‐yet undetermined amount between.”“I like it so far… although I’m insecure about proper protocol. How do I walk? Is it my style? How much time do I have to meditate?” asked Tony.“What happens after we die?’ Many have watched their father's die in this place or other similar spaces. We shared in a time of both remembrance and prayer/meditation, participating in the ancient spiritual practice of walking the labyrinth. A labyrinth is a kind of maze, laid out in a circle. It's smaller than I expected, stark and hard‐surfaced, with no landscaping for ornamentation or shade. But that's part of the appeal. I don't know what to expect from it… or from myself. I stand at the entrance, hesitating, trying to clear my mind. This doesn't work very well, so I just start walking. I see it as a linear and chronological symbol of my life's journey. This could be interesting, although I'm not sure what to make of it yet. I'm unsure about proper protocols. How much energy do I need to walk? How slowly should I come? Will I begin to focus my… or to simply allow it to control me? I see the water gush out, bursting forth, covering you from head to toe with God's presence, drenching you in hope, cleansing you from the dust that’�s caked to your feet and renewing you for a new day, a new hour, anew moment basking in the presence of God?“Not blessed are those who are righteous, but those who thirst for righteousness. Blessed are thosewho thirst for relationship with God, to know God,. to see God.’ “Blessed are thoseWho hunger and thirst forrighteousness, for they will be filled,’ said Jesus, “blessed’. ‘We can only make it so long in the desert, so long wandering from one trial to the next, without a drink. We are desperate to be bathed in the water of the Spirit, to feel a sense of belonging to the One who is greater than I. ’ The author walks a path through his life. The path is filled with sharp turns, requiring concentration and skill to negotiate. The final section is large and unrestrictive, a qualitative change from the narrow linear pathway. The circle opens up to welcome the author, who is centered, peaceful and thankful. The author's role is limited, but critical. The familiar succession of life, death and new life seems magical, divinely‐derived, and strangely better than living forever. This is a story about 4 fishermen, Simon, Andrew, James and John. It’s a normal morning at the docks. Each one of them is going about business as usual. They arrived at dawn, bundled up in the cool morning air and started work without much conversation. Simon and Andrew are working on one fishing boat and see the Teacher approaching. “Hey. There he is,” says Andrew, “You can’t go anywhere without hearing about him.” They paddle back to shore, not wanting to miss him. Jesus comes over to talk to them, just to find these two guys just to get them just to the beach. They follow him, leaving their father Zebedee in the boat with the hired workers. They leave their nets in the sea, for they were fishermen. 17 “Come, follow me,’ he said, ‘and I’ll show you how to fish for people” 18 Right away, they left their nets and followed him. 19 After going a little farther, he saw James and. John, in their boat repairing the fishing nets. 20 At that very moment he called them. They followed him, left their father, and left their boat. The story is called ‘Simon and Andrew’ and ‘The Teacher’.’ The author has written several books about his journey through life, including “The Power of Now” and “How to Be Free”. The book is published by Simon & Schuster, a division of Penguin Books, and is available on Amazon.com for $24.99. For more information on how to be free, visit www.simonandschuster.co.uk. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, or click here for details. In the U.S. call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential. support in the UK, call 08457 909090 or visit the Samaritans, or click here for information. In Europe, visit the National suicide prevention Lifeline, or see http:// www.samaritans.org. For confidential support in Europe, call the National Suicide Prevention Helpline on 0800 8255. In the United States, call the United States on 1 800 909090, or visit the U.S., or UK for information on the UK Samaritans on 0800 809090. This is a story about four fisherman who decided to follow Jesus. Jesus uses a kind of puzzling image about fishing. He says, “Come, follow me, and I’ll show you how to fish for people.” The road Jesus invites these four fishermen to follow him on will mean casting a net of love and welcome to people that they do not anticipate. Jesus will cast his net into the sea of a broken world, filled with sinners, people who have messed up, and people who are outsiders, who don’t belong. He will stay in the homes of poor, be guilty of associating prostitutes and touching the hands of people with communicable diseases. And he will eventually be arrested and executed because those in power decided his fishing net included a few too many of the wrong people. If life becomes all about what we do for Jesus, something is missing. Some of us might not be the best at casting Jesus’ loving net to our brothers and sisters. His net is sometimes, a bit more expansive than we might be comfortable with. He calls us to be like him and yet, sometimes we can barely get into the water for others, no matter how deep into the net at all, he would include us in the water he casts. He would make us fishers of men. He has called us and we must decide. If this is only a stories about fishing, have some of us failed? Is it too late for us? It’s also, and perhaps, most importantly, a story of God. He is the great love of our lives. He wants us all to walk in his footsteps, to do what he does. And they do it, don't they? Simon, Andrew, James, John, they do do it! They decide and they do follow Jesus, imperfectly at that. Still, it's a lot of pressure, a lots of responsibility. It's a big deal! The four normal guys, working a normal job, on a normal morning, decide to following Jesus. The story doesn't tell us. There's nothing about how they felt. It doesn't say they were excited, or moved, or scared, or joyful or resistant. This story gives us only verbs. I realized this week why his image is so confusing to me. Once I realized that fishing was primarily a crack of dawn activity, I knew it wasn't really for me. And I don't know about you, but this I find this to be very strange. I've always interpreted what Jesus said this way. I will make you fishers. of men, fishers Of men, Fishers Of Men, if you follow me. I'll make you Fishers of. men, If you follow. me. The invitation you first heard when you first hear about Jesus? Wasn't that the invitation to be his disciples? The invitation to walk behind him, to trace his every step, to follow after him, is the invitation we first heard about Jesus. That's what Jesus wants us to do, to walk with him. That is what Jesus calls us. It is not to be a follower of him, but to be one of his disciples. It isn't to be just one of the disciples, to be part of his flock, to join in with him and follow him in his work. The life of faith consists of seasons. Acknowledging where we find ourselves in a particular season can allow us to be honest about where we are in relation to God. The Psalms correspond to these seasons of orientation, disorientation, and reorientation. Praying the Psalms helps us to envision God’s future when we can’t see it ourselves. Using their words in prayer brings us into direct conversation with the living God, in language we may never have imagined would come from our lips. Click here to see today's readings, subscribe to the daily readings by email, or download the app. The Daily Discussion is a written version of this blog post. It is intended to be used in conjunction with the daily lectionary, with set Scriptures to read each day. For instructions on praying in color, see Sybil MacBeth's book, ""Praying in Color"" (available on her website). For more information about the Daily Discussion, visit the www.daily-discussion.org. For more on the daily discussion, see the weekly Newsquiz, which is also available on the website and can be downloaded as a free download from the App Store and Google Play. The weekly News Quiz is held on the third Thursday of each month at 10:30 a.m. and 11:30 p.m., and is open to all members of the church. For the next week, we will be discussing the theme of the week, “The Way of the Cross,” and discussing how we can use the Bible to help others in similar situations. We will also be discussing how to pray in color in the next few weeks. The next week we will discuss the topic of “the way of the cross” in the context of the New Testament, and discuss how to use it to help other people in similar circumstances. The final week of the series will be devoted to the theme, ‘The Way Of The Cross’ (January 25-28, 2015). We will discuss this topic in the second week of January, when we will look at the themes of ‘the Way’ and the ‘way of the Lord’. The third week will be dedicated to the “way to the Cross” (January 28, 2015) and the fourth week of February, when the topic will be ”the way to the cross,’ which is about the way God leads us to the Throne of Grace.” The fifth and final week will focus on the ”way’ of the Christ”, which focuses on the relationship between God and those who follow him. This week we look at how God shows us his love for us in the story of the Bible, and how he shows us how to follow him in our daily lives. We can use these themes to help us understand our relationship with God and each other in a new way. The sixth and final point of this post is that God is the source of all good news, and he is the only one who can give us the answers we need to find out how to live a better life for ourselves and for our loved ones. The last point is that the Bible is a book of stories, not just a collection of verses, and that we can learn about God by reading them. Pray a Psalm from the category of life within which you find yourself. These Psalms reflect a confident belief that the world is well ordered, reliable, and life-giving to the person of faith. Praying the Psalms can be done individually or in groups. There are also instructions for doing lectio divina in color from Sybil MacBeth’s book. For more information on Praying The Psalms, visit www.praythepsalms.org. For the full list of Psalms and instructions, go to: http://www.praisethepalm.org/praying-the-psalm-symptons-and-directions-by-Sybil-MacBeth-in-color-from-lectio-divina-on-l lectio- divina on Lectio Divina 1. The Psalm of Joseph tells the story of Joseph, the youngest son of Jacob and his brothers. Joseph was taken off to Egypt where he worked as a slave. After a series of unlikely events, some terrible and some remarkable, Joseph rose to power and became an adviser to the king of Egypt. He led his country in a famine, putting food away on reserve during the years of plenty. The surrounding lands had to lean on Egypt for food or else they would starve. Joseph shows his brother how to be a good man, even when it is hard to do so. He shows us how to love our brothers, no matter how hard it is for us to love them. He tells us how we can help each other, even if it is difficult to do it at the beginning of the day. He says, “I’m reminded of the importance of friends’.” He says: “Dude, listen, you have some dreams where you’re awesome and your brothers treat you like a king. They hate you, man. Keep your dreams to yourself.’” The Psalmist tells us to “love your brothers’ brothers, even though they hate you” and “keep your dreams’ to yourself” “You can’t help it when you want to feel bad for Joseph,” he says. “Just when you. want to help him, show him sympathy, ‘Poor kid,’ he adds. ‘He can�’ve been through a lot. He’ll get through it. You can help him.'’ ‘You can do it.‘’ Joseph says to his brother. � ‘I love you, brother’.' ’You can help me. ’’ The two of them say it, and they hug each other’ and they say ‘Thank you.�' ‘‘You’ can help it, Joseph says, and you can help them’ '’I. love you.' ‘'You can. do it.' ” ‘Joseph says, 'I have a dream about the future, where he rules over his brothers, as if he’�s ruling over them like a. king' ’ ’ 'You. can. help it.' Joseph and his brothers were told by their father to forgive Joseph for their wrongs. Joseph wept when they spoke to him, and they fell down before him. The brothers put a great deal of distance between themselves and Joseph. Fear is a powerful force. Fear motivates and fear paralyzes, and it can cause us to accept things for how they are. It’s not “our crime” that “we committed,” but the crime of these others. Joseph has already offered them forgiveness, but they haven’t fully received it. Perhaps their views of themselves were so low that they didn't see themselves worthy of forgiveness. The road to releasing fear and offering and receiving forgiveness may not come without weeping. It's a long way from “What if…he still bears a grudge against us?” What if we confess our evil, he says to Joseph, and he says “I’m even saving you!” It is the final chapter of the story of Joseph and his 11 brothers. It begins with the brothers saying to one another, ‘What if one of us still bears an evil grudge?’ It ends with the words, � “what if…” “We are here as your slaves.” The brothers then pack up and move their entire family, including their elderly father Jacob, to Egypt to be near Joseph.. He has them go home, packing up and moving their whole family to Egypt. It is a long journey, but it is worth it in the end. It was the right thing to do for the sake of their family. It would have been better if they had stayed in the U.S. instead of going to Egypt, where they would have had to leave their father behind. It could have been worse for them if they stayed in Egypt and had to deal with the death of their father. They would have to face the loss of their mother and father, who they loved. They could have lost their father and their mother. They might have lost all of their possessions. They may have been forced to leave Egypt and go to a new country. It might have been harder for them to leave the country than to go to Egypt and return to their father’S house. They were scared for their lives the moment their father breathes his last. They are scared for the future. They fear what life will be like without it. It can paralyze them. It also can cause them to accept how they’re at things for what they are, and accept things they are at things. They don�’ve become so much an engrained part of their identity, they fear receiving Joseph’’ They fear forgiving themselves. They distance themselves from Joseph and from the message that their dad supposedly gave them to pass along. They refer to themselves in third person—“the servants of the God of your father” They do it again. They say, “Our father told us to tell you…’ Instead of saying “Your father to us toTell you,’ they say,  “your father to them to tellYou…“ And then, they distance themselves again, saying,   “Please forgive the crime.’ Now therefore please forgive the wrong they did in harming you” The Jewish culture that Jesus was born into has a rich history of mourning or practicing lament, stretching back hundreds of years before he was born. Jesus is referencing the kind of mourning that is a response to injustice and oppression. God doesn’t desire that jealousy and revenge rule our lives. Rather, God is able to overcome evil and transform it. God can overcome evil! When Jesus was captured, tried as a criminal and sentenced to death, God overcame death, raising Jesus from the death. Jesus says that this woman and all her sisters and brothers that mourn with her are blessed. Once again, Jesus’ words are a paradox. They are outlandish and another beatitude and another paradox. We do not have to live in fear. We don't have to be motivated or paralyzed by it. It’s OK to ask for forgiveness and he denies it, and he says, “no”? “What if…I never get out of here?” ‘What if I fail as a parent? ‘ ‘’ ‘If I can never forgive myself?’‘‘What If I stand up for what I believe is right and it costs me my reputation?' ’’ '’If I make a mistake at work and lose my job? ’ ’'What if my body fails me?' ‘'’ A woman stumbled into the simple service, interrupting the solemn mood without realizing that a group was gathered and someone was speaking. She stood silent for a few moments, listening to the speaker. When she realized that the speaker was talking about domestic violence, she began to interrupt, asking questions to the speakers, sharing details from her own experience with abuse. ‘How is this weeping woman, this victim of abuse, blessed? She mourns the injustices she’ve experienced, her suffering, the ways her life has been shaped by pain and her inability to free herself from her oppression.’ The vigil began as planned, simple, but meaningful, to remember victims of this tragedy and raise awareness about the suffering that takes place behind closed doors. '“Blessed are those who mourn, for they will be comforted,” Jesus proclaims in the second line of the beatitudes.” '”What would you do…what would youdo if…?“ she cried. “I barely got the words out…I could hardly project my shaking voice over her loud sobs,’ I said. 'What if no one notices I’m gone?' '‘I’ll be gone in a few minutes’, she said.' 'What would I do if…?' 'I won't be able to go back to my family, my friends, my husband, my kids, my wife, my family.' 'I will be gone. I will have a new life, a new beginning, a better life,' she said' 'I'll be free!' 'I'm not going back to that place where I was. I'm going to a better place, a place that I can be with people who love me, who care about me' Our culture tends to restrict mourning or public displays of emotion to something appropriate for home life or private time. Spending time in mourning may be quickly relegated to a waste of time or an inactive posture. Mourn is a verb. In fact, mourning elicits action and engagement. Mourning exposes the powers, shows their true colors. Seeing people in mourning is disorienting. It interrupts the lives we lead that are detached from suffering and injustice, forcing us to take another look, to pause, to listen, and to join in. The woman who interrupted our solemn vigil for victims of domestic violence exposed the powers with her loud wailing. She made me feel uncomfortable, like I wanted to look away and get away from her as quickly as possible. And yet, her cries made it impossible for me to forget her. The sound of her weeping echoed in my ears for weeks following and if I try, I can still hear them now, over nine months later. Her mourning moves me to engage in seeking justice for others who have suffered like she has. In God's empire, mourners are not written off or ignored as uncivilized, uneducated, or badly behaved. Instead, in God’s empire, they are the ones who receive God's comfort and consolation; God's hears their cries. The Beatitudes for Today, by James Howell, is published by Westminster John Knox Press, 2005., 45., 45.2, 45.3, 46, 45., 46.4, 46.5, 45, 47, 45,. 46, 47.6, 46,. 47, 48, 48."
What types of data did the authors use in their experiments?,"\section{Introduction}
\label{sec:introduction}

Probabilistic models have proven to be very useful in a lot of applications in signal processing where signal estimation is needed \cite{rabiner1989tutorial,arulampalam2002tutorial,ji2008bayesian}. Some of their advantages are that 1) they force the designer to specify all the assumptions of the model, 2) they provide a clear separation between the model and the algorithm used to solve it, and 3) they usually provide some measure of uncertainty about the estimation.

On the other hand, adaptive filtering is a standard approach in estimation problems when the input is received as a stream of data that is potentially non-stationary. This approach is widely understood and applied to several problems such as echo cancellation \cite{gilloire1992adaptive}, noise cancellation \cite{nelson1991active}, and channel equalization \cite{falconer2002frequency}.

Although these two approaches share some underlying relations, there are very few connections in the literature. The first important attempt in the signal processing community to relate these two fields was the connection between a linear Gaussian state-space model (i.e. Kalman filter) and the RLS filter, by Sayed and Kailath \cite{sayed1994state} and then by Haykin \emph{et al.} \cite{haykin1997adaptive}. The RLS adaptive filtering algorithm emerges naturally when one defines a particular state-space model (SSM) and then performs exact inference in that model. This approach was later exploited in \cite{van2012kernel} to design a kernel RLS algorithm based on Gaussian processes.

A first attempt to approximate the LMS filter from a probabilistic perspective was presented in \cite{park2014probabilistic}, focusing on a kernel-based implementation. The algorithm of \cite{park2014probabilistic} makes use of a Maximum a Posteriori (MAP) estimate as an approximation for the predictive step. However, this approximation does not preserve the estimate of the uncertainty in each step, therefore degrading the performance of the algorithm.

In this work, we provide a similar connection between state-space models and least-mean-squares (LMS). Our approach is based on approximating the posterior distribution with an isotropic Gaussian distribution. We show how the computation of this approximated posterior leads to a linear-complexity algorithm, comparable to the standard LMS. Similar approaches have already been developed for a variety of problems such as channel equalization using recurrent RBF neural networks \cite{cid1994recurrent}, or Bayesian forecasting \cite{harrison1999bayesian}. Here, we show the usefulness of this probabilistic approach for adaptive filtering.

The probabilistic perspective we adopt throughout this work presents two main advantages. Firstly, a novel LMS algorithm with adaptable step size emerges naturally with this approach, making it suitable for both stationary and non-stationary environments. The proposed algorithm has less free parameters than previous LMS algorithms with variable step size \cite{kwong1992variable,aboulnasr1997robust,shin2004variable}, and its parameters are easier to be tuned w.r.t. these algorithms and standard LMS. Secondly, the use of a probabilistic model provides us with an estimate of the error variance, which is useful in many applications.

Experiments with simulated and real data show the advantages of the presented approach with respect to previous works. However, we remark that the main contribution of this paper is that it opens the door to introduce more Bayesian machine learning techniques, such as variational inference and Monte Carlo sampling methods \cite{barber2012bayesian}, to adaptive filtering.\\


\section{Probabilistic Model}

Throughout this work, we assume the observation model to be linear-Gaussian with the following distribution,

\begin{equation}
p(y_k|{\bf w}_k) = \mathcal{N}(y_k;{\bf x}_k^T {\bf w}_k , \sigma_n^2),
\label{eq:mess_eq}
\end{equation}
where  $\sigma_n^2$ is the variance of the observation noise, ${\bf x}_k$ is the regression vector and ${\bf w}_k$ is the parameter vector to be sequentially estimated, both $M$-dimensional column vectors.


In a non-stationary scenario, ${\bf w}_k$ follows a dynamic process. In particular, we consider a diffusion process (random-walk model) with variance $\sigma_d^2$ for this parameter vector:


\begin{equation}
p({\bf w}_k|{\bf w}_{k-1})= \mathcal{N}({\bf w}_k;{\bf w}_{k-1}, \sigma_d^2 {\bf I}),
\label{eq:trans_eq}
\end{equation}
where $\bf I$ denotes the identity matrix. In order to initiate the recursion, we assume the following prior distribution on ${\bf w}_k$

\begin{equation}
p({\bf w}_0)= \mathcal{N}({\bf w}_0;0, \sigma_d^2{\bf I}).\nonumber
\end{equation}

\section{Exact inference in this model: Revisiting the RLS filter}

Given the described probabilistic SSM, we would like to infer the posterior probability distribution $p({\bf w}_k|y_{1:k})$.
Since all involved distributions are Gaussian, one can perform exact inference, leveraging the probability rules in a straightforward manner. The resulting probability distribution is
\begin{equation}
p({\bf w}_k|y_{1:k}) =  \mathcal{N}({\bf w}_k;{\bf\boldsymbol\mu}_{k}, \boldsymbol\Sigma_{k}), \nonumber
\end{equation}
in which the mean vector ${\bf\boldsymbol\mu}_{k}$ is given by
\begin{equation}
{\bf\boldsymbol\mu}_k = {\bf\boldsymbol\mu}_{k-1} + {\bf K}_k (y_k - {\bf x}_k^T {\bf\boldsymbol\mu}_{k-1}){\bf x}_k, \nonumber
\end{equation}
where we have introduced the auxiliary variable
\begin{equation}
{\bf K}_k = \frac{ \left(\boldsymbol\Sigma_{k-1} + \sigma_d^2 {\bf I}\right)}{{\bf x}_k^T  \left(\boldsymbol\Sigma_{k-1} + \sigma_d^2 {\bf I}\right)  {\bf x}_k + \sigma_n^2}, \nonumber
\end{equation}
and the covariance matrix $\boldsymbol\Sigma_k$ is obtained as
\begin{equation}
\boldsymbol\Sigma_k = \left( {\bf I} -  {\bf K}_k{\bf x}_k {\bf x}_k^T \right) ( \boldsymbol\Sigma_{k-1} +\sigma_d^2), \nonumber
\end{equation}
Note that the mode of $p({\bf w}_k|y_{1:k})$, i.e. the maximum-a-posteriori estimate (MAP), coincides with the RLS adaptive rule
\begin{equation}
{{\bf w}}_k^{(RLS)} = {{\bf w}}_{k-1}^{(RLS)} + {\bf K}_k (y_k - {\bf x}_k^T {{\bf w}}_{k-1}^{(RLS)}){\bf x}_k .
\label{eq:prob_rls}
\end{equation}
This rule is similar to the one introduced in \cite{haykin1997adaptive}.

Finally, note that the covariance matrix $\boldsymbol\Sigma_k$ is a measure of the uncertainty of the estimate ${\bf w}_k$ conditioned on the observed data $y_{1:k}$. Nevertheless, for many applications a single scalar summarizing the variance of the estimate could prove to be sufficiently useful. In the next section, we show how such a scalar is obtained naturally when $p({\bf w}_k|y_{1:k})$ is approximated with an isotropic Gaussian distribution. We also show that this approximation leads to an LMS-like estimation.
 


\section{Approximating the posterior distribution: LMS filter }

The proposed approach consists in approximating the posterior distribution $p({\bf w}_k|y_{1:k})$, in general a multivariate Gaussian distribution with a full covariance matrix, by an isotropic spherical Gaussian distribution 

\begin{equation}
\label{eq:aprox_post}
\hat{p}({\bf w}_{k}|y_{1:k})=\mathcal{N}({\bf w}_{k};{\bf \hat{\boldsymbol\mu}}_{k}, \hat{\sigma}_{k}^2 {\bf I} ).
\end{equation}

In order to estimate the mean and covariance of the approximate distribution $\hat{p}({\bf w}_{k}|y_{1:k})$, we propose to select those that minimize the Kullback-Leibler divergence with respect to the original distribution, i.e., 

\begin{equation}
\{\hat{\boldsymbol\mu}_k,\hat{\sigma}_k\}=\arg \displaystyle{  \min_{\hat{\boldsymbol\mu}_k,\hat{\sigma}_k}} \{ D_{KL}\left(p({\bf w}_{k}|y_{1:k}))\| \hat{p}({\bf w}_{k}|y_{1:k})\right) \}. \nonumber
\end{equation}

The derivation of the corresponding minimization problem can be found in Appendix A. In particular, the optimal mean and the covariance are found as
\begin{equation}
{\hat{\boldsymbol\mu}}_{k} = {\boldsymbol\mu}_{k};~~~~~~ \hat{\sigma}_{k}^2 = \frac{{\sf Tr}\{ \boldsymbol\Sigma_k\} }{M}.
\label{eq:sigma_hat}
\end{equation}


We now show that by using \eqref{eq:aprox_post} in the recursive predictive and filtering expressions we obtain an LMS-like adaptive rule. First, let us assume that we have an approximate posterior distribution at $k-1$, $\hat{p}({\bf w}_{k-1}|y_{1:k-1}) =  \mathcal{N}({\bf w}_{k-1};\hat{\bf\boldsymbol\mu}_{k-1}, \hat{\sigma}_{k-1}^2 {\bf I} )$. Since all involved distributions are Gaussian, the predictive distribution
is obtained as %
\begin{eqnarray}
\hat{p}({\bf w}_k|y_{1:k-1}) &=& \int p({\bf w}_k|{\bf w}_{k-1}) \hat{p}({\bf w}_{k-1}|y_{1:k-1}) d{\bf w}_{k-1} \nonumber\\
&=& \mathcal{N}({\bf w}_k;{\bf\boldsymbol\mu}_{k|k-1}, \boldsymbol\Sigma_{k|k-1}), 
\label{eq:approx_pred}
\end{eqnarray}
where the mean vector and covariance matrix are given by
\begin{eqnarray}
\hat{\bf\boldsymbol\mu}_{k|k-1} &=& \hat{\bf\boldsymbol\mu}_{k-1} \nonumber \\
\hat{\boldsymbol\Sigma}_{k|k-1} &=& (\hat{\sigma}_{k-1}^2 + \sigma_d^2 ){\bf I}\nonumber.
\end{eqnarray}

From \eqref{eq:approx_pred}, the posterior distribution at time $k$ can be computed using Bayes' Theorem and standard Gaussian manipulations (see for instance \cite[Ch. 4]{murphy2012machine}). Then, we approximate the posterior $p({\bf w}_k|y_{1:k})$ with an isotropic Gaussian,
\begin{equation}
\hat{p}({\bf w}_k|y_{1:k}) =  \mathcal{N}({\bf w}_k ; {\hat{\boldsymbol\mu}}_{k}, \hat{\sigma}_k^2 {\bf I} ),\nonumber
\end{equation}
where 
\begin{eqnarray}
{\hat{\boldsymbol\mu}}_{k} &= & {\hat{\boldsymbol\mu}}_{k-1}+ \frac{ (\hat{\sigma}_{k-1}^2+ \sigma_d^2)  }{(\hat{\sigma}_{k-1}^2+ \sigma_d^2)  \|{\bf x}_k\|^2 + \sigma_n^2} (y_k - {\bf x}_k^T {\hat{\boldsymbol\mu}}_{k-1}){\bf x}_k  \nonumber  \\
&=& {\hat{\boldsymbol\mu}}_{k-1}+ \eta_k (y_k - {\bf x}_k^T {\hat{\boldsymbol\mu}}_{k-1}){\bf x}_k . 
\label{eq:prob_lms}
\end{eqnarray}
Note that, instead of a gain matrix ${\bf K}_k$ as in Eq.~\eqref{eq:prob_rls}, we now have a scalar gain $\eta_k$ that operates as a variable step size.


Finally, to obtain the posterior variance, which is our measure of uncertainty, we apply \eqref{eq:sigma_hat} and the trick ${\sf Tr}\{{\bf x}_k{\bf x}_k^T\}= {\bf x}_k^T{\bf x}_k= \|{\bf x}_k \|^2$,

\begin{eqnarray}
\hat{\sigma}_k^2 &=& \frac{{\sf Tr}(\boldsymbol\Sigma_k)}{M} \\
&=& \frac{1}{M}{\sf Tr}\left\{ \left( {\bf I} -  \eta_k {\bf x}_k {\bf x}_k^T \right) (\hat{\sigma}_{k-1}^2 +\sigma_d^2)\right\} \\
&=& \left(1 - \frac{\eta_k \|{\bf x}_k\|^2}{M}\right)(\hat{\sigma}_{k-1}^2 +\sigma_d^2).
\label{eq:sig_k}
\end{eqnarray}
If MAP estimation is performed, we obtain  an adaptable step-size LMS estimation

\begin{equation}
{\bf w}_{k}^{(LMS)} = {\bf w}_{k-1}^{(LMS)} + \eta_k (y_k - {\bf x}_k^T {\bf w}_{k-1}^{(LMS)}){\bf x}_k, 	
\label{eq:lms}
\end{equation}
with
\begin{equation}
\eta_k = \frac{ (\hat{\sigma}_{k-1}^2+ \sigma_d^2)  }{(\hat{\sigma}_{k-1}^2+ \sigma_d^2)  \|{\bf x}_k\|^2 + \sigma_n^2}.\nonumber
\end{equation}
At this point, several interesting remarks can be made:

\begin{itemize}

\item The adaptive rule \eqref{eq:lms} has linear complexity since it does not require us to compute the full matrix $\boldsymbol\Sigma_k$.

\item For a stationary model, we have $\sigma_d^2=0$ in \eqref{eq:prob_lms} and \eqref{eq:sig_k}. In this case, the algorithm remains valid and both the step size and the error variance, $\hat{\sigma}_{k}$, vanish over time $k$. 

\item Finally, the proposed adaptable step-size LMS has only two parameters, $\sigma_d^2$ and $\sigma_n^2$, (and only one, $\sigma_n^2$, in stationary scenarios) in contrast to other variable step-size algorithms \cite{kwong1992variable,aboulnasr1997robust,shin2004variable}. More interestingly, both $\sigma_d^2$ and $\sigma_n^2$ have a clear underlying physical meaning, and they can be estimated in many cases. We will comment more about this in the next section. 
\end{itemize}



\section{Experiments}
\label{sec:experiments}

We evaluate the performance of the proposed algorithm in both stationary and tracking experiments. In the first experiment, we estimate a fixed vector ${\bf w}^{o}$ of dimension $M=50$. The entries of the vector are independently and uniformly chosen in the range $[-1,1]$. Then, the vector is normalized so that $\|{\bf w}^o\|=1$. Regressors $\boldsymbol{x}_{k}$ are zero-mean Gaussian vectors with identity covariance matrix. The additive noise variance is such that the SNR is $20$ dB. We compare our algorithm with standard RLS and three other LMS-based algorithms: LMS, NLMS \cite{sayed2008adaptive}, VSS-LMS \cite{shin2004variable}.\footnote{The used parameters for each algorithm are: for RLS $\lambda=1$, $\epsilon^{-1}=0.01$; for LMS $\mu=0.01$; for NLMS $\mu=0.5$; and for VSS-LMS $\mu_{max}=1$, $\alpha=0.95$, $C=1e-4$.} The probabilistic LMS algorithm in \cite{park2014probabilistic} is not simulated because it is not suitable for stationary environments.

In stationary environments, the proposed algorithm has only one parameter, $\sigma^2_n$. We simulate both the scenario where we have perfectly knowledge of the amount of noise (probLMS1) and the case where the value $\sigma^2_n$ is $100$ times smaller than the actual value (probLMS2). The Mean-Square Deviation (${\sf MSD} = {\mathbb E} \| {\bf w}_0 - {\bf w}_k \|^2$), averaged out over $50$ independent simulations, is presented in Fig. \ref{fig:msd_statationary}.



\begin{figure}[htb]
\centering
\begin{minipage}[b]{\linewidth}
  \centering
  \centerline{\includegraphics[width=\textwidth]{results_stationary_MSD}}
\end{minipage}
\caption{Performance in terms of MSD of probabilistic LMS with both optimal (probLMS1) and suboptimal (probLMS2) compared to LMS, NLMS, VS-LMS, and RLS.}
\label{fig:msd_statationary}
\end{figure}

The performance of probabilistic LMS is close to RLS (obviously at a much lower computational cost) and largely outperforms previous variable step-size LMS algorithms proposed in the literature. Note that, when the model is stationary, i.e. $\sigma^2_d=0$ in \eqref{eq:trans_eq},  both the uncertainty $\hat{\sigma}^2_k$, and the adaptive step size $\eta_k$, vanish over time. This implies that the error tends to zero when $k$ goes to infinity. Fig. \ref{fig:msd_statationary} also shows that the proposed approach is not very sensitive to a bad choice of its only parameter, as demonstrated by the good results of probLMS2, which uses a $\sigma^2_n$ that is $100$ times smaller than the optimal value. 


\begin{figure}[htb]
\centering
\begin{minipage}[b]{\linewidth}
  \centering
  \centerline{\includegraphics[width=\textwidth]{fig2_final}}
\end{minipage}
\caption{Real part of one coefficient of the measured and estimated channel in experiment two. The shaded area represents two standard deviations from the prediction {(the mean of the posterior distribution)}.}
\label{fig_2}
\end{figure}


\begin{table}[ht]
\begin{footnotesize}
\setlength{\tabcolsep}{2pt}
\def1.5mm{1.5mm}
\begin{center}
\begin{tabular}{|l@{\hspace{1.5mm}}|c@{\hspace{1.5mm}}|c@{\hspace{1.5mm}}|c@{\hspace{1.5mm}}|c@{\hspace{1.5mm}}|c@{\hspace{1.5mm}}|c@{\hspace{1.5mm}}|}
\hline
Method &  LMS &  NLMS & LMS-2013 & VSSNLMS & probLMS & RLS \\
\hline
\hline
MSD (dB) &-28.45 &-21.07 &-14.36 &-26.90 &-28.36 &-25.97\\
\hline                                                                     
\end{tabular}
\end{center}
\caption{Steady-state MSD of the different algorithms for the tracking of a real MISO channel.}
\label{tab:table_MSD}
\end{footnotesize}

\end{table}
\newpage
In a second experiment, we test the tracking capabilities of the proposed algorithm with {real} data of a wireless MISO channel acquired in a realistic indoor scenario. More details on the setup can be found in \cite{gutierrez2011frequency}. Fig. \ref{fig_2} shows the real part of one of the channels, and the estimate of the proposed algorithm. The shaded area represents the estimated uncertainty for each prediction, i.e. $\hat{\mu}_k\pm2\hat{\sigma}_k$. Since the experimental setup does not allow us to obtain the optimal values for the parameters, we fix these parameters to their values that optimize the steady-state mean square deviation (MSD). \hbox{Table \ref{tab:table_MSD}} shows this steady-state MSD of the estimate of the MISO channel with different methods. As can be seen, the best tracking performance is obtained by standard LMS and the proposed method. 





\section{Conclusions and Opened Extensions}
\label{sec:conclusions}

{We have presented a probabilistic interpretation of the least-mean-square filter. The resulting algorithm is an adaptable step-size LMS that performs well both in stationary and tracking scenarios. Moreover, it has fewer free parameters than previous approaches and these parameters have a clear physical meaning. Finally, as stated in the introduction, one of the advantages of having a probabilistic model is that it is easily extensible:}

\begin{itemize}
\item If, instead of using an isotropic Gaussian distribution in the approximation, we used a Gaussian with diagonal covariance matrix, we would obtain a similar algorithm with different step sizes and measures of uncertainty, for each component of ${\bf w}_k$. Although this model can be more descriptive, it needs more parameters to be tuned, and the parallelism with LMS vanishes.
\item Similarly, if we substitute the transition model of \eqref{eq:trans_eq} by an Ornstein-Uhlenbeck process, 

\begin{equation}
p({\bf w}_k|{\bf w}_{k-1})= \mathcal{N}({\bf w}_k;\lambda {\bf w}_{k-1}, \sigma_d^2), \nonumber
\label{eq:trans_eq_lambda}
\end{equation}
a similar algorithm is obtained but with a forgetting factor $\lambda$ multiplying ${\bf w}_{k-1}^{(LMS)}$ in \eqref{eq:lms}. This algorithm may have improved performance under such a kind of autoregresive dynamics of ${\bf w}_{k}$, though, again, the connection with standard LMS becomes dimmer.

\item As in \cite{park2014probabilistic}, the measurement model \eqref{eq:mess_eq} can be changed to obtain similar adaptive algorithms for classification, ordinal regression, and Dirichlet regression for compositional data. 

\item A similar approximation technique could be applied to more complex dynamical models, i.e. switching dynamical models \cite{barber2010graphical}. The derivation of efficient adaptive algorithms that explicitly take into account a switch in the dynamics of the parameters of interest is a non-trivial and open problem, though the proposed approach could be useful.

\item Finally, like standard LMS, this algorithm can be kernelized for its application in estimation under non-linear scenarios.

\end{itemize}


\begin{appendices}

\section{KL divergence between a general gaussian distribution and an isotropic gaussian}
\label{sec:kl}

 We want to approximate  $p_{{\bf x}_1}(x) = \mathcal{N}({\bf x}; \boldsymbol\mu_1,\boldsymbol\Sigma_1)$ by $p_{{\bf x}_2}({\bf x}) = \mathcal{N}({\bf x}; \boldsymbol\mu_2,\sigma_2^2 {\bf I})$. In order to do so, we have to compute the parameters of $p_{{\bf x}_2}({\bf x})$, $\boldsymbol\mu_2$ and $\sigma_2^2$, that minimize the following Kullback-Leibler divergence,

\begin{eqnarray}
D_{KL}(p_{{\bf x}_1}\| p_{{\bf x}_2}) &=&\int_{-\infty}^{\infty} p_{{\bf x}_1}({\bf x}) \ln{\frac{p_{{\bf x}_1}({\bf x})}{p_{{\bf x}_2}({\bf x})}}d{\bf x} \nonumber  \\
&= &  \frac{1}{2} \{ -M + {\sf Tr}(\sigma_2^{-2} {\bf I}\cdot \boldsymbol\Sigma_1^{-1})  \nonumber \\
  & &  + (\boldsymbol\mu_2 - \boldsymbol\mu_1 )^T \sigma^{-2}_2{\bf I} (\boldsymbol\mu_2 - \boldsymbol\mu_1 )  \nonumber \\
 & &   +  \ln \frac{{\sigma_2^2}^M}{\det\boldsymbol\Sigma_1} \}.  
\label{eq:divergence}
\end{eqnarray}
Using symmetry arguments, we obtain 
\begin{equation}
\boldsymbol\mu_2^{*} =\arg \displaystyle{  \min_{\boldsymbol\mu_2}} \{ D_{KL}(p_{{\bf x}_1}\| p_{{\bf x}_2}) \} = \boldsymbol\mu_1.
\end{equation}
Then, \eqref{eq:divergence} gets simplified into 

\begin{eqnarray}
D_{KL}(p_{{\bf x}_1}\| p_{{\bf x}_2}) = \frac{1}{2}\lbrace { -M + {\sf Tr}(\frac{\boldsymbol\Sigma_1}{\sigma_2^{2}}) + \ln \frac{\sigma_2^{2M}}{\det\boldsymbol\Sigma_1}}\rbrace.
\end{eqnarray}
The variance $\sigma_2^2$ is computed in order to minimize this Kullback-Leibler divergence as

\begin{eqnarray}
\sigma_2^{2*} &=& \arg\min_{\sigma_2^2} D_{KL}(P_{x_1}\| P_{x_2}) \nonumber \\
 &=& \arg\min_{\sigma_2^2}\{ \sigma_2^{-2}{\sf Tr}\{\boldsymbol\Sigma_1\} + M\ln \sigma_2^{2} \} .
\end{eqnarray}
Deriving and making it equal zero leads to

\begin{equation}
\frac{\partial}{\partial \sigma_2^2} \left[ \frac{{\sf Tr}\{\boldsymbol\Sigma_1\}}{\sigma_2^{2}} + M \ln \sigma_2^{2} \right] = \left. {\frac{M}{\sigma_2^{2}}-\frac{{\sf Tr}\{\boldsymbol\Sigma_1\}}{(\sigma_2^{2})^2}}\right|_{\sigma_2^{2}=\sigma_2^{2*}}\left. =0 \right. .
\nonumber
\end{equation}
Finally, since the divergence has a single extremum in $R_+$,
\begin{equation}
\sigma_2^{2*} = \frac{{\sf Tr}\{\boldsymbol\Sigma_1\}}{M}.
\end{equation}




\end{appendices}

\vfill
\clearpage

\bibliographystyle{IEEEbib}
",['The authors used simulated data and real data from a wireless MISO channel.'],2554,multifieldqa_en,en,,1f3b1e37c3d2ead7ab4fe7dd9d5cddd55b2c76d28e6bfc86," Probabilistic models have proven to be very useful in a lot of applications in signal processing where signal estimation is needed. adaptive filtering is a standard approach in estimation problems when the input is received as a stream of data that is potentially non-stationary. We provide a similar connection between state-space models and least-mean-squares (LMS) Our approach is based on approximating the posterior distribution with an isotropic Gaussian distribution. We show how the computation of this approximated posterior leads to a linear-complexity algorithm, comparable to the standard LMS. The main contribution of this paper is that it opens the door to introduce more Bayesian machine learning techniques, such as variational inference and Monte Carlo sampling methods, to adaptive filtering. The proposed algorithm has less free parameters than previous LMS algorithms with variable step size and its parameters are easier to be tuned w.r.t. these algorithms and standard L MS. The use of a probabilistic model provides us with an estimate of the error variance, which is useful in many applications. We assume the observation model to be linear-Gaussian with the following distribution, i.e. the distribution of the observed data is a linear Gaussian. We conclude that the proposed algorithm is suitable for both stationary and non- stationary environments and that it can be applied to a variety of problems such as channel equalization using recurrent RBF neural networks or Bayesian forecasting. The results of the study were published in the open-access issue of the Journal of Applied Probabilistic Research (JABR) (http://www.jabr.org/2013/01/07/14/bi-probabilists-and-adaptive-filtering-algorithm-in-a-new-look-at-the-world-of-machine-learning-algorithms-with-variate-step-size-inhibiting-parameters.html#storylink=cpy) and the online version is available on JABR's website (http:www.jadr.com/2013-01/06/bi/probablistic/adaptive.html) and on the JBAR website ( http:www.-jabracr.gov/ 2013-01-07/12/abi-probs/ adaptive-filming-alumni.html). The paper is also available in the online edition of the journal, which includes an abridged version of the article with a number of corrections and improvements. The online version includes the full text of the JABr article, as well as an image of the code for the adaptive filtering algorithm. The full version of this article can be downloaded for free from the JBRC website (available on JBCR’s website for free. You can also buy a copy of the paper by clicking here: http://www-jbrc.org.uk/ 2013/01-06/08/jbracr-online-article-1.html. The article also includes a video of a demonstration of the algorithm being used in a real-world environment. In a non-stationary scenario, ${\bf w}_k$ follows a dynamic process. In order to initiate the recursion, we assume the following prior distribution. The resulting probability distribution is the maximum-a-posteriori estimate (MAP), which coincides with the RLS adaptive rule. In the next section, we show how such a scalar is obtained when such a data set is conditioned on the observed data $y_k. Nevertheless, many applications could prove to be useful to prove the estimate could be sufficiently useful to be sufficiently powerful to be of interest to a large number of people. For example, we could use it to prove that a single scalar for a single set of data could be useful for a large group of people to work with. We will also use the model to show that such scalar could be used to prove many other applications, such as in the case of a large data set with a large sample size and a large distribution of observed data. We conclude that the model can be used as a basis for a number of different types of machine-learning algorithms, for example, to predict the outcome of a test on a set of real-world data. For the purposes of this article, we will use the Bayes' Bayes’ Bayes test, which is based on a large-scale Bayes distribution of the data set $y-k$. We will conclude that this test can be applied to a larger data set in the future by using a more complex Bayes-based Bayes model. We hope to use this model to help people understand how to use Bayes Bayes models in the field of computer-based machine learning. We end the section with a discussion of how to apply the model in the real world to a variety of applications. We are now ready to move on to the next part of the study. We have already covered the first section of the paper. We can now move onto the second section, which covers the third and fourth sections of the book. The fourth and fifth sections will be devoted to the second and the fifth sections of this paper, which are dedicated to the study of the model’s ‘effects’ on the data and the ‘variance’ of the noise in the data. The third and final section will focus on the “effects” of the ’sigma_d^2’ rule on the noisy data. This rule is similar to the one introduced in the 1997kinite in ‘Caykin’’, but with a different ‘adaptive rule’. The model is then used to explain how the noise can be ‘transformed’ into a more ‘accurate’ version of the original noise. It is also used in the fourth section to show how to ‘prove’ that the noise is ‘unlikely’ to be noisy in a given data set. The last section will discuss how to interpret the data in this model. The next section will look at how the data can be transformed into a ‘real-world’ data set, such that it can be shown to be more “accurately’ or ‘inconsistently’ noisy in the model. It will also show that the data are ‘expressed’ in a way that makes it easier to understand how the model ‘expects’ the data to be “transformed.’ The model can then be used for the analysis of this data. We now show that by using the recursive predictive and filtering expressions we obtain an LMS-like adaptive rule. First, let us assume that we have an approximate posterior distribution at $k-1. Then, we approximate the posterior $p({\bf w}_k|y_{1:k})$ with an isotropic Gaussian. Finally, we apply the scalar gain, which operates as a variable step size, to obtain the measure of uncertainty, which is our measure of the posterior variance. The result is a Gaussian distribution with a gain of a gain matrix of size 1/3 of the mean vector and a covariance matrix of 2/3 the square root of the value of the gain matrix. The posterior distribution can be computed using Bayes' Theorem and standard Gaussian manipulations (see for instance \cite[Ch. 4]{murphy2012machine} for more details. For more details on the Bayes Theorem, see http://www.cnn.com/2013/01/29/science/features/bayes-theorem-predictive-prediction.html#storylink=cpy. To see the rest of the article, please visit: http://science-news.co.uk/2013-01/30/science-science-features/predicting-predicted-predictions-lms-like-adaptive-rule-lMS.html and http://science.news/features lms.html/2014-01-29/lMS-sigma-preditional-rule.html%. To see more about the LMS adaptive rule, visit: www.lmsscience.org/2014/02/28/mms- adaptive-rules.html. To read the rest, please go to: http: //www.mssci.org/. For more information, see:http:www.sigma.org.uk /sigma/prediction/lms_prediction_lMS_post.html, http:://sigma_predition.html?p=1,http: //sigma prediction lMS post:1,1,0,1.0,0.0.1. For the full version of this article, go to http://www.msci.gov.uk/. For the second version, see www.sci.news.uk.com/. The full version will be published in the next issue of the magazine, which will be available in the week of September 14, 2014, in the form of a print-out with the headline “LMS Prediction’s LMS Adaptive Rule.” The print-outs will be provided in two sizes, one for each of the two cases: 1/1 and 2/1. See http:/www.mls.gov/2012/09/14/LMS-Prediction-LMS/MMS-Adaptive-Rule.html for more information on how to apply the rule to your own situation and the second for a range of different situations. For example, consider the case where you are trying to predict the outcome of a game. For each game, you can try to predict whether the outcome will be positive or negative, or even if it will be the same as the previous one. For other games, try to guess the outcome by looking at the score of the game and the score. We evaluate the performance of the proposed algorithm in both stationary and tracking experiments. We compare our algorithm with standard RLS and three other LMS-based algorithms: LMS, NLMS, VS-LMS, and RLS. The performance of probabilistic LMS is close to RLS (obviously at a much lower computational cost) and largely outperforms previous variable step-size LMS algorithms proposed in the literature. We simulate both the scenario where we have perfectly knowledge of the amount of noise (probLMS1) and the case where the value $\sigma^2_n$ is $100$ times smaller than the actual value. The Mean-Square Deviation (MSD) averaged out over $50$ independent simulations is presented in Fig. 1. The algorithm is not simulated because it is not suitable for stationary environments. It also shows that the proposed approach is not very sensitive to a bad choice of its only parameter, as demonstrated by the good results of probLMS2, which uses a $100 $ smaller value than the optimal value. We conclude that when the model is stationary, i.e. both the uncertainty and adaptive step size vanish over time. This implies that the error tends to zero when $k$ goes to infinity. For more details on the algorithm, see http://www.park2014probabilistic.org/prob-lMS-algorithm-park-2014-park.html/. For more information on how the algorithm works, see the Park2014 Prob-L MSD and the Park2014 LMS Algorithm Manifestation and Manifold. For details on how to use the algorithm in stationary environments, please visit the http://www park2014 prob lMS algorithm manifold or the Park 2014 LMS website. For information on using the algorithm in tracking environments, visit thehttp http  http www.Park2014LMS.com/LMS-tracking-algorithms-manifolder.html. For a more detailed description of the algorithm and its features, see  http:// www.park 2014LMS  Algorithm-Manifolder Manivolder.com/. For a detailed discussion of the algorithms, see  http http-www. park2014LLS.org/. for a full version of this article, please see http  http-www Park 2014 LMS. and the LMS site. for more details, such as how the algorithms work in tracking environments and how they are used in tracking. for a fuller version of the article, click here. For the full version, please go to http-http-http www-park2014lMS.org. For an example of a tracking experiment, please click here for a printout of the results from the experiment. for the full report, please note that the results are not yet available for the entire experiment. The full report will be available in the next issue of the journal, which is scheduled to be released in the fall. for publication in the Spring 2014 issue of The Journal of the American Mathematical Society (July 2014). For the next month, we will publish the results of the experiment in the online edition of the Journal of Computer Vision and Machine Learning (August 2014). We will also publish the findings of the second article in the September 2014 issue, which will be published in the journal The American Mathematical Society (September 2014). We have presented a probabilistic interpretation of the least-mean-square filter. Since the experimental setup does not allow us to obtain the optimal values for the parameters, we fix these parameters to their values that optimize the steady-state mean square deviation. The resulting algorithm is an adaptable step-size LMS that performs well both in stationary and tracking scenarios. The algorithm can be kernelized for its application in estimation under non-linear scenarios. A similar approximation technique could be applied to more complex dynamical models. The derivation of efficient adaptive algorithms that explicitly take into account a switch in the dynamics of the parameters of interest is a non-trivial and open problem, though the proposed approach could be useful in this case. We want to approximate  $p 1 (x) = \mathcal{N}({\bf x}; \boldsymbol\mu_2,  ‘sigma_2’, ‘’sigma’ I’;. ‘p’ (x), “p” (x) (‘x’), ’’‘”’(x),’. ’.”.’,.’., ‘;’ ‘.‘,’ .’: ‘, ’,”,. ’;”, ”. ”,.”., ’,. ‘:’!’ , ’.,’%.’].’ The algorithm may have improved performance under such a kind of autoregresive dynamics of ${\bf w}_{k}$, though, again, the connection with standard LMS becomes dimmer. The measurement model can be changed to obtain similar adaptive algorithms for classification, ordinal regression, and Dirichlet regression for compositional data. We would obtain a similar algorithm with different step sizes and measures of uncertainty, for each component of the MISO channel. Although this model needs more parameters to be tuned, and the parallelism with LMS vanishes, it can be used to get a similar result. The proposed method could also be used for more complex models, i.e. switching dynamical model, such as the Ornstein-Uhlenbeck process. It could be used as a starting point for the development of more complex algorithms such as Bayes' Bayes’ Bayes test. It can also be applied in the form of a ‘Bayes test’ or a “Bayes-like’ test. For example, it could use a Gaussian distribution instead of an isotropic Gaussian with a diagonal diagonal covariance matrix. It is possible to use a more complex model to get the same results. The method can be adapted for more complicated models such as machine learning and computer vision. It has fewer free parameters than previous approaches and these parameters have a clear physical meaning. It also has a more general ‘probabilistic’ approach than the one used in the previous article. The results are presented in the ‘Conclusions and Opened Extensions’ section of the paper. The ‘conclusions and opened extensions’ part of the article is the section on how to use the method to improve the M ISO channel tracking performance in a variety of ways. It includes the “open extensions” section. The final section is the part on how the method could be adapted to different data types. The variance $sigma_2^2$ is computed in order to minimize this Kullback-Leibler divergence as $R_+$. Since the divergence has a single extremum in $R +$, making it equal zero leads to the equation: $Sigma 2 2* = M R R_R_1 + M}. The formula is: $RRR_2* = M R R_R 1 + M S S_S_1+M S-S R-S-R-M. The result is that $R+$ is equal to $R-R+1, or $S+R+2, or $S+S+2 +M+S-2, or $R+S + M+S-.2-.2. The equation is as follows:  $sigma-2-2*=0, $R*=1, $S* = 1, and $S' is the sum of $R' and $s' - 1, with $S&S' being the difference between 2 and 1. The expression for the divergence is: “Sigma_1-2”, where “S” is the root of the “sigma’s’” factor. The formula for the variance $S’ is: “M’ R’-1,”  ”“” ““s” = “1” and “m” means ‘m’."
What is the purpose of the baseline in the layout procedure?,"Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go ""perfectly."" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane.
This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes.
While building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying ""banana"" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built.
First understand that the plans show the finished form of the plane. They show the ""projected"" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat.
Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel ""undevelopable"" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition ""developable"". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a ""compounded"" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain.
Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home), the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock.
This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later.
Layout to ensure a fair and true fuselage starts by drawing a reference line (baseline) on the building surface. Refer to figures 2 & 3 and use a wire guide to draw a very straight baseline. About 500 lbs. Of tension should be adequate. One could use a chalk line, but we're talking airplanes here, not house framing.
The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Although the baseline will still be the reference, the top and bottom longerons will be laid separately.
Layout differences don't end there. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. Another major difference is that the bottom & side panels are applied after forming the fuselage box section. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape.
Refer to figure 1 for the layout of the new developed side panel. The firewall (station a) is layed out perpendicular to the baseline. Longitudinal (station) measurements are given along the length of the baseline from the firewall. Vertical dimensions are given to reference the angle and breadths of the station at the baseline.
Notice that the top longeron is bowed outward and that the stations are spaced slightly greater than called out in the plans. When the panels are formed into the box frame section ,they will work into the dimensions specified in the plans.
Strike a centerline, longer than is needed on the building surface using a wire guide. Draw off the firewall line perpendicular to the centerline at one end.
Using the distances listed in the balloons, mark them off on the centerline. Distances are measured to the nearest sixteenth of an inch. Take time to mark them off carefully. Don't mark off the distances in a cumulative fashion. Use the firewall as a common reference.
Using the angles listed at each station, mark off a station line longer than is needed. The angles are measured to the nearest hundredth of a degree. Take time to mark them off carefully.
At each station, start by marking off each short (bottom longeron) line distance from the centerline. Use your set of trammels or beam compass for doing this. Mark the intersection of the short line with the station line.
At each station, mark off each long (top longeron) line distance from the intersection of the short line distance and the station line. Again the trammels or beam compass is best for completing this step. Mark the intersection of the long line distance with the station line.
Using the longeron as a batten, trace out the inside and outside curves of the longeron. After the batten is secure, in between each station, fasten a keeper block inside and outside to preserve the shape of the longeron taking care to avoid potential future interference with the diagonal members to be installed later. The fairing blocks can be removed or left in place if they won't interfere with building. The vertical station members and their diagonals can now be measured and positioned. Remember to refer to the plans for the material thickness direction.
After vertical and diagonal members are cut and fitted, take time to draw their outlines on the building surface to cut down on time and confusion when laying out the opposite side.
Finishing the side panel is accomplished in a manner similar to that called for in the handbook with the exception that the side and bottom skin panels will be attached later.
The next article in the series will discuss jigging and building techniques to ensure alignment and straightness of the flat built side panels. Also covered will be building a ""strongback"" jig to assure alignment of the side panels when they are formed into their final shape.
Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the side and bottom ply skins.
U.S. Mail: Densmore Associates, inc.
ANSI ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand.
""Scarfing"" is the practice of splicing plywood so that short pieces of plywood can be used to span long distances. On the KR, it is required on both the fuselage skins and spar webs. The angle of the splice should be 10 to 12 degrees to maintain strength across the joint. Also, joints should coincide with structural members, such as spar webs or fuselage truss members.
This scarfer is made by mating a regular plunge router (this one costs about $50) to a table saw. Obviously, you really only need a table saw to cut the chamfer, but it does make a nice heavy table for scarfing. You could just as easily use a large work table as the base.First, set the table saw for a 5.5 degree cut (for a 1:12 joint, or 6.5 degree cut for a 10:1 joint), and run a 1 x 6 through on edge to chamfer a corner on the board. Then drill the board for three router mounting holes (two are countersunk) and connect the assembly to the table saw with two 1/4 inch bolts. Use a long (2-3 inch) straight cutting bit to do the cutting. Adjust the bit so it doesn't interfere with your table top, and go to town. Keep pressure on the plywood to ensure contact with the table while you're scarfing. Make sure you feed your material from the same end as you would if you were sawing, or the router will take your plywood away from you and put a big dent in your garage door.
In the late 60's Ken Rand and Stuart Robinson were working as flight system engineers for Douglas Avionics. Ken was working as an electrical engineer, having previously worked for Sperry as an autopilots project engineer, while Stu's degree was in aeronautical engineering from Northrop University. They were two of the guys at the end of the DC-8,9, and 10 assembly lines responsible for correcting some of the nits and picks in various systems before delivery to the customer.
They both wanted to build a fast, inexpensive airplane which was also economical to maintain. Several designs were considered, and plans were bought first for the Jeanie's Teenie and then the Taylor Monoplane. The Monoplane was more to their liking, but would require some modification to fit their needs. A cooperative redesign effort ensued, with virtually no dimensions left untouched. Only the basic fuselage structure, airfoil, and powerplant were retained. The tail shape was Stu's, and came directly from the big DC-8s parked on the ramp outside his office window. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport.
Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Ken's wife Jeanette became owner of RR overnight, and stepped up to keep the plans and parts coming. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79.
To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5 KR2Ss now in the air. Much of the development work done on KR's is now done by the builders themselves. KR builders tend to be innovative, which leads to some interesting modifications. Some of the mods that work eventually creep into the plans. The KR2S is a case in point. Many builders who'd heard of the pitch sensitivity and tight cabin of the KR2 began to build an enlarged version, with the length determined by the most commonly available longeron material. The result is a KR2 that is stretched 2"" between firewall and main spar, and 14"" behind the main spar. Higher gross weights dictated more wing area, with the new standard becoming the Diehl wing skin. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts.
Mike Stearns addresses the KR Forum crowd.
This year's KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Mike Stearns spoke on several topics, including the many sources for KR and homebuilding information available on the Internet. He also mentioned KRNet, the list server devoted entirely to KR aircraft, as well as several notable World Wide Web home pages. He also brought a sample of the new Rand Robinson wing skins with him, and discussed their high temperature core prepreg construction. His KR2S will receive the first set, which is currently being installed at Hinson Composites.
Steve Trentman spoke on his turbine installation. It uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus. sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. More on this exciting development in next month's issue of KROnline.
Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. After researching the KR series, and reading Neil Bingham's ""A Critical Analysis of the KR2"" (Jan 88 Sport Aviation), Les decided to build his as a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. His fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. It is skinned with 1/8"" birch plywood. Spars are covered with plywoood on both fore and aft sides, ala KR2S. Diehl wing skins provide the lift. Horizontal stabilizer and elevator were stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. . The fuselage to cowling junction was made more graceful by adding 1.5 inches to the height of the firewall end of the fuselage sides.
Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. The canopy frame fits snugly into a recess in the foward deck, providing an excellent wind and water seal. The fiberglass work is exemplary.
Seating is luxurious for one.
The cowling is also a work of art, and uses NACA ducts for efficiency. Female molds were made for all the fiberglass parts on Les's plane, so he could proabably be persuaded to make more, if demand dictates. Les also machines a multitude of KR aluminum and steel parts which he now offers for sale.
The firewall was reinforced with aluminum brackets and angles bolted between the longerons in anticipation of the 200 lb Subaru EA-81 engine installation. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy.
Originally built as a taildragger, the fixed gear is made from 4130 steel tubing. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. An early taxi test accident damaged the main gear, and prompted Les to change to tricycle gear. Again, he designed his own fiberglass main gear, and uses a Diehl nose wheel fork with a 4130 strut and 6"" wheel up front.
Early tests revealed cooling problems, which prompted a radiator move from the firewall to a lower cowling location.
The first flight was almost a disaster, as test pilot Randy Smith lost power right after takeoff. He managed a 180 with a safe downwind landing with only minor nosewheel pant damage. The culprit proved to be a spark plug with too much reach, which was quickly remedied. Subsequent flights have shown water temp to be about 210 degrees, oil temp is 220-230, and airspeed is about 180 mph.
Shopping for the Partially Built KR.
This story starts about twenty years ago when I first started looking at the KR-2 as the plane I'd like to build. The only problem at that time was a lack of money, lack of knowledge, and a lack of job stability. I liked the design, except for the low ground clearance of the retractable gear and that a KR was going to be a tight fit for me to fly.
Over the past twenty years I've owned a number of planes, but still always wanted to build my own. I needed one that would fit me, my budget requirements, and have the speed and performance that I wanted. When ""KITPLANES"" published the article featuring Roy Marsh's new KR-2S, it was the first I had heard of any major modifications or improvements to the same old KR design. I believe that article and Roy Marsh's workmanship have probably been the greatest boon to Rand Robinson (RR) in the last twenty years. It certainly caught my eye! Here was the same design I had decided I wanted to build twenty years ago, with all of the improvements I wanted. It was sitting on fixed gear with some reasonable ground clearance. It had the capability to be built large enough to accommodate me. It has enough prefab parts available that it didn't have to be 100% scratch built if I decided to hurry the project along. And it had the speed I wanted. I knew that Roy's published speeds were probably not realistic expectations for the average KR, but after knocking around for the last three years in my Champ, anything over 90 mph seems pretty fast to me.
After purchasing the info kit and the sales video from Rand Robinson, the next step after deciding for sure to build this plane was to order the KR-2 plans and the KR-2S addendum. I finally got my plans and was putting together my first order to start the plane, when my partner in the Champ pointed out that there was a partially completed KR-2S for sale in Trade-a-plane. My initial answer was ""No, I don't even want to look at it. I want to build my own from scratch."" My partner insisted that for the advertised price and the fact that it wasn't too far away, I ought to at least give the guy a call and investigate it. ""No, I don't think I want to buy someone else's problems,"" I persisted. That night I went home and crunched up some numbers on the calculator and finally came to the conclusion that for the sake of my budget for the next several years, I really should give this guy a call.
Three days later, I flew to his place about 400 miles away to take a look at his project. At this point I should probably mention that I consider myself to be fairly knowledgeable about airplane construction, although the vast majority of my experience is with tube and fabric. The rest of this article deals with what I looked for and more importantly what I missed and have had to repair in the last year since I purchased the project.
When we went to the seller's house, I found that the left wing was built using the Dan Diehl wing skins and the right wing skins were leaning against the wall inside the house. Also the canopy was in the house with the canopy covered with paper and tape. I wanted to inspect the fuselage first, so off we went to the shop.
There I found a fuselage sitting on it's gear painted in primer gray. The first step was to inspect the quality of workmanship of what could be seen as it sat. The interior of the fuselage looked as if it had been built with a great deal of care. The fit and finish of all of the interior wood was very nice. Even the gussets looked like they had been painstakingly perfectly fitted. The glass work on the turtle back also looked very precise and clean. It was evenly faired into the vertical and horizontal stabs. The tail also appeared to be well built with the exception of a depression directly over the front and rear spars in the horizontal stabs. He explained that when he moved recently, that he had shot the plane with gray primer to protect it from the weather since he wouldn't have ready access to a shop to put it in right away. It ended up sitting out in the hot south Texas summer sun for a few weeks before he got a shop rented to work in. That caused the glass (or possibly the foam inside the horizontal stab) to swell, except that it held onto the spar, so it was slightly ballooned in front of and behind the spars. His recommendation was to fill it back smooth with micro.
I also found a small linear crack in the lower left wing spar cap on the left wing stub. It appeared to be from over tightening the rear spar wing attach fitting bolts. His explanation was that the crack wasn't important because the rear spars only job is to keep the wings from folding back. I also noticed that the holes for attaching the outer wing to the wing stub were badly rounded out on the rear spar. He explained that the Diehl wing skins require the rear spar to be swept slightly more forward than the stock wings. This won't allow you to use the rear spar attach fittings from RR and that I would need to fabricate a new set of rear spar attach fittings.
I also found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. I couldn't check for function since the right bellcrank and sheeve wasn't installed, the left wing also wasn't installed, and the right wing didn't exist yet.
Next we pulled the inspection panels off of the fuselage and tail and looked at everything I could see with a good flashlight. I didn't find anything else that might be questionable about the fuselage except for a cracked elevator trim tab that was damaged when it fell off it's hanging place on the wall.
Next we spent some time going over his builders log and builders photo album. I still hadn't seen anything that would dissuade me from buying this project.
At this point it was starting to get late and my ride down needed to get airborne for the flight home. I needed to make a decision about whether I wanted this project or not, but I hadn't inspected the wings and canopy yet. I took a cursory look at the left wing and saw lots on micro built up on it and some bubbles in the leading edge, but nothing that looked seriously wrong to my amateur eye. The right wing was only a set of spars in the shop and the Diehl wing skins in the house, so there wasn't much to look at there. The canopy was wrapped in paper and tape, so there wasn't much to look at there either. I decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the project. For the advertised price, I could build a new set of wings and still be way ahead financially. We negotiated a final price, shook hands, took my ride to the airport, and started off in search of a U-haul to haul the project home.
Now, at this point, some of you are thinking about what I surely must have forgotten to inspect and why didn't I take a local A & P or EAA member along for the ride. First of all, I don't know any mechanics locally that have any experience with glass and our EAA chapter of which I am VP is woefully lacking in fiberglass knowledge. Secondly, as you will see, I missed plenty. Some by ignorance, some by just not looking close enough.
Now for a list of the problems that I found over the last year and a few of the fixes that I came up with.
I found that the lower set of rear spar attach fittings on the left rear spar were installed backwards with the longer spaced hole towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Also in the same area he had drilled through the rear spar with a hole saw to create a place for the aileron cable to pass through and managed to cut out the second from the outside vertical brace in the spar. Then he chose to install the aileron bellcranks in front of the rear spar, and cut another hole through the rear spar for the aileron push rod. He also managed to cut out the outside vertical brace in the spar. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in, cut the whole rear spar carrythrough out of the fuselage including ruining the left lower wing skin, or do something else creative to reinforce the spar cap and install a custom built set of attach fittings.
I also found that after I built and installed the right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. The cable that crosses between the two bellcranks had a sharp uphill from the sheeve to the bellcrank in the last 12 inches on either side. This combined with the radius that the bellcranks turn caused the cross cable to pull up tight when the ailerons were pushed to either end of their travel, but allowed the cables to go very slack when the ailerons were centered. Also the Aileron pushrods needed to pass directly through the lower set of rear wing attach fittings to attach to the aileron. This whole rear spar and aileron bellcrank setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it.
I decided that I had to remove the rear fittings from the left wing to be replaced with the new set that my neighborhood machinist was cutting out for me. When I put the wing on the work bench to start removing the rear fittings, I thought I had better take a closer look at the bubbles in the leading edge. I found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leading edge. What I found was that the leading edge had been floxed together and glassed over, but the mold release had never been scrubbed off the leading edge of the wing. It peeled apart for rebuild quite easily.
When I got back to removing the rear spar attach fittings, I noticed that the woodwork inside the wing looked awfully dull. The reason was that the wing had been closed up without varnishing any of the woodwork. This was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer.
I also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. Retapping the fitting the right direction seemed to be a good fix for that problem.
When I finally got around to attaching the wing to the fuselage, I found that the front spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts.
On the fuselage I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. I must complement the manufacturers for taking care of that problem. One call to the Cleveland factory and they shipped me a new set of wheels and brakes even though the receipt for this set was over four years old and in the original builders name. Their only concern was that this set had never been placed in service yet.
I chose to sand the load of micro off the left wing to see what it was covering. When I got down to the glass, I found that there was no glass for the aft inch and a half of the underside of the wing in front of the aileron hinge. With the Diehl wing skins, you build the wings, then cut the ailerons out of trailing edge of the wing. He had mismeasured and cut too much material off the bottom side of the trailing edge in front of the aileron. It was filled by floxing a piece of spruce into the gap to fill the space between the back edge of the fiberglass and the aileron mount. I chose to wrap the trailing edge of that wing, and the other wing to match with a couple of lay-ups of glass.
When I sanded the primer off the aforementioned damaged trim tab, I found that the hinge was floxed to the leading edge of the foam insides of the tab, but not the glass. I also chose to wrap the front of the trim tab with a lay-up of glass.
I decided to pull the paper off the canopy and take a look at it before I'm ready to bolt it on and fly. The original builder had blown his own canopy and after some of the previous problems, I was beginning to have some concerns about not having looked it over closely enough. The canopy turned out to have been blow a little too large. It ended up with a little larger bubble for headroom, which I didn't object to. However, it had more headroom on the right side than the left. Yes, it was just a little bit lopsided. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm.. My fear was that this is just thin enough that it may decide to lay on my head or in my lap when flying on a warm day. It will have to be replaced.
I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and I'm sure that there could have been many other problems that didn't but could have existed on this project. This is also not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. This article is intended to feature some of the problems that you may run into in buying someone else's project.
The final question is, knowing what I have found over the past year, would I have still purchased this project. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. There are a few things that I would have done differently, but nothing that I can't live with. Although I won't be able to say that I built it all from scratch, I have built and rebuild enough of the plane that I should have no problem qualifying under the 51% rule.
You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"".
Here is an brief explanation of how I built my turtledecks. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo.
After the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade.
After the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out.
At this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the ""backbone"" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees.
You can also send me email at: mikemims@pacbell.net if you have any questions or want to share your ideas.
KROnline is an online KR Newsletter devoted to sharing KR information with other builders and pilots in a timely manner. The first issue (September 96) is now available as a zipped MicroSoft Word file at http://members.aol.com/bshadr or as an html document at kronline9.html. If you'd like to submit articles or photos, email Randy Stein at BSHADR@aol.com ------------------------------------------------------------ Don't bother to email Randy though. KROnline has been retired since the KR Newsletter has improved.",['The baseline is used as a reference for the mid point of the firewall for the developed side panel.'],6340,multifieldqa_en,en,,33bfe67a3d40e71a5e5351ea5db4ea55df61a018d071074b," This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 &KR-2 series planes. The solution starts by understanding the three dimensional relationship of the assembled parts being built. The problem starts when the side panels are bent and sloped to form the fuselage box section. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat. This is important when laying out the side and bottom panels onto flat plywood. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, andSloped to Form a Conical section, it takes on an elliptical shape firewall to tailstock. It should be stressed that although this method borrows heavily from proven techniques used in the marine trades, it should not be stressed at this point in the process that it is not a complete solution to the problem of building an airplane with pre-fabricated parts. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to anydegree of satisfaction. If the sides were not sloped, the section formed would be cylINDrical and the longons would lie flat. Since they were not tumbled home, the part formed is now conical. If you want to build an experimental aircraft, you need to start with a minimum of pre- fabricated parts, especially when using the preformed fiberglass parts. You need to build the plane with a basic understanding of the materials you are using. The best way to start is to start by understanding that the parts you are building are not going to last for long periods of time, and that they will need to be replaced with new parts that will last for longer periods of the construction process. The next step is to learn how to use the parts that you have already built to make the plane more reliable. The most important thing is to understand that the plane will not last longer than a few months or even a few years before it is ready to fly. The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape. The next article in the series will discuss jigging and building techniques to ensure alignment and strai. The layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later. Back to Mail Online home. back to the page you came from. Back To the pageyou came from, Back to the pages you came From. The fuselage layout guide is available in English, French, German, Italian, Spanish, and Portuguese. For more information on how to layout a fuselage, visit the fuselage. layoutguide.org. For the layout guide in English and French, visit www.fuselage.com/fuselage-laid-in-the-world-for-aircraft-builders-and-engine-builders. For a guide to layout in Spanish and Portuguese, visit http://www. fuselageguide.co.uk/ fuselage-layout-guide.html. For an overview of the layout procedure in German and Italian, visit: fuselagelayoutguide.com/. For a breakdown of layout procedures in the U.S., visit: www.u.S.A. Aircraft Building Guide. For details on layout in the United States, see: http:/www.uS/Aircraft-Building-Guide.html/. For more details on the layout in Europe and the rest of the world, go to: http:// www.australia.org/airplane-building-guide/faselage- layout-guide-1-2-3-4-5-6-7-7. For information on layout for Russia and the Middle East, see www.aircraftbuilder.com%. For information about layout in other countries, visit  the fuselage building guide. for the Middle Eastern and South American countries, such as Russia and South Africa, see http://aircraftbuilding.co/home.uk/. for more information about the layout process in South Africa and the West African and South America, see  http:www.aastralia-building.org/. For the South American and South African versions of this article, click on the link to the home page for a detailed layout guide. Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the sides and bottom ply skins. ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand. To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5KR2Ss now in the air. KR2 builders tend to be innovative, which leads to interesting mods that the KR2 has. The KR2 is a case in point: Many who'd heard of the pitch and tightness of the cabin of theKR2 began to modify the plans to suit their needs. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport. Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79. Some of the work on KR2's is now done on some modifications on the KR1 and KR2. Some modifications eventually creep into some of the mods that KR2 builder's have done on their KR2 aircraft. The most recent modification is the addition of ailerons to the fuselage to make it easier for the plane to take off and land on a tight angle, such as with aileron-only landing gear, or with a fixed landing gear instead of a roll-up landing gear. It is also the case in which the landing gear came directly from the big DC-8s parked on the ramp outside his office window. The tail shape was Stu's, and came from thebig DC-9s parked in the parking lot outside hisoffice window. It was also the tail shape that led to the design of the KR-2, which is now the world's most popular single-engine fighter jet. It's also the most popular fighter jet in the world, followed by the F/A-18 Hornet and the F-18 Super Hornet. The F-16 Hornet is the most advanced fighter jet on the market today, with a top speed of nearly 100 mph. It has a range of more than 1,200 miles (1,600 km) and can carry a crew of up to 20,000 pounds (9,000 km) on a single engine. It also has the largest wingspan of any U.S. fighter jet, and is the only one of its kind that can take off from a standstill and land vertically. It can also be easily converted into a glider by using a special frame that can be attached to the front of the plane with a piece of plywood, or by attaching it to the back of it with a screwdriver. The fuselage can be made to look more like a conventional fuselage by cutting it in half.  KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. The KR2 is stretched 2"" between firewall and main spar, and 14"" behind the main spar. The fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy Hegy. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. The cowling is also a work of art, and uses NACA ducts for efficiency. The fiberglass work is exemplary. The cockpit is a fiberglass main, with a Diehl nose nose and a Diee nosecone. The instrument panel is made of fiberglass, with an aluminum frame. The landing gear is a tricycle gear, and he designed his own test accident test accident gear. He also offers a multitude of KR aluminum and steel parts which he now offers for sale. He uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. More on this exciting development in next month's issue of KROnline. It is also possible to build a KR2 with a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. The result is aKR2 that is stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts. It will receive the first set of the new Rand Robinson wing skins, which are currently being installed at Hinson Composites. The first set is expected to arrive in the next few weeks, and the KR2S will get the second set later this year. It has a high temperature core prepreg construction, and will be made of high-performance polyethylene terephthalate (HETT) material. It also has a large number of parts that can be pre-assembled by hand. The KR-2S was designed by Rand Robinson in the 1980s. The plane was designed to fly at speeds of up to 180 mph. It was designed with low ground clearance and a large fuselage. It has been partially built by a team of four people. The project has been in the works for more than 20 years and is still in the planning stages. It is not yet clear if the plane will ever be fully built by Robinsons' team. For more information on the project, visit www.kitplans.com. For information on building your own KR, visit http://www.kitspans.co.uk/kits-pans/KR-2s-partially-built-by-randy-robertson-and-the-team-of-four-who-have-been-working-on-it-for-20-years. For details on building a KR, go to kitspanners.com/kickspanners/kips-panners-partial-built.html. For the rest of the story, visit kitsplanners.org/kps-kips/kpinners/kpp-kinspinners-part-1-kickspinners. For all the information on how to build your own kips, visit the kipspanners site.com/. For the full story on the kpinners site, visit kipspaners.org/. For more info on the Kipspinners website, visit: http://kipsbaners.com%. For the complete story on kipspanners.com, visit  www. kipskinners.org. for the complete list of kipspinner's kipsons.For more information about the Kinspanners, visit their website: http:www.kspinner.com/?p=kipspiners. For a full list of the kpsons' kipspins, see http: www.skipspin.org%. For a list of all the kenspinners, see www.kspins.gov.uk/. For a complete list, visithttp:http: http:\/www.skype.com /skyps/skype/skypes/skypiners/skypaners/skips/skypins/sky-paners-skypin-skyps-sky-pins-skypan-skypans-skype-skypes-skyPins-SkyPins. For an overview of the world's most popular kipspeners, see:http://skyps.com/""skyps""/. For an in-depth look at the best of the best kipspunters, see ""skyps"", ""skype"" or ""skypin"" . For a short time, I was the only person in the world who could fly the KR, so I didn't know what to do with it. I decided to build it. CNN.com's John Sutter sat down with a group of friends to try to find a way to get to the top of the list. Sutter started out by looking at the glass work on the turtle back. He found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. He decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the new set of wings and still be way ahead of the rest of the group. Now for a list of the problems that I found over the last year and a few of the fixes that I came up with that I didn't know any of the people at the center of the story about. The top 10 list: ""Some by ignorance, some by just not looking close enough. Some by ignorance and some by not looking very close enough."" The bottom 10 list includes: ""I found a small linear crack in the lower left wing cap on the left wing stub,"" and ""I didn't find anything else that might be questionable about the right-hand side of the wall"" and ""It was evenly faired into the vertical and horizontal stabs"" on the right wing. The bottom ten list includes ""the top 10 things that I could see with a good flashlight, but nothing that looked seriously wrong to my eye"" and the bottom 10 items that I couldn't see with my own eyes, but that I thought would be nice to look at at some point. The list ends at the top with ""The top ten things that you can see with your own eyes,"" and the top 10 items are ""the things you didn't see that you should have seen that you wished you had seen more of"" and a ""top 10 list of things to do with the time you spent on this project"" and someplace else that you wish you had spent more time on it, but didn't have the time to do it on the other side of it. It's time to move on to the next part of the show. The next part is ""The Top 10 Things You Need To Know About The Top 10 List,"" which will be posted at the bottom of the page on Monday. The final part is the ""Top 10 Things to Know about the Top Ten List"" which will feature the most important things you learned from this week's episode of CNN.com iReporters and iReporter photos and videos from around the world. It will also feature some of the best photos of the U.S. from the last few weeks of the year, as well as a look back at some of our favorite moments from the past year. The show will be available on CNN.co.uk and CNN.uk on Monday, July 25. The episode will also be posted on iReport.com on Tuesday, July 26, and Wednesday, July 27, and Thursday, July 28, at 10 a.m. and 7 p.m., July 28. Le towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in. Or do something else creative to reinforce the spar cap. Also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. Retapping the fitting the right direction seemed to be a good fix for that problem. I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. One call to the Cleveland factory and they shipped me a new set of wheels andbrakes even though the set for this set was four years old and in the original builders name. Their only receipt was over receipt was a receipt for the set that was over four years ago and in their co-owner's name. The problem was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer. The rear spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts. I also found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leads edge. It peeled apart for rebuild quite easily. The leading edge had been floxing together and glassed over, but the mold release had never been scrubbed off the lead edge of the wing. It was rectify with asmall hole saw and modified undercoat sprayer to get it back to its original state. The right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. Also the Aileron pushrods needed to pass directly through the lower set of rear wingattach fittings to attach to the ailerson. This whole rear spar and bellcranks setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it. It needed to be replaced with the new set that my neighborhood mach inist was cutting out for me. It had to be removed the rear fittings from theleft wing to be replacing with thenew set that his new set. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. It also had a sharp uphill from the sheeve to the bellcranking in the last 12 inches on either side. This combined with the radius that the bell cranks turn caused the cross cable to pull up tight when the aiderons were pushed to either end of their travel, but allowed the cables to go very slack when they were centered. This article is intended to feature some of the problems that you may run into in buying someone else's project. The canopy turned out to have been blow a little too large. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. This will give you an idea of how tall your aft bulkhead need to be. As far as location, I placed my aft Bulkhead just forward of the lower/front of my vertical fin. I constructed the fuselage, it is glued together with automotive bondoed to the jig. I used the stringers I ripped from the 1x 4s and gave me a male poster form to cover with thin plastic or plastic. This was the only form of cover I could find that was thin enough to cover. The fuselage was glued together. I was able to cover it with a thin plastic form to give it a male cover. It is now ready to fly. You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"". You can also send them to jscott.gov@mailonline.co.uk or call the FAA at 1-800-447-8255. The FAA has no comment on this article or any of the other questions you may have about the Turtledecks. The author has no plans to fly the aircraft in the near future. He plans to continue to build his own turtledecaks in the coming months. He has built and rebuild enough of the plane that he should have no problem qualifying under the 51% rule. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and there could have been many other problems that didn't but could have existed on this project. It's not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. Yes, it was just a little bit lopsided. It had more headroom on the right side than the left. It will have to be replaced. The final question is, would I have still purchased this project? I don't know if I'll ever be able to fly it in the first place, but I'll still live with the knowledge that I've built it all from scratch, even if it's not in service yet. I can't live with that. I'll live with it until I fly it. After the lay-up cured (2 or 3 days) it was removed from the jig. The fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck looked great and only weighed about 5lbs. But I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtlingeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft. In effect these would act as composite stringers. I covered the foam stringers with one layer of 8oz bid @ 45degrees. I scuff sanded and glued the foamstringers in with micro. board. You can also send me email at: mikemims@pacbell.net if you have any questions or want to share your ideas.KROnline is an online KR Newsletter devoted to sharing KR information with other builders and pilots in a timely manner. The first issue (September 96) is now available as a zipped MicroSoft Word file at http://members.aol.com/bshadr or as an html document at kronline9.html. Don't bother to email Randy though. KROnline has been retired since the KR Newsletter has improved. Don’t bother toemail Randy though; KROnline is a great way to stay up to date with the latest news in the hobby and aviation industry. It's free to sign up for the newsletter and it's a free service. You'll get a free download of the first issue of KROnline every month. The KR Newsletter is available in Word, PDF, and E-mail. It can also be downloaded as a zip file at: http://www.kronline 9.html/members/krononline.oard.html#v1.0.0-v1-0-0. It’s also free to download as a ZIP file from the KR website at:http://kronlines.org/newsletter/KronOnline.oard/News.html?v1_0-1-3-3. It includes the September 96 issue of the KR newsletter, as well as the September 1998 issue of Kronlines magazine. The September 1998 edition of the Kron Lines magazine is also available on Kronline.org. The Kron Line 9.0 issue is available on the KR site. The August 1998 issue is the issue that includes the November 1998 issue, and the September 1999 issue."
What does the new Iraqi Body Count organization do?,"Ann's Mega Dub: 12/19/10 - 12/26/10
Got o have a penis to be an expert
Thursday on NPR's Fresh Air, Terry Gross wanted to talk film and music. Since women don't know a thing about either and aren't interested in either, Terry had to find men who were 'experts.'This is C.I.'s "" Iraq snapshot Friday, December 24, 2010. Chaos and violence continue, Nouri's incomplete Cabinet continues to receive criticism, a father offers an 'excuse' for killing his own daughter, and more.Marci Stone (US Headlines Examiner) reports, ""Friday afternoon, Santa is currently in Baghdad, Iraq and on his next stop is Moscow, Russia, according to the 2010 NORAD Santa Tracker. The North American Aerospace Defense Command (NORAD) has been tracking Santa as he makes his annual journey throughout the world."" Gerald Skoning (Palm Beach Post) quotes Santa saying, ""We send our special wishes for peace and goodwill to all. That includes the people of Iraq, Afghanistan, Iran and North Korea."" Please note that this is Santa's seventh trip to Iraq since the start of the Iraq War and, as usual, his journey was known in advance. No waiting until he hit the ground to announce he was going to Iraq -- the way George The Bully Boy Bush had to and the way US President Barack Obama still has to. In the lead up to Santa's yearly visit, many 'authorities' in Iraq began insisting that Christmas couldn't be celebrated publicly, that even Santa was banned. Gabriel Gatehouse (BBC News) quotes Shemmi Hanna stating, ""I wasn't hurt but I wish that I had been killed. I wish I had become a martyr for this church, but God kept me alive for my daughters."" Shemmi Hanna was in Our Lady of Salvation Church in Baghdad when it was assaulted October 31st and she lost her husband, her son, her daughter-in-law and her infant grandson in the attack. The October 31st attack marks the latest wave of violence targeting Iraqi Christians. The violence has led many to flee to northern Iraq (KRG) or to other countries. Zvi Bar'el (Haaretz) notes, ""This week the Iraqi legislature discussed the Christians' situation and passed a resolution in principle to help families who fled. However, the parliament does not know where the Christians are, how many are still in Iraq, in their homes, and how many have found asylum in Iraqi Kurdistan."" John Leland (New York Times) reports:The congregants on Friday night were fewer than 100, in a sanctuary built for four or five times as many. But they were determined. This year, even more than in the past, Iraqi's dwindling Christian minority had reasons to stay home for Christmas. ""Yes, we are threatened, but we will not stop praying,"" the Rev. Meyassr al-Qaspotros told the Christmas Eve crowd at the Sacred Church of Jesus, a Chaldean Catholic church. ""We do not want to leave the country because we will leave an empty space."" Raheem Salman (Los Angeles Times) reports, ""Rimon Metti's family will go to Christian services on Christmas Day, but his relatives will be praying for their own survival and wondering whether this is their last holiday season in Baghdad. If they had any grounds for optimism about the future of their faith in Iraq, it vanished this year amid repeated attacks on fellow believers."" Shahsank Bengali (McClatchy Newspapers) adds, ""Nearly two months after a shocking assault by Islamist militants, Our Lady of Salvation Catholic Church will commemorate Christmas quietly, with daytime mass and prayers for the dead, under security fit more for a prison than a house of worship. It is the same at Christian churches across Baghdad and northern Iraq, where what's left of one of the world's oldest Christian communities prepares to mark perhaps the most somber Christmas since the start of the Iraq war.""Meanwhile Taylor Luck (Jordan Times) reports on Iraqi refugees in Jordan:Although the calendar will say December 25, for Theresa, Saturday will not be Christmas. There will be no cinnamon klecha cooling on the dining room table, no outdoor ceramic nativity scene, no readings of hymns with relatives. The 63-year-old Iraqi woman has even refused to put up Christmas lights in the crowded two-room Amman hotel apartment she has called home since fleeing Baghdad last month.""There is no holiday spirit. All we have is fear,"" she said.This holiday will instead mark another year without news from her 46-year-old son, who was kidnapped outside Baghdad in late 2006.From Turkey, Sebnem Arsu (New York Times -- link has text and video) notes the increase in Iraq refugees to the country since October 31st and quotes Father Emlek stating, ""I've never seen as many people coming here as I have in the last few weeks. They also go to Lebanon, Jordan and Syria but it seems that Turkey is the most popular despite the fact that they do not speak the language."" Jeff Karoub (AP) reports on the small number of Iraqi refugees who have made it to the US and how some of them ""struggle with insomnia, depression and anxiety.""One group in Iraq who can openly celebrate Christmas are US service members who elect to. Barbara Surk (AP) reports that tomorrow Chief Warrant Officer Archie Morgan will celebrate his fourth Christmas in Iraq and Captain Diana Crane is celebrating her second Christmas in Iraq: ""Crane was among several dozen troops attending a Christmas Eve mass in a chapel in Camp Victory, an American military base just outside Baghdad."" Marc Hansen (Des Moines Reigster) speaks with six service members from Iowa who are stationed in Iraq. Sgt 1st Class Dennis Crosser tells Hansen, ""I certainly understand from reading the paper what's going on in Afghanistan and the attention definitely needs to be on the troops there. But everyone serving here in Operation New Dawn appreciates a little bit of attention as we finish this up.""Today Jiang Yu, China's Foreign Minister, issued the following statement, ""We welcome and congratulate Iraq on forming a new government. We hope that the Iraqi Government unite all its people, stabilize the security situation, accelerate economic reconstruction and make new progress in building its country."" James Cogan (WSWS) reports:US State Department official Philip Crowley declared on Wednesday that Washington had not ""dictated the terms of the government"". In reality, constant American pressure was applied to Maliki, Allawi, Kurdish leaders and other prominent Iraqi politicians throughout the entire nine-month process to form a cabinet. The US intervention included numerous personal phone calls and visits to Baghdad by both President Barack Obama and Vice President Joe Biden.The key objective of the Obama administration has been to ensure that the next Iraqi government will ""request"" a long-term military partnership with the US when the current Status of Forces Agreement (SOFA) expires at the end of 2011. The SOFA is the legal basis upon which some 50,000 American troops remain in Iraq, operating from large strategic air bases such as Balad and Tallil and Al Asad. US imperialism spent billions of dollars establishing these advanced bases as part of its wider strategic plans and has no intention of abandoning them.Cogan's only the second person to include the SOFA in his report. Some are impressed with the 'feat' of taking nearly ten months to form a government, stringing the country along for ten months while no decisions could go through. The editorial board of the Washington Post, for example, was full of praise yesterday. Today they're joined by Iran's Ambassador to Iraq, Hassan Danaiifar. The Tehran Times reports that Danaiifar was full of praise today hailing the ""positive and final step which ended the 10-month political limbo in Iraq."" However, Danaiifar was less pie-in-the-sky than the Post editorial board because he can foresee future problems as evidenced by his statement, ""We may witness the emergence of some problems after one and half of a year -- for example, some ministers may be impeached."" Of course, there are already many clouds on the horizon, even if Iranian diplomats and Post editorial boards can't suss them out. For example, Ben Bendig (Epoch Times) noted the objection of Iraq's female politicians to Nouri al-Maliki's decision to nominate only one woman (so far) to his Cabinet: ""Some 50 female lawmakers went to the country's top leadership, the United Nations and the Arab League to voice their concern and desire for increased representation."" BNO notes that protest and also that a group of Iraqi MPs are alleging that Iraqiya bought seats in the Cabinet via money exchanged in Jordan. UPI adds, ""Maliki, a Shiite who has a long history of working with Tehran, has named himself acting minister of defense, interior and national security, three most powerful and sensitive posts in the government he is stitching together. Although Maliki appears to be bending over backward to accommodate rivals among Iraq's Shiite majority as well as minority Sunnis and Kurds in his administration in a spirit of reconciliation, he is unlikely to relinquish those ministries that dominate the security sector."" DPA reports, ""Sheikh Abdel-Mahdi al-Karbalaei, a confident of influential Shiite spiritual leader Ayatollah Ali al-Sistani, said that the new cabinet is 'below the standards' Iraqi citizens had hoped for and suggested it could prove to be weaker than the previous government."" Ranj Alaaldin (Guardian) also spots clouds on the horizon:Lasting peace and stability depends on resolving outstanding disputes with the Kurds on oil, revenue-sharing, security and the disputed territories (Kirkuk in particular). The Kurds, rather than exploiting their kingmaker position to take a stronger proportion of ministries in Baghdad (they are taking just one major portfolio – the foreign ministry), are instead banking on guarantees from Maliki to implement their list of 19 demands that includes resolving the above disputes in their favour.They may have been naive, though. With their historical and federalist partners, the Islamic supreme council of Iraq in decline, the Kurds may be isolated in the new government – a government dominated by the nationalistic and centrist characteristics of the INM, the Sadrists and indeed State of Law.Maliki may, therefore, turn out to be unable to grant concessions even if he wanted to and could use Osama Nujayfi, the new ultra-nationalist speaker of parliament and Kurdish foe, to absorb the Kurdish criticism and insulate himself from any attacks.AP reports that Iraqi police sought out a 19-year-old woman because of rumors that she was working with al Qaida in Mesopotamia only to be greeted with the news that her father allegedly killed her and the father showed the police where he buried the woman . . . last month. The story begs for more than it offers. The most obvious observation is: what does it say that a woman's allegedly killed by her father and no one says a word for over a month? After that, it should probably be noted that there are many men in Iraq killing women who, no doubt, would love to also be able to pin the blame on al Qaida. In other violence, Reuters notes a house bombing in Haswa which claimed the life of Mohammed al-Karrafi, ""his wife, two sons and a nephew"" -- as well as injuring four more people, and a Samarra roadside bombing which claimed the lives of 2 police officers. DPA notes it was two homes bombed in Haswa and that the Samarra roadside bombing also injured four Iraqi soldiers. Jomana Karadsheh (CNN) reports, ""Another policeman was wounded in Baghdad Friday night when a roadside bomb detonated by a police patrol, an Interior Ministry official told CNN.""And we'll close with this from Peace Mom Cindy Sheehan's latest Al Jazeera column:The recent repeal of the US military policy of ""Don't ask, don't tell"" is far from being the human rights advancement some are touting it to be. I find it intellectually dishonest, in fact, illogical on any level to associate human rights with any military, let alone one that is currently dehumanising two populations as well as numerous other victims of it's clandestine ""security"" policies.Placing this major contention aside, the enactment of the bill might be an institutional step forward in the fight for ""equality""; however institutions rarely reflect reality.Do we really think that the US congress vote to repeal the act and Obama signing the bill is going to stop the current systemic harassment of gays in the military?While I am a staunch advocate for equality of marriage and same-sex partnership, I cannot - as a peace activist - rejoice in the fact that now homosexuals can openly serve next to heterosexuals in one of the least socially responsible organisations that currently exists on earth: The US military.It is an organisation tainted with a history of intolerance towards anyone who isn't a Caucasian male from the Mid-West. Even then I'm sure plenty fitting that description have faced the terror and torment enshrined into an institution that transforms the pride and enthusiasm of youth into a narrow zeal for dominating power relations.And we'll close with this from Francis A. Boyle's ""2011: Prospects for Humanity?"" (Global Research):Historically, this latest eruption of American militarism at the start of the 21st Century is akin to that of America opening the 20th Century by means of the U.S.-instigated Spanish-American War in 1898. Then the Republican administration of President William McKinley stole their colonial empire from Spain in Cuba, Puerto Rico, Guam, and the Philippines; inflicted a near genocidal war against the Filipino people; while at the same time illegally annexing the Kingdom of Hawaii and subjecting the Native Hawaiian people (who call themselves the Kanaka Maoli) to near genocidal conditions. Additionally, McKinley's military and colonial expansion into the Pacific was also designed to secure America's economic exploitation of China pursuant to the euphemistic rubric of the ""open door"" policy. But over the next four decades America's aggressive presence, policies, and practices in the ""Pacific"" would ineluctably pave the way for Japan's attack at Pearl Harbor on Dec. 7, 194l, and thus America's precipitation into the ongoing Second World War. Today a century later the serial imperial aggressions launched and menaced by the Republican Bush Jr. administration and now the Democratic Obama administration are threatening to set off World War III. By shamelessly exploiting the terrible tragedy of 11 September 2001, the Bush Jr. administration set forth to steal a hydrocarbon empire from the Muslim states and peoples living in Central Asia and the Persian Gulf under the bogus pretexts of (1) fighting a war against international terrorism; and/or (2) eliminating weapons of mass destruction; and/or (3) the promotion of democracy; and/or (4) self-styled ""humanitarian intervention."" Only this time the geopolitical stakes are infinitely greater than they were a century ago: control and domination of two-thirds of the world's hydrocarbon resources and thus the very fundament and energizer of the global economic system – oil and gas. The Bush Jr./ Obama administrations have already targeted the remaining hydrocarbon reserves of Africa, Latin America, and Southeast Asia for further conquest or domination, together with the strategic choke-points at sea and on land required for their transportation. In this regard, the Bush Jr. administration announced the establishment of the U.S. Pentagon's Africa Command (AFRICOM) in order to better control, dominate, and exploit both the natural resources and the variegated peoples of the continent of Africa, the very cradle of our human species. This current bout of U.S. imperialism is what Hans Morgenthau denominated ""unlimited imperialism"" in his seminal work Politics Among Nations (4th ed. 1968, at 52-53): The outstanding historic examples of unlimited imperialism are the expansionist policies of Alexander the Great, Rome, the Arabs in the seventh and eighth centuries, Napoleon I, and Hitler. They all have in common an urge toward expansion which knows no rational limits, feeds on its own successes and, if not stopped by a superior force, will go on to the confines of the political world. This urge will not be satisfied so long as there remains anywhere a possible object of domination--a politically organized group of men which by its very independence challenges the conqueror's lust for power. It is, as we shall see, exactly the lack of moderation, the aspiration to conquer all that lends itself to conquest, characteristic of unlimited imperialism, which in the past has been the undoing of the imperialistic policies of this kind…. On 10 November 1979 I visited with Hans Morgenthau at his home in Manhattan. It proved to be our last conversation before he died on 19 July 1980. Given his weakened physical but not mental condition and his serious heart problem, at the end of our necessarily abbreviated one-hour meeting I purposefully asked him what he thought about the future of international relations. iraqbbc newsgabriel gatehousethe new york timesjohn lelandhaaretzzvi bar'elthe jordan timestaylor luckthe associated pressjeff karoubthe los angeles timesraheem salmancnnjomana karadsheh
Terry thinks she's a man
Yesterday on NPR's Fresh Air the hour went to a male TV critic. It's always a man with Terry. Always. And somebody tell her that a snotty, snooty TV critic really doesn't make for good programming.This is C.I.'s ""Iraq snapshot:"" Thursday, December 23, 2010. Chaos and violence continue, Iraqi women make clear their displeasure over the Cabinet make up, Daniel Ellsberg and Veterans for Peace get some recognition, and more. Last Thursday a protest held outside the White House. One of the organizers was Veterans for Peace and Pentagon Papers whistle blower Daniel Ellsberg participated and spoke. Juana Bordas (Washington Post) advocates for both of them to be named persons of the year: Veterans for Peace and Daniel Ellsberg should be this year's person of the year because of their courage and bravery to stand up for all of us who believe that ""war is not the answer."" Moreover in a time of economic recession, the war machine is bankrupting our country. As John Amidon, a Marine Corps veteran from Albany asked at the White House protest, ""How is the war economy working for you?""While unemployment rates hover near 10 percent, there is no doubt that the U.S. economy and quality of life is faltering. Worldwide we are 14th in education, 37th in the World Health Organization's ranking on medical systems, and 23rd in the U.N. Environmental Sustainability Index on being most livable and greenest benefits. There is one place we take the undeniable world lead. The US military spending accounts for a whopping 46.5 percent of world military spending--the next ten countries combined come in at only 20.7 percent. Linda Pershing (Truthout) reports, ""Responding to a call from the leaders of Stop These Wars(1) - a new coalition of Veterans for Peace and other activists - participants came together in a large-scale performance of civil resistance. A group of veterans under the leadership of Veterans for Peace members Tarak Kauff, Will Covert and Elaine Brower, mother of a Marine who has served three tours of duty in Iraq, sponsored the event with the explicit purpose of putting their bodies on the line. Many participants were Vietnam War veterans; others ranged from Iraq and Afghanistan war veterans in their 20s and 30s to World War II vets in their 80s and older. They were predominately white; men outnumbered women by at least three to one. After a short rally in Lafayette Park, they formed a single-file procession, walking across Pennsylvania Avenue to the solemn beat of a drum. As they reached the police barricade (erected to prevent them from chaining themselves to the gate, a plan they announced on their web site), the activists stood shoulder to shoulder, their bodies forming a human link across the 'picture postcard' tableau in front of the White House."" Maria Chutchian (Arlington Advocate) quotes, participant Nate Goldshlag (Vietnam veteran) stating, """"There was a silent, single file march around Lafayette Park to a drum beat. Then we went in front of the White House,. There were barricades set up in front of white house fence. So when we got there, we jumped over barricades and were able to get right next to the White House fence."" Participant Linda LeTendre (Daily Gazette) reports: At the end of the rally, before the silent, solemn procession to the White House fence, in honor of those killed in Iraq and Afghan wars of lies and deceptions, the VFP played taps and folded an American flag that had been left behind at a recent funeral for the veteran of one of those wars. Two attendees in full dress uniform held and folded the flag. I had the image of all of the people who stood along the roads and bridges when the bodies of the two local men, Benjamin Osborn and David Miller, were returned to the Capital District. I thought if all of those people were here now or spoke out against war these two fine young men might still be with us.I was blessed enough to be held in custody with one of those in uniform; a wonderful young man who had to move from his hometown in Georgia because no one understood why as a veteran he was against these wars. Even his family did not understand. (He remains in my prayers.)Our plan was to attach ourselves to the White House fence until President Obama came out and talked to us or until we were arrested and dragged away. I don't have to tell you how it ended.Mr. Ellsberg was one of 139 people arrested at that action. We've noted the protest in pretty much every snapshot since last Thursday. If something else comes out that's worth noting on the protest, we'll include it. We will not include people who don't have their facts and it's really sad when they link to, for example, Guardian articles and the links don't even back them up. It's real sad, for example, when they're trashing Hillary (big strong men that they are) and ripping her apart and yet Barack? ""Obama's inaccurate statements""??? What the hell is that? You're inferring he lied, say so. Don't be such a little chicken s**t. It's especially embarrasing when you're grandstanding on 'truth.' Especially when you're the little s**t that clogged up the public e-mail account here in the summer of 2008 whining that you were holding Barack to a standard, then admitting that you weren't, then whining that if you did people would be mean to you. Oh, that's sooooooo sad. Someone might say something bad about you. The horror. You must suffer more than all the people in Iraq and Afghanistan combined. While the action took place in DC, actions also took place in other cities. We've already noted NYC's action this week, Doug Kaufmann (Party for Socialism & Liberation) reports on the Los Angeles action: Despite heavy rain, over 100 people gathered in Los Angeles on the corner of Hollywood and Highland to demand an end to the U.S. wars on Afghanistan and Iraq. People came from as far as Riverside to protest, braving what Southern California media outlets have dubbed the ""storm of the decade."" The demonstration, initiated and led by the ANSWER Coalition, broke the routine of holiday shopping and garnered support from activists and even passers by, who joined in chanting ""Money for jobs and education -- not for war and occupation!"" and ""Occupation is a crime -- Iraq, Afghanistan, Palestine!"" Protesters held banners reading, ""U.S./NATO Out of Afghanistan!"" and ""Yes to jobs, housing and education -- no to war, racism and occupation!""Speakers at the demonstration included representatives of Korean Americans for Peace, ANSWER Coalition, KmB Pro-People Youth, Veterans for Peace, Party for Socialism and Liberation and National Lawyers Guild. Tuesday, Nouri al-Maliki managed to put away the political stalemate thanks to a lot of Scotch -- tape to hold the deal together and booze to keep your eyes so crossed you don't question how someone can claim to have formed a Cabinet when they've left over ten positions to be filled at a later date. One group speaking out is women. Bushra Juhi and Qassmi Abdul-Zahra (AP) report, ""Iraq's female lawmakers are furious that only one member of the country's new Cabinet is a woman and are demanding better representation in a government that otherwise has been praised by the international community for bringing together the country's religious sects and political parties."" As noted Tuesday, though represenation in Parliament is addressed in Iraq's Constitution, there is nothing to address women serving in the Cabinet. Aseel Kami (Reuters) notes one of the most damning aspects of Nouri's chosen men -- a man is heaing the Ministry of Women's Affairs. Iraqiya's spokesperson Maysoon Damluji states, ""There are really good women who could do wel . . . they cannot be neglected and marginalized."" Al-Amal's Hanaa Edwar states, ""They call it a national (power) sharing government. So where is the sharing? Do they want to take us back to the era of the harem? Do they want to take us back to the dark ages, when women were used only for pleasure."" Deborah Amos (NPR's All Things Considered) reports that a struggle is going on between secular impulses and fundamentalist ones. Gallery owner Qasim Sabti states, ""We know it's fighting between the religious foolish man and the civilization man. We know we are fighting like Gandhi, and this is a new language in Iraqi life. We have no guns. We do not believe in this kind of fighting."" Deborah Amos is the author of Eclipse of the Sunnis: Power, Exile, and Upheaval in the Middle East. Meanwhile Nizar Latif (The National) reports that distrust is a common reaction to the new government in Baghdad and quotes high school teacher Hussein Abed Mohammad stating, ""Promises were made that trustworthy, competent people would be ministers this time around, but it looks as if everything has just been divided out according to sectarian itnerests. No attention has been paid to forming a functioning government, it is just a political settlement of vested interests. I'm sure al Maliki will have the same problems in his next four years as he had in the last four years."" Days away from the ten months mark, Nouri managed to finally end the stalemate. Some try to make sense of it and that must have been some office party that the editorial board of the Washington Post is still coming down from judging by ""A good year in Iraq."" First up, meet the new Iraqi Body Count -- an organization that provides cover for the war and allows supporters of the illegal war to point to it and insist/slur ""Things aren't so bad!"" Sure enough, the editorial board of the Post does just that noting the laughable ""civilian deaths"" count at iCasualities. As we noted -- long, long before we walked away from that crap ass website, they're not doing a civilian count.",['It provides cover for the war and allows supporters of the illegal war to point to it.'],4467,multifieldqa_en,en,,048c12357868d908dfb979ef3c5c6cf7f6d85053acf1cac4," This is C.I.I.'s "" Iraq snapshot Friday, December 24, 2010. Chaos and violence continue, Nouri's incomplete Cabinet continues to receive criticism, a father offers an 'excuse' for killing his own daughter, and more. This is Santa's seventh trip to Iraq since the start of the Iraq War and, as usual, his journey was known in advance. This year, even more than in the past, Iraqi's dwindling Christian minority had reasons to stay home for Christmas. ""Yes, we are threatened, but we will not stop praying,"" the Rev. Meyassr al-Qaspotros told the Christmas Eve crowd at the Sacred Church of Jesus, a Chaldean Catholic church. ""We do not want to leave the country because we will leave an empty space,"" he said. Although the calendar will say December 25, for Theresa, Saturday will not be Christmas. There will be no cinnamon cinnamon cooling on the dining room table, no outdoor ceramic nativity scene, and no klecha cooling on an outdoor ceramic plate. The North American Aerospace Defense Command (NORAD) has been tracking Santa as he makes his annual journey throughout the world. He is currently in Baghdad, Iraq and on his next stop is Moscow, Russia, according to the 2010 NORAD Santa Tracker. It is the same at Christian churches across Baghdad and northern Iraq, where what's left of one of the world's oldest Christian communities prepares to mark perhaps the most somber Christmas since the beginning of the war. If they had any grounds for optimism about the future of their faith in Iraq, it vanished this year amid repeated attacks on fellow believers. The Iraqi legislature discussed the Christians' situation and passed a resolution in principle to help families who fled. However, the parliament does not know where the Christians are, how many are still in Iraq,. in their homes, and how many have found asylum in Iraqi Kurdistan. ""I wish I had become a martyr for this church, but God kept me alive for my daughters,"" says Shemmi Hanna, who lost her husband, her son, her daughter-in-law and her infant grandson in the October 31st attack. The attack marks the latest wave of violence targeting Iraqi Christians and has led many to flee to northern Iraq (KRG) or to other countries. The congregants on Friday night were fewer than 100, in a sanctuary built for four or five times as many. But they were determined. They will go to Christian services on Christmas Day, but his relatives will be praying for their own survival and wondering whether this is their last holiday season in Baghdad. The church will commemorate Christmas quietly, with daytime mass and prayers for the dead, under security fit more for a prison than a house of worship. It will be the first time in two months that the church has been attacked by Islamist militants. It was attacked on October 31th and the attack was carried out by a group of around 20 people, including two of the country's top security officials. It has been the deadliest attack on a Christian church since the fall of Saddam Hussein. Iraqi woman refuses to put up Christmas lights in Amman hotel apartment she has called home since fleeing Baghdad last month. US service members who elect to. openly celebrate Christmas in Iraq. Iran's Ambassador to Iraq hails the ""positive and final step which ended the 10-month political limbo in Iraq"" Some are impressed with the 'feat' of taking nearly ten months to form a government, stringing the country along for ten months while no decisions could go through. The key objective of the Obama administration has been to ensure that the next Iraqi government will ""request"" a long-term military partnership with the US when the current Status of Forces Agreement (SOFA) expires at the end of 2011. The SOFA is the legal basis upon which some 50,000 American troops remain in Iraq, operating from large strategic air bases such as Balad and Tallil and Al Asad. US imperialism spent billions of dollars establishing these advanced bases as part of its wider strategic plans and has no intention of abandoning them. ""We hope that the Iraqi Government unite all its people, stabilize the security situation, accelerate economic reconstruction and make new progress in building its country,"" says Jiang Yu, China's Foreign Minister. The Tehran Times reports that Hassan Danaiifar was less pie-in-the-sky than the Washington Post editorial board because he can foresee future problems as evidenced by his statement, ""We may witness the emergence of some problems after one and half of a year -- for example, some ministers may be impeached"" The Washington Post's editorial board was full of praise yesterday. Today they're joined by Iran's ambassador to Iraq, Hassan Danaifar, who said, ""Everyone serving here in Operation New Dawn appreciates a little bit of attention as we finish this up"" The Post is the only paper in the U.S. that included the SOFA in its report on the Iraqi government's formation. It's only the second person to include the SoFA in his report. The Washington Times' report on Iraq's formation of a new government was published on October 31st. It was accompanied by a photo of Maliki, Allawi, Kurdish leaders and other prominent Iraqi politicians throughout the entire nine-month process to form the cabinet. In reality, constant American pressure was applied to Maliki and Allawi during the entire process. The State Department official Philip Crowley declared on Wednesday that Washington had not ""dictated the terms of the government"". In. reality, the US intervention included numerous personal phone calls and visits to Baghdad by both President Barack Obama and Vice President Joe Biden. The Post's report also included a picture of President Obama, Vice President Biden and Iraqi Prime Minister Nouri al-Maliki during a visit to Baghdad on November 30th. The article was published in the New York Times' online edition on November 29th. Nouri al-Maliki's new cabinet is 'below the standards' Iraqi citizens had hoped for. Peace Mom Cindy Sheehan: The repeal of the US military policy of ""Don't ask, don't tell"" is far from being the human rights advancement some are touting it to be. In other violence, Reuters notes a house bombing in Haswa which claimed the life of Mohammed al-Karrafi, ""his wife, two sons and a nephew"" -- as well as injuring four more people. A Samarra roadside bombing claimed the lives of 2 police officers. And a 19-year-old woman was allegedly killed by her father and no one says a word for over a month. It should probably be noted that there are many men in Iraq killing women who, no doubt, would love to also be able to pin the blame on al Qaida. They may be isolated in the new government – a government dominated by the nationalistic and centrist characteristics of the INM, the Sadrists and indeed State of Law. The Kurds, rather than exploiting their kingmaker position to take a stronger proportion of ministries in Baghdad (they are taking just one major portfolio – the foreign ministry), are instead banking on guarantees from Maliki to implement their list of 19 demands that includes resolving the above disputes in their favour. Maliki may, therefore, turn out to be unable to grant concessions even if he wanted to and could use Osama Nujayfi, the new ultra-nationalist speaker of parliament and Kurdish foe, to absorb the Kurdish criticism and insulate himself from any attacks. The story begs for more than it offers. I find it intellectually dishonest, in fact, illogical on any level to associate human rights with any military, let alone one that is currently dehumanising two populations aswell as numerous numerous numerous others. It's time to end the war in Iraq with a cease-fire and a return to normalcy in the Middle East, writes Peace Mom Sheeran. The U.S. military is still in charge of the country, she says, and it's time for the U.N. to do its part in the fight against al-Qaeda and the Islamic State of Iraq and Levant (ISIL) to come to an end. The time is now for the United States to end its war in Afghanistan and end the conflict in the region, she adds. It is time to put an end to the bloodshed in Iraq and the destruction in Syria and elsewhere in the world, she writes. The war is over and it is time for a new era of peace and stability in Iraq, she argues. The only thing standing in the way of that is the Iraqi people and their right to a better life and a better future for their children and grandkids, and that's what Maliki is trying to give them. The world is divided on how to get to that point. Do we really think that the US congress vote to repeal the act and Obama signing the bill is going to stop the current systemic harassment of gays in the military? While I am a staunch advocate for equality of marriage and same-sex partnership, I cannot rejoice in the fact that now homosexuals can openly serve next to heterosexuals. The US military is an organisation tainted with a history of intolerance towards anyone who isn't a Caucasian male from the Mid-West. This current bout of U.S. imperialism is what Hans Morgenthau denominated ""unlimited imperialism"" in his seminal work Politics Among Nations (4th ed. 1968, at 52-53): The outstanding historic examples of unlimited imperialism are the expansionist policies of Alexander the Great, Rome, the Arabs in the seventh and eighth centuries, Napoleon I, and Hitler. They all hav. all haved. The serial imperial aggressions launched and menaced by the Republican Bush Jr. administration and now the Democratic Obama administration are threatening to set off World War III. Only this time the geopolitical stakes are infinitely greater than they were a century ago: control and domination of two-thirds of the world's hydrocarbon resources and thus the very fundament and energizer of the global economic system – oil and gas. The Bush Jr./ Obama administrations have already targeted the remaining hydrocarbon reserves of Africa, Latin America, and Southeast Asia for further conquest or domination, together with the strategic choke-points at sea and on land required for their transportation. Other victims of it's clandestine ""security"" policies are the Muslim states and peoples living in Central Asia and the Persian Gulf under the bogus pretexts of (1) fighting a war against international terrorism; and/or (2) eliminating weapons of mass destruction; and (3) the promotion of democracy; and-or (4) self-styled ""humanitarian intervention."" The Bush Sr. administration announced the establishment of the Pentagon's Africa Command (AFRICOM) in order to better control, dominate, and exploit both the natural resources and the variegated peoples of the continent of African, the very cradle of our human species. But over the next four decades America's aggressive presence, policies, and practices in the ""Pacific"" would ineluctably pave the way for Japan's attack at Pearl Harbor on Dec. 7, 194l, and thus America's precipitation into the ongoing Second World War. And we'll close with this from Francis A. Boyle's ""2011: Prospects for Humanity?"" (Global Research) (global.research.com): ""This latest eruption of American militarism at the start of the 21st Century is akin to that of America opening the 20th Century by means of the Spanish-American War in 1898"" Veterans for Peace and Pentagon Papers whistle blower Daniel Ellsberg participated in a protest outside the White House. C.I.A.'s ""Iraq snapshot:"" Chaos and violence continue, Iraqi women make clear their displeasure over the Cabinet make up, and Veterans for Peace get some recognition. Terry Jones thinks she's a man yesterday on NPR's Fresh Air the hour went to a male TV critic. It's always a man with Terry. Always. And somebody tell her that a snotty, snooty TV critic really doesn't make for good programming. The U.S. military spending accounts for a whopping 46.5 percent of world military spending--the next ten countries combined come in at only 20.7 percent. As John Amidon, a Marine Corps veteran from Albany asked, ""How is the war economy working for you?"" While unemployment rates hover near 10 percent, there is no doubt that the U.s. economy and quality of life is faltering. There is one place we take the undeniable world lead. The US is 14th in education, 37th in the World Health Organization's ranking on medical systems, and 23rd in the United Nations Environmental Sustainability Index on being most livable and greenest benefits. In a time of economic recession, the war machine is bankrupting our country. It is time to end the war in Iraq and stop the wars in Afghanistan and Syria. The time has come to stop the war-mongering in the Middle East and in the rest of the world, and to put an end to the use of force in any form. The war is not the answer, and it is time for a new generation of leaders to take the lead and lead the world in the fight against poverty, hunger, disease, and ignorance. The world needs a generation of young men and women who are willing to stand up and say, ""Enough is enough."" The time is now for the world to start listening to the voices of those who have served and sacrificed for this country and its people, and for those who are still serving and dying for this cause. The battle is not over, but the battle for the future is not yet won, and the battle is for a better, more just, and more just world. The fight for a just, just and fair world is the battle of our generation, and our generation is the only one that has the power to change the course of history and the fate of the planet. The only way to win is to keep fighting and to keep the faith in the power of the American people to do what is right and just and right and fair and just. The struggle is not a war, it is a fight for freedom and justice and equality and the right to a better life for all people, no matter how long it takes, and no matter what the cost or the length of the struggle. The Struggle for a Just and Just World is the Struggle for the Rights of All. Vietnam veteran Nate Goldshlag: ""We jumped over barricades and were able to get right next to the White House fence"" People came from as far as Riverside to protest, braving what Southern California media outlets have dubbed the ""storm of the decade"" We will not include people who don't have their facts and it's really sad when they link to, for example, Guardian articles and the links don't even back them up. ""Obama's inaccurate statements""??? What the hell is that? You're inferring he lied, say so. Don't be such a little chicken s**t. It's especially embarrasing when you're grandstanding on 'truth.' Especially when you were the little s**T that clogged up the public e-mail account here in the summer of 2008 whining that you were holding Barack to a standard, then admitting that you weren't, then whining that if you did people would be mean to you. Oh, that's sooooooo sad. Someone might say something bad about you. The horror. You must suffer more than all the people in Iraq and Afghanistan combined. ""Money for jobs and education -- not for war and occupation!"" and ""Occupation is a crime -- Iraq, Afghanistan, Palestine!"" Protesters held banners reading, ""U.S./NATO Out of Afghanistan!"" and, ""Yes to jobs, housing and education - no to war, racism and occupation! We've already noted NYC's action this week, Doug Kaufmann (Party for Socialism & Liberation) reports on the Los Angeles action: Despite heavy rain, over 100 people gathered in Los Angeles on the corner of Hollywood and Highland to demand an end to the U.S. wars on Afghanistan and Iraq. The demonstration, initiated and led by the ANSWER Coalition, broke the routine of holiday shopping and garnered support from activists and even passers by, who joined in chanting, ""Occupations are a crime! Occupations are illegal!"" The action took place in Washington, D.C., and New York City, and 139 people were arrested at that action. We've noted the protest in pretty much every snapshot since last Thursday. If something else comes out that's worth noting on the protest, we'll include it. The action was led by Veterans for Peace, KmB Pro-People Youth, Party for Socialism and Liberation and National Lawyers Guild. It was held in honor of the two local men, Benjamin Osborn and David Miller, who were killed in the Iraq and Afghan wars of lies and deceptions. I thought if all of those people were here now or spoke out against war these two fine young men might still be with us. I was blessed enough to be held in custody with one of those in uniform; a wonderful young man who had to move from his hometown in Georgia because no one understood why as a veteran he was against these wars. Even his family did not understand. (He remains in my prayers.)Our plan was to attach ourselves to theWhite House fence until President Obama came out and talked to us or until we were arrested and dragged away. Iraqi female lawmakers are furious that only one member of the country's new Cabinet is a woman. A man is heaing the Ministry of Women's Affairs. Deborah Amos: A struggle is going on between secular impulses and fundamentalist ones. The Washington Post editorial board is still coming down from judging by ""A good year in Iraq"" ""Things aren't so bad!"" is the laughable ""civilian deaths"" count at iCasualities.com. The group provides cover for the war and allows supporters of the illegal war to point to it and insist/slur ""things aren'tSo bad!"" ""We know we are fighting like Gandhi, and this is a new language in Iraqi life,"" says a gallery owner. ""We have no guns. We do not believe in this kind of fighting"" says a man who owns a gallery in the capital, Baghdad. ""It is just a political settlement of vested interests. I'm sure al Maliki will have the same problems in his next four years as he had in the last four years,"" a high school teacher says of the new government. ""They call it a national (power) sharing government. So where is the sharing? Do they want to take us back to the era of the harem?"" says a woman who works in the ministry. ""There are really good women who could do wel . . . they cannot be neglected and marginalized,"" says Iraqiya's spokesperson Maysoon Damluji. ""I don't know how you can claim to have formed a Cabinet when they've left over ten positions to be filled at a later date,"" says an Iraqi journalist. ""You don't question how someone can claims to have form a Cabinet"" says an analyst. ""This is not a functioning government, it's just a government,"" says another. ""No attention has been paid to forming a functioninggovernment, it is just an election-style settlement,"" says one analyst. 'It looks as if everything has just been divided out according to sectarian itnerests,' says a teacher. 'We know it's fighting between the religious foolish man and the civilization man,' says Qasim Sabti, a Gallery owner in the city of Baghdad."
What is the main advantage of the proposed method in terms of computation time?,"Paper Info

Title: Incorporating Human Path Preferences in Robot Navigation with Minimal Interventions
Publish Date: 16 Mar 2023
Author List: Oriana Peltzer, Dylan Asmar, Mac Schwager, Mykel Kochenderfer

Figure

Hyperplane arrangement of a twodimensional space containing two obstacles (colored in gray).The robot is located inside the pink polytope, surrounded by three adjacent obstacle-free polytopes.Each hyperplane on the boundary of the robot's polytope corresponds to one of the nonredundant constraints in eq.(4).(b)Graph derived from the hyperplane arrangement.The nodes on the graph designate polytopes, and edges designate transitions to adjacent polytopes.To estimate the human's preference, the robot updates a posterior over the goal and over which of the graph transitions φ 1 , φ 2 and φ 3 is preferred by the human.(c)Example preference defined over the graph.The location of the goal is indicated in yellow in the lower right polytope.For each node, the outgoing pink arrow designates the edge on the graph corresponding to the preferred transition between polytopes.
Simple, 10 × 10, 8 polytopes.(b) Map 2: Office, 10 × 10, 56 polytopes.(c) Map 3: Classroom, 20 × 20, 73 polytopes.(d) Sampled observations and robot's executed trajectories.
Fig.5: Maps used for simulating the robot navigation problem with path preferences.In (d), the heading angles observed are indicated with arrows.The goal is indicated with a pink circle, and the orange robot corresponds to the starting location.The blue robot follows a policy that accounts for path preference, while the green robot does not.The opacity of the robots increases with time.
Map 1 problem setup and example realizations for goal-only (green) and path preference (blue) solution methods.The robot starts at the lower left corner of the environment, and the goal of the task (pink circle) is in the upper left area.The robot does not know which goal, among 10 options (shown in light blue squares), is the correct goal.The human provides noisy observations, indicated by arrows, at each iteration.The green robot selects actions according to the goal-only baseline, and the blue robot uses our proposed method to infer path preferences.The polytopes composing G are drawn in blue.Probability of correct goal.WLPHVWHS +J (c) Entropy of goal distribution g.
Fig. 6: Probability of the correct goal, fig.6b, and entropy of the goal belief distribution P (g), fig.6c, for the same problem setup, fig.6a.In this problem instance, the human's preference is to go to the goal by passing on the right side of the obstacle.Results are averaged over 50 runs and the area filled represents one standard deviation above and below the mean value.The goal-only baseline shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference.
Success rates in the simple environment (Map 1).The results are averaged over 6 randomly sampled problem instances (start location, goal location, and goal possibilities), and over 50 runs per problem instance.∆T is the number of time steps separating two consecutive human inputs.The robot's mission time is Tmax = 30 time steps.We selected γ h = 1.5, corresponding to relatively noisy human inputs and making the problem more difficult to solve for the robot.
Computation times for Goal Only and Path Preference methods on Map 1 (fig.5a),Map 2 (fig.5b), and Map 3 (fig.5c),averaged over 100 runs with randomly sampled problem instances.The 95 % confidence interval is provided with the mean.We evaluate computation time at the first iteration of each run (where the search depth takes on its highest value Tmax).

abstract

Robots that can effectively understand human intentions from actions are crucial for successful human-robot collaboration. In this work, we address the challenge of a robot navigating towards an unknown goal while also accounting for a human's preference for a particular path in the presence of obstacles.
This problem is particularly challenging when both the goal and path preference are unknown a priori. To overcome this challenge, we propose a method for encoding and inferring path preference online using a partitioning of the space into polytopes. Our approach enables joint inference over the goal and path preference using a stochastic observation model for the human.
We evaluate our method on an unknown-goal navigation problem with sparse human interventions, and find that it outperforms baseline approaches as the human's inputs become increasingly sparse. We find that the time required to update the robot's belief does not increase with the complexity of the environment, which makes our method suitable for online applications.

INTRODUCTION

Collaboration between humans and robots has become increasingly important and one key aspect of this collaboration is the ability for robots to adapt to human decisions. In many scenarios, such as a robot navigating through a busy room to deliver an item, it is important for the robot to take into account human preferences.
For instance, humans may prefer a specific path that would allow their colleagues to notice the item being delivered, but this preference may change dynamically based on various factors such as changes in the environment or unforeseen circumstances. While some preferences can be incorporated into the path-planning process, accommodating dynamic user preferences in real-time remains challenging.
In this paper, we propose a way to enable robots to adapt to human preferences dynamically by leveraging real-time feedback to inform decision-making. In this work, we tackle the problem of robot navigation in which the robot cannot observe the goal or the preferred path to the goal, but must make navigation decisions that are influenced by humans through recommended actions.
Prior work has explored how to adapt to a human's preference through feedback, but such approaches often require a high level of intervention, which can be time-consuming and impractical in real-world scenarios. To optimize the use of human input and quickly infer the human's preference, Fig. : An autonomous robot navigates in a simulated classroom towards a goal location (pink circle).
At the start of its mission, it receives direction indications (arrows) from a human that indicate which path it should take to get to the goal. In this scenario, the human wants the robot to go around the desks on the right side of the classroom. A robot that does not reason over path preferences (green) will take the shortest path to the goal regardless of the human's input.
Our method (blue) infers the human's path preference from these indications and adapts to their recommendations. we propose an approach that leverages probabilistic representations of human preference and incorporates real-time feedback. Previous research by Bajcsy et al. considered an online adaptation problem in a manipulation task, where the person can apply forces to the robot to indicate their preferences.
By allowing the robot to continue its task while taking into account a probabilistic representation of human preference, their approach does not require frequent inputs. Building on this idea, we adopt a similar approach to adapt to a human's preference in the context of a robot autonomously navigating through a known environment, such as a cluttered office space.
Specifically, we focus on allowing the human to influence the robot's trajectory with respect to obstacles, by providing guidance on preferred routes or paths, while the robot continues to execute its task. Paths can be represented using homotopy classes . However, homotopies can pose computational challenges when used to encode and infer human preferences.
When the robot maintains a belief over homotopy classes, the inference problem can become exponentially complex with the number of obstacles in the space. Additionally, when the goal is unknown, the number of variables increases with the number of candidate destinations. This complexity can render the decision-making problem intractable.
Our solution is to encode path preference based on a partitioning of the environment into polytopes . This representation allows path preferences to be expressed as sets of preferred transitions between adjacent polytopes. Paths belonging to different homotopy classes correspond to different sequences of transitions.
By leveraging conditional independence assumptions, we can make the Bayesian inference problem tractable. These assumptions exploit the fact that human actions provide information about the path in a piece-wise manner. For example, indicating a preference for navigating around a particular obstacle only provides information about the local area and not the entire path.
Finally, after updating its belief representation over the human's preference, the robot can adapt to indications by replanning online. Our contributions are as follows. • We formulate the human-robot collaboration problem as a Partially Observable Markov Decision Process (POMDP) where both the goal of the task and the human's path preference are unknown random variables.
• We propose an encoding of a human's path preference using a partitioning of the environment into polytopes, along with conditional independence assumptions that make the Bayesian inference problem tractable to infer the task goal and path preference online. • Through simulations in two environments of different sizes and complexity, we show that our method is effective for solving problems where the robot must reach a goal that is unknown a-priori while simultaneously adapting to a human's indications.
Our method shows higher success rates compared to baseline approaches when the human inputs are sparse. Our approach enables a robot to make effective navigation decisions in collaboration with a human, even when the goal and path preference are not known in advance, and with minimal human input. In recent years, there has been a growing interest in shared autonomy and interactive systems, where humans and robots work together to accomplish tasks.
Several approaches have been proposed to address the challenge of enabling effective collaboration between human and robot agents while still achieving high task performance. Losey et al. and Jeon, Losey, and Sadigh propose a framework where a human operator is given control of a task-relevant latent action space while an autonomous system handles the rest.
Dragan and Srinivasa present a formalism for arbitrating between a user's input and a robot's policy when both human and robot share control of the same action space. Cognetti et al. [7] provide a method for real-time modifications of a path, . . . Fig. : We model the intent inference problem with the above diagram.
At each step in time, the robot receives an observation ot from the human conditioned on its current location st, the intended goal g, and the human's path preference θ. The robot updates its belief over g and θ and transitions to a next location st+1. while Hagenow et al. present a method that allows an outside agent to modify key robot state variables and blends the changes with the original control.
However, a common challenge of these approaches is the high level of intervention required from humans. Best and Fitch propose a method for predicting an agent's intended trajectory from observations. Rather than maintaining a belief over the agent's future path, they infer the agent's intended goal among a set of candidate locations at the boundary of the space.
This approach provides information on where the agent is heading and generates a distribution of candidate future trajectories for the agent. Inferring the goal of the task among a discrete set of candidates is also relevant to the area of shared autonomy. Javdani, Srinivasa, and Bagnell propose a formalism for shared control of a robotic arm, where the robot must assist the human in picking up an object but needs to infer which object the human has chosen from joystick inputs.
Planning with homotopy class constraints is useful in problems where the robot's requirements are given with respect to obstacles, and Yi, Goodrich, and Seppi consider topological constraints provided by human operators. Bhattacharya propose an efficient algorithm for solving pathplanning problems under homotopic constraints.
However, the number of homotopy classes for a given problem can be infinite, and as the robot changes location and updates its representation of the world, carrying out inference over homotopy classes in a dynamic environment requires recomputing the set of homotopies at every iteration, making the belief update challenging.
Prior work has addressed the challenge of shared autonomy by considering how robots can infer a human's intended goal, or how they can infer the preferred path to a goal. However, we argue that inferring the goal and the path as separate problems can lead to over-confidence in incorrect beliefs about the user's preferences.
To illustrate this point, consider the following scenario: a robot and a human are collaborating to move an object from one end of a room to Fig. : Using the hyperplanes composing the H-representation of each obstacle, we construct a hyperplane arrangement of the obstacle-free space (a). We define the human's preference for the robot's one step action choices as the posterior distribution (given all human input up to that point) over transitions from the current to the neighboring polytopes, i.e. edges on the graph.
Each time the robot transitions to a new polytope, the set of neighbor polytopes and the distribution over human preferences are updated. another, but there is an obstacle in the way. The human would like the robot to take a path around the obstacle on the left, even though the goal is on the right. If the robot only infers the goal from the human's inputs, it may incorrectly assume that the goal is on the right, and become over-confident in this belief.
On the other hand, if the robot only infers the preferred path, it may mistakenly assume that the goal is on the left, leading to a failure in completing the task. To overcome these challenges, our work proposes a joint inference approach that considers both the human's intended goal and their preferred path to that goal.
Specifically, we model the human's preference over different homotopy classes and leverage a conditional independence assumption to provide a tractable solution. In our approach, we assume that the human's inputs are noisily rational conditioned on both the goal and the preference. By jointly inferring the goal and path preference, we can avoid over-confidence in incorrect beliefs about the user's preferences, leading to improved system performance.
We consider the problem of robot navigation in a known environment to an unknown destination, where a human can intervene and provide a heading direction to the robot using a joystick or force cues. The human also has a preference on which path the robot should take with respect to obstacles, and our objective is for the robot to understand the human's intentions and execute the task with minimal interventions.
Let g be a discrete random variable denoting the goal of the task, belonging to a set of candidates Ω g , and let θ be a discrete-valued random variable representing the human's path preference, belonging to a set of possible preferences Θ. The physical location of the robot at time index t is denoted by s t ∈ R 2 , and the robot's action at time index t, belonging to some action space A, is denoted by a t .
The transition model T (s t+1 | s t , a t ) is deterministic, meaning the robot has full control over its future location. At any time step, the human may provide an observation to the robot. When the human intervenes, the robot receives a direction (heading angle) that can be mapped to a future location in space.
More specifically, we map the direction to an intended location, which is the resulting robot location after advancing in the indicated direction for one time step. For simplicity, we consider that the robot directly makes an observation o t of the location indicated by the human. We assume that the robot has a stochastic observation model for the human P (o t | s t , g, θ) that is conditioned on both the goal of the task g and the human's preferred path θ.
We further assume that having chosen a goal and path preference, the human takes actions to noisily minimize a cost function C g,θ that measures the cost of moving from the robot's current location to the goal along the preferred path. For example, C g,θ (s t , o t ) can be the length of the shortest path from location s t to the goal g after taking a first step to o t , and constrained by path preference θ.
We use C g,θ to induce a probability distribution over observations, given by: where γ h is a hyperparameter that designates the rationality coefficient. This model assumes the human will pick the lowest cost action with the highest probability and the likelihood of an action decreases exponentially with the increase in cost .
Our inclusion of the path preference θ sets our approach apart from . The model is shown in fig. represented as a Bayesian Network.

Inference

At each time step where the human provides an observation, the posterior P (g, θ) is given through the Bayesian update We note that the number of Bayesian updates required at each iteration to update the belief is equal to the cardinality of Ω g × Θ. In addition, each Bayesian update involves computing C g,θ ( .
, . ) in eq. ( ), which involves solving an optimization problem (such as a shortest path problem). In section IV, we propose a specific encoding of preference θ for resolving eq. ( ), while ensuring the number of computations of the cost C g,θ (., .) per update does not grow exponentially with the number of obstacles.

Decision Making

We consider a navigation problem where the robot receives reward according to the model R(s t , g, θ, a t ). We wish to find the optimal policy π that maximizes the expected discounted sum of future rewards, with discount factor γ. The above problem is a Partially Observable Markov Decision Process (POMDP) .
In this section, we propose an encoding of human's path preference θ for computing the posterior in eq. ( ). Devifrom the concept of homotopy classes, we define the preference according to a partitioning of the environment into polytopes, as shown in fig. , creating a hyperplane arrangement of the space.
Hyperplane arrangements have been used by Vincent and Schwager in the context of Neural Network verification. In our setting, we leverage this representation to define path preferences as preferred transitions between adjacent regions of the space.

Hyperplane Arrangement

We assume a two-dimensional environment composed of m polytopic obstacles, each defined by their half-space representation (H-representation) where A i ∈ R di×2 and b i ∈ R di , and where d i is the number of edges (hyperplanes) composing polytope i. Let n = i d i be the total number of hyperplanes. We leverage each obstacle's H-representation to construct a hyperplane arrangement of the environment as shown in fig.
.e. a partitioning of the space into polytopes. More specifically, each location in space belongs to a polytope j for which we can write an H-representation of the form where α j i ∈ {−1, 1} di is a vector specific to polytope j and obstacle i corresponding to the relative position of any point in the set with respect to each hyperplane in O i .
Fig. : Intent inference model in a hyperplane arrangement of the obstacle free space. We spatially decompose the preference θ into a set of preferred neighboring polytopes per region of the space. Within each polytope j, the human preference pj is a discrete distribution over the preferred neighbor in N (j).
We assume that for a location st belonging to polytope j, and given goal g and preference pj, the observation ot and any other preference p i,i =j are conditionally independent. Concatenating elements from each obstacle's Hrepresentation, we can write polytope j's H-representation as where Some of the constraints in eq. ( ) (corresponding to rows of A, b and α j ) are redundant, i.e. the set P j does not change upon their removal.
We can further reduce the Hrepresentation of a polytope to include only non-redundant constraints. By removing the rows corresponding to redundant constraints, we obtain new matrices A j e , b j e and α j e such that we can write the polytope's reduced H-representation as The non-redundant constraints correspond to edges of the polytope.
In other words, as the robot continually moves in space, the first hyperplane that it will cross upon exiting the polytope will correspond to one of the polytope's nonredundant constraints. Vincent and Schwager outline an iterative method for removing redundant constraints by solving n linear programs.
We use this method in practice for computing α j e for each polytope. We can now characterize each polytope by a vector α j e ∈ {−1, 1} n j e , where n j e ≤ n is the number of essential constraints of the polytope. The polytopes P j partition the environment into a hyperplane arrangement.

Path Preference

In this section, we provide a definition of preference θ according to a graphical representation of the environment based on the hyperplane arrangement. Under this representation, a path preference corresponds to a set of preferred transitions. In other words, for each polytope in the space, the human will have a preference to which neighboring polytope they wish to transition.
Let G := (V, E) be an undirected graph, where vertices are obstacle-free polytopes, and edges connect two adjacent polytopes. Each polytope is described by a unique vector α j as defined in eq. ( ). Two polytopes are adjacent if they share non-redundant constraints (rows in eq. ( )) corresponding to the same hyperplane (i.e. they are on opposite sides of the hyperplane).
Let N (v) be the set of neighbors of a vertex v. For each vertex, we denote p v the discrete-valued random variable describing which edge in N (v) the human intends to transition to. Using this formalism, we define a path preference as the set of preferred transitions over all nodes in the graph, Let m θ = v∈V |N (v)| be the cardinality of Θ, and m g = |Ω g | the number of possible goals.
A priori, the number of Bayesian updates required to update the belief at every iteration should be m θ × m g . Now, let us assume the conditional independence relationships described by the new problem diagram in fig. . More specifically, we introduce the assumption that conditioned on a robot location s t , the goal g, and the preference for the corresponding vertex p v in the graph, the observation o t and the preference for any other vertex are conditionally independent.
In other words, the observations the human provides can be defined conditioned only on the robot location, the goal, and the human's preference for its current vertex p v . By introducing this assumption, each update step only requires updating the joint (p v , g), reducing the number of cost computations to |N (v)| × m g .
We can notice that by introducing this assumption, we removed the direct relationship between the number of polytopes in the environment and the complexity of the Bayesian update in eq. ( ). In practice, components of θ are not mutually independent. For example, if the human preference at a vertex v 1 is
, it is unlikely that the human will also prefer p v2 = (v 2 , v 1 ) (turning back). We can improve our model by assuming a dependent relationship between preferences for adjacent edges, which does not significantly increase the complexity of the inference problem. An interesting property of our encoding is that any two paths that belong to different homotopy classes will cross different sequences of polytopes, i.e. they correspond to a different sequence of edges on G.
This can be proved by contradiction. Let us suppose that two continuous trajectories ξ 1 and ξ 2 , with the same start and end points and that do not intersect any obstacle, traverse the same regions in G in the same order. From the construction of the hyperplane arrangement, each polytope that the paths traverse through is obstacle-free.
Therefore, within each polytope, there is no obstacle in the area located in between the portions of ξ 1 and ξ 2 that belong to the region. A smooth transformation of ξ 1 into ξ 2 can be obtained by transforming each portion of ξ 1 belonging to the polytopes it intersects into the corresponding portion of ξ 2 for the same polytopes, where the extremities of the trajectory portions are connected to one another along the polytope's edges (where the same edge is crossed by both paths).
Along this transformation, the paths do not intersect any obstacle, and therefore ξ 1 and ξ 2 belong to the same homotopy class.

EXPERIMENTS

We evaluate our model on a simulated navigation task where the robot must reach a goal that is unknown a priori while respecting the path preferences indicated by a human. The robot navigates in a grid world containing obstacles. The transition model is deterministic: the robot selects an adjacent location on the grid to reach at the next time step.
The robot is also allowed to take diagonal actions. Each location s t in the map can be mapped to a vertex v t ∈ G. Therefore, the actions leading to locations mapped to different vertices correspond to edges on the graph. We note f (s t , a t ) the edge crossed by taking action a t from location s t .
The robot is given a mission time limit T max for reaching the goal. In this problem, we assume that the human selects actions to noisily minimize a cost function C g,θ , where θ is defined as per eq. ( ), corresponding to the length of the shortest path to the goal constrained by the preference (where the robot is only allowed to make transitions on G along preferred edges).
More specifically, where δ(s t , g | o t , p vt ) designates the length of the shortest path from s t to g passing by o t and constrained by preference p vt . This is a slight variant of the cost function proposed by Best and Fitch , where we add in a conditioning on the path preference. We compute costs by running the A path planning algorithm on the environment maps (grid worlds with diagonal actions) and impose preference constraints by pruning invalid transitions from the search tree.
Reward model. At each step in time, the robot receives a reward which is a sum of three components: a goal-specific reward a preference-specific reward or penalty We compute solutions to the POMDP defined in section III-B with the online solver POMCP , and with the particularity that within the rollouts, the robot does not expect to collect human inputs.
Each time a solution is computed, the robot takes an action and may receive an observation. If it does, it updates its belief distribution over the unknown problem variables and resolves the POMDP over a receding horizon.

Baselines

• Goal only. The robot solves the POMDP while ignoring the effects of path preference. Similarly to , we assume the human is taking action to minimize a goaldependent cost C g (s t , o t ) = δ(s t , g | o t ), where the conditioning on the preference is removed. We also omit the path preference's contribution to the reward R pref .
• Compliant. The robot complies with the human input, but does not take an initiative. If the user stops providing information, the robot continues in the last direction indicated for 5 time steps (conserving its momentum), then stops. • Blended. We designed an arbitration function to decide between our proposed policy (accounting for path preferences) and the user's recommendation when the robot receives inputs.
Our metric to evaluate confidence in the robot's prediction for the purpose of arbitration is the entropy of the intention distribution H(g, p i ), where p i denotes the preferred neighbor for the current region. Because our representation of the world is discrete, the arbitration is given by a step function.
Denoted by U , the action corresponding to the human's input, and P , the robot's prediction for the optimal action, we write the policy where we chose h = 1.6 as the confidence threshold.

Results

When evaluating the algorithm, we consider that a run is successful if the robot reached the goal within its allocated mission time T max and only made transitions between graph vertices corresponding to the human's preferences. We vary the time delay between human inputs, from constant guidance (∆ T = 1) to only a single observation (∆ T ≥ T max ).
Success rates. Table I reports the success rates for experiments conducted over six randomly sampled problem instances and 50 runs per instance in Map 1 (fig. ). When the human provides inputs at every iteration, the compliant policy shows the highest success rates. However, as ∆ T increases, the compliant robot is not able to accomplish the task within the allotted time as it does not receive sufficient inputs to do so, and performance decreases compared to the autonomous baselines.
We find that in these runs, accounting for path preference consistently improves performance compared with the goal-only baseline. Results also show that blending the user's input with the robot's policy (Path Preference + Blend) when the human provides information leads to improved performance. Belief entropy.
Figure shows a challenging problem instance where the directions the human provides do not align directly with the shortest path to the goal. By ignoring the effects of preferences in the problem model (goal only), the robot quickly infers from observations that the upper left goal is less likely than others (P (g) drops).
The strong decrease in entropy shows that the robot becomes overconfident in this prediction. Overconfidence in an incorrect goal will prevent the agent from finding the correct goal once the human's indications directly align with it, as it needs to correct for the wrong predictions, as shown in the path realization (fig.
). In this realization, the goal-only method (green robot) fails to search the upper left area within the allotted time. By accounting for path preferences in its model, the blue robot's entropy over the goal distribution decreases more steadily, allowing for it to leverage the human's latest observations and reach the goal successfully.
shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference. Computation time. In table II we provide the time required to solve the POMDP, and the time required to update the robot's belief as it receives new observations.
We compute solutions on three maps: a simple 10 × 10 grid world with 8 polytopes (fig. ), a 10 × 10 grid world with 56 polytopes (fig. ), and a 20×20 grid world with 73 polytopes (fig. ). The latter environment being larger, we increase the mission time and the depth of the search tree in POMCP from T max = 30 (Map 1 and Map 2) to T max = 60 (Map 3).
We do not notice an increase in the time required to update the robot's belief with an increase in problem complexity, which is consistent with our observation that the complexity of the Bayesian update should not increase with the number of obstacles or polytopes. On the contrary, the belief update time on Map 2 and Map 3, containing more obstacles, is reduced compared to the first map.
More obstacles result in fewer iterations when solving the constrained shortest path problem with A . Adding constraints due to the obstacles and polytopes reduces the size of the A search tree. C. Limitations Simulation environments. In our simulations, we hardcoded the preference policy over the maps (e.g. in Map 1, go around the table counter-clockwise).
We randomly sampled problem instances (start and goal locations, and goal options) to reduce the bias introduced by these preference choices. To best evaluate and compare the different approaches, it would be best to sample preferences among a distribution of preferences chosen by a human (for example, from benchmarks resulting from a collection of data).
Creating such a benchmark is an interesting direction for future work. Hyperplane arrangement construction. The main limitation of our approach is that the size and geometry of each polytope depends strongly on the geometry of the obstacles, as seen in fig. . Because of this, the robot can make predictions over preferences that are too refined compared with the topology of the environment.
A direct consequence is that when the size of the polytopes is small, the information provided by the human can be incorrectly interpreted as a preference on the robot's immediate action. Our method can be improved by changing the structure of the hyperplane arrangement so that it relies on the topology of the environment, but does not vary strongly with the geometry of the features in the environment.
For this purpose, topometric maps and region construction algorithms are promising directions. We presented an approach for encoding and inferring a human's path preference in an environment with obstacles. By leveraging a partitioning of the space into polytopes and a stochastic observation model, our method allows for joint inference over the goal and path preference even when both are unknown a-priori.
Our experiments on an unknown-goal navigation problem with sparse human interventions demonstrate the effectiveness of our approach and its suitability for online applications. The time required to update the robot's belief does not increase with the complexity of the environment, which further highlights the practicality of our method.",['The time required to update the belief does not increase with the complexity of the environment.'],5665,multifieldqa_en,en,,aeb34faf1b5507b32d6d5b49474ea5c71d4ec2aa84a82d93," Robots that can understand human intentions are crucial for human-robot collaboration. We address the challenge of a robot navigating towards an unknown goal while avoiding obstacles. The goal is indicated with a pink circle, and the orange robot corresponds to the starting location. The blue robot follows a policy that accounts for path preference, while the green robot does not. The opacity of the robots increases with time. We evaluate computation time at the first iteration of each run (where the search depth takes on its highest Tmax). In this work, crucial value of successful human- robot collaboration is addressed. We present our proposed method to incorporate human path preferences in robot navigation with minimal interventions. We also present our results for the goal-only (green) and path preference (blue) solution methods. The results are averaged over 6 randomly sampled problem instances (start location, goal location, and goal possibilities), and over 50 runs per problem instance. The 95 % confidence interval is provided with the mean. The robot's mission time is Tmax = 30 time steps, corresponding to relatively noisy human inputs and making the problem more difficult to solve for the robot. We selected γ h = 1.5 to make the problem easier to solve. We have also published a paper on how to incorporate path preferences into robot navigation. The paper is titled Incorporating Human Path Preferences in Robot Navigation with Minimal Interventions.Publish Date: 16 Mar 2023.Author List: Oriana Peltzer, Dylan Asmar, Mac Schwager, Mykel Kochenderfer, and Mykel Wochenderf. The study was published in the journal of Theoretical and Computational Robotics (http://www.the journal of Computer Science & Engineering (CSE) and the corresponding open-access version is available on the CSE website at: www.thecse.org/content/2013/02/16/14/14-cse-path-preferences-in- robot-navigation-with-minimal-interventions.html. It is also available as a free download from the C SE website at www.cseonline.com/cse/pathpreference.html/. The study is available in the online version of the paper is available for download at: http://www thecSE.org/. The online version also includes the corresponding free download of the book, ""Path Preference in Robot Navigation with Minimally Involved Interventions"", which is published at the same time as the paper. The book is available at the CSC website at http:// www.TheCSE.co.uk/books/Path-Preference-In-Robot-Navigation-With-Minimally-Involved-Interventions/Path Preferences in-Robots-With Minimal Interventions in Robot Navigation.html, as well as on the CSE site at: http://thecSC.org and the online version of this article. The article has been updated to include the following information:  “Path preference in Robot Navigating with Minimal  Interventions”. This problem is particularly challenging when both the goal and path preference are unknown a priori. To overcome this challenge, we propose a method for encoding and inferring path preference online using a partitioning of the space into polytopes. We find that the time required to update the robot's belief does not increase with the complexity of the environment, which makes our method suitable for online applications. We evaluate our method on an unknown-goal navigation problem with sparse human interventions, and find that it outperforms baseline approaches as the human's inputs become increasingly sparse. We propose an approach that leverages probabilistic representations of human preference and incorporates real-time feedback to inform decision-making in real-world scenarios. Back to Mail Online home.Back to the page you came from.. The online version of this article was published by the American Chemical Society (ACS) on November 14, 2013. Back into the page that you came From: http://www.acS.com/news/features/2013/11/14/the-online-adaptation-problem-for-robots-and-human-preferences-and real-time-feedback-in-real-world-scenarios.html#storylink=cpy. We are happy to clarify that this article is based on the ACS version of the article, which was published on November 13, 2013, and is available for free on ACS’s website and on theACS. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. In the U.S., call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http:// www.suicidepreventionlifeline.org/. In the UK, contact the National suicide Prevention Line on 0800-923-7255 or see www.samaritans.org for details on how to get in touch with Samaritans in the UK or the National Crime Agency (NCA) in the United Kingdom. For more information, visit the NSPL website or the NSCL’S website (UK) or the NCAA’ s website (U.S. and UK) for more information. The NCL is a non-profit organisation that provides support to people in need of support. It is also available on the NCCL website (www.nccl.org) and the NCS’ UK website ( www.ncs.org.uk/uk/news/. For more on the NCL, see the NCL website (http:www.ncl.uk). We propose an encoding of a human's path preference using a partitioning of the environment into polytopes. This representation allows path preferences to be expressed as sets of preferred transitions. After updating its belief representation over the human's preference, the robot can adapt to indications by replanning online. Our approach enables a robot to make effective navigation decisions in collaboration with a human, even when the goal and path preference are not known in advance. We show that our method is effective for solving problems where the robot must reach a goal that is unknown a-priori while simultaneously adapting to a human’s indications. Our method shows higher success rates compared to baseline approaches when the human inputs are sparse. We model the intent inference problem with the above diagram. At each step in time the robot receives an observation from the human conditioned on its current location st, the intended goal g, and the human s path preference θ. The robot updates its belief over g and θ and transitions to a next location st+1. This approach provides information on where the agent is heading and generates a distribution of candidate future trajectories for the agent. Inferring the goal of the task among a discrete set of candidates is also relevant to the area of shared autonomy. We propose a formalism for shared  autonomy and interactive systems where both human and robot share control of the same action space. For example, Cognetti et al. [7] provide a method for real-time modifications of a path, . . . Fig. : We show how to make changes to a path in real time using a computer program that can be controlled by a human operator. For more information, visit: http://www.sciencemag.org/content/early/2014/01/29/science-magazine-article-4.html#storylink=cpy. This article is an open-source version of the article that was originally published by the University of California, San Diego, and has been updated to reflect the latest developments in the field of machine learning and artificial intelligence. To see the latest version of this article, please go to: http:www.sagepub.com/news/features/2014-01-29/stories/science/top-stories/topstories/storyline-machine-learning-and-robot-collaboration-with-human-roots-in-interactive-systems.html. The article has been amended to reflect that the author has since written a follow-up article on the topic of human-ro robot collaboration. The original version also stated that the authors had written a paper on how humans and robots work together to accomplish tasks. Planning with homotopy class constraints is useful in problems where the robot's requirements are given with respect to obstacles. Yi, Goodrich, and Seppi consider topological constraints provided by human operators. They argue that inferring the goal and the path as separate problems can lead to over-confidence in incorrect beliefs about the user's preferences. They propose a joint inference approach that considers both the human's intended goal and their preferred path to that goal. The aim is for the robot to understand the human’s intentions and execute the task with minimal interventions. At any time step, the human may provide an observation to the robot. When the human intervenes, the robot receives a direction (heading angle) that  the robot receives. The transition model T (s t+1 | s t , a t ) is deterministic, meaning the robot has full control over its future location. The number of homotopic classes for a given problem can be infinite, and as the robot changes location and updates its representation of the world, carrying out inference over homotopies in a dynamic environment requires recomputing the set of homOTopies at every iteration, making the belief update challenging. The work proposes a conditional independence assumption to provide a tractable solution to the problem of robot navigation in a known environment to an unknown destination, where a human can intervene and provide a heading direction to a robot using a joystick or force cues. In our approach, we assume that thehuman's inputs are noisily rational conditioned on both thegoal and the preference. We define a human's preference as the posterior distribution (given all human input up to that point) over transitions from the current to the neighboring polytopes, i.e. edges on the graph. If the robot only infers the preferred path, it may mistakenly assume the goal is on the left, leading to a failure in completing the task. If it infers both, it can avoid over- confidence in incorrect belief about the users' preferences, which would lead to improved system performance. The approach is described in the paper, which also includes an algorithm for solving pathplanning problems under homOTopic constraints. It is published in the open-access journal, Theoretical Computer Vision. The paper is open-source and free to download. It can be downloaded from the project's website here: http://www.theproject.org/project/pathplanning-algorithms-under-homotopic-constraints-with-conditional-independence-assumption-to-provide-a-t tractable-solution-for-robot-navigation-instructions-in-an-known-environment- to-an unknown destination. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here. We consider a navigation problem where the robot receives reward according to the model R(s t , g, θ, a t ). We wish to find the optimal policy π that maximizes the expected discounted sum of future rewards. The above problem is a Partially Observable Markov Decision Process (POMDP) . In section IV, we propose an encoding of human's path preference θ for computing the posterior in eq. ( ). Devifrom the concept of homotopy classes, we define the preference according to a partitioning of the environment into polytopes, as shown in fig. , creating a hyperplane arrangement of the space. We use C g,θ to induce a probability distribution over observations, given by: where γ h is a hyperparameter that designates the rationality coefficient. This model assumes the human will pick the lowest cost action with the highest probability and the likelihood of an action decreases exponentially with the increase in cost. We note that the number of Bayesian updates required at each iteration to update the belief is equal to the cardinality of Ω g × Θ. In addition, each Bayesian update involves computing C g,.θ ( .                ), which involves solving an optimization problem (such as a shortest path problem). We propose a specific encoding of preferenceθ for resolving the above problem, while ensuring that number of computations per update does not grow exponentially with number of obstacles. The model is shown infig. ( ), represented as a Bayesian Network. The preferred region of space per region of each polytope is a discrete distribution over the preferred neighboo distribution. We spatially decompose the preference into a set of neighboring polytopes, which can be mapped to a future location in space. For simplicity, we consider a two-dimensional environment composed of m polytopic obstacles, each defined by their half-space representation (H-representation) where A i ∈ R di×2 and b i ∉ R di , and where d i is the total number of edges (hyperplanes) composingpolytope i. We leverage this representation to define path preferences as preferred transitions between adjacent regions of thespace. More specifically, each location in. space belongs to a polytOPE j for which we can write an H-representations of the form where α j i is a vector specific to polytoped j and obstacle i corresponding to the relative position of any point in the set with respect to each hyperplane in O i . We conclude that the preferred region pj j is the direction to an intended location, which is the resulting robot location after advancing in the indicated direction for one time step. Back to Mail Online home. back to the page you came from. For more information, visit: http://www.dailymail.co.uk/blogs/science/article-news/2013/01/29/robotic-navigation-problem-in-a-new-look-at-the-world-of-robots-and-how-it-can-be-mapped-to-an-intended-location-in.html#storylink=cpy. We assume that for a location st belonging to polytope j, and given goal g and preference pj, the observation ot and any other preference p i,i =j are conditionally independent. We can further reduce the Hrepresentation of a polytopes to include only non-redundant constraints. We define a path preference as the set of preferred transitions over all nodes in the graph. The number of Bayesian updates required to update the belief at every iteration should be m θ × m g . We can notice that by introducing this assumption, we removed the direct relationship between the number of polytopes in the environment and the complexity of the Bayesian update in eq. ( ). In practice, components of θ are not mutually independent. For example, if the human preference at a vertex v 1 is 1, it is unlikely that it will also prefer p v2 = (v 2 , v 2 ) (turning back). We can improve our model by assuming a dependent relationship between edges and preferences for edges, which does not significantly increase the complexity. An interesting property of our encoding is that any two paths that belong to any of our two paths is that they belong to the same set of possible goals, and that any other path that belongs to that set of goals is not dependent on the other path. We use this method in practice for computing α j e for eachpolytope. Back to Mail Online home. Back To the page you came from. Click here to read the next part of the article. Back into the page we came from, and click here to see the next section of the story. The next page includes the results of our analysis of the new problem diagram in fig. (1). Back to the page where you can find the rest of the information about the problem diagram, including how to solve it and how to reduce the cost of computing the Bayes' Bayes equation to a single-valued function. The final page contains the results from our analysis on how to use the new method to solve the problem in practice. We hope that this will help you understand how to improve the model of Bayes’s Bayes equations in a more efficient way. The last section of this article will provide a definition of preference θ according to a graphical representation of the environment based on the hyperplane arrangement. We will then look at how this representation can be used to help you with the problem of choosing the best path for a robot in a given environment. We are going to use this definition to help us with our model of the inference problem in this section. The first question is: What is the optimal path to follow for a human in a hyperplane? We will use the definition of path preference in the section below to help with this question. The second question is about how we can use the graphical representation to help the human with the Bayeuse's Bayes problem. The answer is that the human will have a preference to which neighboring polytopes they wish to transition to. The third question is whether the human has a preference for a particular hyperplane that is on opposite sides of the same hyperplane (i.e. the same as the same side of thehyperplane). We evaluate our model on a simulated navigation task where the robot must reach a goal that is unknown a priori while respecting the path preferences indicated by a human. The robot navigates in a grid world containing obstacles. The transition model is deterministic: the robot selects an adjacent location on the grid to reach at the next time step. We assume that the human selects actions to noisily minimize a cost function C g,θ. We compute costs by running the A path planning algorithm on the environment maps. We impose preference constraints by pruning invalid transitions from the search tree. At each step in time, the robot receives a reward which is a sum of three components: a goal-specific reward, a preference- specific reward or penalty. We also omit the path preference's contribution to the reward R pref . The robot complies with the human input, but does not take an initiative. We designed an arbitration function to decide between our model and the user's recommendation when the robot takes an action. Our metric is similar to the proposed policy to evaluate co-operation between the user and the robot. If the user stops providing information, the Robot continues in the last direction indicated for 5 time steps (conserving its momentum), then stops. If it does, it updates its belief distribution over the unknown problem variables and resolves the POMDP over a receding horizon. We conclude that our model can be used to improve the design of robots in the field of artificial intelligence. We hope to use our model in the development of robots that can be programmed to solve complex problems in a variety of ways. For example, a robot that can solve the complex problem of how to build a prosthetic leg in 3D could be used in a future version of the DARPA program. We believe that this could be a useful tool for developing robots that are able to do complex tasks in a number of different ways. We are also interested in the potential uses of machine learning in the fields of medicine, medicine, robotics and other fields. We will publish a paper on the subject in the next few months in the Journal of Computer Vision and Pattern Recognition (JCVPR) – a new form of computer vision and pattern recognition that could be applied to a range of real-world problems. The paper will also be published in the Proceedings of the National Academy of Sciences of the United States of America (NACS) – an open-source version of JCVPR will be released in the spring of 2015. The full version of this article can be downloaded for free from the NACS website, along with a copy of the J CVPR paper, and a version of it for the Mac OS X operating system (Mac OS X version 8.1.0). The J CVRP paper is available for download from the JAVS website. For more information on how to use the JCVRP, visit the JCPPR website or the JCS website, or see www.jcs.org/jcvp/jvr/javr.html. fidence in the robot's prediction for the purpose of arbitration is the entropy of the intention distribution H(g, p i ), where p i denotes the preferred neighbor for the current region. Table I reports the success rates for experiments conducted over six randomly sampled problem instances and 50 runs per instance in Map 1. Results also show that blending the user's input with the robot’s policy (Path Preference + Blend) when the human provides information leads to improved performance. Computation time. More obstacles result in fewer iterations when solving the constrained shortest path problem with A . Adding constraints due to the obstacles and polytopes reduces the size of the A search tree. C. Limitations Simulation environments. The complexity of the Bayesian update should not increase with the number of obstacles orpolytopes. We compute solutions on three maps: a simple 10 × 10 grid world with 8 poly topes, a 10×10 grid world. with 56 poly Topes, and a 20×20 grid. world with 73 polytope. The latter environment being larger, we increase the mission time and the depth of the search tree in POMCP from T max = 30 (Map 1 and Map 2) to Tmax = 60 (Map 3). C. The time required to solve the POMDP, and the time needed to update the Robot's belief as it receives new observations, are shown in table II. The results are published in the open-source version of the book, “Bayesian Bayes’ Algorithm for Machine Learning,” which is published by the University of California, San Diego, and is available for download on the Google Play store. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or see www.samaritans.org. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org for details. In Europe, contact the National suicide prevention helpline on 0800- 273-7255 or go to http://www.sophia.org/sophie-suicide-helpline/ or www.nhs.uk/s/sophelion/solution-solution. For more information on Bayes' algorithm, see the online version of this article and the corresponding article, ‘Bayes' and ‘SOPhelion’. The online version is also available at http:// www.soroptimists.com/bayes/Bayes-Algorithm-For-Machine-Lunging-Algorithms-Forcing-Bayes.html. The second part of the article is titled ‘The Bayes Algorithms for Machine Lunging’ and will be published later this year. n our simulations, we hardcoded the preference policy over the maps (e.g. in Map 1, go around the table counter-clockwise). We randomly sampled problem instances (start and goal locations, and goal options) to reduce the bias introduced by these preference choices. The main limitation of our approach is that the size and geometry of each polytope depends strongly on the geometry of the obstacles. Because of this, the robot can make predictions over preferences that are too refined compared with the topology of the environment. Our method can be improved by changing the structure of the hyperplane arrangement so that it relies on theTopology of. the environment, but does not vary strongly with the geometry. of the features in that environment. For this purpose, topometric maps and region construction algorithms are promising directions. We presented an approach for encoding and inferring a human's path preference in an environment with obstacles. By leveraging a partitioning of the space into polytopes and a stochastic observation model, our method allows for joint inference over the goal and path preference even when both are unknown a-priori. The time required to update the robot's belief does not increase with the complexity of the environments. The method is suitability for online applications, which further highlights the practicality of our method. It would be best to sample preferences among a distribution of preferences chosen by a human (for example, from benchmarks resulting from a collection of data).Creating such a benchmark is an interesting direction for future work."
When did the Tevatron Collider Run II start and when is it expected to end?,"\section{INTRODUCTION}
The Tevatron Collider Run II started in March 2002 and is expected
to continue until the end of this decade. The Tevatron and the 
two detectors, CDF and D\O, have been performing  well in 2004,
each experiment is collecting data at the rate 
of $\approx$10 pb$^{-1}$ per week.
The total  luminosity accumulated by August 2004 is $\approx$500 pb$^{-1}$
per detector.
The rich physics program includes the
production and precision measurement of properties of  standard model (SM)
objects, as well as searches for phenomena beyond standard model.
In this brief review we focus on areas of most interest 
to the lattice community. We present
new results on the top quark mass
and their implication for the mass of the SM Higgs boson, 
on searches for the SM Higgs boson, on evidence for the $X(3872)$ state, 
on searches for pentaquarks, and on $b$ hadron properties.
All Run II results presented here are preliminary. 

\section{TOP QUARK MASS}

The experiments CDF and D\O\ published several direct  measurements of
the top quark pole mass, $\ensuremath{M_{\mathrm{top}}}$, 
based on Run I data (1992-1996).
The ``lepton $+$ jets'' channel yields the most precise determination of
$\ensuremath{M_{\mathrm{top}}}$. Recently, the
D\O\ collaboration published a new measurement~\cite{Mtop1-D0-l+j-new},
based on a powerful analysis technique yielding  greatly improved precision.
The differential probability 
that the measured variables in any event correspond to the signal
is calculated as a function of $\ensuremath{M_{\mathrm{top}}}$. 
The maximum in the product of the individual event probabilities 
provides the best estimate of $\ensuremath{M_{\mathrm{top}}}$.
The critical differences from previous analyses 
in the lepton $+$ jets decay channel lie in 
the assignment of more 
weight to events that are well measured or more likely to correspond to  
$t \bar t$ signal, 
and  the handling of the combinations of final-state objects
(lepton, jets, and imbalance in transverse momentum) 
and their identification with
top-quark decay products in an event. 
The new combined value for the top-quark mass from Run I is 
$\ensuremath{M_{\mathrm{top}}}  =  178.0\pm4.3~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.

In Run II, both collaborations  have been exploring several different techniques 
for $\ensuremath{M_{\mathrm{top}}}$
measurements. The best single CDF result comes from a dynamic likelihood method
(DLM). The method is similar to
the technique used in Ref.~\cite{Mtop1-D0-l+j-new}.
The result is $\ensuremath{M_{\mathrm{top}}} = 177.8^{+4.5}_{-5.0} (stat) \pm  6.2 (syst) ~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.
The joint likelihood of the selected events is shown in Fig. ~\ref{fig:cdf_tml}. 
The Run II goal is a 1\% uncertainty on $\ensuremath{M_{\mathrm{top}}}$. 




\begin{figure}[htb]
\vspace*{-5mm}
\includegraphics[height=5.8cm,width=8.1cm]  {data_22ev_likelihood.eps}
\vspace*{-1.2cm}
\caption{The joint likelihood of top candidates(CDF).}
\label{fig:cdf_tml}
\end{figure}




\section{SEARCH FOR SM HIGGS BOSON}


The constraints on the SM Higgs ($H$)  boson  mass from
published  measurements, updated to include the new D\O\ top mass
measurement~\cite{Mtop1-D0-l+j-new}, are
$M_H = 117 ^{+67}_{-45}~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$, $M_H < 251~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$ at 95\% C.L.
The  new most likely  value of $M_H$
is above the experimentally excluded range,
and sufficiently low for $H$ to be observed at the Tevatron.


\begin{figure}[htb]
\vspace*{-5mm}
\includegraphics[height=7.5cm,width=7.8cm]  {d0_wbb_fig_3_err.eps}
\vspace*{-1.1cm}
\caption{Distribution of the dijet
invariant mass for $W+2 b$-tagged jets  events,
compared to the expectation (D\O). 
}
\label{fig:d0_wbb_2tag}
\end{figure}



D\O\  has conducted a search for $H$ at $M_H < 140~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$ 
in the production channel  
$p \bar{p} \rightarrow WH \rightarrow  e \nu b \bar{b}$. 
The experimental signature of  $WH \rightarrow e \nu b \bar{b}$
is a final state with 
one high $p_T$ electron, two  $b$ jets, and
large missing transverse energy  resulting from
the undetected neutrino.
The dominant backgrounds to $WH$ production
are  $W b \bar{b}$, $t \bar{t}$ and single-top production.
The distribution 
of the dijet mass for events with two $b$-tagged jets is shown in
Fig.~\ref{fig:d0_wbb_2tag}. 
Also shown is the  expected contribution ($0.06$ events)  
from the $b \bar{b}$ decay of a
SM Higgs boson with $M_H =$ 115 $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.
No events are observed in the  dijet mass window of 85--135  $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.
D\O\ sets a limit on the cross section
for $\sigma( p\bar{p} \rightarrow WH) \times B(H \rightarrow b \bar{b}) $
of 9.0 pb at the 95\% C.L.,  for a 115  $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$ Higgs boson.
The results for mass points 105, 125, and 135 $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$
 are 11.0, 9.1 and 12.2 pb,  respectively.



\begin{figure}[htb]
\vspace*{-1.2cm}
\includegraphics[height=0.33\textheight,width=8.0cm]{whww_aps04_bw.eps}

\vspace*{-1.2cm}
\caption{95\% limits on the $H$ production (CDF).}
\label{fig:cdf_whww}
\end{figure}


CDF  has done  a similar search, allowing either an  electron or a muon  
in the final state.  Both groups have also searched for $H$ produced in
gluon-gluon fusion, with subsequent decay to a pair of $W$ bosons.
The CDF results for both channels  are shown in Fig.~\ref{fig:cdf_whww}. 



\section{THE STATE X(3872)}


\begin{figure}[htb]

\includegraphics[height=8.0cm,width=7.5cm]  {X3872cdfPRL1FullM.eps}
\vspace*{-1cm}
\caption{The $X(3872)$ signal (CDF).}
\label{fig:cdf_x}
\end{figure}




 The existence of the $X(3872)$ state discovered by 
the Belle Collaboration~\cite{Belle-X}
 has been confirmed 
 in $p \bar{p}$ collisions by  CDF~\cite{cdf-X} (see Fig.~\ref{fig:cdf_x})
and D\O~\cite{d0-X}.
 It is still unclear whether this particle is a $c\bar{c}$ state,
 or a more complex object.  When the data are separated according to
production and decay variables, D\O\  finds no significant
differences between the $X(3872)$ and
the $c \bar{c}$ state $\psi(2S)$.
CDF has analysed the ``lifetime'' distribution of the $X(3872)$ events in order to
quantify what fraction of this state arises from decay of $B$ hadrons, as opposed to
those produced promptly. The authors find that for the selected samples
28.3$\pm$1.0$(stat)\pm$0.7$(syst)$\% of $\psi(2S)$ candidates are from $b$ decays,
whereas 16.1$\pm$4.9$(stat)\pm$2.0$(syst)$\% of $X$ mesons arise from such decays.





\section{SEARCH FOR PENTAQUARKS}



\begin{figure}[htb]

\includegraphics[height=0.27\textheight,width=7.6cm]  {mpks_1stminbias.eps}
\vspace*{-1.2cm}

\caption{Invariant mass distribution of an identified proton and a $K^0_s$ candidate. (CDF)
}
\label{fig:pqtheta}
\end{figure}



\begin{figure}[htb]

\vspace*{-0.9cm}
\includegraphics[height=0.25\textheight,width=8.0cm]  {CM_xicst_cc_1.eps}
\vspace*{-1.2cm}
\caption{Invariant mass distribution of the $(\Xi^-,\pi^+)$ system. (CDF) 
}
\label{fig:pqxi}
\end{figure}


\begin{figure}[htb]
\vspace*{-0.9cm}

\includegraphics[height=0.25\textheight,width=7.6cm]  {theta_note_dstp_dedx_pt.eps}
\vspace*{-1.2cm}
\caption{Mass of the ($D^{*+}\bar p$) system. The arrow indicates the position of 
the $\Theta_c$ state (CDF).}
\label{fig:pqthetac}
\end{figure}



Following reports of evidence for exotic
baryons containing five quarks (pentaquarks), CDF has analysed 
its data for evidence of the following pentaquarks:
$\Theta^+$ ($uud\bar d \bar s$), doubly strange states 
$\Xi_{3/2}$, charmed states $\Theta_c$, and, most recently, 
a state $(udus\bar b)$, dubbed $R^+_s$, through its weak decay to $(J/\psi, p)$. 
With its excellent particle indentification and mass resolution,
CDF has a unique capability to search for  pentaquark states.
The signals of known states: $\phi$, $\Lambda$,
$\Lambda(1520)$, $K^*$, $\Xi$, 
compare favorably with those provided
by the authors of  the pentaquark evidence.
The group finds no evidence for pentaquark states, see Figs 
~\ref{fig:pqtheta},{\ref{fig:pqxi},\ref{fig:pqthetac}.
This can be interpreted as an indication that the pentaquark production 
in $p \bar p$ collisions is heavily suppressed compared to the conventional
hadron production, or as an evidence against the existence of pentaquarks.

\clearpage

\section{RECENT B PHYSICS RESULTS}


\subsection{Spectroscopy}

CDF has measured the mass of $b$ hadrons in exclusive $J/\psi$ channels.
The measurements of the $B_s$ and $\Lambda_b$ (Fig. \ref{fig:masslb})
masses are the current world's best.\\

$m(B^+)$ = 5279.10$\pm$0.41$(stat)\pm$0.36$(syst)$,

$m(B^0)$ = 5279.63$\pm$0.53$(stat)\pm$0.33$(syst)$,

$m(B_s)$ = 5366.01$\pm$0.73$(stat)\pm$0.33$(syst)$,

$m(\Lambda_b)$ = 5619.7$\pm$1.2$(stat)\pm$1.2$(syst)$ MeV/$c^2$.\\


\begin{figure}[htb]
\vspace*{-1mm}
\includegraphics[height=0.30\textheight,width=7.5cm]  {lambdav1c.eps}
\vspace*{-1cm}

\caption{The mass spectrum of $\Lambda_b$ candidates (CDF).}
\label{fig:masslb}
\end{figure}


D\O\ reports the first observation of the excited $B$ mesons 
$B_1$ and $B^*_2$ as two separate states in fully reconstructed
decays to $B^{(*)}\pi$. The mass of $B_1$ is measured to be
5724$\pm$4$\pm$7 MeV/c$^2$, and the mass difference $\Delta M$ between
$B^*_2$ and $B_1$ is 23.6$\pm$7.7$\pm$3.9 MeV/c$^2$
(Fig.  \ref{fig:d0_bexc}).

D\O\ observes semileptonic $B$ decays to narrow $D^{**}$ states,
the orbitally excited states  of the $D$ meson
seen as resonances in the $D^{*+}\pi^-$ invariant mass spectrum.
The $D^*$ mesons are reconstructed through the decay sequence 
$D^{*+} \rightarrow D^0\pi^+$, $D^0\rightarrow K^-\pi^+$.
The invariant mass  of oppositely charged $(D^*,\pi)$ pairs
is shown in Fig.  \ref{fig:d0_dstst}.
The mass peak between 2.4 and 2.5 GeV/$c^2$ can be interpreted as two merged 
narrow $D^{**}$ states, $D^0_1(2420)$ and $D^0_2(2460)$.
The combined branching fraction is 
$ {\cal B}(B\rightarrow D^0_1,D^0_2)\cdot {\cal B}(D^0_1,D^0_2\rightarrow D^{*+}\pi^-)=(0.280\pm0.021(stat)\pm0.088(syst)$\%. The systematic error includes the unknown phase between the
two resonances. Work is in progress on extracting the two Breit-Wigner
amplitudes.


\begin{figure}[htb]
\vspace*{-2mm}
\hspace*{-3mm}
\includegraphics[height=0.28\textheight,width=8.3cm]  {B08F02.eps}

\vspace*{-1cm}
\caption{Mass difference $\Delta M = M(B\pi)-M(B)$ for exclusive $B$ decays.
The background-subtracted signal is a sum of 
$B^*_1 \rightarrow B^* \pi$, $B^* \rightarrow B \gamma $ (open area)
and $B^*_2 \rightarrow B^*\pi$ $B^*\rightarrow B \gamma$ (lower peak in the shaded area)
and $B^*_2 \rightarrow B \pi$ (upper peak in the shaded area)  
(D\O).}
\label{fig:d0_bexc}
\end{figure}


\begin{figure}[htb]
\includegraphics[height=0.25\textheight,width=7.5cm]  {B05F03.eps}

\vspace*{-1cm}
\caption{The invariant mass distribution of
$(D^*,\pi)$ pairs, opposite sign (points) and same-sign (solid histogram).}
\label{fig:d0_dstst}
\end{figure}






\subsection{Lifetimes}


CDF and D\O\ have measured  lifetimes of $b$ hadrons through the exclusively
reconstructed decays $B^+ \rightarrow J/\psi K^+$, $B^0 \rightarrow J/\psi K^{*0}$,
$B_s \rightarrow J/\psi \phi$, 
and $\Lambda_b \rightarrow J/\psi \Lambda$
(Fig. \ref{fig:d0_lbctau}).
The latest results are:  \\



 $\tau(B^+)$=1.65 $\pm$ 0.08 $^{+0.096}_{-0.123}$  ps ~(D\O\ 2003),

 $\tau(B^+)$=1.662 $\pm$ 0.033  $\pm$ 0.008  ps ~(CDF),

 $\tau(B^0_d)$=1.473  $^{+0.052}_{-0.050}$ $\pm$ 0.023    ps ~(D\O).

 $\tau(B^0_d)$=1.539 $\pm$ 0.051  $\pm$ 0.008  ps ~(CDF),

 $\tau(B^0_s)$=1.444   $^{+0.098}_{-0.090}$ $\pm$ 0.020   ps ~(D\O),

 $\tau(B^0_s)$=1.369 $\pm$ 0.100 $\pm$ $^{+0.008}_{0.010}$  ps ~(CDF),


 $\tau(\Lambda_b)$=1.221 $^{+0.217}_{-0.179}$ $\pm$ 0.043  ps ~(D\O),


 $\tau(\Lambda_b)$=1.25 $\pm$ 0.26 $\pm$ 0.10  ps ~(CDF 2003).\\



The measured lifetimes correspond to the following lifetime ratios:\\

$\tau(B^+)/\tau(B^0_d)$   =  1.080$\pm$0.042     ~(CDF),
 
$\tau(B^0_s)/\tau(B^0_d)$ =  0.890$\pm$0.072    ~(CDF),

$\tau(B^0_s)/\tau(B^0_d)$ = 0.980$ ^{+0.075}_{-0.070}   \pm$0.003    ~(D\O),

$\tau(\Lambda_b)/\tau(B^0_d)$ =  0.874$ ^{+0.169}_{-0.142}   \pm$0.028    ~(D\O).\\



\begin{figure}[htb]
\includegraphics[height=0.3\textheight,width=8.2cm]  {d0_lbctau_B11F02.eps}
\vspace*{-1cm}

\caption{ Fit projection on  $c\tau$ for the $\Lambda_b$ candidates.  (D\O)}
\label{fig:d0_lbctau}
\end{figure}


The $B_s$ lifetime measurements listed above are results of
a single-lifetime fit to data, integrated over the decay angles.
Because  of the presence of  final
states  common to ${B_s^0}$\ and its charge conjugate ${\overline{B}_s^0}$,
the two meson states   are expected
to mix in such a way that the two CP  eigenstates may have a relatively
large lifetime difference.
It is possible to
separate the two CP components of ${B_s^0 \rightarrow J/\psi \phi}$\ and thus to measure the
lifetime difference by studying the time evolution of the
polarization states of the vector mesons in the final state.
CDF has carried out a combined analysis of $B_s$ lifetimes 
and polarization amplitudes. The results for the lifetimes of the
low mass (CP even) and high mass (CP odd) eigenstates, and the relative 
width difference are:\\

 $\tau_L = 1.05 ^{+0.16}_{-0.13} \pm 0.02$ ~ps,
 
 $\tau_H = 2.07 ^{+0.58}_{-0.46} \pm 0.03$ ~ps,

 $\Delta \Gamma /\overline \Gamma   = 0.65 ^{+0.25}_{-0.33} \pm 0.01$.\\

Figure \ref{fig:cdf_dg} shows  the scan of the likelihood function 
for $\Delta \Gamma /\overline \Gamma$.
Pseudoexperiments tossed with $\Delta \Gamma /\overline \Gamma =0$
yield the betting odds for observing the above results at
1/315. For $\Delta \Gamma /\overline \Gamma = 0.12$ (SM prediction,
which has recently been updated to 0.14$\pm$0.05~\cite{dg_un}) the betting odds are
1/84.

\begin{figure}[htb]
\vspace*{-1mm}
\includegraphics[height=0.3\textheight,width=8.2cm]  {cdf_scan-dg-un.eps}

\vspace*{-1cm}
\caption{Scan of the likelihood function 
for $\Delta \Gamma /\overline \Gamma$ (CDF).
}
\label{fig:cdf_dg}
\end{figure}




D\O\ has used a novel technique to  measure the lifetime ratio
of the charged and neutral $B$ mesons, exploiting the large
semileptonic sample. $B$ hadrons were reconstructed in the channels
$B\rightarrow \mu^+ \nu D^*(2010)^-X$, which are dominated by $B^0$ decays, 
and  $B\rightarrow \mu^+ \nu D^0X$, which are dominated by $B^+$ decays.
The lifetime ratio was
obtained from the variation of the ratio of the number of events in these two
processes at different decay lengths.
The result is \\


$\tau(B^+)/\tau(B^0_d)$   =  1.093$\pm$0.021$\pm$0.022.      ~(D\O)




\subsection{Towards $B_s$ mixing}

Measurement of the $B_s$ oscillation frequency via ${B_s^0}$ -${\overline{B}_s^0}$ ~mixing
will provide an important constraint on the CKM matrix. The oscillation
frequency is proportional to the mass difference between the mass eigenstates,
$\Delta m_s$, and is related to the CKM matrix through 
$\Delta m_s \propto |V_{tb}V_{ts}|$. When combined with the
$B_d$ mass difference, $\Delta m_d$ it helps in extraction of $|V_{td}|$,
and thereby the CP violating phase. 

As a benchmark for future $B_s$ oscillation measurement, both groups
study  $B_d$ mixing, gaining an understanding of the different components
of a $B$ mixing analysis (sample composition, flavor tagging, vertexing,
asymmetry fitting). For a sample of partially reconstructed decays
$B\rightarrow D^*(2010)^+\mu^-X$, D\O\ obtains 
$\Delta m_d = 0.506 \pm 0.055 (stat) \pm  0.049 (syst))$ ps$^{-1}$ and
$\Delta m_d = 0.488 \pm 0.066 (stat) \pm  0.044 (syst))$ ps$^{-1}$
when employing  opposite side muon tagging and the same side tagging,
respectively.

The CDF result for semileptonic channels is
$\Delta m_d = 0.536 \pm 0.037 (stat) \pm  0.009 (s.c.) \pm 0.015 (syst)$ ps$^{-1}$.
CDF also reports a result on $B$ oscillations using fully reconstructed
decays:
$\Delta m_d = 0.526 \pm 0.056 (stat) \pm  0.005 (syst))$ ps$^{-1}$.

Reconstructing $B_s$ decays into different final states is another
important
 step in the ${B_s^0}$ -${\overline{B}_s^0}$ ~mixing analysis.
Thanks to the  large muon and tracking coverage,   D\O\ is accumulating
a  high statistics sample of semileptonic $B_s$ decays.
D\O\ reconstructs the $B_s \rightarrow D^+_s \mu^- X$ decays, with
$D^+_s \rightarrow \phi \pi^+ $ and
$D^+_s \rightarrow K^* K^+ $,
at a rate of $\approx$ 40(25) events per pb$^{-1}$,  respectively.
Figure \ref{fig:d0_bsdsphipi} shows the mass distribution of the
$D^+_s \rightarrow \phi \pi$ candidates.


\begin{figure}[htb]
\vspace*{-5mm}
\includegraphics[height=0.3\textheight,width=8.0cm]  {blds-250.eps}
\vspace*{-1.2cm}
\caption{  $D^+_s \rightarrow \phi \pi^+$  signal. (D\O)}
\label{fig:d0_bsdsphipi}
\end{figure}


\begin{figure}[htb]
\vspace*{-10mm}
\hspace*{-4mm}
\includegraphics[height=0.35\textheight,width=7.9cm]  {cdf_Bs-DsPi-PhiPi.eps}

\vspace*{-1.0cm}
\caption{ $B_s \rightarrow D_s \pi$, $D_s \rightarrow \phi \pi$  signal.  (CDF)}
\label{fig:cdf_bsdsphipi}
\end{figure}


CDF has clean signals for fully hadronic, flavor-specific  $B_s$ decays,
providing the best sensitivity to $B_s$ oscillations at high
$\Delta m_s$. Figure \ref{fig:cdf_bsdsphipi} shows the signal for
the best channel, $B_s \rightarrow D_s \pi$, $D_s \rightarrow \phi \pi$.

\clearpage


\subsection{Rare decays}

The purely leptonic decays $B_{d,s}^0 \rightarrow \mu^+
\mu^-$ are flavor-changing neutral current (FCNC) processes.
In the standard model, these decays are forbidden at the tree level and
proceed at a very low rate through higher-order diagrams.
The latest SM prediction~\cite{sm_ref3}
is ${\cal B}(B^0_s \rightarrow \mu^+ \mu^-)=(3.42\pm 0.54)\times
10^{-9}$, where the error is dominated by non-perturbative uncertainties. The
leptonic branching fraction of the $B_d^0$ decay is suppressed by CKM matrix elements $|V_{td}/V_{ts}|^2$
leading to a predicted SM branching fraction of $(1.00\pm0.14)\times 10^{-10}$.
The best published experimental bound (Fig.~\ref{fig:cdf_bsmumu})
 for the branching fraction
of $B^0_s$ $(B^0_d)$ is presently
${\cal B}(B^0_s \, (B^0_d) \rightarrow \mu^+\mu^-)<7.5\times 10^{-7}\, 
(1.9\times 10^{-7})$ at the 95\% C.L.~\cite{cdfII}.
The decay amplitude of $B^0_{d,s} \rightarrow \mu^+ \mu^-$ can be
significantly enhanced in some extensions of the SM. 

\begin{figure}[htb]
\includegraphics[height=8.3cm,width=7.9cm]  {cdfbsmumu_results_prl.eps}

\vspace*{-1cm}
\caption{Invariant mass for the events passing all requirements. (CDF)}
\label{fig:cdf_bsmumu}
\end{figure}


Assuming no contributions 
from the decay $B^0_d\rightarrow \mu^+\mu^-$ in the signal region,
D\O\  finds the conservative upper limit on the branching fraction 
to be ${\cal B}(B^0_s \rightarrow \mu^+ \mu^-) \leq 4.6\times 10^{-7}$ 
at the 95\% C.L. (Fig.~\ref{fig:d0_bsmumu}).






\begin{figure}[htb]
\includegraphics[height=5.0cm,width=8.0cm]  {B06F03.eps}
\vspace*{-1cm}
\caption{Invariant mass for the events  passing all requirements. (D\O)}
\label{fig:d0_bsmumu}
\end{figure}

",['The Tevatron Collider Run II started in March 2002 and is expected to continue until the end of this decade.'],2431,multifieldqa_en,en,,d70ca68d05af951cd8ff2052095597d1b4dca557bfb0b40b," The Tevatron Collider Run II started in March 2002 and is expected to continue until the end of this decade. We present new results on the top quark mass and their implication for the mass of the SM Higgs boson. We also look at evidence for the $X(3872)$ state, on searches for pentaquarks, and on $b$ hadron properties. All Run II results presented here are preliminary and are based on data from the CDF and D\O detectors. The joint likelihood of the selected events is shown in Fig. 2 in the figure below. The new combined value for the top-quark mass from Run I is  178.0\pm4.3 (stat)  6.2 (syst)    ‘’’. ‘””. “’ ”.” “. ’”, ”’, ’.’,. ”,”,. ’,’;.  . . ”,.”., ”;.‚’.,  ,  ,. “, ,. ,“” and   ”;  ., “;” ,  ; ;  !” .  +.  %.  -.   . ”., +. %. !  ? ?  : “This article includes a brief review of the results of Run II on the ‘top quark pole mass’ and other areas of interest to the lattice community. The full text of this article can be found at: http://www.cnn.com/2004/01/29/science/top-quarks-top-q-mass-top.html#storylink=cpy.“;. The article also includes information on how to use the CDf and D’O detectors to search for the SM. Higgs. boson, as well as searches for phenomena beyond standard model (SM) objects, such as the pentaquark state and the Hadron. The final chapter of the article includes an analysis of the data from Run II for the “X( 3872)” state and other searches for $b’ hadron phenomena, including the $x(3870) hadron state. It also includes a discussion of how to interpret the results from the new D.D.O. measurements, updated to include the new top mass measurements, from the D.DF-1-0-l+l+j-new experiment. The paper concludes with a list of some of the key findings from the Run II data, including how to analyse the data for the X(3871) state and how to make sense of the evidence from the $B’ Hadron state, and a summary of the latest results from D.O.-1-L+J-new. The book is also available as a free download from: www.cns.org/doi/10.1145/cnn/2004-01/11/1136/1/2/4/3/4.0/1.0.1/3.1.1/.. The. book’s. title is “The Top Quark Pole Mass: Theoretical and Practical Implications of the Top-Quark Mass” The experimental signature of $X(3872)$ is a final state with one high $p_T$ electron, two $b$ jets, and large missing transverse energy resulting from the undetected neutrino. No events are observed in the dijet mass window of 85--135. It is still unclear whether this state is a more complex object. The authors find that for the samples selected for the selected samples that were promptly produced, no significant differences between those produced and those that were not. The existence of the state has been confirmed in $p \bar{p}$ collisions by  CDF~\cite{cdf-X} (see Fig. 1). The authors have also searched for $H$ produced in gluon-gluon fusion, with subsequent decay to a pair of $W$ bosons. The results for mass points 105, 125, and 135 are 11.0, 9.1 and 12.2 pb,  respectively, for a 115  $\ensuremath{\mathrm{ Ge\kern -0.1em V }/c^2 }$ Higgs boson with $M_H =$ 115 $W b \bar {b}$ and single-top production. They find no significant difference between these results and those for the $B$ production, as opposed to those for $T b $ and $t b $ (Fig. 1) The authors also find that the $CDF-CDF results for both channels are shown in Fig. 2. They conclude that the existence of this state has not been confirmed by the CDF, but that it is likely to be more complex than previously thought and that it could be the result of a new type of superconducting particle or a new form of quark-like neutrinos. They also conclude that this state could be used as a basis for a new class of supernovae. The study was published in the journal The Astrophysics of Radioactivity (ASR) (http://www.astrophysics.org/news/2013/01/26/the-astrophysiology-of-radioactive-neutrinos-in-a-new-state-and-the-quantum-experiment-that-could-lead-to-a new-type-of superconducting-particles.html). For more information, visit the ASR website. For more details on the X(3870) state, see the X3872cdfPRL1FullMeps page. For the full report, see: http:// www.astrophys.org.uk/x3870/x/3870.html. The article has been adapted from the original version by the Belle Collaboration (Belle-X) (www.belle-x.com/2013-01/27/x-3872/X/3872.html) and is available in hard-copy and soft-copy versions as well as in the online version. For confidential support, call the National Institute of Standards and Technology (NIST) on 1-800-273-8255 or visit the NIST website (http: http:www.nist.org/. For confidential.gov/research/article/X3870-X3872-x/28.html/. For more on the $W-B-T-1-2-3-4-5-1.html, see http: www.nIST.gov/. For the latest version of this article, please visit the website. ique capability to search for  pentaquark states. The group finds no evidence for pentaquarks. The mass of $b$ hadrons in exclusive $J/\psi$ channels is the current world's best. This can be interpreted as an indication that the pentquark production in $p \bar p$ collisions is heavily suppressed compared to the conventionalhadron production, or as an evidence against the existence of pentquarks. Work is in progress on extracting the two resonances in the Breitenbach-Wigner-Wagner model. Theoretically, this could lead to the discovery of a new type of superconducting superconductor, known as a ‘superconductor’ or ‘Superconducting Superconductor Superconductor (SSTS)’. SSTS is a type of synchrotron that has been used to study superconductors in the past. The SSTs have been used in a number of other fields, such as superconductivity and magnetohydrodynamics, as well as in the study of supernovae in the 1970s and 1980s. The study of the SST’s in the 1980s and 1990s was the first to show that they could be used as a source of information about the state of the universe. The results of this study were published in the Journal of Superconductivity in the 1990s and have been influential in the development of the theory of supersymmetric superconductions. The current study is the first of its kind to look at the mass spectrum of $B$ mesons in a variety of ways, including by measuring the mass peak between 2.4 and 2.5 GeV/c$2.5. It also reports the first observation of the excited $B# mesons as two separate states, $B_1$ and $B^*_2$ as separate states in fully reconstructed neoclassical models of the $D*$ meson decay sequence. It is hoped that this will lead to a better understanding of the origin of the superconduction in the universe and how it behaves in space and time. It has been reported that the mass difference between the two states is 23.6$\pm$7.7$’pm$3.9 MeV/ c$2$2, and that this difference could be the result of a “bridge’” between the excited and non-excited $B* mesons. It was also reported that $B1$ decays to narrow $D^{**}$ states, and the orbitally excited states of $D$ are seen as resonances of the ‘D*’ meson. This could be interpreted to mean that $D’ and ‘B’ are two merged $D**’ states, with the “D” and “B” being two separate ‘d” mesons at the same time. This is the most likely explanation for why the two “b” states are so close to each other in the ’70s and ’80s, and why they are so different in size and shape. The ‘b’-’B$-‘B#’ decay sequence is called ‘the D* decay sequence’ (Fig. 1) and is based on the decay sequence of the D* meson, which has a branching fraction of 1.2:1. The $B_s$ lifetime measurements listed above are results of a single-lifetime fit to data, integrated over the decay angles. The results for the lifetimes of the low mass (CP even) and high mass ( CP odd) eigenstates, and the relative  grotesquewidth difference are:. The betting odds for observing the above results at the time of this article are at 1/315. For the likelihood function, the odds are 0.12/0.14$ (SM prediction, which has recently been updated to 0.14/1.05/cite), and 0.01/1/315 (the odds of observing the results at 0.04/1, which is the same as the SM prediction, but the odds of seeing the results are lower). The odds of spotting the results were 0.02/1 (the SM prediction) for the $Lambda_b candidates. The odds for spotting the candidates for the CP odd eigenstate is 0.03/1-0.05 (this has been updated from 0.08/0, which was previously 0.07/1), and the odds for detecting the CP eveneigenstate are 0,0.01/. (this was originally 0.09/0,. but it is now 0.05). The probability of spotting a candidate for theCP odd eigene is 0,1.5%. The odds that a candidate will be found to be a CP oddeigene are at 0,2.5 (the probability of finding a CP evenEigene, or 0,3.5, is 0,.5). The probabilities for spotting both CP oddes and CP evenes are at the same time at 0,.3/. (CDF 2003) and 0,4/. (DDF 2003). The chances of observing both CP evens and CP odds are at least 0.4%. The probabilities of observing either CP odd orCP evens are also 0.5/3-0,4.5. The probability for observing bothCP odds is at the rate of 0.6/3, or the probability of observing one of the two CP eigens at the speed of light is at a rate of 1.3%. The probability that a sample will be observed at a speed of 1,000 km/h is at least 1/3.4. The chances that a study will be successful are at about 0.7/4.0. The probabilities are at around 0.8/5.0, and this is based on a novel technique to measure the lifetime ratio of the charged and neutral $B$ mesons, exploiting the largeseptonic channels in the large-septons, the large septonic sample. The likelihood function for this technique is: 0.3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/24/23/25/26/28/26, which can be used to estimate the lifetime ratios for the two types of meson states. The lifetime ratios are: 1.25/4, 0.2/3/5, 1.4/6, 2.0/7, 3.0%, 3.5%, 4.0% and 5.0%. The lifetime ratio is the ratio of charged and Neutral mesons in the $B# sample. It is possible to study the time evolution of the polarization states of the vector mesons of the B# meson. CDF has clean signals for fully hadronic, flavor-specific $B_s$ decays. The purely leptonic decays $B_{d,s]^0 are flavor-changing neutral current (FCNC) processes. These decays are forbidden at the tree level and proceed at a very low rate through higher-order diagrams. The latest SM prediction is dominated by non-perturbative uncertainties. The experimental bound for the branching fraction at the 95% Cdf level is presently at 95%. The amplitude of the decay $0_0_d$ is presently 1.7 times the amplitude of $0.1_d#d$ (CDF) This is the best prediction for the best branching fraction for the $B-d$ decay at the CDF level. The prediction is based on the fact that $B$ oscillations are not affected by CKM matrix elements in the Cdf-CDF model. The predicted SM branching fraction is 1.6 times higher than the current SM prediction for $B.1#d. This means that the prediction is more likely to be correct than the previous SM prediction of 1.5 times the magnitude of the B-d-d decay. For more information on the SM prediction, please visit: http://www.cdf.org/sm/sm_ref3/sm-ref3-sm-s-m-s.html#sm-m_s-s_m_d-m, or see: http:// www.cDF.org /sm ref3.html/. For more details on how the SM predictions work, see:http:/ / www.CDF.gov/Sm/sm.html?sm-f=1, http:://www CDF/sm%20ref3,%20s%20m-m%20f%20F%20S%20CDF%, http:%20www.Cdf.gov%20/sm%,%20http%20cdf%20#sm% 20s% 20CDF% 20S% 20F% 20% 20E% 20D% 20G% 20M% 20R% 20A% 20H% 20P% 20W% 20B% 20%. For more info, visit:http:\/\/www.csf.org\/news/2013/01/07/14/cdf-s/m-b-d%20decays.html%20and%20the-best-prediction-for-the-B%20b%20of-the B_d% 20decay%20is%20likely-to-be% 20true.html%. For the full report on $B#decays, please go to: http:\/www.sf.c\/s/Cdf/m/s/B/decays/B-D/B_D%20Decays%20with%20a.html.%20(1) . For the rest of the article, please refer to the article for further information on how to structure $B%2C%20 decays into different final states. For the remainder of the paper, please see: www.sfc.org.uk/sf/cDF/B#/b-D#decay.html,. For the entire article, go to http: //www.scf.com/sfc/sfs/B$/B&s/decay/B0/B1/B2/B3/B4/B5/B6/B7/B8/B9/B10/B11/B12/B13/B14/B15/B16/B17/B18/B19/B20/B21/B22/B23/B24/B26/B28/B"
How many brother does Njoroge have?,"Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. It was among the African Writers Series. It was the first English language|English novel to be published by an East African. Thiong'o's works deal with the relationship between Africans and white settlers in colonial Kenya, and are heavily critical of colonial rule. Specifically, Weep Not, Child deals with the Mau Mau Uprising, and ""the bewildering dispossession of an entire people from their ancestral land."" Ngũgĩ wrote the novel while he was a student at Makerere University.

The book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement.

Plot summary

Njoroge, a little boy, is urged to attend school by his mother. He is the first one of his family able to go to school. His family lives on the land of Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Njoroge's brother Kamau works as an apprentice to a carpenter, while Boro, the eldest living son, is troubled by his experiences while in forced service during World War II, including witnessing the death of his elder brother. Ngotho, Njoroge's father and a respected man in the surrounding area, tends Mr. Howlands' crops, but is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty.

One day, black workers call for a strike to obtain higher wages. Ngotho is ambivalent about participating in the strike because he fears he will lose his job. However, he decides to go to the gathering, even though his two wives do not agree. At the demonstration, there are calls for higher wages. Suddenly, the white police inspector brings Jacobo to the gathering to pacify the native people. Jacobo tries to put an end to the strike. Ngotho attacks Jacobo, and the result is a riot where two people are killed. Jacobo survives and swears revenge. Ngotho loses his job and Njoroge’s family is forced to move. Njoroge’s brothers fund his education and seem to lose respect for their father.

Mwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school.

For a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta – a revered leader of the movement. Many blacks think that he is going to bring forth Kenya’s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population.

Jacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods.

One day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition.

Several months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau. Ngotho soon dies from his injuries and Njoroge finds out that his father was protecting his brothers. Kamau has been imprisoned for life. Only Njoroge and his two mothers remain free, and Njoroge is left as the sole provider of his two mothers. Njoroge fears that he cannot make ends meet; he gives up hope of continuing in school and loses faith in God.

Njoroge asks Mwihaki's for support, but she is angry because of her father’s death. When he finally pledges his love to her, she refuses to leave with him, realizing her obligation to Kenya and her mother. Njoroge decides to leave town and makes an attempt at suicide; however, he fails when his mothers find him before he is able to hang himself. The novel closes with Njoroge feeling hopeless, and ashamed of cowardice.

Characters in Weep Not, Child
 Njoroge: the main character of the book whose main goal throughout the book is to become as educated as possible.
 Ngotho: Njoroge's father. He works for Mr.Howlands and is respected by him until he attacks Jacobo at a workers strike. He is fired and the family is forced to move to another section of the country. Over the course of the book his position as the central power of the family weakened, to the point where his self-realization that he has spent his whole life waiting for the prophecy (that proclaims the blacks will be returned their land) to come true rather than fighting for Kenyan independence, leads to his depression.
 Nyokabi and Njeri: the two wives of Ngotho. Njeri is Ngotho's first wife, and mother of Boro, Kamau, and Kori. Nyokabi is his second wife, and the mother of Njoroge and Mwangi.
 Njoroge has four brothers: Boro, Kamau, Kori and Mwangi (who is Njoroge's only full brother, who died in World War II).
 Boro: Son of Njeri who fights for the Allies in World War II. Upon returning his anger against the colonial government is compounded by their confiscation of the his land. Boro's anger and position as eldest son leads him to question and ridicule Ngotho, which eventually defeats their father's will (upon realizing his life was wasted waiting and not acting). It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as ""entering politics"") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo.
 Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother.
 Jacobo: Mwihaki's father and an important landowner. Chief of the village.
 Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school.

Themes and motifs
Weep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt.

The novel also ponders the role of saviours and salvation. The author notes in his The River Between: ""Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people."" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, ""Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel."" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government.",['Four.'],1414,multifieldqa_en,en,,69e69439e349a539ca4cff96ae35aa8499ca61886801d488," Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. The book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement. The novel was the first English language|English novel to be published by an East African. It is heavily critical of colonial rule in Kenya and the Mau Mau Uprising, and ""the bewildering dispossession of an entire people from their ancestral land"" It was among the African Writers Series, and was published by Penguin Books in the UK in 1964. It has been cited as one of the best books of the 20th century by the American scholar John MacIntosh, who called it the ""greatest novel of all time"" and called it ""a masterpiece of African literature"" The novel is published in two parts, one in English and the other in the Nairobi-based Nambia-based English-language edition of the book, which was published in the U.S. in the same year as the first edition of Weep not, Child. The English-speaking edition has been published by Simon & Schuster in the US in the form of a hardback, priced at $24.99 (about £16.50). The English version has been released in hardback in the United States, with a price tag of $30 (about $25). The book has been translated into English as ""Weep not child"" and ""Weeping Not Child"" by John Macintosh, Jr. and published in paperback in the USA in the year 2000. It can be ordered by clicking here for a copy of the English version of the novel, or clicking here to read the English-only version. The U.K. version of this article has been amended to read: ""Weeps Not Child"", with the emphasis on the word ""child"" replaced by ""child"". The English translation of the second part has been changed to ""Child"" and to reflect that the book is about a boy, Njorosge, and not a girl, Mwihaki, as well as the events surrounding her departure from boarding school. The author has also changed the name of the girl to ""Mwihki"" to protect the identity of the character, which has been removed from the book. The story is about the relationship between a boy and his father, who is a respected man in the surrounding area, and a man who is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty. It also tells the story of the rise and fall of a man known as Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Jacobo is brutally murdered in his office by a member of the Mau-Mau movement. Several months later, it is revealed that it is eventually revealed to be a connection between the murder and the family and the murder of Jacobo's family. The family is forced to move away from their home. Njoroge is the main character of Weep Not, Child. His main goal throughout the book is to become as educated as possible. He is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt. The novel explores the negative aspects of colonial rule over Kenya. It also ponders the role of saviours and salvation in The River Between. The author says, ""Jomo Kenyatta had had a duty to lead the people and save the people of Kenya, and I say from the same tree, a son shall rise in me, and be his duty to save his people"" The author notes: ""Salvation shall come from the hills in his attempt. And from the blood that flows that flows in me ... I say: 'Salvation in me' "" The author also says: ""The author notes that in his attempts to save Kenya, the saviour and savior have been the people, and not the other way around"" ""The River Between"" is a collection of short stories written by the author, written in English and Kiswahili, about his experiences in Kenya during the Second World War. The book is published by Simon & Schuster, and is available in hardback for £12.99 (US$16.99) and paperback for £16.95 (UK). It is published in English with an abridged version of the book available in the UK for £14.99, and in the US with an unabridged copy of the English version for £15.99. It has been published in two editions, the second of which is a hardback version with a paperback edition of £15 (UK) and the third of which comes in a hardcover version of £16 (US) with an additional £5.99 ($16.50) for the paperback version. It is also available in paperback in the U.S. and the UK with an autographed copy for £13.99 (£15.95) and a paperback version of $15.50 (£16.00). The book has been translated into English and published in the United States by Simon and Schuster. The UK edition has been released in paperback with an English translation of the first half of the novel, and the second half by the US publisher, Simon &Schuster, for £11.95 (£14.95). It has also been published on the Kindle version of this article, with a US edition of the same name, but with a different cover. The paperback version has a different front cover and a different back cover. For more information on the book, see: http://www.simonandschuster.com/books/Weep-Not-Child-The-River-Between.html. For the full version, see www.samaritans.co.uk/pages/weep-not-child-the-river-between.html?page-1-2.html%. For the second part of the story, see http://sales.sales-and-purchases/Weeping-Not, Child-The. River- Between.html/. For the third part, see http:// www.sages.com/. The fourth and final part of this section, the book will be released on September 11, 2013. een his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel"""
What are the stability conditions for a solution of $-\Delta u = f(u)$?,"\section{Introduction and main results}


In this note we are interested in the existence versus non-existence of stable sub- and super-solutions of equations of the form
\begin{equation} \label{eq1}
-div( \omega_1(x) \nabla u ) = \omega_2(x) f(u) \qquad \mbox{in $ {\mathbb{R}}^N$,}
\end{equation} where $f(u)$ is one of the following non-linearities: $e^u$,  $ u^p$ where $ p>1$ and $ -u^{-p}$ where $ p>0$.  We assume that $ \omega_1(x)$ and $ \omega_2(x)$, which we call \emph{weights},  are smooth positive functions (we allow $ \omega_2$ to be zero at say a point) and which satisfy various growth conditions at $ \infty$.    Recall that we say that a solution $ u $ of $ -\Delta u = f(u)$ in $ {\mathbb{R}}^N$ is stable provided
\[ \int f'(u) \psi^2 \le \int | \nabla \psi|^2, \qquad \forall \psi \in C_c^2,\] where $ C_c^2$ is the set of $ C^2$ functions defined on $ {\mathbb{R}}^N$ with compact support.  Note that the stability of $u$ is just saying that the second variation at $u$ of the energy associated with the equation is non-negative.     In our setting this becomes:  We say a $C^2$ sub/super-solution $u$ of (\ref{eq1}) is \emph{stable} provided
\begin{equation} \label{stable}
\int \omega_2 f'(u) \psi^2 \le \int \omega_1 | \nabla \psi|^2 \qquad \forall \psi \in C_c^2.
\end{equation}
One should note that (\ref{eq1}) can be re-written as
\begin{equation*}
- \Delta u  + \nabla \gamma(x) \cdot \nabla u ={ \omega_2}/{\omega_1}\ f(u) \qquad \text{ in $ \mathbb{R}^N$},
\end{equation*}
where
$\gamma = - \log( \omega_1)$ and on occasion we shall take this point of view.


\begin{remark} \label{triv} Note that  if $ \omega_1$ has enough integrability then it is immediate that if $u$ is a stable solution  of (\ref{eq1}) we have $ \int \omega_2 f'(u) =0 $  (provided $f$ is increasing).   To see this let $ 0 \le \psi \le 1$ be supported in a ball of radius $2R$ centered at the origin ($B_{2R}$) with $ \psi =1$ on $ B_R$ and such that $  | \nabla \psi | \le \frac{C}{R}$ where $ C>0$ is independent of $ R$.   Putting this $ \psi$ into $ (\ref{stable})$ one obtains
\[ \int_{B_R} \omega_2 f'(u) \le \frac{C}{R^2} \int_{R < |x| <2R} \omega_1,\] and so if the right hand side goes to zero as $ R \rightarrow \infty$ we have the desired result.

\end{remark}





The existence versus non-existence of stable solutions of  $ -\Delta u =  f(u)$ in $ {\mathbb{R}}^N$ or $ -\Delta u = g(x) f(u)$ in $ {\mathbb{R}}^N$ is now quite well understood, see  \cite{dancer1, farina1, egg, zz, f2, f3, wei, ces, e1, e2}.  We remark that some of these results are examining the case where $ \Delta $ is replaced with $ \Delta_p$ (the $p$-Laplacian) and also in many cases the authors are interested in finite Morse index solutions or solutions which are stable outside a compact set.
  Much of the interest in these Liouville type theorems  stems from the fact that the non-existence of a stable solution is related to the existence of a priori estimates for stable solutions of a related equation on a bounded domain.




       In \cite{Ni}  equations similar to  $ -\Delta u = |x|^\alpha u^p$  where examined on the unit ball in $ {\mathbb{R}}^N$ with zero Dirichlet boundary conditions.  There it was shown that for $ \alpha >0$ that  one can obtain positive solutions for $ p $ supercritical with respect to Sobolev embedding and so one can view that the term $ |x|^\alpha$ is restoring some compactness.      A similar feature happens for equations of the form
\[ -\Delta u = |x|^\alpha f(u) \qquad \mbox{in $ {\mathbb{R}}^N$};\]    the value of $ \alpha$ can vastly alter the existence versus non-existence of a stable solution,  see \cite{e1, ces, e2, zz, egg}.

We now come to our main results and for this we need to define a few quantities:

\begin{eqnarray*}
I_G&:=& R^{-4t-2} \int_{ R < |x|<2R} \frac{ \omega_1^{2t+1}}{\omega_2^{2t}}dx , \\
 J_G&:=& R^{-2t-1} \int_{R < |x| <2R} \frac{| \nabla \omega_1|^{2t+1} }{\omega_2^{2t}} dx  ,\\I_L&:=& R^\frac{-2(2t+p-1)}{p-1}  \int_{R<|x|<2R }{ \left(   \frac{w_1^{p+2t-1}}{w_2^{2t}}    \right)^{\frac{1}{p-1} }  }  dx,\\ J_L&:= &R^{-\frac{p+2t-1}{p-1} }  \int_{R<|x|<2R }{ \left(   \frac{|\nabla w_1|^{p+2t-1}}{w_2^{2t}}     \right)^{\frac{1}{p-1} }  }  dx,\\
I_M  &:=&   R^{-2\frac{p+2t+1}{p+1} }  \int_{R<|x|<2R }{ \left(   \frac{w_1^{p+2t+1}}{w_2^{2t}}    \right)^{\frac{1}{p+1} }  } \ dx, \\
J_M  &:= &  R^{-\frac{p+2t+1}{p+1} }  \int_{R<|x|<2R }{ \left(   \frac{|\nabla w_1|^{p+2t+1}}{w_2^{2t}}     \right)^{\frac{1}{p+1} }  }  dx.
\end{eqnarray*}


The three equations we examine are
\[ -div( \omega_1 \nabla u ) = \omega_2 e^u \qquad \mbox{ in $ {\mathbb{R}}^N$ } \quad  (G), \]
\[  -div( \omega_1 \nabla u ) = \omega_2 u^p \qquad \mbox{ in $ {\mathbb{R}}^N$ } \quad  (L), \]
\[ -div( \omega_1 \nabla u ) = - \omega_2 u^{-p} \qquad \mbox{ in $ {\mathbb{R}}^N$ } \quad  (M),\]  and where we restrict $(L)$ to the case $ p>1$ and $(M)$ to $ p>0$.    By solution we always mean a $C^2$ solution.   We now come to our main results in terms of abstract $ \omega_1 $ and $ \omega_2$.    We remark that our approach to non-existence of stable solutions is the approach due to Farina, see \cite{f2,f3,farina1}.

\begin{thm} \label{main_non_exist} \begin{enumerate}


\item   There is no  stable sub-solution of $(G)$ if $ I_G, J_G \rightarrow 0$ as $ R \rightarrow \infty$   for some   $0<t<2$.
 \item    There is no positive   stable sub-solution (super-solution) of $(L)$  if  $ I_L,J_L \rightarrow 0$ as $ R \rightarrow \infty$   for some   $p- \sqrt{p(p-1)}   < t<p+\sqrt{p(p-1)} $  ($0<t<\frac{1}{2}$).
\item   There is no positive   stable super-solution of (M) if  $ I_M,J_M \rightarrow 0$ as $ R \rightarrow \infty$  for some $0<t<p+\sqrt{p(p+1)}$.

\end{enumerate}

\end{thm}     If we assume that $ \omega_1$ has some monotonicity we can do  better.   We will assume that the monotonicity conditions is satisfied for big $x$  but really all ones needs is for it to be satisfied on a suitable sequence of annuli.

\begin{thm} \label{mono}  \begin{enumerate} \item There is no  stable sub-solution of $(G)$ with $  \nabla \omega_1(x) \cdot  x \le 0$ for big $x$ if  $ I_G \rightarrow 0$ as $ R \rightarrow \infty$   for some   $0<t<2$.

\item There is no positive  stable sub-solution of  $(L)$ provided  $  I_L \rightarrow 0$ as $ R \rightarrow \infty$ for either:
\begin{itemize}
\item  some $  1 \le t < p + \sqrt{p(p-1)}$ and $  \nabla \omega_1(x) \cdot x \le 0$ for big $x$,  or  \\

\item some $ p - \sqrt{p(p-1)} < t \le 1$ and $ \nabla \omega_1(x) \cdot x \ge 0$ for big $ x$.
\end{itemize}
 There is no positive super-solution of $(L)$ provided $ I_L \rightarrow 0$ as $ R \rightarrow \infty$ for some $ 0 < t < \frac{1}{2}$ and $ \nabla \omega_1(x) \cdot x \le 0$ for big $x$.



\item  There is no positive stable super-solution of $(M)$ provided  $ I_M \rightarrow 0$ as $ R \rightarrow \infty$  for some  $0<t<p+\sqrt{p(p+1)}$.

\end{enumerate}

\end{thm}


\begin{cor}  \label{thing}  Suppose $ \omega_1 \le C  \omega_2$  for big $ x$, $ \omega_2 \in L^\infty$,  $ \nabla \omega_1(x) \cdot  x \le 0$ for big $ x$.
\begin{enumerate} \item There is no stable sub-solution of $(G)$ if $ N \le 9$.

\item  There is no positive stable sub-solution of $(L)$ if  $$N<2+\frac{4}{p-1} \left( p+\sqrt{p(p-1)}  \right).$$

\item  There is no positive stable super-solution of $(M)$ if $$N<2+\frac{4}{p+1} \left( p+\sqrt{p(p+1)}  \right).$$

\end{enumerate}

\end{cor}


If one takes $ \omega_1=\omega_2=1$ in the above corollary, the results obtained for $(G)$ and  $(L)$,  and for some values of $p$ in $(M)$, are optimal, see \cite{f2,f3,zz}.





We now drop all monotonicity conditions on $ \omega_1$.

\begin{cor} \label{po} Suppose  $ \omega_1 \le C \omega_2$ for big $x$, $ \omega_2 \in L^\infty$, $ | \nabla \omega_1| \le C \omega_2$ for big $x$.
\begin{enumerate} \item  There is no stable sub-solution of $(G)$ if $ N \le 4$.

\item  There is no positive stable sub-solution of $(L)$ if $$N<1+\frac{2}{p-1} \left( p+\sqrt{p(p-1)}  \right).$$

\item There is no positive super-solution of $(M)$ if $$N<1+\frac{2}{p+1} \left( p+\sqrt{p(p+1)}  \right).$$

\end{enumerate}

\end{cor}

Some of the conditions on $ \omega_i$ in Corollary \ref{po} seem somewhat artificial.  If we shift over to the advection equation (and we take $ \omega_1=\omega_2$  for simplicity)
\[ -\Delta u + \nabla \gamma \cdot \nabla u = f(u), \] the conditions on $ \gamma$ become: $ \gamma$ is bounded from below and has a bounded gradient.



In what follows we examine the case where $ \omega_1(x) = (|x|^2 +1)^\frac{\alpha}{2}$ and $ \omega_2(x)= g(x) (|x|^2 +1)^\frac{\beta}{2}$,  where $ g(x) $ is positive except at say a point, smooth and where $ \lim_{|x| \rightarrow \infty} g(x) = C \in (0,\infty)$.     For this class of weights we can essentially obtain optimal results.


\begin{thm} \label{alpha_beta}   Take $ \omega_1 $ and $ \omega_2$ as above.
\begin{enumerate}

\item If $ N+ \alpha - 2 <0$ then there is no stable sub-solution for $(G)$, $(L)$ (here we require it to be positive) and in the case of $(M)$ there is no positive  stable  super-solution. This case is the trivial case, see Remark \ref{triv}.  \\



\textbf{Assumption:} For the remaining cases we assume that $ N + \alpha -2 > 0$.

  \item If  $N+\alpha-2<4(\beta-\alpha+2)$ then there is no  stable sub-solution for $ (G)$.

\item If $N+\alpha-2<\frac{ 2(\beta-\alpha+2)   }{p-1} \left( p+\sqrt{p(p-1)}  \right)$ then there is  no positive stable sub-solution of $(L)$.

\item If $N+\alpha-2<\frac{2(\beta-\alpha+2)   }{p+1} \left( p+\sqrt{p(p+1)}  \right)$ then there is no positive stable super-solution of $(M)$.

\item Further more 2,3,4 are optimal in the sense if $ N + \alpha -2 > 0$ and the remaining inequality is not satisfied (and in addition we assume we don't have equality in the inequality) then we can find a suitable function $ g(x)$ which satisfies the above properties and a stable sub/super-solution $u$ for the appropriate equation.

\end{enumerate}

\end{thm}

\begin{remark}  Many of the above results can be extended to the case of equality in either the $ N + \alpha - 2 \ge 0$ and also the other inequality which depends on the equation we are  examining.  We omit the details because one cannot prove the results in a unified way.

\end{remark}






















In showing that an explicit solution is stable we will need the   weighted Hardy inequality given in \cite{craig}.
\begin{lemma} \label{Har}
Suppose $ E>0$ is a smooth function.  Then one has
\[ (\tau-\frac{1}{2})^2 \int E^{2\tau-2} | \nabla E|^2 \phi^2 + (\frac{1}{2}-\tau) \int (-\Delta E) E^{2\tau-1} \phi^2 \le \int E^{2\tau} | \nabla \phi|^2,\] for all $ \phi \in C_c^\infty({\mathbb{R}}^N)$ and $ \tau \in {\mathbb{R}}$.
\end{lemma}  By picking an appropriate function $E$ this gives,

\begin{cor} \label{Hardy}
For all $ \phi \in C_c^\infty$ and $ t , \alpha \in {\mathbb{R}}$. We have
  \begin{eqnarray*}
\int (1+|x|^2)^\frac{\alpha}{2} |\nabla\phi|^2 &\ge& (t+\frac{\alpha}{2})^2 \int |x|^2 (1+|x|^2)^{-2+\frac{\alpha}{2}}\phi^2\\
&&+(t+\frac{\alpha}{2})\int  (N-2(t+1)   \frac{|x|^2}{1+|x|^2}) (1+|x|^2)^{-1+\frac{\alpha} {2}} \phi^2.
\end{eqnarray*}
 \end{cor}























\section{Proof of main results}

\textbf{ Proof of Theorem \ref{main_non_exist}.}   (1). Suppose $ u$ is a stable sub-solution of $(G)$ with $ I_G,J_G \rightarrow 0$ as $ R \rightarrow \infty$ and  let $ 0 \le \phi \le 1$ denote a smooth compactly supported function.  Put $ \psi:= e^{tu} \phi$ into (\ref{stable}), where $ 0 <t<2$, to arrive at
\begin{eqnarray*}
\int \omega_2 e^{(2t+1)u} \phi^2 &\le & t^2 \int \omega_1 e^{2tu} | \nabla u|^2 \phi^2 \\
&& +\int \omega_1 e^{2tu}|\nabla \phi|^2 + 2 t \int \omega_1 e^{2tu} \phi \nabla u \cdot \nabla \phi.
\end{eqnarray*}  Now multiply $(G)$ by $ e^{2tu} \phi^2$ and integrate by parts to arrive at
\[ 2t \int \omega_1 e^{2tu} | \nabla u|^2 \phi^2 \le \int \omega_2 e^{(2t+1) u} \phi^2 - 2 \int \omega_1 e^{2tu} \phi \nabla u \cdot \nabla \phi,\]
and now if one equates like terms they arrive at
\begin{eqnarray} \label{start}
\frac{(2-t)}{2} \int \omega_2 e^{(2t+1) u} \phi^2  & \le & \int \omega_1 e^{2tu} \left( | \nabla \phi |^2 - \frac{ \Delta \phi}{2} \right) dx \nonumber \\
&& - \frac{1}{2} \int e^{2tu} \phi \nabla \omega_1 \cdot \nabla \phi.
\end{eqnarray}   Now substitute $ \phi^m$ into this inequality for $ \phi$ where $ m $ is a big integer to obtain
\begin{eqnarray} \label{start_1}
\frac{(2-t)}{2} \int \omega_2 e^{(2t+1) u} \phi^{2m}  & \le & C_m \int \omega_1 e^{2tu} \phi^{2m-2}  \left( | \nabla \phi |^2 + \phi |\Delta \phi|  \right) dx \nonumber \\
&& - D_m \int e^{2tu} \phi^{2m-1} \nabla \omega_1 \cdot \nabla \phi
\end{eqnarray} where $ C_m$ and $ D_m$ are positive constants just depending on $m$.   We now estimate the  terms on the right but we mention that when ones assume the appropriate monotonicity on $ \omega_1$ it is the last integral on the right which one is able to drop.

\begin{eqnarray*}
\int \omega_1 e^{2tu} \phi^{2m-2} | \nabla \phi|^2 & = & \int \omega_2^\frac{2t}{2t+1} e^{2tu} \phi^{2m-2}  \frac{ \omega_1 }{\omega_2^\frac{2t}{2t+1}} | \nabla \phi|^2  \\
& \le &  \left( \int \omega_2 e^{(2t+1) u} \phi^{(2m-2) \frac{(2t+1)}{2t}} dx \right)^\frac{2t}{2t+1}\\ &&\left( \int \frac{ \omega_1 ^{2t+1}}{\omega_2^{2t}} | \nabla \phi |^{2(2t+1)} \right)^\frac{1}{2t+1}.
\end{eqnarray*}
Now, for fixed $ 0 <t<2$ we can take $ m $ big enough so $ (2m-2) \frac{(2t+1)}{2t} \ge 2m $ and since $ 0 \le \phi \le 1$ this allows us to replace the power on $ \phi$ in the first term on the right with $2m$   and hence we obtain
 \begin{equation} \label{three}
 \int \omega_1 e^{2tu} \phi^{2m-2} | \nabla \phi|^2  \le \left( \int \omega_2 e^{(2t+1) u} \phi^{2m} dx \right)^\frac{2t}{2t+1} \left( \int \frac{ \omega_1 ^{2t+1}}{\omega_2^{2t}} | \nabla \phi |^{2(2t+1)} \right)^\frac{1}{2t+1}.
 \end{equation}    We now take the test functions $ \phi$ to be such that $ 0 \le  \phi \le 1$ with $ \phi $ supported in the ball $ B_{2R}$ with $ \phi = 1 $ on $ B_R$ and $ | \nabla \phi | \le \frac{C}{R}$ where $ C>0$ is independent of $ R$.   Putting this choice of $ \phi$ we obtain
 \begin{equation} \label{four}
 \int \omega_1 e^{2tu} \phi^{2m-2} | \nabla \phi |^2 \le \left( \int \omega_2 e^{(2t+1)u} \phi^{2m} \right)^\frac{2t}{2t+1} I_G^\frac{1}{2t+1}.
 \end{equation}  One similarly shows that
 \[ \int \omega_1 e^{2tu} \phi^{2m-1} | \Delta \phi| \le \left( \int \omega_2 e^{(2t+1)u} \phi^{2m} \right)^\frac{2t}{2t+1} I_G^\frac{1}{2t+1}.\]
 So, combining the results we obtain

 \begin{eqnarray} \label{last} \nonumber \frac{(2-t)}{2} \int \omega_2 e^{(2t+1) u} \phi^{2m} &\le& C_m \left( \int \omega_2 e^{(2t+1) u} \phi^{2m} dx \right)^\frac{2t}{2t+1} I_G^\frac{1}{2t+1}\\
 &&- D_m \int e^{2tu} \phi^{2m-1}  \nabla \omega_1 \cdot \nabla \phi.
 \end{eqnarray}
 We now estimate this last term.  A similar argument using H\""{o}lder's inequality shows that
 \[ \int e^{2tu} \phi^{2m-1} | \nabla \omega_1| | \nabla \phi| \le \left(  \int \omega_2 \phi^{2m} e^{(2t+1) u} dx \right)^\frac{2t}{2t+1} J_G^\frac{1}{2t+1}. \] Combining the results gives that
\begin{equation} \label{last}
(2-t) \left( \int \omega_2 e^{(2t+1) u} \phi^{2m} dx \right)^\frac{1}{2t+1} \le I_G^\frac{1}{2t+1} + J_G^\frac{1}{2t+1},
\end{equation} and now we send $ R \rightarrow \infty$ and use the fact that $ I_G, J_G \rightarrow 0$ as $ R \rightarrow \infty$ to see that
\[ \int \omega_2 e^{(2t+1) u} =0, \] which is clearly a contradiction.   Hence there is no stable sub-solution of $(G)$.

(2).   Suppose that $u >0$ is a stable sub-solution (super-solution) of $(L)$.  Then a similar calculation as in (1) shows that for  $ p - \sqrt{p(p-1)} <t < p + \sqrt{p(p-1)}$,  $( 0 <t<\frac{1}{2})$ one has

\begin{eqnarray}   \label{shit}
(p  -\frac{t^2}{2t-1}   )\int \omega_2 u^{2t+p-1} \phi^{2m} & \le & D_m \int \omega_1 u^{2t} \phi^{2(m-1)} (|\nabla\phi|^2  +\phi |\Delta \phi |) \nonumber \\
&& +C_m \frac{(1-t)}{2(2t-1)} \int u^{2t} \phi^{2m-1}\nabla \omega_1 \cdot  \nabla \phi.
 \end{eqnarray}    One now applies H\""{o}lder's argument as in (1) but the terms $ I_L$ and $J_L$ will appear on the right hand side of the resulting
 equation.      This shift from a sub-solution to a super-solution depending on whether $ t >\frac{1}{2}$ or $ t < \frac{1}{2}$ is a result from the sign change of $ 2t-1$ at $ t = \frac{1}{2}$.   We leave the details for the reader.


(3).  This case is also similar to (1) and (2).


\hfill $ \Box$

 \textbf{Proof of Theorem \ref{mono}.}   (1).  Again we suppose there is a stable sub-solution $u$ of $(G)$.  Our starting point  is (\ref{start_1}) and we wish to be able to drop the term
 \[ - D_m \int e^{2tu} \phi^{2m-1} \nabla \omega_1 \cdot \nabla \phi, \] from (\ref{start_1}).    We can choose $ \phi$ as in the proof of Theorem \ref{main_non_exist} but also such that $ \nabla \phi(x) = - C(x) x$ where $ C(x) \ge 0$.     So if we assume that $ \nabla \omega_1 \cdot x \le 0$ for big $x$ then we see that this last term is non-positive and hence we can drop the term.  The the proof is as before but now we only require that $ \lim_{R \rightarrow \infty} I_G=0$.

 (2).     Suppose that $ u >0$ is a stable sub-solution of $(L)$  and so (\ref{shit}) holds for all $  p - \sqrt{p(p-1)} <t< p + \sqrt{p(p-1)}$.   Now we wish to use monotonicity to drop the term from (\ref{shit}) involving the term $ \nabla \omega_1 \cdot \nabla \phi$.      $ \phi$ is chosen the same as in (1)  but here one notes that the co-efficient for this term changes sign at $ t=1$ and hence by restriction $t$ to the appropriate side of 1 (along with the above condition on $t$ and $\omega_1$) we can drop the last term depending on which monotonicity we have and hence to obtain a contraction we only require that $ \lim_{R \rightarrow \infty} I_L =0$.   The result for the non-existence of a stable super-solution is similar be here one restricts $ 0 < t < \frac{1}{2}$.


(3).  The proof here is similar to (1) and (2) and we omit the details.




 \hfill $\Box$













\textbf{Proof of Corollary \ref{thing}.}   We suppose  that $ \omega_1 \le C  \omega_2$  for big $ x$, $ \omega_2 \in L^\infty$,  $ \nabla \omega_1(x) \cdot  x \le 0$ for big $ x$.     \\
(1).    Since $ \nabla \omega_1 \cdot x \le 0$ for big $x$ we can apply Theorem \ref{mono} to show the non-existence of a stable solution to $(G)$.   Note that  with the above assumptions on $ \omega_i$ we have that
\[ I_G \le \frac{C R^N}{R^{4t+2}}.\]  For $ N \le 9$  we can take $ 0 <t<2$  but close enough to $2$ so the right hand side goes to zero as $ R \rightarrow \infty$.

Both (2) and (3) also follow directly from applying Theorem \ref{mono}.   Note that one can say more about (2) by taking the multiple cases as listed in Theorem \ref{mono} but we have choice to leave this to the reader.


\hfill $ \Box$


\textbf{Proof of Corollary \ref{po}.}   Since we have no monotonicity conditions now we will need both $I$ and $J$ to go to zero to show the non-existence of a stable solution.   Again the results are obtained immediately by applying Theorem \ref{main_non_exist}  and we prefer to omit the details.


\hfill $\Box$
















\textbf{Proof of Theorem \ref{alpha_beta}.}  (1).  If $ N + \alpha -2 <0$ then using Remark \ref{triv}  one easily sees there is no stable sub-solution of $(G)$ and $(L)$ (positive for $(L)$) or a positive stable super-solution of $(M)$.   So we now assume that $ N + \alpha -2 > 0$.     Note that the monotonicity of $ \omega_1$ changes when $ \alpha $ changes sign and hence one would think that we need to consider separate cases if we hope to utilize the monotonicity results.   But a computation shows that in fact $ I$ and $J$ are just multiples of each other in all three cases so it suffices to show, say, that $ \lim_{R \rightarrow \infty} I =0$. \\
(2).   Note that for $ R >1$ one has
\begin{eqnarray*}
I_G & \le  & \frac{C}{R^{4t+2}} \int_{R <|x| < 2R} |x|^{ \alpha (2t+1) - 2t \beta} \\
& \le  &  \frac{C}{R^{4t+2}}  R^{N + \alpha (2t+1) - 2t \beta},
\end{eqnarray*} and so to show the non-existence we want to find some $ 0 <t<2$ such that
$  4t+2 > N  + \alpha(2t+1) - 2 t \beta$,   which is equivalent to  $ 2t ( \beta - \alpha +2) > (N + \alpha -2)$.    Now recall that we are assuming that $ 0 < N + \alpha -2 < 4 ( \beta - \alpha +2) $ and hence we have the desired result by taking $  t <2$ but sufficiently close.
The proof of the non-existence results for
(3) and (4) are similar and we omit the details.   \\
(5).  We now assume that $N+\alpha-2>0$.  In showing the existence of stable sub/super-solutions we need to consider  $ \beta - \alpha + 2 <0$ and $ \beta - \alpha +2 >0$ separately.

\begin{itemize} \item $(\beta - \alpha + 2 <0)$  Here we take $ u(x)=0$ in the case of $(G)$ and $ u=1$ in the case of $(L)$ and $(M)$. In addition we take $ g(x)=\E$.  It is clear that in all cases $u$ is the appropriate sub or super-solution.  The only thing one needs to check is the stability.    In all cases this reduces to trying to show that we have
\[ \sigma \int       (1+|x|^2)^{\frac{\alpha}{2}  -1}    \phi^2 \le \int      (1+|x|^2)^{\frac{\alpha}{2}}   | \nabla\phi  |^2,\]  for all $ \phi \in C_c^\infty$ where  $ \sigma $ is some small positive constant; its either $ \E$ or $ p \E$ depending on which equation were are examining.
To show this we use the result from Corollary \ref{Hardy} and we drop a few positive terms to arrive at
\begin{equation*}
\int (1+|x|^2)^\frac{\alpha}{2} |\nabla\phi|^2\ge (t+\frac{\alpha}{2})\int \left (N-2(t+1)   \frac{|x|^2}{1+|x|^2}\right) (1+|x|^2)^{-1+\frac{\alpha} {2}}
\end{equation*} which holds for all $ \phi \in C_c^\infty$ and $ t,\alpha \in {\mathbb{R}}$.
  Now, since $N+\alpha-2>0$, we can choose $t$ such that $-\frac{\alpha}{2}<t<\frac{n-2}{2}$.  So, the integrand function in the right hand side is positive and since for small enough $\sigma$ we have
  \begin{equation*}
\sigma \le  (t+\frac{\alpha}{2})(N-2(t+1)   \frac{|x|^2}{1+|x|^2})  \ \ \ \text {for all} \ \ x\in \mathbb{R}^N
\end{equation*}
 we get stability.



\item ($\beta-\alpha+2>0$) In the case of $(G)$ we take   $u(x)=-\frac{\beta-\alpha+2}{2} \ln(1+|x|^2)$ and $g(x):= (\beta-\alpha+2)(N+(\alpha-2)\frac{|x|^2}{1+|x|^2})$. By a computation one sees that $u$ is a sub-solution of $(G)$ and hence we need now to only show the stability, which amounts to showing that
\begin{equation*}
\int \frac{g(x)\psi^2}{(1+|x|^{2   })^{-\frac{\alpha}{2}+1}}\le \int\frac{|\nabla\psi|^2}{    (1+|x|^2)^{-\frac{\alpha}{2}}     },
\end{equation*} for all $ \psi \in C_c^\infty$.  To show this we use  Corollary \ref{Hardy}.  So  we  need to choose an appropriate $t$ in   $-\frac{\alpha}{2}\le t\le\frac{N-2}{2}$  such that for all $x\in {\mathbb{R}}^N$ we have
 \begin{eqnarray*}
 (\beta-\alpha+2)\left(    N+  (\alpha-2)\frac{|x|^2}{1+|x|^2}\right)         &\le& (t+\frac{\alpha}{2})^2 \frac{ |x|^2 }{(1+|x|^2}\\
&&+(t+\frac{\alpha}{2}) \left(N-2(t+1)   \frac{|x|^2}{1+|x|^2}\right).
\end{eqnarray*}
With a  simple calculation one sees we need just to have
   \begin{eqnarray*}
 (\beta-\alpha+2)&\le& (t+\frac{\alpha}{2}) \\
  (\beta-\alpha+2)     \left(    N+  \alpha-2\right)      &   \le&  (t+\frac{\alpha}{2}) \left(N-t-2+\frac{\alpha}{2}) \right).
 \end{eqnarray*}     If one takes $ t= \frac{N-2}{2}$ in the case where $ N \neq 2$ and $ t $ close to zero in the case for $ N=2$ one easily sees the above inequalities both hold, after considering all the constraints on $ \alpha,\beta$ and $N$.

 We now consider the case of $(L)$.  Here one takes $g(x):=\frac {\beta-\alpha+2}{p-1}(    N+  (\alpha-2-\frac{\beta-\alpha+2}{p-1})
\frac{|x|^2}{1+|x|^2})$ and $ u(x)=(1+|x|^2)^{ -\frac  {\beta-\alpha+2}{2(p-1)} }$.   Using essentially the same approach as in $(G)$ one shows that $u$ is a stable sub-solution of $(L)$ with this choice of $g$.   \\
For the case of $(M)$ we take   $u(x)=(1+|x|^2)^{ \frac  {\beta-\alpha+2}{2(p+1)}   }$ and $g(x):=\frac {\beta-\alpha+2}{p+1}(    N+  (\alpha-2+\frac{\beta-\alpha+2}{p+1})
\frac{|x|^2}{1+|x|^2})$.



\end{itemize}


\hfill  $  \Box$








","[""$\\int f'(u) \\psi^2 \\le \\int | \\nabla \\psi|^2, \\forall \\psi \\in C_c^2$.""]",3743,multifieldqa_en,en,,135b82aecda3c83cfd73d1447e8f92a8a44de50ffdb09444," In this note we are interested in the existence versus non-existence of stable sub- and super-solutions of equations of the form. We say that a solution $ u $ of $ -\Delta u = f(u)$ in $ {\mathbb{R}}^N$ is stable provided $f$ is non-negative. We remark that some of these results are quite well understood, and we see that the case where these are not the case is quite different. We conclude that $u$ is a stable solution of (\ref{eq1}) where $ 0 is supported in a ball of radius $2R$ centered at the origin with $ \psi =1$ on $ B_R$ and such that $ C>0$ is independent of $ R$. In the right hand hand side we have the result that if $u $ is stable we have $ \int \omega_2 f' (u) =0 $ (provided $ f$ is increasing) and so if $ R goes to zero we have  $R < R \ right hand side. We also see that $C_c^2$ is the set of $ C^2 $ functions defined on $ $ R$ with compact support. We end up with the following results: $R = 0, C = 1, R < R, R \right hand side, c = 2, e1, e2, f3, f4, e5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, f21, f22, f23, f24, f25, f26, f27, f28, f29, f30, f31, f32, f33, f34, f35, f36, f37, f38, f39, f40, f41, f42, f43, f44, f45, f46, f47, f48, f49, f50, f51, f52, f53, f54, f55, f56, f57, f58, f59, f60, f63, f64, f65, f66, f67, f68, f69, f70, f71, f72, f73, f74, f75, f76, f77, f78, f79, f80, f81, f82, f83, f84, f85, f87, f88, f90, f91, f93, f92, f94, f95, f96, f97, f98, f99, f100, f102, f103, f104, f105, f106, f107, f108, f109, f110, f111, f112, f113, f120, f130, f123, f143, f144, f145, f150, f151, f152, f153, f160, f185, f183, f186, f187, f190, f223, f194, f195, f205, f206, f207, f208, f209, f240, f234, f250, f230, f255, f260, f242, f237, f239, f244, f245, f254, f246, f253, f258, f259, f263, f265, f256, f267, f270, f268, f273, f274, f275, f264, f277, f269, f278, f299, f284, f300, f286, f303, f308, f340, f344, f345, f353, f357, f338, f364, f5 There is no positive   stable sub-solution of $(G) if $ I_G, J_G \rightarrow 0$ as $ R   for some $0<t<2$. We remark that our approach to non-existence of stable solutions is the approach due to Farina. If we assume that the monotonicity conditions is satisfied for big $x$ we can do better than Farina's approach. We will now come to our main results in terms of abstract $ \omega_1 $ and $\omega-2$. The three equations we examine are: -div(    - div(         (G), -div (   1  2 )   (L), - div (   1 2 ) (L)  (M) (G) (L, M) (R) (C) (N) (P) (S) (B) (M, L) (X) (D) (E) (W) (F) (H) (A) (Z) (J) (K) (x) (Y) (z) (f) (r) (g) (h) (m) (b) (s) (e) (n) (l) (p) (i) (u) (j) (w) (a) (t) (c) (d) (1) (2) (expressed in $ {\mathbb{R}}^N$ } (G, M, L, J, R) (V) (Q) (v) (q) (k) (3) (4) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (28) (29) (30) (31) (32) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (46) (47) (54) (55) (56) (57) (58) (59) (62) (63) (64) (65) (74) (75) (76) (78) (83) (84) (85) (86) (87) (88) (90) (bis) (93) (92) (94) (95) (96) (97) (98) (99) (100) (104) (111) (113) (114) (130) (115) (131) (133) (138) (125) (128) (135) (136) (129) (139) (140) (141) (142) (143) (145) (150) (151) (153) (164) (165) (167) (174) (169) (170) (191) (172) (173) (194) (195) (196) (197) (190) (214) (215) (216) (223) (229) (height) (220) (235) (222) (233) (234) (244) (225) (251) (227) (238) (236) (273) (274) (affordance: $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $19, $20, $21, $22, $23, $24, $25, $26, $27, $28, $29, Some of the conditions on $ \omega_i$ seem somewhat artificial. We omit the details because one cannot prove the results in a unified way. For this class of weights we can essentially obtain optimal results. Many of the above results can be extended to the case of equality in either the $ N + \alpha - 2 \ge 0$ and also the other inequality which depends on the equation we are  examining. If $ N+ \ alpha - 2 <0$ then there is no stable sub-solution for $ (G)$. If $N+\alpha-2 <4 then there are no  stable sub/super-solutions of $ (L)$. For more 2,3,4 are optimal in the sense if the remaining inequality is not satisfied then we can find a suitable function $ g(x)$ which satisfies the above properties and a stableSub-Super-Solution $u$ for the appropriate equation. For the remaining cases we assume that $ N. + \Alpha -2 > 0$. For this case we require that $N. +\alpha -2 <0$. For the rest of the cases we require it to be positive. We will show that an explicit Hardy inequality given in this case is given in the next section. The solution is a function of the form E>0, E>1, E2, E3, E4, E5, E6, E7, E8, E9, E10, E11, E12, E13, E14, E15, E16, E17, E18, E19, E20, E21, E22, E23, E24, E25, E26, E27, E28, E29, E30, E31, E32, E33, E34, E35, E36, E37, E38, E39, E40, E41, E42, E43, E44, E45, E46, E47, E48, E49, E51, E52, E53, E54, E55, E56, E57, E58, E59, E60, E61, E62, E63, E64, E73, E74, E83, E84, E85, E86, E87, E88, E89, E90, E91, E92, E93, E94, E95, E96, E97, E98, E99, E102, E103, E104, E105, E107, E108, E110, E111, E112, E113, E114, E130, E1, EF, EF12, EF13, EF14, EF15, EF16, EF17, EF18, EF19, EF20, EF21, EF22, EF23, EF24, EF25, EF26, EF27, EF28, EF2, EF3, EF4, EF5, EF6, EF7, EF8, EF9, EF10, EF11,EF12,EF13,EF14,EF15,EF16,EF17,EF18,EF19,EF20,EF21,EF22,EF24,EF2,EF3,EF4,EF5,EF6,EF7,EF8,EF9,EF10,EF11, EF1,EF1,FP2,FP3,FP4,FP5,FP6,FP7,FP8,FP9,FP10,FP11,FP12,FP13,FP14,FP15,FP16,FP17,FP18,FP19,FP20,FP21,FP22,FP23,FP24,FP25,FP26,FP27,FP28,FP29,FP30,FP31,FP32,FP33,FP36,FP We start with the inequality $ \phi$ where $ m $ is a big integer to obtain $ C_m$ and $ D_m. We now estimate the terms on the right but we mention that when ones assume the appropriate monotonicity on $ \omega_1$ it is the last integral on the left which one is able to drop. For fixed $ 0 <t<2$ we can take $m $ big enough so $ (2m-2) \frac{(2t+1)}{2t} \ge 2m $ and since $ 0 \le \phi \le 1$ this allows us to replace the power on $    with $2m$  and hence we obtain $C_m$. We now take the test functions $phi$ to be such that $ 0 phi$ is independent of $ R$. We obtain the choice of this of this $C-R$ we call ‘Hercules’ and ‘B-R’. The choice of $ C-R and $R-B’ is called ‘The Hercules Test’, which is a test of the choice $ R-B$ of $ B-R. We then take the choice ‘R-C’ to be the test of this choice of ‘C-C,’ which is the test for $ R. We call this ‘the Hercule test’ ’ and we call it the ‘P-R test�’ This test is the ’P-S test,” where ‘r’ stands for ‘positive’ or ‘negative’ in the case of a positive constant, and � ‘s’ for a negative constant, such as ‘m’ (‘M’ = ‘1’). We then say that $ C’s ‘p’ means ‘power’ on $ B_R$ and that $ B’’P’= ‘2’ so that the test is ‘S’ if and only if there is a ‘n’ between 1 and 2.’ We now say that the tests for $ B and B are equal to $1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 28,. 32, 33, 34, 33,. 34, 35, 34,. 35, 36, 37, 38, 39, 39,. 36, 38,. 38, 37,. 39, 40, 41, 40,. 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 57,. 58, 58,. 59, 59,. 60, 60,. 61, 62, 63, 64, 63,. 62, 64,. 64, 65, 69, 68, 69,. 66, 69!, 64, 70, 70,. 64,. 68, 68,. 69, 73, 64! 64, 69! 68, 73,. 64! 69, 72, 70! 64,. 69,. 68! 69,. 72, 73! 68,. 70, 74, 79, 72,. 68,. 73, 78, 78,. 69!, 68, 79,. 72,. 70,. 69! 72, 78! 69!, 72, 79! 74, 73!, 73, 80, 74! 72,. 72!, 73,. 74, 75, 78!, 74, 77, 74!, 72,. 73,. 78, 79!, 74,. 74,. 73! 74,. 72%, 73, 74,. 78,. 74!, 73!, 74!, 74! 73, 75,. Theorem: A super-solution is a shift from a sub-solutions to a super-Solution depending on whether $ t >\frac{1}{2}$ or $ t < $ t. Theorem: The term $ u >0$ is a stable sub- solution of $(L)$ and so the co-efficient for this term changes sign at $ t=1$ and hence by restriction $t$ to the appropriate side of 1 we can drop the last term depending on which monotonicity we have and hence to obtain a contraction we only require that $ I_L$ and $J_L will appear on the right hand side of the resulting equation. The proof is similar to (1) and (2) and we omit the details. The result for the non-existence of a stable super- s solution is similar be here one restricts $ 0 < t <  1 /2 /3 /4 /5 /6 /7 /8 /9 /10 /11 /12 /13/14 /14/15/16/17/18/19/20/21/22/23/24/24, and so we follow directly from (3) to (4) in the same way as (5) in (6) Theorem can be rewritten as: Theorem is the same as in (1), (2), (3), (4), (5), (6), (7), (8), (9), (10), (11), (12), (13), (14), (15), (16), (17), (18), (19), (20), (21), (22), (23), (24), (25), (26), (27), (28), (29), (30), (31), (32), (33), (34), (36), (37), (38), (39), (40), (41), (42), (43), (44), (45), (46), (47), (48), (49), (50), (52), (53), (54), (55), (56), (57), (58), (59), (60), (61), (63), (64), (65), (66), (67), (68), (69), (70), (72), (73), (74), (75), (76), (77), (78), (79), (82), (83), (84), (85), (86), (87), (88), (90), (93), (94), (95), (96), (97), (98), (100), (102), (103), (104), (105), (106), (107), (108), (109), (110), (111), (112), (113), (130), (131), (132), (133), (138), (123), (134), (135), (136), (137), (139), (140), (141), (142), (143), (144), (145), (146), (147), (153), (148), (149), (151), (152), (157), (159), (160), (163), (164), (165), (167), (168), (170), (173), (174), (179), (182), (183), (184), (185), (187), (190), (189) and(190) are all the same. For example, we can take $t < 0 < 9$ and we can apply Theorem to show the existence of a solution to $(G) with the above assumptions. The proof of the non-existence results for (3) and (4) are similar and we omit the details. We now assume that $N+\alpha-2>0$. In showing the existence of stable sub/super-solutions we need to consider  $ \beta - \alpha + 2 <0$ and $ \ beta - \ alpha +2 >0$ separately. In all cases $u$ is the appropriate sub or super-solution. The only thing one needs to check is the stability. In the case of $x$ we take $u(=0) and $g(=1) and we drop a few positive terms to arrive at the result from Corollary \ref{Hardy. For small enough $\sigma$ we have a formula that holds for all $ \phi \in C_c^\infty and $ t,\alpha \in {\mathbb{R}}$. We can choose $t$ such that $-\frac{\alpha}{2}<t<\frac{n-2}{2}. So, the integrand function in the right hand side is positive and since for small enough $ \sigma $ we have the formula for the stability of a sub/Super-Super-Solution. It is clear that in all cases the sub or Super-Super is a sub-Super. We can now show that the stability amounts to $1/2, which is the same as $u/2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/24/25/26/27/28/28-28/27-28-29/29/30/31/31-32/32/33/34/36/37/38/36-37/37-38/38-38-37-36-36/39/37+28/36+37/36%/38%/37%/39%/42%/40%/41/42/42+36/47/48/38+38% /36+36% /37% /38% &36% &37% &38% -36% -37% -38% +36% +37% +38% (2t+1) -2t - 2t \beta) - 2 t \beta. We get the desired result by taking $  t <2$ but sufficiently close. We take $ u(x)=0$ in the cases of $(G) and $(M) and hence we need only to show that we have $u=1$ in both cases. We have the result for $N + \alpha -2 < 4 (\beta - alpha +2) $ and we get $t <2 (N-2(t-2) - (N) + 4t+2 (T) + 2t + 2 t (T+2) (T-2 (t) + (R) + N + \ alpha (2T+1 (T + N) + 3t + 3 t (2 t + 2) (N+ \alpha(2t + 1) - 3 t + (N +  N) - 4t + N (R +   N +  R +  4t    (R+  N  + N   +  2t  (N  + \beta(2 t) - 1 t + 1 t (N -  R) +  3 t   2 t  (T - N)   T + N+  R   R (R - N + n + N -  N - 1t + R + 1t (N. +  n +  1t  R - N. + N = N + (2 rac {\beta-\alpha+2}{p+1}(    N+  (\alpha-2+\frac{\beta-‘a’+‘b’ + ‘c’) N+ (’alpha’-’a‘’’: ‘beta’, ‘a,’ ‘b,” ‘‘d’; ‘e’. ‘f’): ‘n’ : ‘0’,. ‘1’.: ‘2’%: ‘3’%, ‘4’?: ‘5’), ‘6’!: ‘"
What algorithm is engaged in the PLMS-PPIC method?,"\section{Introduction}\label{S1}

The multiple access interferences (MAI) is the root of user
limitation in CDMA systems \cite{R1,R3}. The parallel least mean
square-partial parallel interference cancelation (PLMS-PPIC) method
is a multiuser detector for code division multiple access (CDMA)
receivers which reduces the effect of MAI in bit detection. In this
method and similar to its former versions like LMS-PPIC \cite{R5}
(see also \cite{RR5}), a weighted value of the MAI of other users is
subtracted before making the decision for a specific user in
different stages \cite{cohpaper}. In both of these methods, the
normalized least mean square (NLMS) algorithm is engaged
\cite{Haykin96}. The $m^{\rm th}$ element of the weight vector in
each stage is the true transmitted binary value of the $m^{\rm th}$
user divided by its hard estimate value from the previous stage. The
magnitude of all weight elements in all stages are equal to unity.
Unlike the LMS-PPIC, the PLMS-PPIC method tries to keep this
property in each iteration by using a set of NLMS algorithms with
different step-sizes instead of one NLMS algorithm used in LMS-PPIC.
In each iteration, the parameter estimate of the NLMS algorithm is
chosen whose element magnitudes of cancelation weight estimate have
the best match with unity. In PLMS-PPIC implementation it is assumed
that the receiver knows the phases of all user channels. However in
practice, these phases are not known and should be estimated. In
this paper we improve the PLMS-PPIC procedure \cite{cohpaper} in
such a way that when there is only a partial information of the
channel phases, this modified version simultaneously estimates the
phases and the cancelation weights. The partial information is the
quarter of each channel phase in $(0,2\pi)$.

The rest of the paper is organized as follows: In section \ref{S4}
the modified version of PLMS-PPIC with capability of channel phase
estimation is introduced. In section \ref{S5} some simulation
examples illustrate the results of the proposed method. Finally the
paper is concluded in section \ref{S6}.

\section{Multistage Parallel Interference Cancelation: Modified PLMS-PPIC Method}\label{S4}

We assume $M$ users synchronously send their symbols
$\alpha_1,\alpha_2,\cdots,\alpha_M$ via a base-band CDMA
transmission system where $\alpha_m\in\{-1,1\}$. The $m^{th}$ user
has its own code $p_m(.)$ of length $N$, where $p_m(n)\in \{-1,1\}$,
for all $n$. It means that for each symbol $N$ bits are transmitted
by each user and the processing gain is equal to $N$. At the
receiver we assume that perfect power control scheme is applied.
Without loss of generality, we also assume that the power gains of
all channels are equal to unity and users' channels do not change
during each symbol transmission (it can change from one symbol
transmission to the next one) and the channel phase $\phi_m$ of
$m^{th}$ user is unknown for all $m=1,2,\cdots,M$ (see
\cite{cohpaper} for coherent transmission). According to the above
assumptions the received signal is
\begin{equation}
\label{e1} r(n)=\sum\limits_{m=1}^{M}\alpha_m
e^{j\phi_m}p_m(n)+v(n),~~~~n=1,2,\cdots,N,
\end{equation}
where $v(n)$ is the additive white Gaussian noise with zero mean and
variance $\sigma^2$. Multistage parallel interference cancelation
method uses $\alpha^{s-1}_1,\alpha^{s-1}_2,\cdots,\alpha^{s-1}_M$,
the bit estimates outputs of the previous stage, $s-1$, to estimate
the related MAI of each user. It then subtracts it from the received
signal $r(n)$ and makes a new decision on each user variable
individually to make a new variable set
$\alpha^{s}_1,\alpha^{s}_2,\cdots,\alpha^{s}_M$ for the current
stage $s$. Usually the variable set of the first stage (stage $0$)
is the output of a conventional detector. The output of the last
stage is considered as the final estimate of transmitted bits. In
the following we explain the structure of a modified version of the
PLMS-PIC method \cite{cohpaper} with simultaneous capability of
estimating the cancelation weights and the channel phases.

Assume $\alpha_m^{(s-1)}\in\{-1,1\}$ is a given estimate of
$\alpha_m$ from stage $s-1$. Define
\begin{equation}
\label{e6} w^s_{m}=\frac{\alpha_m}{\alpha_m^{(s-1)}}e^{j\phi_m}.
\end{equation}
From (\ref{e1}) and (\ref{e6}) we have
\begin{equation}
\label{e7} r(n)=\sum\limits_{m=1}^{M}w^s_m\alpha^{(s-1)}_m
p_m(n)+v(n).
\end{equation}
Define
\begin{subequations}
\begin{eqnarray}
\label{e8} W^s&=&[w^s_{1},w^s_{2},\cdots,w^s_{M}]^T,\\
\label{e9}
\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!X^{s}(n)\!\!\!&=&\!\!\![\alpha^{(s-1)}_1p_1(n),\alpha^{(s-1)}_2p_2(n),\cdots,\alpha^{(s-1)}_Mp_M(n)]^T.
\end{eqnarray}
\end{subequations}
where $T$ stands for transposition. From equations (\ref{e7}),
(\ref{e8}) and (\ref{e9}), we have
\begin{equation}
\label{e10} r(n)=W^{s^T}X^{s}(n)+v(n).
\end{equation}
Given the observations $\{r(n),X^{s}(n)\}^{N}_{n=1}$, in modified
PLMS-PPIC, like the PLMS-PPIC \cite{cohpaper}, a set of NLMS
adaptive algorithm are used to compute
\begin{equation}
\label{te1} W^{s}(N)=[w^{s}_1(N),w^{s}_2(N),\cdots,w^{s}_M(N)]^T,
\end{equation}
which is an estimate of $W^s$ after iteration $N$. To do so, from
(\ref{e6}), we have
\begin{equation}
\label{e13} |w^s_{m}|=1 ~~~m=1,2,\cdots,M,
\end{equation}
which is equivalent to
\begin{equation}
\label{e14} \sum\limits_{m=1}^{M}||w^s_{m}|-1|=0.
\end{equation}
We divide $\Psi=\left(0,1-\sqrt{\frac{M-1}{M}}\right]$, a sharp
range for $\mu$ (the step-size of the NLMS algorithm) given in
\cite{sg2005}, into $L$ subintervals and consider $L$ individual
step-sizes $\Theta=\{\mu_1,\mu_2,\cdots,\mu_L\}$, where
$\mu_1=\frac{1-\sqrt{\frac{M-1}{M}}}{L}, \mu_2=2\mu_1,\cdots$, and
$\mu_L=L\mu_1$. In each stage, $L$ individual NLMS algorithms are
executed ($\mu_l$ is the step-size of the $l^{th}$ algorithm). In
stage $s$ and at iteration $n$, if
$W^{s}_k(n)=[w^s_{1,k},\cdots,w^s_{M,k}]^T$, the parameter estimate
of the $k^{\rm th}$ algorithm, minimizes our criteria, then it is
considered as the parameter estimate at time iteration $n$. In other
words if the next equation holds
\begin{equation}
\label{e17} W^s_k(n)=\arg\min\limits_{W^s_l(n)\in I_{W^s}
}\left\{\sum\limits_{m=1}^{M}||w^s_{m,l}(n)|-1|\right\},
\end{equation}
where $W^{s}_l(n)=W^{s}(n-1)+\mu_l \frac{X^s(n)}{\|X^s(n)\|^2}e(n),
~~~ l=1,2,\cdots,k,\cdots,L-1,L$ and
$I_{W^s}=\{W^s_1(n),\cdots,W^s_L(n)\}$, then we have
$W^s(n)=W^s_k(n)$, and therefore all other algorithms replace their
weight estimate by $W^{s}_k(n)$. At time instant $n=N$, this
procedure gives $W^s(N)$, the final estimate of $W^s$, as the true
parameter of stage $s$.

Now consider $R=(0,2\pi)$ and divide it into four equal parts
$R_1=(0,\frac{\pi}{2})$, $R_2=(\frac{\pi}{2},\pi)$,
$R_3=(\pi,\frac{3\pi}{2})$ and $R_4=(\frac{3\pi}{2},2\pi)$. The
partial information of channel phases (given by the receiver) is in
a way that it shows each $\phi_m$ ($m=1,2,\cdots,M$) belongs to
which one of the four quarters $R_i,~i=1,2,3,4$. Assume
$W^{s}(N)=[w^{s}_1(N),w^{s}_2(N),\cdots,w^{s}_M(N)]^T$ is the weight
estimate of the modified algorithm PLMS-PPIC at time instant $N$ of
the stage $s$. From equation (\ref{e6}) we have
\begin{equation}
\label{tt3}
\phi_m=\angle({\frac{\alpha^{(s-1)}_m}{\alpha_m}w^s_m}).
\end{equation}
We estimate $\phi_m$ by $\hat{\phi}^s_m$, where
\begin{equation}
\label{ee3}
\hat{\phi}^s_m=\angle{(\frac{\alpha^{(s-1)}_m}{\alpha_m}w^s_m(N))}.
\end{equation}
Because $\frac{\alpha^{(s-1)}_m}{\alpha_m}=1$ or $-1$, we have
\begin{eqnarray}
\hat{\phi}^s_m=\left\{\begin{array}{ll} \angle{w^s_m(N)} &
\mbox{if}~
\frac{\alpha^{(s-1)}_m}{\alpha_m}=1\\
\pm\pi+\angle{w^s_m(N)} & \mbox{if}~
\frac{\alpha^{(s-1)}_m}{\alpha_m}=-1\end{array}\right.
\end{eqnarray}
Hence $\hat{\phi}^s_m\in P^s=\{\angle{w^s_m(N)},
\angle{w^s_m(N)+\pi, \angle{w^s_m(N)}-\pi}\}$. If $w^s_m(N)$
sufficiently converges to its true value $w^s_m$, the same region
for $\hat{\phi}^s_m$ and $\phi_m$ is expected. In this case only one
of the three members of $P^s$ has the same region as $\phi_m$. For
example if $\phi_m \in (0,\frac{\pi}{2})$, then $\hat{\phi}^s_m \in
(0,\frac{\pi}{2})$ and therefore only $\angle{w^s_m(N)}$ or
$\angle{w^s_m(N)}+\pi$ or $\angle{w^s_m(N)}-\pi$ belongs to
$(0,\frac{\pi}{2})$. If, for example, $\angle{w^s_m(N)}+\pi$ is such
a member between all three members of $P^s$, it is the best
candidate for phase estimation. In other words,
\[\phi_m\approx\hat{\phi}^s_m=\angle{w^s_m(N)}+\pi.\]
We admit that when there is a member of $P^s$ in the quarter of
$\phi_m$, then $w^s_m(N)$ converges. What would happen when non of
the members of $P^s$ has the same quarter as $\phi_m$? This
situation will happen when the absolute difference between $\angle
w^s_m(N)$ and $\phi_m$ is greater than $\pi$. It means that
$w^s_m(N)$ has not converged yet. In this case where we can not
count on $w^s_m(N)$, the expected value is the optimum choice for
the channel phase estimation, e.g. if $\phi_m \in (0,\frac{\pi}{2})$
then $\frac{\pi}{4}$ is the estimation of the channel phase
$\phi_m$, or if $\phi_m \in (\frac{\pi}{2},\pi)$ then
$\frac{3\pi}{4}$ is the estimation of the channel phase $\phi_m$.
The results of the above discussion are summarized in the next
equation
\begin{eqnarray}
\nonumber \hat{\phi}^s_m = \left\{\begin{array}{llll} \angle
{w^s_m(N)} & \mbox{if}~
\angle{w^s_m(N)}, \phi_m\in R_i,~~i=1,2,3,4\\
\angle{w^s_m(N)}+\pi & \mbox{if}~ \angle{w^s_m(N)}+\pi, \phi_m\in
R_i,~~i=1,2,3,4\\
\angle{w^n_m(N)}-\pi & \mbox{if}~ \angle{w^s_m(N)}-\pi, \phi_m\in
R_i,~~i=1,2,3,4\\
\frac{(i-1)\pi+i\pi}{4} & \mbox{if}~ \phi_m\in
R_i,~~\angle{w^s_m(N)},\angle
{w^s_m(N)}\pm\pi\notin R_i,~~i=1,2,3,4.\\
\end{array}\right.
\end{eqnarray}
Having an estimation of the channel phases, the rest of the proposed
method is given by estimating $\alpha^{s}_m$ as follows:
\begin{equation}
\label{tt4}
\alpha^{s}_m=\mbox{sign}\left\{\mbox{real}\left\{\sum\limits_{n=1}^{N}
q^s_m(n)e^{-j\hat{\phi}^s_m}p_m(n)\right\}\right\},
\end{equation}
where
\begin{equation} \label{tt5}
q^{s}_{m}(n)=r(n)-\sum\limits_{m^{'}=1,m^{'}\ne
m}^{M}w^{s}_{m^{'}}(N)\alpha^{(s-1)}_{m^{'}} p_{m^{'}}(n).
\end{equation}
The inputs of the first stage $\{\alpha^{0}_m\}_{m=1}^M$ (needed for
computing $X^1(n)$) are given by
\begin{equation}
\label{qte5}
\alpha^{0}_m=\mbox{sign}\left\{\mbox{real}\left\{\sum\limits_{n=1}^{N}
r(n)e^{-j\hat{\phi}^0_m}p_m(n)\right\}\right\}.
\end{equation}
Assuming $\phi_m\in R_i$, then
\begin{equation}
\label{qqpp} \hat{\phi}^0_m =\frac{(i-1)\pi+i\pi}{4}.
\end{equation}
Table \ref{tab4} shows the structure of the modified PLMS-PPIC
method. It is to be notified that
\begin{itemize}
\item Equation (\ref{qte5}) shows the conventional bit detection
method when the receiver only knows the quarter of channel phase in
$(0,2\pi)$. \item With $L=1$ (i.e. only one NLMS algorithm), the
modified PLMS-PPIC can be thought as a modified version of the
LMS-PPIC method.
\end{itemize}

In the following section some examples are given to illustrate the
effectiveness of the proposed method.

\section{Simulations}\label{S5}

In this section we have considered some simulation examples.
Examples \ref{ex2}-\ref{ex4} compare the conventional, the modified
LMS-PPIC and the modified PLMS-PPIC methods in three cases: balanced
channels, unbalanced channels and time varying channels. In all
examples, the receivers have only the quarter of each channel phase.
Example \ref{ex2} is given to compare the modified LMS-PPIC and the
PLMS-PPIC in the case of balanced channels.

\begin{example}{\it Balanced channels}:
\label{ex2}
\begin{table}
\caption{Channel phase estimate of the first user (example
\ref{ex2})} \label{tabex5} \centerline{{
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{6}{*}{\rotatebox{90}{$\phi_m=\frac{3\pi}{8},M=15~~$}} & N(Iteration) & Stage Number& NLMS & PNLMS  \\
&&&&\\
\cline{2-5} & \multirow{2}{*}{64}& s = 2 &  $\hat{\phi}^s_m=\frac{3.24\pi}{8}$ & $\hat{\phi}^s_m=\frac{3.18\pi}{8}$ \\
\cline{3-5} & & s = 3 & $\hat{\phi}^s_m=\frac{3.24\pi}{8}$ & $\hat{\phi}^s_m=\frac{3.18\pi}{8}$ \\
\cline{2-5} & \multirow{2}{*}{256}& s = 2 &  $\hat{\phi}^s_m=\frac{2.85\pi}{8}$ & $\hat{\phi}^s_m=\frac{2.88\pi}{8}$ \\
\cline{3-5} & & s = 3 & $\hat{\phi}^s_m=\frac{2.85\pi}{8}$ & $\hat{\phi}^s_m=\frac{2.88\pi}{8}$ \\
\cline{2-5} \hline
\end{tabular} }}
\end{table}
Consider the system model (\ref{e7}) in which $M$ users
synchronously send their bits to the receiver through their
channels. It is assumed that each user's information consists of
codes of length $N$. It is also assumd that the signal to noise
ratio (SNR) is 0dB. In this example there is no power-unbalanced or
channel loss is assumed. The step-size of the NLMS algorithm in
modified LMS-PPIC method is $\mu=0.1(1-\sqrt{\frac{M-1}{M}})$ and
the set of step-sizes of the parallel NLMS algorithms in modified
PLMS-PPIC method are
$\Theta=\{0.01,0.05,0.1,0.2,\cdots,1\}(1-\sqrt{\frac{M-1}{M}})$,
i.e. $\mu_1=0.01(1-\sqrt{\frac{M-1}{M}}),\cdots,
\mu_4=0.2(1-\sqrt{\frac{M-1}{M}}),\cdots,
\mu_{12}=(1-\sqrt{\frac{M-1}{M}})$. Figure~\ref{Figexp1NonCoh}
illustrates the bit error rate (BER) for the case of two stages and
for $N=64$ and $N=256$. Simulations also show that there is no
remarkable difference between results in two stage and three stage
scenarios. Table~\ref{tabex5} compares the average channel phase
estimate of the first user in each stage and over $10$ runs of
modified LMS-PPIC and PLMS-PPIC, when the the number of users is
$M=15$.
\end{example}

Although LMS-PPIC and PLMS-PPIC, as well as their modified versions,
are structured based on the assumption of no near-far problem
(examples \ref{ex3} and \ref{ex4}), these methods and especially the
second one have remarkable performance in the cases of unbalanced
and/or time varying channels.

\begin{example}{\it Unbalanced channels}:
\label{ex3}
\begin{table}
\caption{Channel phase estimate of the first user (example
\ref{ex3})} \label{tabex6} \centerline{{
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow{6}{*}{\rotatebox{90}{$\phi_m=\frac{3\pi}{8},M=15~~$}} & N(Iteration) & Stage Number& NLMS & PNLMS  \\
&&&&\\
\cline{2-5} & \multirow{2}{*}{64}& s=2 &  $\hat{\phi}^s_m=\frac{2.45\pi}{8}$ & $\hat{\phi}^s_m=\frac{2.36\pi}{8}$ \\
\cline{3-5} & & s=3 & $\hat{\phi}^s_m=\frac{2.71\pi}{8}$ & $\hat{\phi}^s_m=\frac{2.80\pi}{8}$ \\
\cline{2-5} & \multirow{2}{*}{256}& s=2 &  $\hat{\phi}^s_m=\frac{3.09\pi}{8}$ & $\hat{\phi}^s_m=\frac{2.86\pi}{8}$ \\
\cline{3-5} & & s=3 & $\hat{\phi}^s_m=\frac{2.93\pi}{8}$ & $\hat{\phi}^s_m=\frac{3.01\pi}{8}$ \\
\cline{2-5} \hline
\end{tabular} }}
\end{table}
Consider example \ref{ex2} with power unbalanced and/or channel loss
in transmission system, i.e. the true model at stage $s$ is
\begin{equation}
\label{ve7} r(n)=\sum\limits_{m=1}^{M}\beta_m
w^s_m\alpha^{(s-1)}_m c_m(n)+v(n),
\end{equation}
where $0<\beta_m\leq 1$ for all $1\leq m \leq M$. Both the LMS-PPIC
and the PLMS-PPIC methods assume the model (\ref{e7}), and their
estimations are based on observations $\{r(n),X^s(n)\}$, instead of
$\{r(n),\mathbf{G}X^s(n)\}$, where the channel gain matrix is
$\mathbf{G}=\mbox{diag}(\beta_1,\beta_2,\cdots,\beta_m)$. In this
case we repeat example \ref{ex2}. We randomly get each element of
$G$ from $[0,0.3]$. Figure~\ref{Figexp2NonCoh} illustrates the BER
versus the number of users. Table~\ref{tabex6} compares the channel
phase estimate of the first user in each stage and over $10$ runs of
modified LMS-PPIC and modified PLMS-PPIC for $M=15$.
\end{example}

\begin{example}
\label{ex4} {\it Time varying channels}: Consider example \ref{ex2}
with time varying Rayleigh fading channels. In this case we assume
the maximum Doppler shift of $40$HZ, the three-tap
frequency-selective channel with delay vector of $\{2\times
10^{-6},2.5\times 10^{-6},3\times 10^{-6}\}$sec and gain vector of
$\{-5,-3,-10\}$dB. Figure~\ref{Figexp3NonCoh} shows the average BER
over all users versus $M$ and using two stages.
\end{example}


\section{Conclusion}\label{S6}

In this paper, parallel interference cancelation using adaptive
multistage structure and employing a set of NLMS algorithms with
different step-sizes is proposed, when just the quarter of the
channel phase of each user is known. In fact, the algorithm has been
proposed for coherent transmission with full information on channel
phases in \cite{cohpaper}. This paper is a modification on the
previously proposed algorithm. Simulation results show that the new
method has a remarkable performance for different scenarios
including Rayleigh fading channels even if the channel is
unbalanced.

",['The normalized least mean square (NLMS) algorithm.'],2008,multifieldqa_en,en,,49d0334d26929f712de2cba070a74752f415313c08166f80," The PLMS-PPIC method is a multiuser detector for code division multiple access (CDMA)receivers which reduces the effect of MAI in bit detection. In this method, a weighted value of the MAI of other users issubtracted before making the decision for a specific user. The method uses a set of NLMS algorithms with different step-sizes instead of one NLMS algorithm used in LMS- PPIC. It then makes a new decision on each user on each new variable and outputs a new bit estimate on each variable. The results of the proposed method are shown in the next section of the paper. The paper is published by Springer, a division of Springer Group, and is available for download from the company's website and on the Springer.com website. It is available in English, German, French, Spanish, Italian, Portuguese, and the German version of this paper is available from the Springer website and the English version of the French version is available on the company’s website. For more information, visit Springer.co.uk or the German website. The German version is also available from Spiegel.com or the French website. Back to the page you came from. Click here to read the full article and the link to the Dutch version of it will be available in the coming days. The English version will be published in the week of November 14th, 2014. The Dutch version will appear on the 27th November, 2014, and will be released in the UK on the 28th November 2014, respectively. The UK version will also be released on the 29th November 2013, but the US release will be delayed until the 30th of that year until the 31st November 2014. For the UK version, the release date has been delayed until after the 27 December 2014, when the UK release date will be announced. The U.S. release date for the German release is the 28 December 2014. Click the link for the U.K. release. The American release date is the 29 December 2014,. and the European release date of the 29 September 2014, both of which have been published by the European Union (EU) and the Czech Republic (Czech Republic). The Czech Republic is the only country in which the EU has allowed the use of this method in its CDMA system. The Czech version has been approved by the EU for use in CDMA systems. The Polish version, however, has not been approved for use by the Czech government. The reason for this is that the Czech version is not compatible with the European version of CDMA, which uses a different algorithm for bit detection, which is used in other countries. The US version uses the same algorithm, but with a different step size, so the difference between the two methods is not significant. The European version uses an algorithm that is used for both the Polish version and the Russian version. It also uses a method that is based on the least mean square (NLMS) algorithm. In both of these methods, the weight vector in each stage is the true transmitted binary value of each user divided by its hard estimate value from the previous stage. This is called the $m^{\rm th}$ element of the weight vectors in the stage. In each iteration, the parameter estimate of theNLMS algorithm is chosen whose element magnitudes of cancelation weight estimate have the best match with unity. The $m^{th}$ user has its own code $p_m(.)$ of length $N$, where $p-m(n)(n)\in \{-1,1\}$ for all $n$. It means that for each symbol $N$ bits are transmitted by each user and the processing gain is equal to $N$. timating the cancelation weights and the channel phases. In each stage, the NLMS algorithms areexecuted ($L$ is the step-size of the L$ subintervals) In other words, if the algorithm minimizes our criteria, then it is considered as the parameter estimate at time $n$. In otherwords if the next word in the equation holds at iteration $n, then the algorithm holds as the estimate $k$ of the parameters $T$ and $m$ at time$n. In modified versions of PLMS-PPIC, a set of NLMS.adaptive algorithm are used to compute the algorithm. For example, in modified versions, the algorithm can be used to calculate $W^s$ after iteration $N$. To do so, from the equations (e7) and (e8) we have: From equations (\ref{e7), we have (e9), and from (e10) we are able to compute $W$ after iterations $N, $T, $L, $M, $S, $V, $N and $L$ for $W, $m, $J, $A, $B, $C, $D, $E, $F, $G, $H, $I, $K, $P, $R, $U, $Z, $W and $M$ are used in the algorithm's equations. For instance, we have the equation (e6) which states: From the observations $X^{s}(n) and $N$ we can say that the algorithm is able to find $W$, $T$, $L' and $V' for $T', $M', $L', $N', $S' and ""M"" in $N"" and ""L"" in ""N"" . For the rest of the article, we will use the equation from ( e7) as well as (e4) for our analysis of the algorithm in this article. We will also use (e5) in the next section to understand how the algorithm works in the context of the PLMSPPIC algorithm, which is based on a modified version of PlMS- PPIC. We'll also use the same equations in the final section of this article to explain how the algorithms work in terms of the PIC algorithm. We have: (e1) R(n), (e2) W, (e3) X, (E4) X,. (E5), (E6) X and (E7) X. (E8) X , (E9) X . (E1) X & (E2) X + (E3) x, (-E4), (EF1) x,. (EF2) N, (EF3) N , (EF4) N & (EF5) N and (EF6) N + (EF7) N = W, W, S, N, N & N, n, N + 1, N - N, 1, 2, 3, 4, 5, 6, 6,. (N) & (N, N) & N + 2, 2,. (F7) & 2, 4. (F8) N. (N-1) N-1 N-2 N-3 N-4 N-5 N-6 N-7 N-8 N-9 N-10 N-12 N-11 N-13 N-14 N-15 N-16 N-17 N-18 N-19 N-20 N-22 N-23 N-24 N-21 N-28 N-26 N-27 N-32 N-29 N-34 N-31 N-38 N-33 N-36 N-37 N-42 N-47 N-48 N-43 N-44 N-46 N-53 N-54 N-52 N-45 N We admit that when there is a member of $P^s$ in the quarter of $phi_m$ then $w^s_m(N)$ converges. In this case, the expected value is the optimum choice for channel phase estimation. The results of the above discussion are summarized in the nextequation. The first stage of the first stage is the estimation of the channel phase. The rest of the proposed method is given by estimating the phase of $alpha$ as follows:. The second stage is called the second stage and it is based on the third stage. The third stage is known as the fourth stage and is the third and final stage. This stage is used to estimate the fourth and fifth phases of the fourth phase of the fifth stage. For this stage, we use the following method:. We estimate the third phase as the fifth phase by adding the two fourths of the sixth and seventh stages together. We then estimate the final phase by multiplying these by the second and third stages. The result is the sixth stage, which is the seventh phase. We conclude that this method is the best way to estimate channel phases of $N$ and $M$ in terms of the number of members in $P*$ that are in the same region as $phi#. We also use the seventh stage, the eighth stage, in which we estimate the phase phase by using the same method as the previous one, but with the addition of the seventh and eighth stages. We end the discussion with the last stage, called the ninth stage, where we estimate channel phase $N_i$ and the eighth phase is the ninth phase, or the tenth phase, depending on the order in which the first and second stages are included in $N  and the ninth stage. It is also possible to use the tenth stage to estimate phase phase $S_i#, the tenth and final phase, and the tenth stage, if the first two stages are the same as each other. The final stage is then called the eleventh stage, and it consists of the ninth and tenth stages of $S m_i, the sixth stage and the ninth seventh seventeenth segment, respectively. This is the stage where the phase is estimated using the following methods:. First, we look at how the phase phases are related to each other:. Second, we consider how the channel phases interact with each other and how they are related. Third, we see that the channel phases are related to one another in a way that allows us to estimate them to be more or less accurate than the previous phase. Fourth, we look at how they interact with the other physics in this stage. We then examine the channel phase and refer to the next stage in which we expect the phase to take place. This step is the next stage, known as ""the eighth stage"" or ""the seventh stage"" of the process. The next stage is the final stage, when we arguably estimate the channelphase of $N# and $S#. This involves the estimation of the channel phrase in terms of $s m i, s n, m, n i s, N s and n_i i. The final stage is called ""the ninth stage"" and it involves the estimation  of the channels of $N m and $s-m in N. Consider the system model in which $M$ users send their bits to the receiver. It is assumed that each user's information consists ofcodes of length $N$. It is also assumed that the signal to noise (SNR) is 0dB. In this example there is no power-unbalanced or                channel loss is assumed. The step-size of the NLMS algorithm in a modified LMS-PPIC method is $0.1(1-\sqrt{\frac{M-1}{M)$ and $1 (1-1/2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/24/26/27/28/29/30/31/32/28-28/31-32/33/34/36/37/38/39/40/41/42/43/44/45/46/47/48/49/50/51/52/53/54/56/57/58/59/60/61/62/63/64/65/66/67/68/69/70/72/73/74/75/76/78/79/80/82/83/84/85/86/87/88/89/90/91/92/93/94/95/96/97/98/99/100/102/104/103/104, and so on. The set of step-sizes of the parallel NLMS algorithms in modifiedPLMS- PPIC method are. $Theta=\{0.01,0.05, 0.1,’cdots,1’,”Theta’s”, “Theta.”,.””.’’. “”;” ‘’;’ ”’:’ Theta”.,’ theta. ’‘”:. ”, ”.  ’,. ”,. ’: ”; ’; ”: ’, ’. ; . •’+.’%.’!”+.”%.”!’%’%,”%”%;’%;”#’#”%,’?’}.’%:’&”&’+’$’A’.;’}’!,’@’/’/.’%-’(’a’)’).’!.’()’!:’();’);’ol’=’true’.,”?”}.”%-”True’?,’|’?:’].’;}’ & ’# ’% ’%, ‘% ‘#‘%‘%, ’%. ’,.’.: ’+. ’! ’%:.‘%. ! ,. ‘!�’?!’ !!’ (#’), ’&. ‘%. ‘;‘.‚’)!’ , ‘.%. ”% ”&.‡’$.’ (’Theta-”). ’);. ‚”),’ . ’+. ‡” & ‘+”$”-.’ {’m’ }’});’ (.’)(’M’ });’ er, parallel interference cancelation using adaptive multistage structure and employing a set of NLMS algorithms. Simulation results show that the new method has a remarkable performance for different scenarios including Rayleigh fading channels. In fact, the algorithm has been proposed for coherent transmission with full information on channelipientphases in \cite{cohpaper}. This paper is a modification on the                previously proposed algorithm. It is proposed to work when just a quarter of the                channel phase of each user is known."
Who compiled the 88-page letter to the HHS regarding vaccine safety?,"A special tribute to Del Bigtree (pictured) and his team at ICAN for his stunning 88 page letter to the HHS regarding vaccine safety. As Del reported - in the latest edition of Highwire - the letter, in response to an earlier reply from the then acting Director National Vaccine Program Office, Melinda Wharton, took virtually a year to compile, and is a meticulous piece of research. Most sensationally they researched the HHS claim through US government archives that at least some pediatric vaccines had been trialed against genuine placebo, and came to a negative conclusion. Not only that, they established that none of the vaccines those vaccines had been trialed against had ever been trialed against genuine placebo either. At the end of the line the toxic products were only being compared with other toxic products, rather than against saline.
Leave aside the sceptics, for any believer in the vaccine program as a necessary intervention in public health, this should be a devastating finding. Fundamentally, the research into the safety of any of the products before marketing was simply not there. The manufacturers apparently had no faith that their proto-products could withstand this scrutiny, and for the rest they just did not care: under the alleged imperative of protecting the population it seems anything went. So even before all the sham monitoring procedures and reviews which Del and his team dismantle in forensic detail we are left with the proposition that none of the present products being given to US children – and frequently other children across most of the developed world – have any meaningful pre-marketing safety data all. If you are believer in the program you have been let down: if you wanted a program with any pretensions to safety - supposing such a thing to be possible - it looks like you would have to start from scratch. The manufacturers did this: the governments, the politicians and the regulators (internationally) let it happen.
This damning document is published simultaneously with a demand in the UK from the Royal Society for Public Health (which I had never heard of) to shut down comment about vaccines on the web. It echoes calls from Seth Berkley of GAVI, Heidi Larson of the Vaccine Confidence Project and the European Parliament. The pamphlet airily dismisses concerns that vaccines have side effects or that you could possibly have too many. It is pure public relations, and if the RSPH claims to be ""independent"" it also admits that the publication was paid for by Merck, a detail which was reported by British Medical Journal and the Guardian, but not true to form by the BBC. We have, in truth, been building to this moment for two decades: as the evidence piles up that every single aspect of the program lacks integrity or is simply rotten to the core all the perpetrators can do is call for the silencing of their critics, and maintain the products are safe because they say so.
Please help give the ICAN letter the widest possible distribution, particularly to politicians.
""The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system.""
Nope. This makes no sense. Lots of people who seemed vibrant will get a very severe case of the same illness that a vulnerable baby overcomes in a day.
And under the germ theory it doesn't matter how strong your immune system *was*. Once it's been overcome by the pathogen it is every bit as weak as anybody else's with that pathogen.
What you say makes no sense. There's no reason for me to reply to you again.
""Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared?""
Why do you keep asking this question when I've already provided the answer hundreds of times? Why are you so desperate to believe the people who you already recognize are harming our children?
Why would Walter Reed be any more trustworthy than Paul Offit or Senator Pan? Why would Jenner or Pasteur?
And you went no way to explaining my arguments against germ theory. If we are attacked by billions of viruses every day then if even a tiny fraction of them are pathogenic then we couldn't possibly survive. And even if we could, we would already be immune rendering every vaccine pointless. Once we had survived our first few days on earth, then we could never get sick again.
If that's wrong then we must conclude that precisely 0% of germs are pathogenic.
Plus your comment about the immune system completely misunderstood my point. The immune system does not allow us to overcome our math problem. In fact, it makes it worse.
You did provide one solitary example of a patient with what are presumably yellow fever symptoms but you didn't say whether they had been given any toxic medical treatments.
And like I said before, the whole ""incubation period"" is more than a little suspicious. Clearly they never found what they thought they would and just rigged the results to tell them what they want to hear.
Like every other germ theorist/vaccine promoter in history.
Many kinds of bacteria are constantly evolving and changing, like flu viruses. Others are more stable over time, like the yellow fever virus. Those that change develop new ways of infiltrating the cells of the organism being attacked (from our point of view, from its unconscious point of view, it's just carrying out its need to replicate, which it can only do inside the cells of its host). The changes which allow it to better infiltrate are more successful and result in more viruses with those traits.
Our immune system is designed to detect and destroy potentially dangerous invading pathogens. Many bacteria are usually harmless and absolutely necessary. The minority are dangerous, and most people's immune systems do a good job of analyzing them and killing them, often with no signs of disease. Others experience a clinical infection, and the immune system usually mounts a successful attack on them.
The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system. Vaccines are usually effective in giving immunity to the targeted diseases. They also have many dangers which everyone should be aware of, and vaccines should be avoided whenever possible. But in the case of the most dangerous diseases, everyone should learn about them and think about what he wants to do to protect himself and his children from them, considering all the factors involved. And no one can have 100% certainty that he has made the right decision, but that's life. But if you live in the Congo and many people around you are currently dying of yellow fever, then that means that you yourself are at risk of being bitten by a loaded mosquito and getting, often dying, of yellow fever. The yellow fever vaccine is very effective at preventing yellow fever. From there, each person must make a choice.
At the end of this stage there is a remission of two or three days. About 80% of those with clinical disease recover at this point, with permanent immunity. The other 20% enter the toxic stage, with a return of the fever, black vomit (coffee-ground emesis), diarrhea, a slowing of the pulse (Faget's sign), jaundice, yellow eyes, yellow skin, and failure of the kidneys, liver, and heart. The patient gets a strange hiccup (like with Ebola, a related disease), falls into a coma, and dies. About half of those patients who enter the toxic stage dies, even now, even with the best of hospital care. The Faget's sign can also occur at the end of the first stage.
You asked specifically about the symptoms of the Americans on Dr. Reed's team who got yellow fever in Cuba in 1900. I'll give the passage from The American Plague (162-5), which describes the course of Jesse Lazear's illness. ""In his logbook, Lazear wrote an unusual entry on September 13. In all cases before those, page after page of records, Lazear had used the soldier's name and simply the date he was bitten, with no other attention to the mosquito. A one-line entry with a name and a date. On that day, however, in his elegant hand, Lazear did not write the soldier's name, but instead wrote 'Guinea Pig No. 1.' He went on to write that this guinea pig had been bitten by a mosquito that developed from an egg laid by a mosquito that developed from an egg laid by a mosquito that fed on a number of yellow fever cases: Suarez, Hernández, De Long, Ferández. It was a precise, detailed history that proved beyond doubt that the mosquito was loaded with the virus when it bit a healthy soldier...(If he had entered his name, then his death would have been considered medical suicide by the insurance company, and his wife and two children would not have gotten any payment.) For the next few days, Lazear's life continued much as it had over the last few months in Cuba. He fed and cared for the mosquitoes in the lab. ..Then he began to lose his appetite. He skipped a few meals in the mess hall. He didn't mention it to anyone, nor did he ask to see one of the yellow fever doctors; instead, he worked hard in the lab trying to ignore the oncoming headache.
""On September 18, he complained of feeling 'out of sorts,' and stayed in his officer's quarters. His head pounded and L. decided to write a letter. ..(he wrote to his mother, and referred to his one-year old son Houston and the baby his wife Mabel was about to have: they were staying with his mother in the US). ..That night, L. started to feel chilled as the fever came on. He never went to sleep but worked at his desk all through the night, trying to get all the information about the mosquitoes organized. By morning, he showed all the signs of a severe attack of yellow fever. The camp doctors made the diagnosis, and L. agreed to go to the yellow fever ward. ..L. was carried by litter out of the two-room, white pine board house in which he had lived since he and Mabel first arrived in Cuba. ..(In the yellow fever ward, in a separate one-room building), Lena Warner (the immune nurse who had survived the yellow fever in 1878, when she was nine, and was found in her boarded-up house by a former slave who first thought she was dead, and carried her to safety) nursed J.L., recording his vitals. (I put up a link to his case record and vital signs last week. The surgeon general required that this record be made for every yellow fever patient.)... (On September 25,) Lena Warner braced L's arms with all of her weight, shouting for help. Still he bolted from the bed, darting around the small frame-wood room as wildly as a trapped insect beating against glass. Two soldiers ran into the ward, pinning L to his bed, tying restraints around his wrists and elbows. ..Warner sponged his body with iced whiskey and water. She recorded his temperature, which had held at 104 degrees for days, on the chart beside his bed. ..(Warner watched him sleep.) But the quiet did not last. L's body began to lurch, and black vomit rolled from his mouth; through the bar hanging above his hospital cot. He writhed in the bed, and his skin grew deep yellow. His 104 temperature slowly fell, leveling out 99 degrees, and JL died at 8:45 p.m. at the age of thirty-four.""
As is obvious, there are many problems with vaccines. But, that being said, most of them usually work for a period of time to prevent the targeted diseases. The basic science behind vaccines is correct. Why do you think that within a few years (not many) of the introduction of the vaccines for them, pertussis, measles, mumps, rubella, tetanus, diphtheria, Hib disease, and chickenpox (and others) almost entirely disappeared? In the case of the routine childhood diseases, this was a bad thing, but it is a true thing.
Vaccines usually don't cause any obvious reactions. While they usually prevent the diseases, and that's why people continue to get them. With the increasing vaccination schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk.
Your article said as though it had any probative value that 90% of those who get pertussis had been vaxxed. The old DPT vaccine was MUCH more effective at preventing pertussis, but it was so dangerous (again, not to most, but to many), that developed countries replaced it with the acellular version, DTaP. From the beginning about twenty years ago, it was clear that it was not very effective and that huge numbers of vaxxed people got pertussis anyway, including my daughter who got pertussis at eight month old after having gotten three DTaPs. The pertussis vaccine continues to be very dangerous, and I do not recommend that anyone get it. It used to be a killer disease, but evolved to become much milder, to the extent that the disease is very rarely dangerous (usually only to newborns under three months old), while the vaccine is very dangerous. And they're trying to see how they can go back to the old DPT. This does not show that vaccine science has collapsed, but rather that the vaccine they developed to replace the DPT turned out to be much less effective than they first thought, while continuing to be much more dangerous than they first thought.
Your article extrapolated from that that modern medical science in general has collapsed, but that, again, is going too far. A older woman in Mexico City who is like my mother to me had a pacemaker inserted about two months ago to aid her failing heart, and it has restored her to optimism and energy, when she was despondent, weak, and close to death. I took my daughter to the dentist yesterday, who said she has three wisdom teeth coming in and that she said that the lower right one was sore. So, although I am cautious about X-rays, I made an appointment for a panoramic X-ray in a month to assess the wisdom teeth, and, if it seems appropriate, I'll take her to an oral surgeon to have one or more extracted under IV sedation, in his office, if possible (the dentist thought that it would be). And I am confident that there will be no serious problems, but this is thanks to technology and training in modern medicine that haven't been available for that long.
I think that everyone should inform himself on all medical procedures before agreeing to anything, but I also think that he should have access to any medical procedure which is reasonable (and opinions can differ as to that).
One problem is that you have not said how you think people should protect themselves against tetanus, bacterial meningitis, and yellow fever in the relevant cases, for example. These are diseases which healthy, well-nourished people used to die from very readily.
If most people stopped vaxxing and the mortality from these diseases rose to something like pre-vaccine levels, do you think they should just accept dying from them?
I put that in a separate paragraph because it is the crucial issue.
balinaheuchter Air Traffic Control You Tube - Colin Campbell example of - How to ""Fudge a Nudge"" -""Deal"" or ""No Deal"" ""Not in a month of Sundays"" ""No exceptions/no compromise?"" -make a trade off -do an exception- everyone get's a good deal /good outcome!
Hans, you are right that we are looking at one of the biggest crimes in all history. When I read the story of that poor girl who was so healthy and is now confined to a wheelchair after getting her third Gardasil shot I could not believe that Merck could produce such a toxic vaccine and give it out to girls like it was something they absolutely had to have only to be mislead and made into cripples. Merck should be prosecuted for the damage they have done to so many girls who got the Gardasil vaccine and were physically debilitated for life. There is a place for the people who perpetrated this crime on young girls and women and it is called hell. They have destroyed people's lives and gotten away with it. My heart goes out to those who have suffered this damage for no damn good reason except to help make huge profits for Merck!
Here is the reason that the germ theory is nonsense.
1) Everyday we are bombarded with billions of germs. Presumably at least some of them are of the kind that germ theorists believe are dangerous (otherwise we would have to conclude that none of them are dangerous). So how do we survive?
2) Let's just say that we ignore 1 and imagine that, by way of magic, none of the billions of viruses we get bombarded with are pathogenic but all those that are are tucked away somewhere. Ok. But presumably they reside in sick people right? So where are there lots of sick people? Doctor offices and hospitals! So everybody must be dying the moment they enter these places right?
3) I love this one because I have never seen anybody else ever raise it. Under the germ theory there are no negative feedbacks. This makes a stable biological system by definition impossible. The immune system is *not* a negative feedback it is the opposite. It actually reinforces our math problem because the immune system will weaken as the number of pathogens increase.
There is no way of resolving this problem without a discontinuity. A Deus ex Machina as The Almighty Pill so beautifully put it. So the germ theory is quite literally, mathematically impossible.
There is as much chance of it being true as 2+2 = 5.
There are plenty of other massive problems with germ theory such as why did things like SARS and bird flu magically disappear? Why do we have the symptoms that we do? Is our body controlling the symptoms to help fight the germs and if so, why would suppressing the symptoms with antibiotics or Tamiflu be considered a good idea? If the virus is causing the symptoms then why would it cause these kinds of things?",['Del Bigtree and his team at ICAN.'],3150,multifieldqa_en,en,,751053416f74a11311a13e801634fff8fd48649d3921b368," Del Bigtree and his team at ICAN wrote 88 page letter to the HHS regarding vaccine safety. Letter, in response to an earlier reply from the then acting Director National Vaccine Program Office, Melinda Wharton, took virtually a year to compile. Most sensationally they researched the HHS claim through US government archives that at least some pediatric vaccines had been trialed against genuine placebo, and came to a negative conclusion. The manufacturers apparently had no faith that their proto-products could withstand this scrutiny, and for the rest they just did not care. We have, in truth, been building to this moment for two decades: as the evidence piles up that every single aspect of the program lacks integrity or is simply rotten to the core, all the perpetrators can do is call for the silencing of their critics, and maintain the products are safe because they say so. The pamphlet airily dismisses concerns that vaccines have side effects or that you could possibly have too many. It is pure public relations, and if the RSPH claims to be ""independent"" it also admits that the publication was paid for by Merck, a detail which was reported by British Medical Journal and the Guardian, but not true to form by the BBC. It echoes calls from Seth Berkley of GAVI, Heidi Larson of the Vaccine Confidence Project and the European Parliament. The letter is published simultaneously with a demand in the UK from the Royal Society for Public Health (which I had never heard of) to shut down comment about vaccines on the web. Please help give the ICAN letter the widest possible distribution, particularly to politicians. It has been published in the latest edition of Highwire, which is published every week on the 8th and 9th October. For more information on the Highwire series, visit www.highwire.co.uk/vaccine and follow us on Twitter @HighwireVaccine. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. In the UK, contact the National Institute of Health and Social Services on 0800-788-9090. For details on the National Institutes of Health, visit the National Institute for Health and Social Service on the 8th October and or the University of London on the 7th November on 8th November on 08th and 9th November. For information on how to get in touch with the NHS, see www.samaritans.org or  www.nhs.uk. For confidential support on suicide matters call the Samaritans. Vaccines are usually effective in giving immunity to the targeted diseases. They also have many dangers which everyone should be aware of, and vaccines should be avoided whenever possible. But in the case of the most dangerous diseases, everyone should learn about them and think about what he wants to do to protect himself and his children from them. If you live in the Congo and many people around you are currently dying of yellow fever, then that means that you yourself are at risk of being bitten by a loaded mosquito. The yellow fever vaccine is very effective at preventing yellow fever. From there, each person must make a choice about whether or not to get the vaccine. The outcome of disease always depends both on the virulence of the pathogen and the health of the individual immune system. The immune system is designed to detect and destroy potentially dangerous invading pathogens. Many bacteria are usually harmless and absolutely necessary. The minority are dangerous, and most people's immune systems do a good job of analyzing them and killing them, often with no signs of disease. About 80% of those with clinical disease recover at this point, with permanent immunity. The other 20% enter the toxic stage, with a return of the fever, black vomit (coffee-ground emesis), diarrhea, a slowing of the pulse (Faget's sign), jaundice, yellow eyes, yellow skin, and failure of the kidneys, liver, and heart. The patient gets a strange hiccup (like with Ebola, a related disease), falls into a coma, and dies. About half of those patients who enter the Toxic stage dies, even now, even with the best of hospital care. The Fagett's sign can also occur at the end of the first stage. In all cases before and after the vaccine, the patient has a remission of two or three days. In the end, the person who got yellow fever in Cuba in 1900. ""In his logbook, Lazear wrote an unusual entry on September 13. ""I'll give the passage from The American Plague (162-5), which describes the course of Jesse Lazear's illness. I'll give you the details of his illness,"" he writes. ""He wrote in his log book, ""I've been suffering from yellow fever for a few days, and I've been given a shot of the vaccine."" He wrote in the logbook: ""I'm fine, I'm fine. I've had a good day, I've got a good night, I'll have a good tomorrow."" He writes in the next few days he'll be fine, he says. He says in the last few days of his life he's had a bad day, he's been sick, he'll get better. He's been ill, he writes, and he's going to be fine. He goes on to say that he's having a great day, and then he's sick again. He writes: ""The immune system does not allow us to overcome our math problem. In fact, it makes it worse."" Lazear had used the soldier's name and simply the date he was bitten, with no other attention to the mosquito. If he had entered his name, then his death would have been considered medical suicide by the insurance company, and his wife and two children would not have gotten any payment. The basic science behind vaccines is correct. While they usually prevent the diseases, they don't cause obvious reactions. With increasing vaccininatin, people should continue to get them, and that's why people are still getting them, says Dr. David Frum, author of the book, ""Vaccines: The New Science of Vaccines and the Death of The Vaccine,"" which is published by Simon & Schuster at $24.99. For more information on the book and its author, visit www.vaccines.com. For the full interview with Dr. Frum on ""The Death of the Vaccinate,"" visit CNN.com/Heroes, or go to http://www.cnn.co/heroes/2013/09/26/science/death-of-the-vaccine-death-by-jennifer-frum-1.html. The full interview will be published on October 1, 2013, at 9 p.m. ET on CNN.co.uk/ Heroes, the death of thevaccine, will be broadcast on October 2, 2013 at 9:30 p.M. ET, and on October 3, 2014, at 10:30 a.m., 9:45 a.M., 10:50 a.C.E. and 10:55 a.E., respectively. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255 or visit http:// www.suicidepreventionlifeline.org/. For confidential. support on suicide matters call theNational Suicide Prevention Lifeline at 1-856-788-7255 or click here. For support in the United States, call the Salvation Army on 1 (800) 273-TALK (8255) or  visit http: http: www.sending.it/sending-a-compassionate-message or http://www.-sendinga.org/Sendinga/Suffering-A-Compassion-Lonely-Man-To-The-World-Abandoned-By-Your-Widow-and-Children-Awareness-Affected- By-His-Family-On-This-World.html/. For help in the UK, visit the Samaritans on 08457 9/11/13. For help with suicide matters, visit http:/www.samarsitans.com/. For support on the UK Samaritans, click here.  For help on suicide issues in the US, go to the  Samaritans' page on the website. For help on how to get involved in the fight against cancer, call the National Suicide Helpline on 1(800) 856-9255. on schedule, more and more are severely and permanently damaged, and it is immoral to mandate any vaccine for anyone for this reason. But it would also be immoral to prohibit vaccines for those who want them enough to take the risk. The pertussis vaccine continues to be very dangerous, and I do not recommend that anyone get it. Merck should be prosecuted for the damage they have done to so many girls who got the Gardasil vaccine and were physically debilitated for life. There is a place for the people who perpetrated this crime on young girls and women called Thriving Life, a place where life can be lived to its full potential and no one can be put in harm's way. The old DPT vaccine was MUCH more effective at preventing pertussi, but it was so dangerous (again, not to most, but to many) that developed countries replaced it with the acellular version, DTaP. This does not show that vaccine science has collapsed, but rather that the vaccine they developed to replace the DPT turned out to be much less effective than they first thought. If most people stopped vaxxing and the mortality from these diseases rose to something like pre-vaccine levels, do you think they should just accept dying from them? You Tube - Colin Campbell example of - How to ""Fudge a Nudge"" -""Deal"" or ""No Deal"" ""Not in a month of Sundays"" ""No exceptions/no compromise?"" -make a trade off -do an exception- everyone get's a good deal /good outcome!Everyone get's the good deal/good outcome- everyone gets the good outcome! The best way to make a deal is to ""fudge a nudge"" or to ""no-nudge"" a deal, which is a way to get people to agree to a deal that doesn't have to be agreed on by all parties at the end of the deal. The best place to start is with the terms ""no deal"" and ""no compromise,"" which can be very different from ""no exception"" or a compromise, depending on the terms of the ""no agreement"" you're talking about. The most successful deals are the ones where everyone gets a good price for what they get, and the best deals are those that don't include a price for the ""good deal"" or the ""bad deal"" The best deals can be found on the internet, and they are usually available for as little as a few hundred dollars per person, or even a few thousand dollars for a small number of people who want to take a chance on the vaccine. The worst deals are usually the ones that are available for a lot of people, such as those who don't want to risk their lives, and who are willing to put their lives on the line for a few months at a time to get a vaccine that they believe will protect them from a disease that is very rarely dangerous. The ""best deals"" are often available on the Internet, and can be bought for much less than $1,000 per person. y have destroyed people's lives and gotten away with it. My heart goes out to those who have suffered this damage for no damn good reason except to help make huge profits for Merck!Here is the reason that the germ theory is nonsense. Everyday we are bombarded with billions of germs. Presumably at least some of them are of the kind that germ theorists believe are dangerous. This makes a stable biological system by definition impossible. The immune system is *not* a negative feedback it is the opposite. It actually reinforces our math problem because the immune system will weaken as the number of pathogens increase. So the germ Theory is quite literally, mathematically impossible. There is as much chance of it being true as 2+2 = 5. There are plenty of other massive problems with germ theory such as why did things like SARS and bird flu magically disappear? Why do we have the symptoms that we do? Is our body controlling the symptoms to help fight the germs and if so, why would suppressing the symptoms with antibiotics or Tamiflu be considered a good idea? If the virus is causing the symptoms then why would it cause these kinds of things? If it is not causing them then why does it cause them at all? It is a massive mystery to me. I have never seen anybody else ever raise it. I love this one because I have ever seen nobody else ever raising it."
What molecule was the focus of the study?,"\section{Introduction}

Spectral line surveys have revealed that high-mass star-forming
regions are rich reservoirs of molecules from simple diatomic species
to complex and larger molecules (e.g.,
\citealt{schilke1997b,hatchell1998b,comito2005,bisschop2007}).
However, there have been rarely studies undertaken to investigate the
chemical evolution during massive star formation from the earliest
evolutionary stages, i.e., from High-Mass Starless Cores (HMSCs) and
High-Mass Cores with embedded low- to intermediate-mass protostars
destined to become massive stars, via High-Mass Protostellar Objects
(HMPOs) to the final stars that are able to produce Ultracompact H{\sc
  ii} regions (UCH{\sc ii}s, see \citealt{beuther2006b} for a recent
description of the evolutionary sequence). The first two evolutionary
stages are found within so-called Infrared Dark Clouds (IRDCs).  While
for low-mass stars the chemical evolution from early molecular
freeze-out to more evolved protostellar cores is well studied (e.g.,
\citealt{bergin1997,dutrey1997,pavlyuchenkov2006,joergensen2007}),
it is far from clear whether similar evolutionary patterns are present
during massive star formation.

To better understand the chemical evolution of high-mass star-forming
regions we initiated a program to investigate the chemical properties
from IRDCs to UCH{\sc ii}s from an observational and theoretical
perspective. We start with single-dish line surveys toward a large
sample obtaining their basic characteristics, and then perform
detailed studies of selected sources using interferometers on smaller
scales. These observations are accompanied by theoretical modeling of
the chemical processes.  Long-term goals are the chemical
characterization of the evolutionary sequence in massive star
formation, the development of chemical clocks, and the identification
of molecules as astrophysical tools to study the physical processes
during different evolutionary stages. Here, we present an initial
study of the reactive radical ethynyl (C$_2$H) combining single-dish
and interferometer observations with chemical modeling.  Although
C$_2$H was previously observed in low-mass cores and Photon Dominated
Regions (e.g., \citealt{millar1984,jansen1995}), so far it was not
systematically investigated in the framework of high-mass star
formation.

\section{Observations}
\label{obs}

The 21 massive star-forming regions were observed with the Atacama
Pathfinder Experiment (APEX) in the 875\,$\mu$m window in fall 2006.
We observed 1\,GHz from 338 to 339\,GHz and 1\,GHz in the image
sideband from 349 to 350\,GHz.  The spectral resolution was
0.1\,km\,s$^{-1}$, but we smoothed the data to
$\sim$0.9\,km\,s$^{-1}$. The average system temperatures were around
200\,K, each source had on-source integration times between 5 and 16
min. The data were converted to main-beam temperatures with forward
and beam efficiencies of 0.97 and 0.73, respectively
\citep{belloche2006}. The average $1\sigma$ rms was 0.4\,K.  The main
spectral features of interest are the C$_2$H lines around 349.4\,GHz
with upper level excitation energies $E_u/k$ of 42\,K (line blends of
C$_2$H$(4_{5,5}-3_{4,4})$ \& C$_2$H$(4_{5,4}-3_{4,3})$ at
349.338\,GHz, and C$_2$H$(4_{4,4}-3_{3,3})$ \&
C$_2$H$(4_{4,3}-3_{3,2})$ at 349.399\,GHz). The beam size was $\sim
18''$.

The original Submillimeter Array (SMA) C$_2$H data toward the
HMPO\,18089-1732 were first presented in \citet{beuther2005c}. There
we used the compact and extended configurations resulting in good
images for all spectral lines except of C$_2$H. For this project, we
re-worked on these data only using the compact configuration. Because
the C$_2$H emission is distributed on larger scales (see
\S\ref{results}), we were now able to derive a C$_2$H image. The
integration range was from 32 to 35\,km\,s$^{-1}$, and the achieved
$1\sigma$ rms of the C$_2$H image was 450\,mJy\,beam$^{-1}$.  For more
details on these observations see \citet{beuther2005c}.

\section{Results}
\label{results}

The sources were selected to cover all evolutionary stages from IRDCs
via HMPOs to UCH{\sc ii}s. We derived our target list from the samples
of \citet{klein2005,fontani2005,hill2005,beltran2006}.  Table
\ref{sample} lists the observed sources, their coordinates, distances,
luminosities and a first order classification into the evolutionary
sub-groups IRDCs, HMPOs and UCH{\sc ii}s based on the previously
available data. Although this classification is only based on a
limited set of data, here we are just interested in general
evolutionary trends. Hence, the division into the three main classes
is sufficient.

Figure \ref{spectra} presents sample spectra toward one source of each
evolutionary group. While we see several CH$_3$OH lines as well as
SO$_2$ and H$_2$CS toward some of the HMPOs and UCH{\sc ii}s but not
toward the IRDCs, the surprising result of this comparison is the
presence of the C$_2$H lines around 349.4\,GHz toward all source types
from young IRDCs via the HMPOs to evolved UCH{\sc ii}s.  Table
\ref{sample} lists the peak brightness temperatures, the integrated
intensities and the FWHM line-widths of the C$_2$H line blend at
349.399\,GHz. The separation of the two lines of 1.375\,MHz already
corresponds to a line-width of 1.2\,km\,s$^{-1}$. We have three C$_2$H
non-detections (2 IRDCs and 1 HMPO), however, with no clear trend with
respect to the distances or the luminosities (the latter comparison is
only possible for the HMPOs). While IRDCs are on average colder than
more evolved sources, and have lower brightness temperatures, the
non-detections are more probable due to the relatively low sensitivity
of the short observations (\S\ref{obs}). Hence, the data indicate
that the C$_2$H lines are detected independent of the evolutionary
stage of the sources in contrast to the situation with other
molecules.  When comparing the line-widths between the different
sub-groups, one finds only a marginal difference between the IRDCs and
the HMPOs (the average $\Delta v$ of the two groups are 2.8 and
3.1\,km\,s$^{-1}$).  However, the UCH{\sc ii}s exhibit significantly
broader line-widths with an average value of 5.5\,km\,s$^{-1}$.

Intrigued by this finding, we wanted to understand the C$_2$H spatial
structure during the different evolutionary stages.  Therefore, we
went back to a dataset obtained with the Submillimeter Array toward
the hypercompact H{\sc ii} region IRAS\,18089-1732 with a much higher
spatial resolution of $\sim 1''$ \citep{beuther2005c}.  Albeit this
hypercompact H{\sc ii} region belongs to the class of HMPOs, it is
already in a relatively evolved stage and has formed a hot core with a
rich molecular spectrum.  \citet{beuther2005c} showed the spectral
detection of the C$_2$H lines toward this source, but they did not
present any spatially resolved images. To recover large-scale
structure, we restricted the data to those from the compact SMA
configuration (\S\ref{obs}). With this refinement, we were able to
produce a spatially resolved C$_2$H map of the line blend at
349.338\,GHz with an angular resolution of $2.9''\times 1.4''$
(corresponding to an average linear resolution of 7700\,AU at the
given distance of 3.6\,kpc). Figure \ref{18089} presents the
integrated C$_2$H emission with a contour overlay of the 860\,$\mu$m
continuum source outlining the position of the massive protostar. In
contrast to almost all other molecular lines that peak along with the
dust continuum \citep{beuther2005c}, the C$_2$H emission surrounds the
continuum peak in a shell-like fashion.

\section{Discussion and Conclusions}

To understand the observations, we conducted a simple chemical
modeling of massive star-forming regions. A 1D cloud model with a mass
of 1200\,M$_\sun$, an outer radius of 0.36\,pc and a power-law density
profile ($\rho\propto r^p$ with $p=-1.5$) is the initially assumed
configuration. Three cases are studied: (1) a cold isothermal cloud
with $T=10$\,K, (2) $T=50$\,K, and (3) a warm model with a temperature
profile $T\propto r^q$ with $q=-0.4$ and a temperature at the outer
radius of 44\,K. The cloud is illuminated by the interstellar UV
radiation field (IRSF, \citealt{draine1978}) and by cosmic ray
particles (CRP). The ISRF attenuation by single-sized $0.1\mu$m
silicate grains at a given radius is calculated in a plane-parallel
geometry following \citet{vandishoeck1988}. The CRP ionization rate is
assumed to be $1.3\times 10^{-17}$~s$^{-1}$ \citep{spitzer1968}. The
gas-grain chemical model by \citet{vasyunin2008} with the desorption
energies and surface reactions from \citet{garrod2006} is used.
Gas-phase reaction rates are taken from RATE\,06 \citep{woodall2007},
initial abundances, were adopted from the ``low metal'' set of
\citet{lee1998}.

Figure \ref{model} presents the C$_2$H abundances for the three models
at two different time steps: (a) 100\,yr, and (b) in a more evolved
stage after $5\times10^4$\,yr. The C$_2$H abundance is high toward the
core center right from the beginning of the evolution, similar to
previous models (e.g., \citealt{millar1985,herbst1986,turner1999}).
During the evolution, the C$_2$H abundance stays approximately
constant at the outer core edges, whereas it decreases by more than
three orders of magnitude in the center, except for the cold $T=10$~K
model.  The C$_2$H abundance profiles for all three models show
similar behavior.

The chemical evolution of ethynyl is determined by relative removal
rates of carbon and oxygen atoms or ions into molecules like CO, OH,
H$_2$O. Light ionized hydrocarbons CH$^+_{\rm n}$ (n=2..5) are quickly
formed by radiative association of C$^+$ with H$_2$ and hydrogen
addition reactions: C$^+$ $\rightarrow$ CH$_2^+$ $\rightarrow$
CH$_3^+$ $\rightarrow$ CH$_5^+$.  The protonated methane reacts with
electrons, CO, C, OH, and more complex species at later stage and
forms methane.  The CH$_4$ molecules undergo reactive collisions with
C$^+$, producing C$_2$H$_2^+$ and C$_2$H$_3^+$. An alternative way to
produce C$_2$H$_2^+$ is the dissociative recombination of CH$_5^+$
into CH$_3$ followed by reactions with C$^+$.  Finally, C$_2$H$_2^+$
and C$_2$H$_3^+$ dissociatively recombine into CH, C$_2$H, and
C$_2$H$_2$. The major removal for C$_2$H is either the direct
neutral-neutral reaction with O that forms CO, or the same reaction
but with heavier carbon chain ions that are formed from C$_2$H by
subsequent insertion of carbon. At later times, depletion and
gas-phase reactions with more complex species may enter into this
cycle.  At the cloud edge the interstellar UV radiation
instantaneously dissociates CO despite its self-shielding,
re-enriching the gas with elemental carbon.

The transformation of C$_2$H into CO and other species proceeds
efficiently in dense regions, in particular in the ``warm'' model
where endothermic reactions result in rich molecular complexity of the
gas (see Fig.~\ref{model}).  In contrast, in the ``cold'' 10\,K model
gas-grain interactions and surface reactions become important.  As a
result, a large fraction of oxygen is locked in water ice that is hard
to desorb ($E_{\rm des} \sim 5500$~K), while half of the elemental
carbon goes to volatile methane ice ($E_{\rm des} \sim 1300$~K). Upon
CRP heating of dust grains, this leads to much higher gas-phase
abundance of C$_2$H in the cloud core for the cold model compared to
the warm model. The effect is not that strong for less dense regions
at larger radii from the center.

Since the C$_2$H emission is anti-correlated with the dust continuum
emission in the case of IRAS\,18089-1732 (Fig.\,\ref{18089}), we do
not have the H$_2$ column densities to quantitatively compare the
abundance profiles of IRAS\,18089-1732 with our model. However, data
and model allow a qualitative comparison of the spatial structures.
Estimating an exact evolutionary time for IRAS\,18089-1732 is hardly
possible, but based on the strong molecular line emission, its high
central gas temperatures and the observed outflow-disk system
\citep{beuther2004a,beuther2004b,beuther2005c}, an approximate age of
$5\times10^4$\,yr appears reasonable.  Although dynamical and chemical
times are not necessarily exactly the same, in high-mass star
formation they should not differ to much: Following the models by
\citet{mckee2003} or \citet{krumholz2006b}, the luminosity rises
strongly right from the onset of collapse which can be considered as a
starting point for the chemical evolution. At the same time disks and
outflows evolve, which should hence have similar time-scales.  The
diameter of the shell-like C$_2$H structure in IRAS\,18089-1732 is
$\sim 5''$ (Fig.\,\ref{18089}), or $\sim$9000\,AU in radius at the
given distance of 3.6\,kpc.  This value is well matched by the modeled
region with decreased C$_2$H abundance (Fig.\,\ref{model}).  Although
in principle optical depths and/or excitation effects could mimic the
C$_2$H morphology, we consider this as unlikely because the other
observed molecules with many different transitions all peak toward the
central submm continuum emission in IRAS\,18089-1732
\citep{beuther2005c}. Since C$_2$H is the only exception in that rich
dataset, chemical effects appear the more plausible explanation.

The fact that we see C$_2$H at the earliest and the later evolutionary
stages can be explained by the reactive nature of C$_2$H: it is
produced quickly early on and gets replenished at the core edges by
the UV photodissociation of CO. The inner ``chemical'' hole observed
toward IRAS\,18089-1732 can be explained by C$_2$H being consumed in
the chemical network forming CO and more complex molecules like larger
carbon-hydrogen complexes and/or depletion.

The data show that C$_2$H is not suited to investigate the central gas
cores in more evolved sources, however, our analysis indicates that
C$_2$H may be a suitable tracer of the earliest stages of (massive)
star formation, like N$_2$H$^+$ or NH$_3$ (e.g.,
\citealt{bergin2002,tafalla2004,beuther2005a,pillai2006}). While a
spatial analysis of the line emission will give insights into the
kinematics of the gas and also the evolutionary stage from chemical
models, multiple C$_2$H lines will even allow a temperature
characterization. With its lowest $J=1-0$ transitions around 87\,GHz,
C$_2$H has easily accessible spectral lines in several bands between
the 3\,mm and 850\,$\mu$m.  Furthermore, even the 349\,GHz lines
presented here have still relatively low upper level excitation
energies ($E_u/k\sim42$\,K), hence allowing to study cold cores even
at sub-millimeter wavelengths.  This prediction can further be proved
via high spectral and spatial resolution observations of different
C$_2$H lines toward young IRDCs.

\acknowledgments{H.B. acknowledges financial support
  by the Emmy-Noether-Programm of the Deutsche Forschungsgemeinschaft
  (DFG, grant BE2578). }


",['The focus of the study was on the reactive radical ethynyl (C$_2$H).'],2115,multifieldqa_en,en,,b619fa683cbbeb8db3d575fc2e261940701a530452b14eef," The 21 massive star-forming regions were observed with the AtacamaPathfinder Experiment (APEX) in the 875\,$\mu$m window in fall 2006. The first two evolutionarystages are found within so-called Infrared Dark Clouds (IRDCs) The chemical evolution from early molecular.freeze-out to more evolved protostellar cores is well studied (e.g., bergin1997,dutrey1997,pavlyuchenkov2006,joergensen2007) But it is far from clear whether similar evolutionary patterns are present during massive star formation. Long-term goals are the chemical.characterization of the evolutionary sequence in massive star.formation, the development of chemical clocks, and the identification.of molecules as astrophysical tools to study the physical processes. during different evolutionary stages. We present an initial. study of the reactive radical ethynyl (C$_2$H) combining single-dish. observations with chemical modeling. The data were converted to main-beam temperatures with forward and beam efficiencies of 0.97 and 0.73, respectively. The average $1\sigma$ rms was 0.4\,K. The mainspectral features of interest are the C$_1$ lines around 349.4/2/3 and C$2$ lines at 349/3/4/4. We used compact and compact configurations and extended the resulting images for all spectral lines except the C $2$2# lines. The original data were first presented in 2005 in the first in-depth look at the early stages of star formation in the universe. We are happy to make this information available to the public for the first time in an open-access version of this article. We apologise for any inconvenience caused by the use of these images. We also apologize for any confusion that may have been caused by our use of compact configurations. We have extended the original images to include the more compact C$3# lines of C$1# and C##. We hope that this will allow us to provide a more complete picture of the early history of the universe and the formation of massive star regions. We apologize for the errors in the original version of the article, which we believe were due to a lack of clarity on how the images were created. We would like to make clear that the data were taken from the SMA Array (SMA) and not from a single source. We regret any confusion caused by this, and we are pleased to make it clear that all the images have been extended to include all the spectral lines of all the sources used in the study. We thank the authors for their efforts. The study was carried out with the help of the University of California, Los Angeles, for which we are grateful for their support and assistance in the design of the project. For more information, visit the website: http://www.cnn.com/2007/07/29/science/topics/star-building-in-the-sky-and-space-research/star.html#storylink=cpy. The article was originally published in the journal The Astrophysics of Star Formation, with a modified version with the headline “Star-Building in the Milky Way’s Early Years”. It has been updated to reflect that this article has been revised to reflect the more recent developments in the field of star-building in the history of astronomy. The author would also like to point out that the study was conducted with the support of the International Astronomical Union (IAO) in 2007. The sources were selected to cover all evolutionary stages from IRDCs to HMPOs. The surprising result of this comparison is the presence of the C$_2$H lines around 349.4\,GHz toward all source types. The lines are detected independent of the evolutionary stage of the sources in contrast to the situation with othermolecules. The data indicate that the peak brightness temperatures, the integrated.intensities and the FWHM line-widths of the lines at 349.399 \,GHz are independent of evolutionary stages. The C$2$ H lines are the only ones that are clearly distinguishable between the IRDC and HMPO sources. The results are published in the open-access issue of The Astrophysical Journal (ASJ) (http://www. Astrophysics.org/ASJ/2012/01/08/astrophysics-results.html#storylink=cpy. This article includes text from the ASJ 2012 ASJ issue (http: // www.AstrophysicalJournal.com/ 2012/02/07/astrophysiological-results-article.html). The article is also available in the online edition of the journal Astrophysiology (http:\www.astrophysicaljournal.com/. The article also includes the online version of the article from ASJ ( http:// www.astrophysics.com. The article’s title is ‘Astrophysics’, and the article is available in both English and French. The publication date is also the same as that of the previous version of this article, but we have changed the title to ‘astrophysiological results’ to reflect that the article has been updated to reflect the new information.’. The study was originally published in The Astrophysiology journal ( http: //www. astrophys.org/. The author has since changed the name of the publication to “Astrophysology’ and the publication date to be ‘2012/11/08’). The author also has the intention to publish the findings in the AstroPhysiology (A&E) issue (A/E/12/08). The study has been amended to include the additional information about the ‘molecular structure’ of the “C$2’ line blend,’ as well as a ‘spatially resolved’ map of the line blend at the 349.338’GHz. The map was produced to produce a spatially resolved C$#2$ map of. the C#2#H emission at349.338\, GHz with an angular resolution of $29.4''''''’'' ”’’ ‘‘’This map shows the peak-like contour of the massive protostar’ in a shell-like fashion. Incontrast to almost all other lines that appear in the 860,\mu$mcontinuum outlining the source position, this peak is almost almost identical to all other peak position of themassive protostars. ’”. The analysis of the spectra shows that the line- widths between the different types of source types are significantly greater than the average for the H MPOs and the HMCOs. This is because the HMcOs are already in a relatively evolved stage and have formed a hot core with a rich molecular spectrum. The average line width between the two groups is 5.5\,km\,s$^{-1}$.  temperature $T\propto r^q$ with $q=-0.4$ and a temperature at the outer                radius of 44\,K. The cloud is illuminated by the interstellar UVophobicradiation field (IRSF) and by cosmic ray                particles (CRP) The CRP ionization rate is assumed to be $1.3\times 10^{-17}$~s$^{-1}$ \citep{spitzer1968}. The C$_2$H abundance profiles for all three models show a similar behavior. The chemical evolution of ethynyl is determined by relative removal of carbon and oxygen atoms or ions into molecules like CO, OH, and C$2$O. At later times, depletion and gas-phase reactions with more complex species may enter into this cycle. As a result, a large fraction of oxygen is locked in water that is hard to desorb in ice that is 5500$~K desorbance. The gas-grain interactions between surface reactions and surface reactions become important important for the study of the formation of planets and planets in the solar system. In contrast, the heating of dust grains, this leads to much higher-abundance of the gas C2$2, which leads to higher-phase C2, or C$3, gas. In particular, in the end regions in particular in the model in particular, C$1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,32,.32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,56,57,58,59,60,63,64,62,65,69,70,72,73,74,74 and74,75,76,77,78,79,80,82,83,84,86,87,88,89,90,92,93,94,89,.92,90,.93,93,.94,96,98,99,100,102,103,100,.96,104,103,.98,102,.99,104,.97,103 and 104,103. The transformation of methane into CO and other species proceeds in denseefficient, rich molecular complexity of the ``warm'' othermic reactions result in a rich molecular complex of the glycine gas (see Fig. 1) The protonated methane reacts with CO, C, OH and other complex species at later stage andforms methane. The CH$_4$ molecules undergo reactive collisions with C$^+$, producing C$#+$ and C###, which in turn recombine into CH, C#, C #, CH # and C $2#, and CH$3#, C #. In the end, the methane goes to volatile ice ($E_{\rm des}\rm des} in the ``cold'' model in Fig. 10, cold in 10, 10K in 10K, in contrast to the warm in 10k in the cold 10,cold 10,10K, cold 10K model, in which the gas goes to methane ($E_2,10k, 10,000K) upon heating. In high-mass starforming they should not differ to much: the luminosity rises strongly right from the onset of collapse which can be considered as a starting point for the chemical evolution. The fact that we see C$_2$H at the earliest and the later evolutionarystages can be explained by the reactive nature of the structure. The inner ``chemical'' hole observed toward IRAS\,18089-1732 can be. explained by C$-2-H being consumed in the chemical network forming CO and more complex molecules like larger.carbon-hydrogen complexes and/or depletion. The study acknowledges financial support by the Emmy-Noether-Programm of the Deutsche Forschungsgemeinschaft (DFG, grant BE2578) and the German Space Research Institute (SPI) for the study of the early stages of star formation. For more information on the study, visit: http://www.sri.org/sri-research/star-formation-early-stage-of-star-forming-in-IRAS-1808-1809-1731/ and the SRI/SPI/Sri-Research/Star-Forming-Early-Stage-Of IRAS/STAR-Formation-Early.html#sthash.BE2578, as well as the SRA/SRI/Siri/StarFormation/Starform/Early-Birth-Of-IrAS/EarlyBirth.html. For the full report, please visit:http://www.-sri.-research.org/. For the rest of the article, please see:http:www.spris.org.uk/starform/early-birth-of IRAS/Star formation/Earlybirth-Of IRACS/EarlyBase.html/. For more details on this article, see: http:www-sri research.gov/starbeast/earlyBase/Starbeast-EarlyBirth-EarlyBase/Childbirth-EarlyBurst.html%. For the remainder of the paper, please click on the link to the bottom of the page and read the article for more information about the early stage of star-beast formation in IRACS. The article is also available as a free download on the Sari/Sari/StarBeast/EarlyBursts/Starbase/ChildBirth- EarlyBursts.html?content=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,16. For a full list of the findings, please go to: http:\/sari/starbase/earlyBurst/ChildBase/Children%20Bursts%20and%20Other%20Studies%20By%20Scientists%20on%20The%20Study%20of%20IRACs%20with%20This%20Report%20As%20Presentation%20And%20A, http://sari.gov.uk/."
"When was Weep Not, Child first published?","Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. It was among the African Writers Series. It was the first English language|English novel to be published by an East African. Thiong'o's works deal with the relationship between Africans and white settlers in colonial Kenya, and are heavily critical of colonial rule. Specifically, Weep Not, Child deals with the Mau Mau Uprising, and ""the bewildering dispossession of an entire people from their ancestral land."" Ngũgĩ wrote the novel while he was a student at Makerere University.

The book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement.

Plot summary

Njoroge, a little boy, is urged to attend school by his mother. He is the first one of his family able to go to school. His family lives on the land of Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Njoroge's brother Kamau works as an apprentice to a carpenter, while Boro, the eldest living son, is troubled by his experiences while in forced service during World War II, including witnessing the death of his elder brother. Ngotho, Njoroge's father and a respected man in the surrounding area, tends Mr. Howlands' crops, but is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty.

One day, black workers call for a strike to obtain higher wages. Ngotho is ambivalent about participating in the strike because he fears he will lose his job. However, he decides to go to the gathering, even though his two wives do not agree. At the demonstration, there are calls for higher wages. Suddenly, the white police inspector brings Jacobo to the gathering to pacify the native people. Jacobo tries to put an end to the strike. Ngotho attacks Jacobo, and the result is a riot where two people are killed. Jacobo survives and swears revenge. Ngotho loses his job and Njoroge’s family is forced to move. Njoroge’s brothers fund his education and seem to lose respect for their father.

Mwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school.

For a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta – a revered leader of the movement. Many blacks think that he is going to bring forth Kenya’s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population.

Jacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods.

One day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition.

Several months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau. Ngotho soon dies from his injuries and Njoroge finds out that his father was protecting his brothers. Kamau has been imprisoned for life. Only Njoroge and his two mothers remain free, and Njoroge is left as the sole provider of his two mothers. Njoroge fears that he cannot make ends meet; he gives up hope of continuing in school and loses faith in God.

Njoroge asks Mwihaki's for support, but she is angry because of her father’s death. When he finally pledges his love to her, she refuses to leave with him, realizing her obligation to Kenya and her mother. Njoroge decides to leave town and makes an attempt at suicide; however, he fails when his mothers find him before he is able to hang himself. The novel closes with Njoroge feeling hopeless, and ashamed of cowardice.

Characters in Weep Not, Child
 Njoroge: the main character of the book whose main goal throughout the book is to become as educated as possible.
 Ngotho: Njoroge's father. He works for Mr.Howlands and is respected by him until he attacks Jacobo at a workers strike. He is fired and the family is forced to move to another section of the country. Over the course of the book his position as the central power of the family weakened, to the point where his self-realization that he has spent his whole life waiting for the prophecy (that proclaims the blacks will be returned their land) to come true rather than fighting for Kenyan independence, leads to his depression.
 Nyokabi and Njeri: the two wives of Ngotho. Njeri is Ngotho's first wife, and mother of Boro, Kamau, and Kori. Nyokabi is his second wife, and the mother of Njoroge and Mwangi.
 Njoroge has four brothers: Boro, Kamau, Kori and Mwangi (who is Njoroge's only full brother, who died in World War II).
 Boro: Son of Njeri who fights for the Allies in World War II. Upon returning his anger against the colonial government is compounded by their confiscation of the his land. Boro's anger and position as eldest son leads him to question and ridicule Ngotho, which eventually defeats their father's will (upon realizing his life was wasted waiting and not acting). It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as ""entering politics"") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo.
 Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother.
 Jacobo: Mwihaki's father and an important landowner. Chief of the village.
 Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school.

Themes and motifs
Weep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt.

The novel also ponders the role of saviours and salvation. The author notes in his The River Between: ""Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people."" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, ""Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel."" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government.

See also

Things Fall Apart
Death and the King's Horseman

References

External links
Official homepage of Ngũgĩ wa Thiong'o
BBC profile of Ngũgĩ wa Thiong'o
Weep Not, Child at Google Books

British Empire in fiction
Novels set in colonial Africa
Historical novels
Kenyan English-language novels
Novels by Ngũgĩ wa Thiong'o
Novels set in Kenya
1964 novels
Heinemann (publisher) books
Postcolonial novels
African Writers Series
1964 debut novels","['Weep Not, Child was first published in 1964.']",1489,multifieldqa_en,en,,25ebcea4435f88495b4175446d1d7d6dacb1034a8f861ca5," Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. The book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement. The novel was the first English language|English novel to be published by an East African. It is heavily critical of colonial rule in Kenya and the Mau Mau Uprising, and ""the bewildering dispossession of an entire people from their ancestral land"" It was among the African Writers Series, and was published by Penguin Books in the UK in 1964. It has been cited as one of the best books of the 20th century by the American scholar John MacIntosh, who called it the ""greatest novel of all time"" and called it ""a masterpiece of African literature"" The novel is published in two parts, one in English and the other in the Nairobi-based Nambia-based English-language edition of the book, which was published in the U.S. in the same year as the first edition of Weep not, Child. The English-speaking edition has been published by Simon & Schuster in the US in the form of a hardback, priced at $24.99 (about £16.50). The English version has been released in hardback in the United States, with a price tag of $30 (about $25). The book has been translated into English as ""Weep not child"" and ""Weeping Not Child"" by John Macintosh, Jr. and published in paperback in the USA in the year 2000. It can be ordered by clicking here for a copy of the English version of the novel, or clicking here to read the English-only version. The U.K. version of this article has been amended to read: ""Weeps Not Child"", with the emphasis on the word ""child"" replaced by ""child"". The English translation of the second part has been changed to ""Child"" and to reflect that the book is about a boy, Njorosge, and not a girl, Mwihaki, as well as the events surrounding her departure from boarding school. The author has also changed the name of the girl to ""Mwihki"" to protect the identity of the character, which has been removed from the book. The story is about the relationship between a boy and his father, who is a respected man in the surrounding area, and a man who is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty. It also tells the story of the rise and fall of a man known as Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Jacobo is brutally murdered in his office by a member of the Mau-Mau movement. Several months later, it is revealed that it is eventually revealed to be a connection between the murder and the family and the murder of Jacobo's family. The family is forced to move away from their home. Njoroge is the main character of Weep Not, Child. His main goal throughout the book is to become as educated as possible. He is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt. The novel explores the negative aspects of colonial rule over Kenya. It also ponders the role of saviours and salvation in The River Between. The author says, ""Jomo Kenyatta had had a duty to lead the people and save the people of Kenya, and I say from the same tree, a son shall rise in me, and be his duty to save his people"" The author notes: ""Salvation shall come from the hills in his attempt. And from the blood that flows that flows in me ... I say: 'Salvation in me' "" The author also says: ""The author notes that in his attempts to save Kenya, the saviour and savior have been the people, and not the other way around"" ""The River Between"" is a collection of short stories written by the author, written in English and Kiswahili, about his experiences in Kenya during the Second World War. The book is published by Simon & Schuster, and is available in hardback for £12.99 (US$16.99) and paperback for £16.95 (UK). It is published in English with an abridged version of the book available in the UK for £14.99, and in the US with an unabridged copy of the English version for £15.99. It has been published in two editions, the second of which is a hardback version with a paperback edition of £15 (UK) and the third of which comes in a hardcover version of £16 (US) with an additional £5.99 ($16.50) for the paperback version. It is also available in paperback in the U.S. and the UK with an autographed copy for £13.99 (£15.95) and a paperback version of $15.50 (£16.00). The book has been translated into English and published in the United States by Simon and Schuster. The UK edition has been released in paperback with an English translation of the first half of the novel, and the second half by the US publisher, Simon &Schuster, for £11.95 (£14.95). It has also been published on the Kindle version of this article, with a US edition of the same name, but with a different cover. The paperback version has a different front cover and a different back cover. For more information on the book, see: http://www.simonandschuster.com/books/Weep-Not-Child-The-River-Between.html. For the full version, see www.samaritans.co.uk/pages/weep-not-child-the-river-between.html?page-1-2.html%. For the second part of the story, see http://sales.sales-and-purchases/Weeping-Not, Child-The. River- Between.html/. For the third part, see http:// www.sages.com/. The fourth and final part of this section, the book will be released on September 11, 2013. een his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel"" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government. The novel ""Weep Not, Child"" was published in 1964 by Heinemann. It was the first of a series of novels by Ngũgĩ wa Thiong'o to be published in Kenya. The novels were published in English and in the  Kenyan   language. The author has also written novels in the African language and in Portuguese."
What is the relationship between the maximum velocity and the amplitude of the blob or depletion?,"\section{Model equations} \label{sec:equations}

In drift-fluid models the continuity equation
\begin{align}
 \frac{\partial n}{\partial t} + \nabla\cdot\left( n \vec u_E  \right) &= 0 \label{eq:generala} 
\end{align}
describes the dynamics of the electron density $n$. Here
$\vec u_E := (\hat{\vec b} \times \nabla \phi)/B$ gives the electric drift
velocity in a magnetic field $\vec B := B \hat{\vec b}$ and an electric
potential $\phi$. We neglect contributions of the diamagnetic drift~\cite{Kube2016}.




Equation~\eqref{eq:generala} is closed by invoking quasineutrality, i.e. the divergence of the ion polarization, 
the electron diamagnetic and the gravitational drift currents must vanish
\begin{align}
  \nabla\cdot\left( \frac{n}{\Omega} \left( \frac{\partial}{\partial t} 
  + \vec u_E \cdot\nabla  \right)\frac{\nabla_\perp \phi}{B}  + n\vec u_d - n\vec u_g\right) &=0
  . 
 
 
  \label{eq:generalb}
\end{align}
Here we denote 
$\nabla_\perp\phi/B := - \hat{\vec b} \times \vec u_E$, 
the electron diamagnetic drift
$\vec u_d := - T_e(\hat{\vec b} \times\nabla n ) /enB$
with the electron temperature $T_e$,
the ion gravitational drift velocity  
$\vec u_g := m_i \hat{\vec b} \times \vec g /B$
with ion mass $m_i$, and the ion gyro-frequency
$\Omega := eB/m_i$.

Combining Eq.~\eqref{eq:generalb} with Eq.~\eqref{eq:generala} yields
\begin{align}
 \frac{\partial \rho}{\partial t} + \nabla\cdot\left( \rho\vec u_E \right) + \nabla \cdot\left( n(\vec u_\psi + \vec u_d + \vec u_g) \right) &= 0\label{eq:vorticity}
\end{align}
with the polarization charge density 
$\rho = \nabla\cdot( n\nabla_\perp \phi / \Omega B)$ 
and
$\vec u_\psi := \hat{\vec b}\times \nabla\psi /B$ 
with 
$\psi:= m_i\vec u_E^2 /2e$.
We exploit this form of Eq.~\eqref{eq:generalb} in our numerical simulations.

Equations~\eqref{eq:generala} and \eqref{eq:generalb} respectively \eqref{eq:vorticity} have several invariants.
First, in Eq.~\eqref{eq:generala} the relative particle number 
$M(t) := \int \mathrm{dA}\, (n-n_0)$ is conserved over time
$\d M(t)/\d t = 0$. 
Furthermore, we integrate 
$( T_e(1+\ln n) -T_e \ln B)\partial_t n$
as well as
$-e\phi \partial_t\rho - (m_i\vec u_E^2/2+gm_ix - T_e\ln B)\partial_t n$ 
over the domain to get, disregarding boundary contributions,
\begin{align}
  \frac{\d}{\d t}\left[T_eS(t) + H(t) \right] = 0, \label{eq:energya}\\ 
    \frac{\d}{\d t} \left[ E(t) - G(t) - H(t)\right] =  0,
    \label{eq:energyb}
\end{align}
where we define 
the entropy
$S(t):=\int \mathrm{dA}\, [n\ln(n/n_0) - (n-n_0)]$,  
the kinetic energy 
$E(t):=m_i \int \mathrm{dA}\, n\vec u_E^2/2$ 
and the potential energies
$G(t) := m_i g\int \mathrm{dA}\, x(n-n_0)$
and
$H(t) := T_e\int \mathrm{dA}\, (n-n_0) \ln (B^{-1})$.
Note that $n\ln( n/n_0) - n + n_0 \approx (n-n_0)^2/2$ for $|(n-n_0)/n_0| \ll 1$ and $S(t)$ thus reduces to the 
local entropy form in Reference~\cite{Kube2016}. 

We now set up a gravitational field $\vec g = g\hat x$ and a constant homogeneous background
magnetic field $\vec B = B_0 \hat z$ in a Cartesian coordinate system.
Then the divergences of the electric and gravitational drift velocities $\nabla\cdot\vec u_E$ and $\nabla\cdot\vec u_g$
and the diamagnetic current $\nabla\cdot(n\vec u_d)$ vanish, which makes the 
flow incompressible. Furthermore, the magnetic potential energy vanishes $H(t) = 0$.

In a second system we model the inhomogeneous magnetic field present in tokamaks as
$\vec B := B_0 (1+ x/R_0)^{-1}\hat z$ and neglect the gravitational drift $\vec u_g = 0$.
Then, the potential energy $G(t) = 0$. 
Note that 
$H(t) = m_i \ensuremath{C_\mathrm{s}}^2/R_0\int\mathrm{dA}\, x(n-n_0) +\mathcal O(R_0^{-2}) $
reduces to $G(t)$ with the effective gravity $g_\text{eff}:= \ensuremath{C_\mathrm{s}}^2/R_0$ with $\ensuremath{C_\mathrm{s}}^2 := T_e/m_i$. 
For the rest of this letter we treat $g$ and $g_\text{eff}$ as well as $G(t)$ and $H(t)$ on the same footing.
The magnetic field inhomogeneity thus entails compressible flows, which is 
the only difference to the model describing dynamics in a homogeneous magnetic field introduced above. 
Since both $S(t)\geq 0$ and $E(t)\geq 0$ we further derive from Eq.~\eqref{eq:energya} and Eq.~\eqref{eq:energyb} that the kinetic energy
is bounded by $E(t) \leq T_eS(t) + E(t) = T_e S(0)$; a feature absent from the gravitational system with 
incompressible flows, where $S(t) = S(0)$. 

We now show that the invariants Eqs.~\eqref{eq:energya} and \eqref{eq:energyb} present restrictions on the velocity and
acceleration of plasma blobs. 
First, we define the blobs' center of mass (COM) via $X(t):= \int\mathrm{dA}\, x(n-n_0)/M$ and 
its COM velocity as $V(t):=\d X(t)/\d t$. 
The latter is proportional to the total radial particle flux~\cite{Garcia_Bian_Fundamensky_POP_2006, Held2016a}.
We assume
that $n>n_0$ and $(n-n_0)^2/2 \leq [ n\ln (n/n_0) - (n-n_0)]n $ to show for both systems 
\begin{align}
  (MV)^2 &= \left( \int \mathrm{dA}\, n{\phi_y}/{B} \right)^2
  = \left( \int \mathrm{dA}\, (n-n_0){\phi_y}/{B} \right)^2\nonumber\\
 
&\leq 2 \left( \int \mathrm{dA}\, \left[n\ln (n/n_0) -(n-n_0)\right]^{1/2}\sqrt{n}{\phi_y}/{B}\right)^2\nonumber\\
 
  &\leq 4 S(0) E(t)/m_i 
 
  \label{eq:inequality}
\end{align}
Here we use the Cauchy-Schwartz inequality and 
$\phi_y:=\partial\phi/\partial y$. 
Note that although we derive the inequality Eq.~\eqref{eq:inequality} only for amplitudes $\triangle n >0$  we assume that the results also hold for depletions. This is justified by our numerical results later in this letter. 
If we initialize our density field with a seeded blob of radius $\ell$ and amplitude $\triangle n$ as 
\begin{align}
  n(\vec x, 0) &= n_0 + \triangle n \exp\left( -\frac{\vec x^2}{2\ell^2} \right), \label{eq:inita}
 
 
\end{align}
and  
$\phi(\vec x, 0 ) = 0$,
we immediately have $M := M(0) = 2\pi \ell^2 \triangle n$, $E(0) = G(0) = 0$ and 
$S(0) = 2\pi \ell^2 f(\triangle n)$, where $f(\triangle n)$ captures the amplitude dependence of 
the integral for $S(0)$. 

The acceleration for both incompressible and compressible flows can be estimated
by assuming a linear acceleration $V=A_0t$ and $X=A_0t^2/2$~\cite{Held2016a} and using 
$E(t) = G(t) = m_igMX(t)$ in Eq.~\eqref{eq:inequality}
\begin{align}
  \frac{A_0}{g} =  \mathcal Q\frac{2S(0)}{M} \approx \frac{\mathcal Q}{2} \frac{\triangle n }{n_0+2\triangle n/9}.
  \label{eq:acceleration}
\end{align}
Here, we use the Pad\'e approximation of order $(1/1)$ of $2S(0)/M $
and define a model parameter $\mathcal Q$ with $0<\mathcal Q\leq1$ to be determined by numerical simulations.
Note that the Pad\'e approximation is a better approximation than a simple 
truncated Taylor expansion especially for large relative amplitudes of order unity.
Eq.~\eqref{eq:acceleration} predicts that $A_0/g\sim \triangle n/n_0$ for small 
amplitudes $|\triangle n/n_0| < 1$ and $A_0 \sim g $ for very large amplitudes $\triangle n /n_0 \gg 1$, 
which confirms the predictions in~\cite{Pecseli2016} and reproduces the limits discussed in~\cite{Angus2014}.

As pointed out earlier for compressible flows Eq.~\eqref{eq:inequality} can be further estimated
\begin{align}
  (MV)^2  \leq 4 T_eS(0)^2/m_i. 
  \label{}
\end{align}
We therefore have a restriction on the maximum COM velocity for compressible flows, which is absent for incompressible flows
\begin{align}
  \frac{\max |V|}{\ensuremath{C_\mathrm{s}}} = {\mathcal Q}\frac{2S(0)}{M} \approx \frac{\mathcal Q}{2} \frac{|\triangle n| }{n_0+2/9 \triangle n } \approx \frac{\mathcal Q}{2} \frac{|\triangle n|}{n_0}.
  \label{eq:linear}
\end{align}
For $|\triangle n /n_0|< 1$ Eq.~\eqref{eq:linear} reduces to the linear scaling derived in~\cite{Kube2016}. 
Finally, a scale analysis of Eq.~\eqref{eq:vorticity} shows that~\cite{Ott1978, Garcia2005, Held2016a}
\begin{align}
  \frac{\max |V|}{\ensuremath{C_\mathrm{s}}} = \mathcal R \left( \frac{\ell}{R_0}\frac{|\triangle n|}{n_0} \right)^{1/2}.
  \label{eq:sqrt}
\end{align}
This equation predicts a square root dependence of the center of mass velocity 
on amplitude and size. 





We now propose a simple phenomenological model that captures the essential dynamics
of blobs and depletions in the previously stated systems. More specifically 
the model reproduces the acceleration Eq.~\eqref{eq:acceleration} with and without
Boussinesq approximation, the square root scaling for the COM velocity 
Eq.~\eqref{eq:sqrt} for incompressible flows as well as the relation between the 
square root scaling Eq.~\eqref{eq:sqrt} and the linear scaling 
Eq.~\eqref{eq:linear} for compressible flows. 
The basic idea is that the COM of blobs behaves like 
the one of an infinitely long plasma column immersed in an ambient plasma. 
The dynamics of this column reduces to the one of a two-dimensional ball.
This idea is similar to the analytical ``top hat'' density solution for
blob dynamics recently studied in~\cite{Pecseli2016}.
The ball is subject to buoyancy as well as linear and nonlinear friction
\begin{align}
  M_{\text{i}} \frac{d V}{d t} = (M_{\text{g}} - M_\text{p}) g - c_1 V  - \mathrm{sgn}(V ) \frac{1}{2}c_2 V^2.
  \label{eq:ball}
\end{align}
The gravity $g$ has a positive sign in the coordinate system; sgn$(f)$ is the sign function. 
The first term on the right hand side is the buoyancy, where 
$M_{\text{g}} := \pi \ell^2 (n_0 + \mathcal Q \triangle n/2)$ 
is the gravitational mass of the ball with radius $\ell$ and 
$M_\mathrm{p} := n_0 \pi \ell^2 $ 
is the mass of the displaced ambient plasma.
Note that if $\triangle n<0$ the ball represents a depletion and the buoyancy term has a negative sign, i.e. the depletion will rise. 
We introduce an inertial mass 
$M_{\text{i}} := \pi\ell^2 (n_0 +2\triangle n/9)$ 
different from the gravitational mass $M_{\text{g}}$ in order to 
recover the initial acceleration in Eq.~\eqref{eq:acceleration}. 
We interpret the parameters $\mathcal Q$ and $2/9$ as geometrical factors 
that capture the difference of the actual blob form from the idealized
``top hat'' solution. 
Also note that the Boussinesq approximation appears in the model as a neglect of inertia, $M_{\text{i}} = \pi\ell^2n_0$.

The second term is the linear friction term with coefficient $c_1(\ell)$, which
depends on the size of the ball.
If we disregard the nonlinear friction, $c_2=0$, Eq.~\eqref{eq:ball} directly yields a 
maximum velocity $c_1V^*=\pi \ell^2 n g \mathcal Q\triangle n/2$.
From our previous considerations $\max V/\ensuremath{C_\mathrm{s}}=\mathcal Q \triangle n /2n_0$, we thus identify 
\begin{align}
  c_1 = \pi\ell^2 n_0 g/\ensuremath{C_\mathrm{s}}. 
  \label{}
\end{align}
The linear friction coefficient thus depends on the gravity and the size of the
ball. 

The last term in \eqref{eq:ball} is the nonlinear friction. The sign of the force depends on whether
the ball rises or falls in the ambient plasma. 
If we disregard linear friction $c_1=0$, we have the maximum velocity 
$V^*= \sigma(\triangle n)\sqrt{\pi \ell^2|\triangle n| g\mathcal Q/c_2}$, 
which must equal 
$\max V= \sigma(\triangle n) \mathcal R \sqrt{g \ell |\triangle n/n_0|}$ 
and thus
\begin{align}
  c_2 = {\mathcal Q\pi n_0\ell }/{\mathcal R^2}.
  \label{}
\end{align}
Inserting $c_1$ and $c_2$ into Eq.~\eqref{eq:ball}
we can derive the maximum absolute velocity in the form 
\begin{align}
  \frac{\max |V|}{\ensuremath{C_\mathrm{s}}} = 
        \left(\frac{\mathcal R^2}{\mathcal Q}\right) \frac{\ell}{R_0} \left( 
        \left({1+\left( \frac{\mathcal Q}{\mathcal R} \right)^{2} \frac{|\triangle n|/n_0 }{\ell/R_0}}\right)^{1/2}-1 \right)
  \label{eq:vmax_theo}
\end{align}
and thus have a concise expression for $\max |V|$ that captures both the linear
scaling \eqref{eq:linear} as well as the square root scaling \eqref{eq:sqrt}.
With Eq.~\eqref{eq:acceleration} and Eq.~\eqref{eq:sqrt} respectively Eq.~\eqref{eq:vmax_theo} we 
finally arrive at an analytical expression for the time at which the maximum velocity is reached via 
$t_{\max V} \sim \max V/A_0$. Its inverse $\gamma:=t_{\max V}^{-1}$ gives the
global interchange growth rate, for which an empirical expression was
presented in Reference~\cite{Held2016a}.


We use the open source library FELTOR 
to simulate 
Eqs.~\eqref{eq:generala} and \eqref{eq:vorticity} with and without 
drift compression.
For numerical stabilty we added small diffusive terms on the right hand 
sides of the equations.
The discontinuous Galerkin methods employ three polynomial coefficients and a minimum of $N_x=N_y=768$ grid cells. The box size is $50\ell$ in order to mitigate 
influences of the finite box size on the blob dynamics. 
Moreover, we used the invariants in Eqs. \eqref{eq:energya} and \eqref{eq:energyb} as consistency tests to verify the code and repeated simulations 
also in a gyrofluid model. 
No differences to the results presented here were found. 
Initial perturbations on the particle density field are given by Eq.~\eqref{eq:inita},
where the perturbation amplitude $\triangle n/n_0$ was chosen between $10^{-3}$ and $20$ for blobs and $-10^0$ and $ -10^{-3}$ for depletions. 
Due to computational reasons we show results only for $\triangle n/n_0\leq 20$. 


For compressible flows we consider two different cases $\ell/R_0 = 10^{-2}$ and
$\ell /R_0 = 10^{-3}$. 
 For incompressible flows Eq.~\eqref{eq:generala} and \eqref{eq:vorticity}
 can be normalized such that the blob radius is absent from the equations~\cite{Ott1978, Kube2012}. 
 The simulations of incompressible flows can thus be used for both sizes. 
The numerical code as well as input parameters and output data can be found 
in the supplemental dataset to this contribution~\cite{Data2017}.

\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{com_blobs}
    \caption{
      The maximum radial COM velocities of blobs for compressible and incompressible flows are shown. 
      The continuous lines show Eq.~\eqref{eq:vmax_theo} while the 
      dashed line shows the square root scaling Eq.~\eqref{eq:sqrt} with 
      $\mathcal Q = 0.32$ and $\mathcal R=0.85$.
    }
    \label{fig:com_blobs}
\end{figure}
In Fig.~\ref{fig:com_blobs} we plot the maximum COM velocity for blobs 
with and without drift compression.
For incompressible flows blobs follow the square root scaling almost 
perfectly. Only at very large amplitudes velocities are slightly below
the predicted values. 
For small amplitudes we observe that the compressible blobs follow
a linear scaling. When the amplitudes increase there is a transition to the
square root scaling at around $\triangle n/n_0 \simeq 0.5$ for 
$\ell/R_0=10^{-2}$ and $\triangle n/n_0 \simeq 0.05$ for $\ell/R_0=10^{-3}$, which is consistent with Eq.~\eqref{eq:vmax_theo} and Reference~\cite{Kube2016}. 
In the transition regions the simulated velocities are slightly larger than the predicted ones from Eq.~\eqref{eq:vmax_theo}.
Beyond these amplitudes
the velocities of compressible and incompressible blobs align. 

\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{com_holes}
    \caption{
      The maximum radial COM velocities of depletions for compressible and incompressible flows are shown. 
      The continuous lines show Eq.~\eqref{eq:vmax_theo} while the 
      dashed line shows the square root scaling Eq.~\eqref{eq:sqrt} with 
      $\mathcal Q = 0.32$ and $\mathcal R=0.85$.
      Note that small amplitudes are on the right and amplitudes close to unity are on the left side.
  }
    \label{fig:com_depletions}
\end{figure}
In Fig.~\ref{fig:com_depletions} we show the maximum radial COM velocity 
for depletions instead of blobs.
For relative amplitudes below $|\triangle n|/n_0 \simeq 0.5$ (right of unity in the plot) the velocities
coincide with the corresponding blob velocities in Fig.~\ref{fig:com_blobs}. 
 For amplitudes larger than $|\triangle n|/n_0\simeq 0.5$ the 
velocities follow the square root scaling.
We observe that for plasma depletions beyond $90$ percent the velocities 
in both systems reach a constant value that is very well predicted by the
square root scaling. 

\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{acc_blobs}
    \caption{
      Average acceleration of blobs for compressible and incompressible flows are shown.
      The continuous line shows the acceleration in Eq.~\eqref{eq:acceleration} 
      with $\mathcal Q=0.32$
      while the dashed line is a linear reference line, which corresponds to the Boussinesq approximation. 
  }
    \label{fig:acc_blobs}
\end{figure}
In Fig.~\ref{fig:acc_blobs} we show the average acceleration of blobs 
for compressible and incompressible flows computed
by dividing the maximum velocity $\max V$ by the time  
to reach this velocity $t_{\max V}$. 
We compare the simulation results
to the theoretical predictions Eq.~\eqref{eq:acceleration} of our model with and without inertia. 
The results of the compressible and incompressible systems coincide and fit very
well to our theoretical values. 
For amplitudes larger than unity the acceleration deviates significantly from the prediction with Boussinesq approximation.

\begin{figure}[htb]
    \includegraphics[width=\columnwidth]{acc_holes}
    \caption{
      Average acceleration of depletions for compressible and incompressible flows are shown.
      The continuous line shows the acceleration in Eq.~\eqref{eq:acceleration} 
      with $\mathcal Q=0.32$
      while the dashed line is a linear reference line, which corresponds to the Boussinesq approximation. 
    }
    \label{fig:acc_depletions}
\end{figure}
In Fig.~\ref{fig:acc_depletions} we show the simulated acceleration of depletions in the
compressible and the incompressible systems. We compare the simulation results
to the theoretical predictions Eq.~\eqref{eq:acceleration} of our model with and without inertia.
Deviations from our theoretical prediction Eq.~\eqref{eq:acceleration} are visible for amplitudes smaller than $\triangle n/n_0 \simeq -0.5$ (left of unity in the plot). The relative deviations are small at around $20$ percent. 
As in Fig.~\ref{fig:com_depletions} the acceleration reaches a constant values
for plasma depletions of more than $90$ percent.
Comparing Fig.~\ref{fig:acc_depletions} to Fig.~\ref{fig:acc_blobs} the asymmetry between blobs and depletions becomes 
apparent. While the acceleration of blobs is reduced for large 
amplitudes compared to a linear dependence the acceleration 
of depletions is increased. In the language of our simple buoyancy 
model the inertia of depletions is reduced but increased for blobs. 



In conclusion  
  we discuss the dynamics of seeded blobs and depletions in a 
  compressible and an incompressible system.
  With only two fit parameters our theoretical results reproduce the 
  numerical COM velocities and accelerations over five orders of magnitude.
  We derive the amplitude dependence of the acceleration of blobs and depletions from 
  the conservation laws of our systems in Eq.~\eqref{eq:acceleration}. 
  From the same inequality a linear regime is derived in the compressible system for 
  ratios of amplitudes to sizes smaller than a critical value.
   In this regime 
  the blob and depletion velocity depends linearly on the initial amplitude and 
  is independent of size. The regime is absent from the system with incompressible flows.
  Our theoretical results are verified by numerical simulations for all 
  amplitudes that are relevant in magnetic fusion devices.
  Finally, we suggest a new empirical blob model that captures the detailed dynamics of more complicated models. 
  The Boussinesq approximation is clarified as the absence of inertia and a thus altered acceleration of blobs and depletions.
  The maximum blob velocity is not altered by the Boussinesq approximation.

The authors were supported with financial subvention from the Research Council of Norway under grant
240510/F20. M.W. and M.H. were supported by the Austrian Science Fund (FWF) Y398.  The computational
results presented have been achieved in part using the Vienna Scientific Cluster (VSC). Part of this work was performed on the Abel Cluster, owned by the University of Oslo and the Norwegian metacenter
for High Performance Computing (NOTUR), and operated by the Department for Research Computing at USIT,
the University of Oslo IT-department.
This work has been carried out within the framework of the EUROfusion Consortium and has received funding from the Euratom research and training programme 2014-2018 under grant agreement No 633053. The views and opinions expressed herein do not necessarily reflect those of the European Commission.",['The maximum velocity scales with the square root of the amplitude.'],2748,multifieldqa_en,en,,7a503a81877d3baca86f8d7179209e4899823433ab3326f3," In drift-fluid models the continuity equation is closed by invoking quasineutrality, i.e. the divergence of the ion polarization,  the electron diamagnetic and the gravitational drift currents must vanish. We exploit this form of Eq.~\eqref{eq:generalb} in our numerical simulations. The relative particle number $M(t) is conserved over time and is called the relative energy of the system. We integrate the model using the following equations: Eq.   “EQ. ”, “E.’, ‘E. ’,” and “’”. We also use the equations ‘’ and ’’ to represent the charge density and charge height of the charged particles. We conclude that the model’s equations can be used to calculate the energy and charge of a charged particle in terms of the charge height and charge weight of the electrons. We call this the ‘energy’ equation ‘e’. The ‘vorticity’ function ‘V’ is used to measure the energy of a particle in the model. We use the equation ’s ’E’ as well as the equations’ ’C’ for ‘voltage’ (‘v’) and ‘f’(’F’), ‘g’,. ’H’). The ’v”’f” function ’c’ includes the charge velocity, charge height, charge weight, and charge density of the particles in a magnetic field and a gravitational drift velocity. We. integrate the equations by adding the charge and velocity of the ions in the magnetic field, the charge weight and the ion gyro-frequency to get the energy. We then use these equations to estimate the energy, charge and mass of a given particle in a given magnetic field. The results are shown in the table below. The equations are based on the equations for the charge, mass, and gravitational drift velocities of electrons in a magnetized magnetic field with an electricpotential of $n$ and an electric polarization charge of $N/N. The model is based on a model of the electron density ‘n’ with an energy of $T_e$ and a charge velocity of $t_e/2e. The energy is expressed as a function of the particle mass $m_i$ and the polarization charge density $n/N, and the charge length is $t-1/2/2E. We show that this is a simple formula for the electric drift velocity of an electrons’ electron density. We can also use this equation to work out how much energy is present in the current magnetic field in a particular magnetic field ‘N’ in a specific magnetic field (B) and how long it takes to charge the electrons to travel through it. The equation is called ‘The Eq’ of the model, and it is written as: ‘T’ = ‘t’ + ‘B’ , ‘C�’ & ‘G’ . We. use the ’A’ operator to denote that the current is ‘m’ or ‘b’ so that we can calculate the current energy of an electron in the same magnetic field as the charge charge density. ference to the model describing dynamics in a homogeneous magnetic field introduced above. The kinetic energy of a plasma blobs is bounded by $E(t) \leq T_eS(t), where $S( t) = S(0)$. The acceleration for both incompressible and compressible flows can be estimated by assuming a linear acceleration $V=A_0tt and $X_0_t/2$ for both systems. We assumethat $n>n_0$ and $n-n-0)^2/2 is the total radial particle flux. We use the Cauchy-Schwartz inequality and $phi_y:=\partial\phi/\partial y$ for our density field. We also use the Padinese approximation of order of order $1/1/2/1, where $M=M(0), $E=E, and $G=G. The results also hold for depletions, which is justified by our numerical results later in this letter. We conclude that our model is correct for both the gravitational and the compressible systems, and that the model is more accurate for the gravitational system than for the compressable one. The model is now ready to be used in the next section of this article. We hope to use the model to explain the effects of the magnetic field on the density field in a more general way. We will also use it to explain how the magnetic density field affects the density of a given plasma blob. We are now ready for the next part of the article, which will focus on the dynamics of a magnetic field with a seeded blob of radius $\ell$ and amplitude $\triangle n$ as well as its effects on density field dynamics. We expect to use this model to help us understand how magnetic fields interact with the magnetic fields in the real world. The next section will look at how these interactions affect the density fields in a number of different ways, including how they interact with each other and with the particle flux in a given density field, such as by using the Padini approximation of the radial particle density. We end up with a model that is more general than the previous one, and we hope to show that it can be used to explain some of the consequences of magnetic fields on density fields. We have already used this model in the previous section to show how the field can be applied to the plasma. We now want to use a different model to understand the dynamics in the universe in a different way, and this will be shown in the following section. We believe that this model can provide a more complete understanding of the plasma dynamics of the current magnetic field and its effects in a real world environment. We call this model the ""magnetic magnetic field model"" and it will be called the ""model of plasma dynamics"" in the second section of the paper. We then show that the velocity and acceleration of the blobs can be calculated using the following equations: $V(t):=\d X(t)/\d t, $X(t:= x(n- n_0)/M, $S (t) = X(0):= X( t), and $E (0) = E(t/m_i) = G (t), $G (t): = G(t = A) = A, and  E (t)/m_t = G, $F(t)(t)= A, $A) = M, $E() = G,. $G(t%) = A,. $M( t):= M(0)(A) & $E) = 2\pi \ell^2 f(A), $F (T) = 1, $M (t)(T) + F (T), $S) = T, $T(t). We now propose a simple phenomenological model that captures the essential dynamics of blobs and depletions in the previously stated systems. The model reproduces the acceleration with and without the Boussinesq approximation. The dynamics of this column reduces to the one of a two-dimensional ball. The ball is subject to buoyancy as well as linear and nonlinear friction. The force of the force depends on whether the ball falls in the ambient plasma or rises in the plasma. We have the maximum velocity $c_1V^*=\pi \ell^2 n g \mathcal Q\triangle n/2$, which must equal equal to the mass velocity $V*= c_1 v/2 g/\ensuremath{C_\mathrm{s}}. The friction coefficient thus depends on the size of the ball and on the sign of the gravity and the friction coefficient. We conclude that the model captures the dynamics of an infinitely long plasma column immersed in an ambient plasma in the form of a ball. We call this the ""top hat'' density solution for Blob dynamics, which was recently studied in a paper by Pecseli2016. We hope the model will be of interest to researchers in the field of plasma physics and the physics of the plasma environment in general. Back to Mail Online home. back to the page you came from.  ""Theory of Blob Dynamics: Theory of Theories of Blobs, Depletions, and Theoretical Modeling of the Blob Density""  ""Pec Seli 2016""  http://www.pecselianet.com/news/features/top-hat-dynamics-of-blobs-and-depletions-theory-of blob-dynamic.html#storylink=cpy. ""Theory of Blob Dynamics of Blobs and Depletions in the Plasma-Plasma Pl plasma"" ""Aims to explain the dynamics of blobs in the plasma-plasma  Plasma"" ""Theoretical Modeling of the Blobs"" ""Briefly, we look at how the dynamics  of the blobs behave in the physically similar to that of an infinitely long pl plasma column"" ""We also look at how this dynamically can be explained in terms of the inertial mass"" ""This model is similar to the analytical `` top hat''density solution forblob dynamics recently studied in~\cite{Pecselsi2016}."" ""It can also be used to explore the physics of plasma dynamics in a number of other relativistic situations"" ""How do we exercise this model?"" ""To examine this model, we first expose the relationship between the gravitational mass and the friction coefficient"" ""And from there we discuss how this can be transformed into a differently dramatic model"" ""What are the properties of this model?"" ""What do you think?"" ""Let's examined this. model reveal the Dynamics of a blob in the environment."" ""What does this mean?"" We use the open source library FELTOR to simulate the global interchange growth rate. The maximum radial COM velocities of blobs for compressible and incompressible flows are shown. We plot the maximum COM velocity for blobs  with and without drift compression. The numerical code as well as input parameters and output data can be found in the supplemental dataset to this contribution. We show results only for $\triangle n/n_0\leq 20$ for depletions. For compressible flows we consider two different cases $10/R_0 = 10^{-2}$ and $20/R-3 = 10-3. We use the invariants in Eqs. \eqref{eq:energya} and    ‘vorticity’ as consistency tests to verify the code and repeated simulations in a gyrofluid model. No differences to the results presented here were found. The results are consistent with Eq. ’Vmax_theo’ and Reference~’Kube2016a’, which is consistent with the predicted values from  Eq.~”Vmax” and Reference’s ‘Vmax.’. We also show the square root scaling of the COM velocity of the blobs in both compressible flow and depletion flow. We observe that the compressible blobs follow a linear scaling when the amplitudes increase. Only at very large amplitudes velocity are slightly below predicted values. We conclude that the results are not consistent with ’’ Vmax’ or ’V max’ which is the predicted value of the ‘vmax�’ number of ‘‘’ in the equations. We are happy with the results for both types of flow, but not for the incompressable flow. The simulations of incompressibly flows can thus be used for both sizes. In the transition regions the simulated veloities are slightly larger than the ones predicted from Eq:’vmax.theglobal interchange growth Rate’  for which an empirical expression was.presented in Reference~\cite{Held2016a}. The results for incompressibility are shown in Fig. 1. The continuous lines show the continuous lines and the dashed line shows the dashed lines for the sqrt scaling of COM velocity. For incompressibles blobs, the lines are shown  almost perfectly. The dashed lines show  the continuous line and the continuous. line for the squareroot scaling. for the depletION flow. For the compressibility flows, the dashed. line shows  the dashedline shows the line and dashed line of the square. root scaling for the com velocity. For depletIONS, the continuous and continuous lines. are shown for both depletible flows and for the compression flows. For all flows, we show the maximum radial. COM velOCities of depletioned blobs are shown, as well. as the dashed dashed line. for compressibles flows and compressible. flows for incompressesible flows for the compressibles. The total. COM velocity is shown in the figure below. It is shown that the maximum. COM speed is greater than the predicted. values for both compressibles and incompressed flows. The. maximum radial speed is also shown for the compressed flows. for both. compressible flowing flows and the compressable flows for deleted flows. This figure is based on the equations ‘V max ’ and ‘ V max ‘ and ’. The authors used the Boussinesq approximation to calculate the acceleration of blobs and depletions in a compressible and an incompressible system. They compared the simulation results with and without inertia. The authors suggest a new empirical model that captures the detailed dynamics of more complicated models that are relevant in magnetic fusion devices. They also suggest that the maximum velocity of the blob is not altered by the subsq approximation. The results of the simulations were supported with financial support from the authors' respective foundations. The study was published in the journal Applied Physics Letters (ASL) (2013) (accessed via the arxiv.org/v1.0.2.1). The paper is open-source and can be downloaded as a hard copy for $20 ($20) or $25 ($25) per page. The author's website is: http://www.arxiv-org/reader/article/13/20/3157/3158/3159/3155/3153/3154/3145/3152/3151/3156.html. The abstract is: “The acceleration of the blobs is reduced for large amplitudes compared to a linear dependence. The inertia of depletion is reduced but increased for blobs.” The authors’ paper is also available as a free download from the arXiv (2012) (www. arXI.org) (http://www.-arXiv.com/reader/. The paper’s abstract is available here: http:www. Arxiv:12.04.1/1157/2157.1.1/. The abstract has been updated to reflect that the authors have now published a new version of the article with the title ‘The Acceleration of the Blobs’ (2014) instead of ‘the Acceleration Of The Blobs,’ with the emphasis on the ‘Blob’. The article has been amended to make clear that the “Blobs” part of the term ‘blobs�’ is not the same as ‘depletions’ as the authors had previously stated. The ‘Depletion’ section of this article has also been updated from ‘depletions” to “Depletions.’ to make it clear that this is a more general term for ‘plasma’ rather than ‘ plasma’, and that the author has not changed the meaning of the word “plasma. ” The article also has a new title, ‘Dynamics of seeded blobs in a magnetic fusion device’ and ‘Plasma Fusion Device (PFD) (2014’) (see: ‘PfD’ for details). The authors have also updated the title to ‘Magnetic Fusion Devices (MFV) to reflect the fact that PfD is a non-magnetic fusion device (MV) and not a ‘m fusion device) (‘MV’ means ‘polar fusion’ or ‘magnetic field’). They have also added a new ‘Fusion Device (FPD) section to discuss the dynamics of seeded Blobs and Depletions (PfV) in a MFV that is more complex than the previous ‘MFD” (“PfF”). The article is also titled ‘Inertia of a Magnetic Fusion Device’ This work has been carried out within the framework of the EUROfusion Consortium. It has received funding from the Euratom research and training programme 2014-2018 under grant agreement No 633053. The views and opinions expressed herein do not necessarily reflect those of the European Commission. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org."
What do dendritic spines contain?,"JoVE | Peer Reviewed Scientific Video Journal - Methods and Protocols
A role for thrombospondin-1 deficits in astrocyte-mediated spine and synaptic pathology in Downs syndrome. Octavio Garcia, Maria Torres, Pablo Helguera, Pinar Coskun, Jorge Busciglio.
PUBLISHED: 07-02-2010	Downs syndrome (DS) is the most common genetic cause of mental retardation. Reduced number and aberrant architecture of dendritic spines are common features of DS neuropathology. However, the mechanisms involved in DS spine alterations are not known. In addition to a relevant role in synapse formation and maintenance, astrocytes can regulate spine dynamics by releasing soluble factors or by physical contact with neurons. We have previously shown impaired mitochondrial function in DS astrocytes leading to metabolic alterations in protein processing and secretion. In this study, we investigated whether deficits in astrocyte function contribute to DS spine pathology.
Analysis of Dendritic Spine Morphology in Cultured CNS Neurons Authors: Deepak P. Srivastava, Kevin M. Woolfrey, Peter Penzes. Published: 07-13-2011 JoVE Neuroscience
Dendritic spines are the sites of the majority of excitatory connections within the brain, and form the post-synaptic compartment of synapses. These structures are rich in actin and have been shown to be highly dynamic. In response to classical Hebbian plasticity as well as neuromodulatory signals, dendritic spines can change shape and number, which is thought to be critical for the refinement of neural circuits and the processing and storage of information within the brain. Within dendritic spines, a complex network of proteins link extracellular signals with the actin cyctoskeleton allowing for control of dendritic spine morphology and number. Neuropathological studies have demonstrated that a number of disease states, ranging from schizophrenia to autism spectrum disorders, display abnormal dendritic spine morphology or numbers. Moreover, recent genetic studies have identified mutations in numerous genes that encode synaptic proteins, leading to suggestions that these proteins may contribute to aberrant spine plasticity that, in part, underlie the pathophysiology of these disorders. In order to study the potential role of these proteins in controlling dendritic spine morphologies/number, the use of cultured cortical neurons offers several advantages. Firstly, this system allows for high-resolution imaging of dendritic spines in fixed cells as well as time-lapse imaging of live cells. Secondly, this in vitro system allows for easy manipulation of protein function by expression of mutant proteins, knockdown by shRNA constructs, or pharmacological treatments. These techniques allow researchers to begin to dissect the role of disease-associated proteins and to predict how mutations of these proteins may function in vivo.
Play ButtonIsolation and Culture of Mouse Cortical AstrocytesAuthors: Sebastian Schildge, Christian Bohrer, Kristina Beck, Christian Schachtrup. Institutions: University of Freiburg , University of Freiburg .Astrocytes are an abundant cell type in the mammalian brain, yet much remains to be learned about their molecular and functional characteristics. In vitro astrocyte cell culture systems can be used to study the biological functions of these glial cells in detail. This video protocol shows how to obtain pure astrocytes by isolation and culture of mixed cortical cells of mouse pups. The method is based on the absence of viable neurons and the separation of astrocytes, oligodendrocytes and microglia, the three main glial cell populations of the central nervous system, in culture. Representative images during the first days of culture demonstrate the presence of a mixed cell population and indicate the timepoint, when astrocytes become confluent and should be separated from microglia and oligodendrocytes. Moreover, we demonstrate purity and astrocytic morphology of cultured astrocytes using immunocytochemical stainings for well established and newly described astrocyte markers. This culture system can be easily used to obtain pure mouse astrocytes and astrocyte-conditioned medium for studying various aspects of astrocyte biology.Neuroscience, Issue 71, Neurobiology, Cellular Biology, Medicine, Molecular Biology, Anatomy, Physiology, brain, mouse, astrocyte culture, astrocyte, fibroblast, fibrinogen, chondroitin sulfate proteoglycan, neuronal regeneration, cell culture, animal model50079Play ButtonImaging Dendritic Spines of Rat Primary Hippocampal Neurons using Structured Illumination MicroscopyAuthors: Marijn Schouten, Giulia M. R. De Luca, Diana K. Alatriste González, Babette E. de Jong, Wendy Timmermans, Hui Xiong, Harm Krugers, Erik M. M. Manders, Carlos P. Fitzsimons. Institutions: University of Amsterdam, University of Amsterdam.Dendritic spines are protrusions emerging from the dendrite of a neuron and represent the primary postsynaptic targets of excitatory inputs in the brain. Technological advances have identified these structures as key elements in neuron connectivity and synaptic plasticity. The quantitative analysis of spine morphology using light microscopy remains an essential problem due to technical limitations associated with light's intrinsic refraction limit. Dendritic spines can be readily identified by confocal laser-scanning fluorescence microscopy. However, measuring subtle changes in the shape and size of spines is difficult because spine dimensions other than length are usually smaller than conventional optical resolution fixed by light microscopy's theoretical resolution limit of 200 nm.
Several recently developed super resolution techniques have been used to image cellular structures smaller than the 200 nm, including dendritic spines. These techniques are based on classical far-field operations and therefore allow the use of existing sample preparation methods and to image beyond the surface of a specimen. Described here is a working protocol to apply super resolution structured illumination microscopy (SIM) to the imaging of dendritic spines in primary hippocampal neuron cultures. Possible applications of SIM overlap with those of confocal microscopy. However, the two techniques present different applicability. SIM offers higher effective lateral resolution, while confocal microscopy, due to the usage of a physical pinhole, achieves resolution improvement at the expense of removal of out of focus light. In this protocol, primary neurons are cultured on glass coverslips using a standard protocol, transfected with DNA plasmids encoding fluorescent proteins and imaged using SIM. The whole protocol described herein takes approximately 2 weeks, because dendritic spines are imaged after 16-17 days in vitro, when dendritic development is optimal. After completion of the protocol, dendritic spines can be reconstructed in 3D from series of SIM image stacks using specialized software.Neuroscience, Issue 87, Dendritic Spine, Microscopy, Confocal, Fluorescence, Neurosciences, hippocampus, primary neuron, super resolution microscopy, structured illumination microscopy (SIM), neuroscience, dendrite51276Play ButtonSetting-up an In Vitro Model of Rat Blood-brain Barrier (BBB): A Focus on BBB Impermeability and Receptor-mediated TransportAuthors: Yves Molino, Françoise Jabès, Emmanuelle Lacassagne, Nicolas Gaudin, Michel Khrestchatisky. Institutions: VECT-HORUS SAS, CNRS, NICN UMR 7259.The blood brain barrier (BBB) specifically regulates molecular and cellular flux between the blood and the nervous tissue. Our aim was to develop and characterize a highly reproducible rat syngeneic in vitro model of the BBB using co-cultures of primary rat brain endothelial cells (RBEC) and astrocytes to study receptors involved in transcytosis across the endothelial cell monolayer. Astrocytes were isolated by mechanical dissection following trypsin digestion and were frozen for later co-culture. RBEC were isolated from 5-week-old rat cortices. The brains were cleaned of meninges and white matter, and mechanically dissociated following enzymatic digestion. Thereafter, the tissue homogenate was centrifuged in bovine serum albumin to separate vessel fragments from nervous tissue. The vessel fragments underwent a second enzymatic digestion to free endothelial cells from their extracellular matrix. The remaining contaminating cells such as pericytes were further eliminated by plating the microvessel fragments in puromycin-containing medium. They were then passaged onto filters for co-culture with astrocytes grown on the bottom of the wells. RBEC expressed high levels of tight junction (TJ) proteins such as occludin, claudin-5 and ZO-1 with a typical localization at the cell borders. The transendothelial electrical resistance (TEER) of brain endothelial monolayers, indicating the tightness of TJs reached 300 ohm·cm2 on average. The endothelial permeability coefficients (Pe) for lucifer yellow (LY) was highly reproducible with an average of 0.26 ± 0.11 x 10-3 cm/min. Brain endothelial cells organized in monolayers expressed the efflux transporter P-glycoprotein (P-gp), showed a polarized transport of rhodamine 123, a ligand for P-gp, and showed specific transport of transferrin-Cy3 and DiILDL across the endothelial cell monolayer. In conclusion, we provide a protocol for setting up an in vitro BBB model that is highly reproducible due to the quality assurance methods, and that is suitable for research on BBB transporters and receptors.Medicine, Issue 88, rat brain endothelial cells (RBEC), mouse, spinal cord, tight junction (TJ), receptor-mediated transport (RMT), low density lipoprotein (LDL), LDLR, transferrin, TfR, P-glycoprotein (P-gp), transendothelial electrical resistance (TEER),51278Play ButtonInducing Plasticity of Astrocytic Receptors by Manipulation of Neuronal Firing RatesAuthors: Alison X. Xie, Kelli Lauderdale, Thomas Murphy, Timothy L. Myers, Todd A. Fiacco. Institutions: University of California Riverside, University of California Riverside, University of California Riverside.Close to two decades of research has established that astrocytes in situ and in vivo express numerous G protein-coupled receptors (GPCRs) that can be stimulated by neuronally-released transmitter. However, the ability of astrocytic receptors to exhibit plasticity in response to changes in neuronal activity has received little attention. Here we describe a model system that can be used to globally scale up or down astrocytic group I metabotropic glutamate receptors (mGluRs) in acute brain slices. Included are methods on how to prepare parasagittal hippocampal slices, construct chambers suitable for long-term slice incubation, bidirectionally manipulate neuronal action potential frequency, load astrocytes and astrocyte processes with fluorescent Ca2+ indicator, and measure changes in astrocytic Gq GPCR activity by recording spontaneous and evoked astrocyte Ca2+ events using confocal microscopy. In essence, a “calcium roadmap” is provided for how to measure plasticity of astrocytic Gq GPCRs. Applications of the technique for study of astrocytes are discussed. Having an understanding of how astrocytic receptor signaling is affected by changes in neuronal activity has important implications for both normal synaptic function as well as processes underlying neurological disorders and neurodegenerative disease.Neuroscience, Issue 85, astrocyte, plasticity, mGluRs, neuronal Firing, electrophysiology, Gq GPCRs, Bolus-loading, calcium, microdomains, acute slices, Hippocampus, mouse51458Play ButtonInhibitory Synapse Formation in a Co-culture Model Incorporating GABAergic Medium Spiny Neurons and HEK293 Cells Stably Expressing GABAA ReceptorsAuthors: Laura E. Brown, Celine Fuchs, Martin W. Nicholson, F. Anne Stephenson, Alex M. Thomson, Jasmina N. Jovanovic. Institutions: University College London.Inhibitory neurons act in the central nervous system to regulate the dynamics and spatio-temporal co-ordination of neuronal networks. GABA (γ-aminobutyric acid) is the predominant inhibitory neurotransmitter in the brain. It is released from the presynaptic terminals of inhibitory neurons within highly specialized intercellular junctions known as synapses, where it binds to GABAA receptors (GABAARs) present at the plasma membrane of the synapse-receiving, postsynaptic neurons. Activation of these GABA-gated ion channels leads to influx of chloride resulting in postsynaptic potential changes that decrease the probability that these neurons will generate action potentials. During development, diverse types of inhibitory neurons with distinct morphological, electrophysiological and neurochemical characteristics have the ability to recognize their target neurons and form synapses which incorporate specific GABAARs subtypes. This principle of selective innervation of neuronal targets raises the question as to how the appropriate synaptic partners identify each other. To elucidate the underlying molecular mechanisms, a novel in vitro co-culture model system was established, in which medium spiny GABAergic neurons, a highly homogenous population of neurons isolated from the embryonic striatum, were cultured with stably transfected HEK293 cell lines that express different GABAAR subtypes. Synapses form rapidly, efficiently and selectively in this system, and are easily accessible for quantification. Our results indicate that various GABAAR subtypes differ in their ability to promote synapse formation, suggesting that this reduced in vitro model system can be used to reproduce, at least in part, the in vivo conditions required for the recognition of the appropriate synaptic partners and formation of specific synapses. Here the protocols for culturing the medium spiny neurons and generating HEK293 cells lines expressing GABAARs are first described, followed by detailed instructions on how to combine these two cell types in co-culture and analyze the formation of synaptic contacts. Neuroscience, Issue 93, Developmental neuroscience, synaptogenesis, synaptic inhibition, co-culture, stable cell lines, GABAergic, medium spiny neurons, HEK 293 cell line52115Play ButtonTwo-Photon in vivo Imaging of Dendritic Spines in the Mouse Cortex Using a Thinned-skull PreparationAuthors: Xinzhu Yu, Yi Zuo. Institutions: University of California, Santa Cruz.In the mammalian cortex, neurons form extremely complicated networks and exchange information at synapses. Changes in synaptic strength, as well as addition/removal of synapses, occur in an experience-dependent manner, providing the structural foundation of neuronal plasticity. As postsynaptic components of the most excitatory synapses in the cortex, dendritic spines are considered to be a good proxy of synapses. Taking advantages of mouse genetics and fluorescent labeling techniques, individual neurons and their synaptic structures can be labeled in the intact brain. Here we introduce a transcranial imaging protocol using two-photon laser scanning microscopy to follow fluorescently labeled postsynaptic dendritic spines over time in vivo. This protocol utilizes a thinned-skull preparation, which keeps the skull intact and avoids inflammatory effects caused by exposure of the meninges and the cortex. Therefore, images can be acquired immediately after surgery is performed. The experimental procedure can be performed repetitively over various time intervals ranging from hours to years. The application of this preparation can also be expanded to investigate different cortical regions and layers, as well as other cell types, under physiological and pathological conditions.Neuroscience, Issue 87, dendritic spine, mouse cortex, in vivo, two-photon microscopy, thinned-skull, imaging51520Play ButtonModeling Astrocytoma Pathogenesis In Vitro and In Vivo Using Cortical Astrocytes or Neural Stem Cells from Conditional, Genetically Engineered MiceAuthors: Robert S. McNeill, Ralf S. Schmid, Ryan E. Bash, Mark Vitucci, Kristen K. White, Andrea M. Werneke, Brian H. Constance, Byron Huff, C. Ryan Miller. Institutions: University of North Carolina School of Medicine, University of North Carolina School of Medicine, University of North Carolina School of Medicine, University of North Carolina School of Medicine, University of North Carolina School of Medicine, Emory University School of Medicine, University of North Carolina School of Medicine.Current astrocytoma models are limited in their ability to define the roles of oncogenic mutations in specific brain cell types during disease pathogenesis and their utility for preclinical drug development. In order to design a better model system for these applications, phenotypically wild-type cortical astrocytes and neural stem cells (NSC) from conditional, genetically engineered mice (GEM) that harbor various combinations of floxed oncogenic alleles were harvested and grown in culture. Genetic recombination was induced in vitro using adenoviral Cre-mediated recombination, resulting in expression of mutated oncogenes and deletion of tumor suppressor genes. The phenotypic consequences of these mutations were defined by measuring proliferation, transformation, and drug response in vitro. Orthotopic allograft models, whereby transformed cells are stereotactically injected into the brains of immune-competent, syngeneic littermates, were developed to define the role of oncogenic mutations and cell type on tumorigenesis in vivo. Unlike most established human glioblastoma cell line xenografts, injection of transformed GEM-derived cortical astrocytes into the brains of immune-competent littermates produced astrocytomas, including the most aggressive subtype, glioblastoma, that recapitulated the histopathological hallmarks of human astrocytomas, including diffuse invasion of normal brain parenchyma. Bioluminescence imaging of orthotopic allografts from transformed astrocytes engineered to express luciferase was utilized to monitor in vivo tumor growth over time. Thus, astrocytoma models using astrocytes and NSC harvested from GEM with conditional oncogenic alleles provide an integrated system to study the genetics and cell biology of astrocytoma pathogenesis in vitro and in vivo and may be useful in preclinical drug development for these devastating diseases.Neuroscience, Issue 90, astrocytoma, cortical astrocytes, genetically engineered mice, glioblastoma, neural stem cells, orthotopic allograft51763Play ButtonPaired Whole Cell Recordings in Organotypic Hippocampal SlicesAuthors: Chantelle Fourie, Marianna Kiraly, Daniel V. Madison, Johanna M. Montgomery. Institutions: University of Auckland, Stanford University.Pair recordings involve simultaneous whole cell patch clamp recordings from two synaptically connected neurons, enabling not only direct electrophysiological characterization of the synaptic connections between individual neurons, but also pharmacological manipulation of either the presynaptic or the postsynaptic neuron. When carried out in organotypic hippocampal slice cultures, the probability that two neurons are synaptically connected is significantly increased. This preparation readily enables identification of cell types, and the neurons maintain their morphology and properties of synaptic function similar to that in native brain tissue. A major advantage of paired whole cell recordings is the highly precise information it can provide on the properties of synaptic transmission and plasticity that are not possible with other more crude techniques utilizing extracellular axonal stimulation. Paired whole cell recordings are often perceived as too challenging to perform. While there are challenging aspects to this technique, paired recordings can be performed by anyone trained in whole cell patch clamping provided specific hardware and methodological criteria are followed. The probability of attaining synaptically connected paired recordings significantly increases with healthy organotypic slices and stable micromanipulation allowing independent attainment of pre- and postsynaptic whole cell recordings. While CA3-CA3 pyramidal cell pairs are most widely used in the organotypic slice hippocampal preparation, this technique has also been successful in CA3-CA1 pairs and can be adapted to any neurons that are synaptically connected in the same slice preparation. In this manuscript we provide the detailed methodology and requirements for establishing this technique in any laboratory equipped for electrophysiology.Neuroscience, Issue 91, hippocampus, paired recording, whole cell recording, organotypic slice, synapse, synaptic transmission, synaptic plasticity51958Play ButtonImaging Intracellular Ca2+ Signals in Striatal Astrocytes from Adult Mice Using Genetically-encoded Calcium IndicatorsAuthors: Ruotian Jiang, Martin D. Haustein, Michael V. Sofroniew, Baljit S. Khakh. Institutions: University of California Los Angeles, University of California Los Angeles.Astrocytes display spontaneous intracellular Ca2+ concentration fluctuations ([Ca2+]i) and in several settings respond to neuronal excitation with enhanced [Ca2+]i signals. It has been proposed that astrocytes in turn regulate neurons and blood vessels through calcium-dependent mechanisms, such as the release of signaling molecules. However, [Ca2+]i imaging in entire astrocytes has only recently become feasible with genetically encoded calcium indicators (GECIs) such as the GCaMP series. The use of GECIs in astrocytes now provides opportunities to study astrocyte [Ca2+]i signals in detail within model microcircuits such as the striatum, which is the largest nucleus of the basal ganglia. In the present report, detailed surgical methods to express GECIs in astrocytes in vivo, and confocal imaging approaches to record [Ca2+]i signals in striatal astrocytes in situ, are described. We highlight precautions, necessary controls and tests to determine if GECI expression is selective for astrocytes and to evaluate signs of overt astrocyte reactivity. We also describe brain slice and imaging conditions in detail that permit reliable [Ca2+]i imaging in striatal astrocytes in situ. The use of these approaches revealed the entire territories of single striatal astrocytes and spontaneous [Ca2+]i signals within their somata, branches and branchlets. The further use and expansion of these approaches in the striatum will allow for the detailed study of astrocyte [Ca2+]i signals in the striatal microcircuitry.Neuroscience, Issue 93, astrocyte, calcium, striatum, GECI, GCaMP3, AAV2/5, stereotaxic injection, brain slice, imaging51972Play ButtonMethods to Assess Subcellular Compartments of Muscle in C. elegansAuthors: Christopher J. Gaffney, Joseph J. Bass, Thomas F. Barratt, Nathaniel J. Szewczyk. Institutions: University of Nottingham.Muscle is a dynamic tissue that responds to changes in nutrition, exercise, and disease state. The loss of muscle mass and function with disease and age are significant public health burdens. We currently understand little about the genetic regulation of muscle health with disease or age. The nematode C. elegans is an established model for understanding the genomic regulation of biological processes of interest. This worm’s body wall muscles display a large degree of homology with the muscles of higher metazoan species. Since C. elegans is a transparent organism, the localization of GFP to mitochondria and sarcomeres allows visualization of these structures in vivo. Similarly, feeding animals cationic dyes, which accumulate based on the existence of a mitochondrial membrane potential, allows the assessment of mitochondrial function in vivo. These methods, as well as assessment of muscle protein homeostasis, are combined with assessment of whole animal muscle function, in the form of movement assays, to allow correlation of sub-cellular defects with functional measures of muscle performance. Thus, C. elegans provides a powerful platform with which to assess the impact of mutations, gene knockdown, and/or chemical compounds upon muscle structure and function. Lastly, as GFP, cationic dyes, and movement assays are assessed non-invasively, prospective studies of muscle structure and function can be conducted across the whole life course and this at present cannot be easily investigated in vivo in any other organism.Developmental Biology, Issue 93, Physiology, C. elegans, muscle, mitochondria, sarcomeres, ageing52043Play ButtonImproved Preparation and Preservation of Hippocampal Mouse Slices for a Very Stable and Reproducible Recording of Long-term PotentiationAuthors: Agnès Villers, Laurence Ris. Institutions: University of Mons.Long-term potentiation (LTP) is a type of synaptic plasticity characterized by an increase in synaptic strength and believed to be involved in memory encoding. LTP elicited in the CA1 region of acute hippocampal slices has been extensively studied. However the molecular mechanisms underlying the maintenance phase of this phenomenon are still poorly understood. This could be partly due to the various experimental conditions used by different laboratories. Indeed, the maintenance phase of LTP is strongly dependent on external parameters like oxygenation, temperature and humidity. It is also dependent on internal parameters like orientation of the slicing plane and slice viability after dissection.
The optimization of all these parameters enables the induction of a very reproducible and very stable long-term potentiation. This methodology offers the possibility to further explore the molecular mechanisms involved in the stable increase in synaptic strength in hippocampal slices. It also highlights the importance of experimental conditions in in vitro investigation of neurophysiological phenomena.Neuroscience, Issue 76, Neurobiology, Anatomy, Physiology, Biomedical Engineering, Surgery, Memory Disorders, Learning, Memory, Neurosciences, Neurophysiology, hippocampus, long-term potentiation, mice, acute slices, synaptic plasticity, in vitro, electrophysiology, animal model50483Play ButtonIn Vivo Modeling of the Morbid Human Genome using Danio rerioAuthors: Adrienne R. Niederriter, Erica E. Davis, Christelle Golzio, Edwin C. Oh, I-Chun Tsai, Nicholas Katsanis. Institutions: Duke University Medical Center, Duke University, Duke University Medical Center.Here, we present methods for the development of assays to query potentially clinically significant nonsynonymous changes using in vivo complementation in zebrafish. Zebrafish (Danio rerio) are a useful animal system due to their experimental tractability; embryos are transparent to enable facile viewing, undergo rapid development ex vivo, and can be genetically manipulated.1 These aspects have allowed for significant advances in the analysis of embryogenesis, molecular processes, and morphogenetic signaling. Taken together, the advantages of this vertebrate model make zebrafish highly amenable to modeling the developmental defects in pediatric disease, and in some cases, adult-onset disorders. Because the zebrafish genome is highly conserved with that of humans (~70% orthologous), it is possible to recapitulate human disease states in zebrafish. This is accomplished either through the injection of mutant human mRNA to induce dominant negative or gain of function alleles, or utilization of morpholino (MO) antisense oligonucleotides to suppress genes to mimic loss of function variants. Through complementation of MO-induced phenotypes with capped human mRNA, our approach enables the interpretation of the deleterious effect of mutations on human protein sequence based on the ability of mutant mRNA to rescue a measurable, physiologically relevant phenotype. Modeling of the human disease alleles occurs through microinjection of zebrafish embryos with MO and/or human mRNA at the 1-4 cell stage, and phenotyping up to seven days post fertilization (dpf). This general strategy can be extended to a wide range of disease phenotypes, as demonstrated in the following protocol. We present our established models for morphogenetic signaling, craniofacial, cardiac, vascular integrity, renal function, and skeletal muscle disorder phenotypes, as well as others. Molecular Biology, Issue 78, Genetics, Biomedical Engineering, Medicine, Developmental Biology, Biochemistry, Anatomy, Physiology, Bioengineering, Genomics, Medical, zebrafish, in vivo, morpholino, human disease modeling, transcription, PCR, mRNA, DNA, Danio rerio, animal model50338Play ButtonDirect Imaging of ER Calcium with Targeted-Esterase Induced Dye Loading (TED)Authors: Samira Samtleben, Juliane Jaepel, Caroline Fecher, Thomas Andreska, Markus Rehberg, Robert Blum. Institutions: University of Wuerzburg, Max Planck Institute of Neurobiology, Martinsried, Ludwig-Maximilians University of Munich.Visualization of calcium dynamics is important to understand the role of calcium in cell physiology. To examine calcium dynamics, synthetic fluorescent Ca2+ indictors have become popular. Here we demonstrate TED (= targeted-esterase induced dye loading), a method to improve the release of Ca2+ indicator dyes in the ER lumen of different cell types. To date, TED was used in cell lines, glial cells, and neurons in vitro. TED bases on efficient, recombinant targeting of a high carboxylesterase activity to the ER lumen using vector-constructs that express Carboxylesterases (CES). The latest TED vectors contain a core element of CES2 fused to a red fluorescent protein, thus enabling simultaneous two-color imaging. The dynamics of free calcium in the ER are imaged in one color, while the corresponding ER structure appears in red. At the beginning of the procedure, cells are transduced with a lentivirus. Subsequently, the infected cells are seeded on coverslips to finally enable live cell imaging. Then, living cells are incubated with the acetoxymethyl ester (AM-ester) form of low-affinity Ca2+ indicators, for instance Fluo5N-AM, Mag-Fluo4-AM, or Mag-Fura2-AM. The esterase activity in the ER cleaves off hydrophobic side chains from the AM form of the Ca2+ indicator and a hydrophilic fluorescent dye/Ca2+ complex is formed and trapped in the ER lumen. After dye loading, the cells are analyzed at an inverted confocal laser scanning microscope. Cells are continuously perfused with Ringer-like solutions and the ER calcium dynamics are directly visualized by time-lapse imaging. Calcium release from the ER is identified by a decrease in fluorescence intensity in regions of interest, whereas the refilling of the ER calcium store produces an increase in fluorescence intensity. Finally, the change in fluorescent intensity over time is determined by calculation of ΔF/F0.Cellular Biology, Issue 75, Neurobiology, Neuroscience, Molecular Biology, Biochemistry, Biomedical Engineering, Bioengineering, Virology, Medicine, Anatomy, Physiology, Surgery, Endoplasmic Reticulum, ER, Calcium Signaling, calcium store, calcium imaging, calcium indicator, metabotropic signaling, Ca2+, neurons, cells, mouse, animal model, cell culture, targeted esterase induced dye loading, imaging50317Play ButtonPreparation of Dissociated Mouse Cortical Neuron CulturesAuthors: Lutz G. W. Hilgenberg, Martin A. Smith. Institutions: University of California, Irvine (UCI).This video will guide you through the process for generating cortical neuronal cultures from late embryo and early postnatal mouse brain. These cultures can be used for a variety of applications including immunocytochemistry, biochemistry, electrophysiology, calcium and sodium imaging, protein and/or RNA isolation. These cultures also provide a platform to study the neuronal development of transgenic animals that carry a late embryonic or postnatal lethal gene mutation. The procedure is relatively straight forward, requires some experience in tissue culture technique and should not take longer than two to three hours if you are properly prepared. Careful separation of the cortical rind from the thalamo-cortical fiber tract will reduce the number of unwanted non-neuronal cells. To increase yields of neuronal cells triturate the pieces of the cortical tissue gently after the enzyme incubation step. This is imperative as it prevents unnecessary injury to cells and premature neuronal cell death. Since these cultures are maintained in the absence of glia feeder cells, they also offer an added advantage of growing cultures enriched in neurons.Neuroscience, Issue 10, cellular, molecular, neurobiology, neuron, calcium/sodium imaging, primary cultures, mouse562Play ButtonAnalysis of Schwann-astrocyte Interactions Using In Vitro AssaysAuthors: Fardad T. Afshari, Jessica C. Kwok, James W. Fawcett. Institutions: University of Cambridge.Schwann cells are one of the commonly used cells in repair strategies following spinal cord injuries. Schwann cells are capable of supporting axonal regeneration and sprouting by secreting growth factors 1,2 and providing growth promoting adhesion molecules 3 and extracellular matrix molecules 4. In addition they myelinate the demyelinated axons at the site of injury 5.
However following transplantation, Schwann cells do not migrate from the site of implant and do not intermingle with the host astrocytes 6,7. This results in formation of a sharp boundary between the Schwann cells and astrocytes, creating an obstacle for growing axons trying to exit the graft back into the host tissue proximally and distally. Astrocytes in contact with Schwann cells also undergo hypertrophy and up-regulate the inhibitory molecules 8-13.
In vitro assays have been used to model Schwann cell-astrocyte interactions and have been important in understanding the mechanism underlying the cellular behaviour.
These in vitro assays include boundary assay, where a co-culture is made using two different cells with each cell type occupying different territories with only a small gap separating the two cell fronts. As the cells divide and migrate, the two cellular fronts get closer to each other and finally collide. This allows the behaviour of the two cellular populations to be analyzed at the boundary. Another variation of the same technique is to mix the two cellular populations in culture and over time the two cell types segregate with Schwann cells clumped together as islands in between astrocytes together creating multiple Schwann-astrocyte boundaries.
The second assay used in studying the interaction of two cell types is the migration assay where cellular movement can be tracked on the surface of the other cell type monolayer 14,15. This assay is commonly known as inverted coverslip assay. Schwann cells are cultured on small glass fragments and they are inverted face down onto the surface of astrocyte monolayers and migration is assessed from the edge of coverslip.
Both assays have been instrumental in studying the underlying mechanisms involved in the cellular exclusion and boundary formation. Some of the molecules identified using these techniques include N-Cadherins 15, Chondroitin Sulphate proteoglycans(CSPGs) 16,17, FGF/Heparin 18, Eph/Ephrins19.
This article intends to describe boundary assay and migration assay in stepwise fashion and elucidate the possible technical problems that might occur.Cellular Biology, Issue 47, Schwann cell, astrocyte, boundary, migration, repulsion2214Play ButtonQuantifying Synapses: an Immunocytochemistry-based Assay to Quantify Synapse NumberAuthors: Dominic M. Ippolito, Cagla Eroglu. Institutions: Duke University, Duke University.One of the most important goals in neuroscience is to understand the molecular cues that instruct early stages of synapse formation. As such it has become imperative to develop objective approaches to quantify changes in synaptic connectivity. Starting from sample fixation, this protocol details how to quantify synapse number both in dissociated neuronal culture and in brain sections using immunocytochemistry. Using compartment-specific antibodies, we label presynaptic terminals as well as sites of postsynaptic specialization. We define synapses as points of colocalization between the signals generated by these markers. The number of these colocalizations is quantified using a plug in Puncta Analyzer (written by Bary Wark, available upon request, c.eroglu@cellbio.duke.edu) under the ImageJ analysis software platform. The synapse assay described in this protocol can be applied to any neural tissue or culture preparation for which you have selective pre- and postsynaptic markers. This synapse assay is a valuable tool that can be widely utilized in the study of synaptic development.Neuroscience, Issue 45, synapse, immunocytochemistry, brain, neuron, astrocyte2270Play ButtonPreparation of Acute Hippocampal Slices from Rats and Transgenic Mice for the Study of Synaptic Alterations during Aging and Amyloid PathologyAuthors: Diana M. Mathis, Jennifer L. Furman, Christopher M. Norris. Institutions: University of Kentucky College of Public Health, University of Kentucky College of Medicine, University of Kentucky College of Medicine.The rodent hippocampal slice preparation is perhaps the most broadly used tool for investigating mammalian synaptic function and plasticity. The hippocampus can be extracted quickly and easily from rats and mice and slices remain viable for hours in oxygenated artificial cerebrospinal fluid. Moreover, basic electrophysisologic techniques are easily applied to the investigation of synaptic function in hippocampal slices and have provided some of the best biomarkers for cognitive impairments. The hippocampal slice is especially popular for the study of synaptic plasticity mechanisms involved in learning and memory. Changes in the induction of long-term potentiation and depression (LTP and LTD) of synaptic efficacy in hippocampal slices (or lack thereof) are frequently used to describe the neurologic phenotype of cognitively-impaired animals and/or to evaluate the mechanism of action of nootropic compounds. This article outlines the procedures we use for preparing hippocampal slices from rats and transgenic mice for the study of synaptic alterations associated with brain aging and Alzheimer's disease (AD)1-3. Use of aged rats and AD model mice can present a unique set of challenges to researchers accustomed to using younger rats and/or mice in their research. Aged rats have thicker skulls and tougher connective tissue than younger rats and mice, which can delay brain extraction and/or dissection and consequently negate or exaggerate real age-differences in synaptic function and plasticity. Aging and amyloid pathology may also exacerbate hippocampal damage sustained during the dissection procedure, again complicating any inferences drawn from physiologic assessment. Here, we discuss the steps taken during the dissection procedure to minimize these problems. Examples of synaptic responses acquired in ""healthy"" and ""unhealthy"" slices from rats and mice are provided, as well as representative synaptic plasticity experiments. The possible impact of other methodological factors on synaptic function in these animal models (e.g. recording solution components, stimulation parameters) are also discussed. While the focus of this article is on the use of aged rats and transgenic mice, novices to slice physiology should find enough detail here to get started on their own studies, using a variety of rodent models.Neuroscience, Issue 49, aging, amyloid, hippocampal slice, synaptic plasticity, Ca2+, CA1, electrophysiology2330Play ButtonMesenteric Artery Contraction and Relaxation Studies Using Automated Wire MyographyAuthors: Lakeesha E. Bridges, Cicely L. Williams, Mildred A. Pointer, Emmanuel M. Awumey. Institutions: North Carolina Central University, Durham, North Carolina Central University, Durham, Wake Forest University School of Medicine.Proximal resistance vessels, such as the mesenteric arteries, contribute substantially to the peripheral resistance. These small vessels of between 100-400 μm in diameter function primarily in directing blood flow to various organs according to the overall requirements of the body. The rat mesenteric artery has a diameter greater than 100 μm. The myography technique, first described by Mulvay and Halpern1, was based on the method proposed by Bevan and Osher2. The technique provides information about small vessels under isometric conditions, where substantial shortening of the muscle preparation is prevented. Since force production and sensitivity of vessels to different agonists is dependent on the extent of stretch, according to active tension-length relation, it is essential to conduct contraction studies under isometric conditions to prevent compliance of the mounting wires. Stainless steel wires are preferred to tungsten wires because of oxidation of the latter, which affects recorded responses3.The technique allows for the comparison of agonist-induced contractions of mounted vessels to obtain evidence for normal function of vascular smooth muscle cell receptors.
Medicine, Issue 55, cardiovascular, resistant arteries, contraction, relaxation, myography3119Play ButtonVisualization and Genetic Manipulation of Dendrites and Spines in the Mouse Cerebral Cortex and Hippocampus using In utero ElectroporationAuthors: Emilie Pacary, Matilda A. Haas, Hendrik Wildner, Roberta Azzarelli, Donald M. Bell, Djoher Nora Abrous, François Guillemot. Institutions: MRC National Institute for Medical Research, National Institute for Medical Research, Université de Bordeaux.In utero electroporation (IUE) has become a powerful technique to study the development of different regions of the embryonic nervous system 1-5. To date this tool has been widely used to study the regulation of cellular proliferation, differentiation and neuronal migration especially in the developing cerebral cortex 6-8. Here we detail our protocol to electroporate in utero the cerebral cortex and the hippocampus and provide evidence that this approach can be used to study dendrites and spines in these two cerebral regions.
Finally, IUE provides a useful tool to identify functional interactions between genes involved in dendrite, spine and/or synapse development. Indeed, in contrast to other gene transfer methods such as virus, it is straightforward to combine multiple RNAi or transgenes in the same population of cells. In summary, IUE is a powerful method that has already contributed to the characterization of molecular mechanisms underlying brain function and disease and it should also be useful in the study of dendrites and spines.Neuroscience, Issue 65, Developmental Biology, Molecular Biology, Neuronal development, In utero electroporation, dendrite, spines, hippocampus, cerebral cortex, gain and loss of function4163Play ButtonImaging Analysis of Neuron to Glia Interaction in Microfluidic Culture Platform (MCP)-based Neuronal Axon and Glia Co-culture SystemAuthors: Haruki Higashimori, Yongjie Yang. Institutions: Tufts University, Tufts Sackler School of Graduate Biomedical Sciences.Proper neuron to glia interaction is critical to physiological function of the central nervous system (CNS). This bidirectional communication is sophisticatedly mediated by specific signaling pathways between neuron and glia1,2 . Identification and characterization of these signaling pathways is essential to the understanding of how neuron to glia interaction shapes CNS physiology. Previously, neuron and glia mixed cultures have been widely utilized for testing and characterizing signaling pathways between neuron and glia. What we have learned from these preparations and other in vivo tools, however, has suggested that mutual signaling between neuron and glia often occurred in specific compartments within neurons (i.e., axon, dendrite, or soma)3. This makes it important to develop a new culture system that allows separation of neuronal compartments and specifically examines the interaction between glia and neuronal axons/dendrites. In addition, the conventional mixed culture system is not capable of differentiating the soluble factors and direct membrane contact signals between neuron and glia. Furthermore, the large quantity of neurons and glial cells in the conventional co-culture system lacks the resolution necessary to observe the interaction between a single axon and a glial cell.
In this study, we describe a novel axon and glia co-culture system with the use of a microfluidic culture platform (MCP). In this co-culture system, neurons and glial cells are cultured in two separate chambers that are connected through multiple central channels. In this microfluidic culture platform, only neuronal processes (especially axons) can enter the glial side through the central channels. In combination with powerful fluorescent protein labeling, this system allows direct examination of signaling pathways between axonal/dendritic and glial interactions, such as axon-mediated transcriptional regulation in glia, glia-mediated receptor trafficking in neuronal terminals, and glia-mediated axon growth. The narrow diameter of the chamber also significantly prohibits the flow of the neuron-enriched medium into the glial chamber, facilitating probing of the direct membrane-protein interaction between axons/dendrites and glial surfaces.Neuroscience, Issue 68, Molecular Biology, Cellular Biology, Biophysics, Microfluidics, Microfluidic culture platform, Compartmented culture, Neuron to glia signaling, neurons, glia, cell culture4448Play ButtonFluorescence Recovery After Photobleaching (FRAP) of Fluorescence Tagged Proteins in Dendritic Spines of Cultured Hippocampal NeuronsAuthors: Chan-Ying Zheng, Ronald S. Petralia, Ya-Xian Wang, Bechara Kachar. Institutions: National Institutes of Health, Bethesda.FRAP has been used to quantify the mobility of GFP-tagged proteins. Using a strong excitation laser, the fluorescence of a GFP-tagged protein is bleached in the region of interest. The fluorescence of the region recovers when the unbleached GFP-tagged protein from outside of the region diffuses into the region of interest. The mobility of the protein is then analyzed by measuring the fluorescence recovery rate. This technique could be used to characterize protein mobility and turnover rate.
This FRAP protocol shows how to perform a basic FRAP experiment as well as how to analyze the data.Neuroscience, Issue 50, Spine, FRAP, hippocampal neurons, live cell imaging, protein mobility2568Play ButtonPrimary Neuronal Cultures from the Brains of Late Stage Drosophila PupaeAuthors: Beatriz Sicaeros, Jorge M. Campusano, Diane K. O'Dowd. Institutions: University of California, Irvine (UCI).In this video, we demonstrate the preparation of primary neuronal cultures from the brains of late stage Drosophila pupae. The procedure begins with the removal of brains from animals at 70-78 hrs after puparium formation. The isolated brains are shown after brief incubation in papain followed by several washes in serum-free growth medium. The process of mechanical dissociation of each brain in a 5 ul drop of media on a coverslip is illustrated. The axons and dendrites of the post-mitotic neurons are sheered off near the soma during dissociation but the neurons begin to regenerate processes within a few hours of plating. Images show live cultures at 2 days. Neurons continue to elaborate processes during the first week in culture. Specific neuronal populations can be identified in culture using GAL4 lines to drive tissue specific expression of fluorescent markers such as GFP or RFP. Whole cell recordings have demonstrated the cultured neurons form functional, spontaneously active cholinergic and GABAergic synapses. A short video segment illustrates calcium dynamics in the cultured neurons using Fura-2 as a calcium indicator dye to monitor spontaneous calcium transients and nicotine evoked calcium responses in a dish of cultured neurons. These pupal brain cultures are a useful model system in which genetic and pharmacological tools can be used to identify intrinsic and extrinsic factors that influence formation and function of central synapses.",['They are rich in actin and have been shown to be highly dynamic.'],6654,multifieldqa_en,en,,4214e20df9d299bae4fb9ba8d1b4792d516812e86960009c," Astrocytes are an abundant cell type in the mammalian brain. In vitro astrocyte cell culture systems can be used to study the biological functions of these glial cells in detail. This video protocol shows how to obtain pure astro Cytes by isolation and culture of mixed cortical cells of mouse pups. The method is based on the absence of viable neurons and the separation of astroCytes, oligodendrocytes and microglia, the three main glial cell populations of the central nervous system, in culture. The images during the first days of culture demonstrate the presence of a mixed cell population and indicate the timepoint, when astro cytes become confluent and should be separated frommicroglia and oligod endrocyts. The video protocol also shows the purity and astrotytic morphology of cultured astropytes using immunocytochemical stainin. The results of the study were published in the journal of the University of Freiburg, the German Academy of Sciences, and the German Journal of Neurophysiology, the Journal of Neuroscience, the Society of Neuropathology and the Society for Neurophysiological Research, the European Association for Neuropathological Research and the European Society for neuropathology, the Association for Neuroscience and the Neurophysiologists, the  Society for Neuroscience, and  the  European Neuropsychiatric Society. For more information on the JoVE video protocol, visit www.jve.org.uk/video/jve-video-protocols/joVE-video.html. For the full article, visit http://www.jves.org/video-journal/jove/videos/jves-video/v2.0/v3.0-v2-v3-v4.0.1.1/jv3/jvis.html#v3_v3v4-v5.1-v6-v7-v8-v9-v10-v11-v12-v1-1. The article has been updated to reflect the fact that the study was conducted in a mouse model of Downs syndrome and not a human model of Down's syndrome. It has also been amended to make clear that the authors' findings were based on a mouse brain model of the syndrome. The study was also amended to reflect that the mouse brain is a different type of brain. The authors conclude that a number of disease states, ranging from schizophrenia to autism spectrum disorders, display abnormal dendritic spine morphology or numbers. They also suggest that these proteins may contribute to aberrant spine plasticity that, in part, underlie the pathophysiology of these disorders. They conclude that the use of cultured cortical neurons offers several advantages, including high-resolution imaging of d endritic spines in fixed cells as well as time-lapse imaging of live cells. Dendritic spines are protrusions emerging from the dendrite of a neuron and represent the primary postsynaptic targets of excitatory inputs in the brain. The quantitative analysis of spine morphology using light microscopy remains an essential problem due to technical limitations associated with light's intrinsic refraction limit. The blood brain barrier (BBB) specifically regulates molecular and cellular flux between the blood and the nervous tissue. The aim was to develop and characterize a highly reproducible rat syngeneic in vitro model of the BBB using co-cultures of primary rat brain endothelial cells (RBEC) and astrocytes to study receptors involved in transcytosis across the endothelial cell monolayer. This culture system can be easily used to obtain pure mouse astroCytes and astracyte-conditioned medium for studying various aspects of astro Cyte biology. It can also be used to study well established and newly described astro cyte markers. It was developed by the University of Amsterdam and the VECT-HORUS SAS, CNRS, NICN UMR 7259. It is based on classical far-field operations and therefore allows the use of existing sample preparation methods and to image beyond the surface of a specimen. The whole protocol described herein takes approximately 2 weeks, because dendriticSpines are imaged after 16-17 days in vitro. After completion of the protocol, dendrites can be reconstructed in 3D from series of SIM image stacks using specialized software. It offers higher effective lateral resolution, while confocal microscopy, due to the usage of a physical pinhole, achieves resolution improvement at the expense of removal of out of focus light. In this protocol, primary neurons are cultured on glass coverslips using a standard protocol. They are transfected with DNA plasmids encoding fluorescent proteins and imaged using SIM. The entire protocol takes approximately two weeks. It's based on a working protocol to apply super resolution structured illumination microscopy (SIM) to the imaging of dendrid spines in primary hippocampal neuron cultures. It has been developed by Yves Molino, Françoise Jabès, Emmanuelle Lacassagne, Nicolas Gaudin, Michel Khrestchatisky and Yves M. Molino at the Vect-HorUS SAS and the CNRS in the Netherlands. The study was published in the journal Neurosciences and is available online in the U.S. and in the European Journal of Neurobiology and Biomedicine. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. For more information on the study, see www.samaritans.org and www.vect-horus.org. For the full article, visit the European Society for Neuroscience and Biomedical Research (ESB) or the European Association for Neuroscience and Biotechnology (ECB) website. For details on how to get involved in the project, visit www.esb.uk. The ability of astrocytic receptors to exhibit plasticity in response to changes in neuronal activity has received little attention. Having an understanding of how astroCytic receptor signaling is affected by changes in. neuronal activity could have important implications for both normal. synaptic function as well as processes underlying neurological disorders and. neurodegenerative disease. Inhibitory Synapse Formation in a Co-culture Model Incorporating GABAergic Medium Spiny Neurons and HEK293 Cells Expressing Gq GPCR Receptors. In essence, a “calcium roadmap” is provided for how to measure plasticity of astracytic GQ GPCRs. The technique could be used to globally scale up or down astrocyclic group I metabotropic glutamate receptors (mGluRs) in acute brain slices. It could also be used in the central nervous system to regulate the spatio-temporal dynamics and co-o-temporo- temporal dynamics of neurons. It was developed by Alison X. Xie, Kelli Lauderdale, Thomas Murphy, Timothy L. Myers, Todd A. Fiacco at the University of California Riverside, and Alex M. Nicholson at University College London. It has been published in the Journal of Neurology and Neurosurgery. It is a peer-reviewed journal and is published by the Association for Computational Neurophysiology (ACN) and the American College of Neurosurgeons (ACNS). For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential. support on suicide matters call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or see www.samaritans.org. For confidential help in the U.S. call 1-877-457-9255 or go to www.s Samaritans. org. For information on suicide support in the UK, visit a Samaritans local branch or click here. The Samaritans can also be contacted on 0800-838-9090. For more information on the Samaritan’s suicide support service, visit http:www.samarsitans.com/. For details on suicide prevention in the United States, visit the National suicide prevention Lifeline on 1-844-788-888 or http: http: www. Samaritans-uk.org/suicide-prevention-lifeline/. For more on the benefits of suicide prevention, see http: //www.saritans- UK.gov/ Suicide-Prevention-Lifeline.html. for details on how to help people in need of suicide support can be reached by contacting Samaritans at 0800 888-9088. For a confidential. helpline, call 0800 909090. for help with suicide matters, or for confidential support on http: //www.Saritans.-uk.gov/. For a guide on suicide Prevention, call http:://www.-saritanitans.uk/Suicide-Prevent-L Prevention.html for details. For an overview of the benefits and risks of suicide, see the article by the American Cancer Society. dination of neuronal networks. In the mammalian cortex, neurons form extremely complicated networks and exchange information at synapses. Changes in synaptic strength, as well as addition/removal of synapses, occur in an experience-dependent manner, providing the structural foundation of neuronal plasticity. As postsynaptic components of the most excitatory synapses in the cortex, dendritic spines are considered to be a good proxy ofsynapses. Two-photon in vivo imaging of Dendritic Spines in the Mouse Cortex Using a Thinned-skull Preparation. Using Cortical Astrocytes or Neural Stem Cells from Conditional, Genetically Engineered Mice. The experimental procedure can be performed repetitively over various time intervals ranging from hours to years. The application of this preparation can also be expanded to investigate different cortical regions and layers, aswell as other cell types, under physiological and pathological conditions. The results indicate that various GABAAR subtypes differ in their ability to promote synapse formation, suggesting that this reduced in vitro model system can be used to reproduce, at least in part, the in vitro conditions required for the recognition of the appropriate synaptic partners and formation of specific synapses for the study of brain plasticity in mice. The study was published in the Journal of Neuroendocrinology and Metabolism. It was written by Xinzhu Yu, Yi Zuo, and Ralf Schmid, and published by the University of California, Santa Cruz. It is the first of its kind and aims to provide information about the development of the brain's plasticity and plasticity over time in a mouse brain. The research was funded by the National Institutes of Health and the National Institute of Neurological Disorders and Stroke of the U.S. Department of Health, among other institutions. For more information, visit www.neuroendocrinologists.org and www.nind.org. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here for details. For support on suicide matters call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.samaritans.org/. For more on how to get involved in the study, visit the Samaritans' page and the Online site on the University of California University of Santa Cruz or the National Sciences Department of Nursing Institute of Health, or The University of Cancer of California, San Safari and the American Institution of Neurology for Research and Research in the U. São Paulo. Current astrocytoma models are limited in their ability to define the roles of oncogenic mutations in specific brain cell types during disease pathogenesis. In order to design a better model system, cells from genetically engineered mice were harvested and grown in culture. The phenotypic consequences of these mutations were defined by measuring proliferation, transformation, and drug response in vitro. Paired whole cell recordings are often perceived as too challenging to perform, but can be performed by anyone trained in whole cell patch clamping. The probability of attaining synaptically connected paired recordings significantly increases with healthy organotypic slices of the brain, according to the authors of the study. It can provide information on the properties of synaptic transmission and plasticity that is not possible with other more crude techniques utilizing extracellular axonal stimulation. The study was published in the journal Neuroscience. It was co-authored by Chantelle Fourie, Marianna Kiraly, Daniel V. Madison, Johanna M. Montgomery, and Kristen K. White, and published by the University of North Carolina School of Medicine, Emory University School of medicine, and the Stanford University Graduate School of Pharmacy. It is the first of its kind to be published by a major U.S. academic journal, and was published by The New England Journal of Medicine and the American Journal of Clinical Pharmacology and Therapeutics, which is a division of the journal of the American College of Pharmacists. For more information, visit www.neuroscience.org and www.the-new-england-journal-of-pharmacy.com. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, or click here for details. The Samaritans can also be found at www.samaritans.org. In the U.K., call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential. support in the UK, call 08457 909090 or visit the Samaritans, or the University of North Carolina School of Medicine and the University of North Carolina School of Medicine in the UK. For information on how to get involved in the study, visit http://www www.sussex-medicine.org or the-university-of North Carolina school of-medics and the University of Carolinas University. For details on the UNC School of Medicines and other medical centers, visit the university's research and medical centers on-campus  websites. Astrocytes display spontaneous intracellular Ca2+ concentration fluctuations. Using genetically encoded calcium indicators (GECIs) such as the GCaMP series, scientists can study astrocyte [Ca2+]i signals in detail within model microcircuits. C. elegans is an established model for understanding the genomic regulation of biological processes of interest. The worm’s body wall muscles display a large degree of homology with the muscles of higher metazoan species. The localization of GFP to mitochondria and sarcomeres allows visualization of these structures in vivo. The further use and expansion of these approaches in the striatum will allow for the detailed study of the striatal microcircuitry. The loss of muscle mass and function with disease and age are significant public health burdens. We currently understand little about the genetic regulation of muscle health with disease or age. These methods, as well as assessment of muscle protein homeostasis, are combined with assessment of whole animal muscle function, in the form of movement assays, to allow the correlation of sub-cellular defects with functional measures of muscle performance. The results are published in the Journal of Neurophysiology, Volume 91, Issue 91 (July 2013) and Issue 92 (July 2014) The study was funded by the National Institutes of Health (NIH) and the University of California, Los Angeles (UCL) The results of the study have been published in Neurophysiological Journal, volume 91, issue 92 (June 2013). The study has been published by the NIH and the UCL Press (September 2013) in the journal of Neuropsychology, Volume 92, issue 103 (September 2012) and issue 104 (October 2013) The findings were published as part of the International Journal of neurophysiology (ICN) series. The ICN is a peer-reviewed, open-access journal. For confidential support, call the U.S. National Institute of Neurology and Neurosurgery on 1-800-273-8255 or visit http://www.ncn.org/. For confidential. support in the UK, contact the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or see www.samaritans.org. For information on how to get involved in the study, visit the www.s Samaritans.com. For more information on the ICN, visit www.the Samaritans, or click here. For details on the Northern European Society for Neurobiology, visit http:www.sarvins.org/NHS/NES/NIS/NUS/NNS/NINS/NPS/NTS/NSS/NMS/NCS/NSC/NDS/NSP/NS/SNS.NSP.NPS.NTS.NSS.NNS.NS is a platform with a stable micromanipulation allowing independent attainment of pre- and postsynaptic whole cell recordings. This technique can be adapted to any neurons that are synaptically connected in the same slice preparation. mpact of mutations, gene knockdown, and/or chemical compounds upon muscle structure and function. As GFP, cationic dyes, and movement assays are assessed non-invasively, prospective studies of muscle structure can be conducted across the whole life course and this at present cannot be easily investigated in vivo in any other organism. Zebrafish (Danio rerio) are a useful animal system due to their experimental tractability. They are transparent to enable facile viewing, undergo rapid development ex vivo, and can be genetically manipulated. Taken together, the advantages of this vertebrate model make zebrafish highly amenable to modeling the developmental defects in pediatric disease, and in some cases, adult-onset disorders. Because the zebra fish genome is highly conserved with that of humans (~70% orthologous), it is possible to recapitulate human disease states inZebrafish. This is accomplished either through the injection of mutant human mRNA to induce dominant negative or gain of function alleles, or utilization of morpholino (MO) antisense oligonucleotides to suppress genes to mimic loss of function variants. Through complementation of MO-induced phenotypes with capped human mRNA, our approach enables the interpretation of the deleterious effect of mutations on human protein sequence based on the ability of mutant mRNA to rescue a measurable, physiologically relevant phenotype. This general strategy can be extended to a wide range of disease phenotypes. It also highlights the importance of experimental conditions in in vitro investigation of neurophysiological phenomena, such as oxygenation, temperature, humidity and orientation of the slicing plane and slice viability after dissection. It can be used to model potentially clinically significant nonsynonymous changes using in vivo complementation in zebra Fish. It is also possible to model the human disease alleles through microinjection of zebraFish embryos with MO and/ or human mRNA at the 1-4 cell stage, and phenotyping up to seven days post fertilization (dpf). This general Strategy can be extend to a Wide range of Disease phenotypes, including those associated with cancer, heart disease, diabetes, and other chronic conditions such as myelodysplastic syndromes. It could also be used for the development of new treatments for Parkinson's disease, stroke, and multiple sclerosis, among others. It has been proposed that the use of this strategy could be used in the treatment of cancer of the pancreas, prostate, and bone marrow, as well as other conditions that are associated with ageing. It would also allow for the study of the effects of ageing on the immune system, including the development and function of the central nervous system, on the nervous system and on the cardiovascular system. It will also help us to better understand the role of ageing in ageing. This video will guide you through the process for generating cortical neuronal cultures from late embryo and early postnatal mouse brain. The cultures can be used for a variety of applications including immunocytochemistry, biochemistry, electrophysiology, calcium and sodium imaging, protein and/or RNA isolation. These cultures also provide a platform to study the neuronal development of transgenic animals that carry a late embryonic or postnatal lethal gene mutation. The procedure is relatively straight forward, requires some separation of the cortical rind from the cortical tissue, and should not take longer than two to three hours if you are properly prepared.s demonstrated in the following protocol. We present our established models for morphogenetic signaling, craniofacial, cardiac, vascular integrity, renal function, and skeletal muscle disorder phenotypes, as well as others. For more information, visit the University of California, Irvine (UCI) and the Max Planck Institute of Neurobiology, Martinsried (Max Planck) (http://www.maxplanck.org/neurobiology/neuronal-development/neuron-development-neurons-and-mammoth-cortical-tissue-cultures.html). For more on the UCI Cortical Neuron Cultures project, visit www.uci.edu/corticular-neuron cultures and www.corticleneuron.org/. For more about the UCi Corticular Neuron Culture Project, see www.uica.edu. and http:www.ucI. and   http: www.cmi.org.uk/. for more information on theUCI Corticle Neurons Project, visitwww.canticleneuron.co.uk and http :www. cmi.uk/cntNeuron-Cultures/NeuronalNeurons.html. For the full article, visit http://www-www.csmonitor.com/news/features/article/50338/calcium-dynamics-in-cell-physiology.html#storylink=cpy. Calcium release from the ER is identified by a decrease in fluorescence intensity in regions of interest, whereas the refilling of the ER calcium store produces an increase in fluorescence intensity. The dynamics of free calcium in the ER are imaged in one color, while the corresponding ER structure appears in red. The latest TED vectors contain a core element of CES2 fused to a red fluorescent protein, thus enabling simultaneous two-color imaging. After dye loading, the cells are analyzed at an inverted confocal laser scanning microscope. Cells are continuously perfused with Ringer-like solutions. The ER calcium dynamics are directly visualized by time-lapse imaging. Cal calcium release is determined by calculation of the change in fluorescent intensity over time is determined in ΔF/F0. This article was originally published in Cellular Biology, Issue 75, Neurobiology and Neuroscience, Biochemistry, Biomedical Engineering, Medicine, Developmental Biology, Anatomy, Physiology, Genomics, Medical, zebrafish. lamo-cortical fiber tract will reduce the number of unwanted non-neuronal cells. To increase yields of neuronal cells triturate the pieces of the cortical tissue gently after the enzyme incubation step. This is imperative as it prevents unnecessary injury to cells and premature neuronal cell death. Since these cultures are maintained in the absence of glia feeder cells, they also offer an added advantage of growing cultures enriched in neurons. In vitro assays have been used to model Schwann cell-astrocyte interactions and have been important in understanding the mechanism underlying the cellular behaviour. Some of the molecules identified using these techniques include N-Cadherins 15, Chondroitin Sulphate proteoglycans(CSPGs) 16,17, FGF/Heparin 18, Eph/Ephrins19. This article intends to describe boundary assay and migration assay in stepwise fashion and elucidate the possible technical problems that might occur. Starting from sample fixation, this protocol details how to quantify synapse number both in dissociated neuronal culture and in brain sections using immunocytochemistry. Using compartment-specific antibodies, we label presynynyn (synapses) using compartment- specific antibodies, and label the synapses as ‘neurons’ or ‘nucular cells’. The protocol is based on the study of mice and the results are published in the Journal of Neurobiology and Biomarkers (JNB) (http://www.jbn.org.uk/jb/neurobiology/neurology/neuron/synapse.html). The study was published in JNB’s online edition, which is available in English and French. The abstract is available on the JNB website. It states: ‘Synapse number is a measure of the connectivity between neurons in the brain and the rest of the body. It can be used to help us understand the early stages of synapse formation.’ The JNB also states that it is a ‘non-invasive’ method of quantifying synapses and that it can be applied to a wide range of human diseases and conditions. The JBN also says that it has been developed to help scientists understand the role of synapses in the development of cancer and other forms of disease. It is available online at www.jb.org/.uk.uk. The article is also available in the online edition of the journal, which includes the article ‘Neuroscience and Biomedicine’ (http:www. JNB.org/Neuromarker/Neuron/Neurobiology-Neuron/Synapse/Synapses.html’, and the online version of the article can be downloaded as a free download. It also includes the JB article, which can be read in English or French. This article outlines the procedures we use for preparing hippocampal slices from rats and transgenic mice for the study of synaptic alterations associated with brain aging and Alzheimer's disease (AD)1-3. Use of aged rats and AD model mice can present a unique set of challenges to researchers accustomed to using younger rats and/or mice in their research. Proximal resistance vessels, such as the mesenteric arteries, contribute substantially to the peripheral resistance. These small vessels of bet bettered betterness can be used to study the effects of stress on the cardiovascular system on the brain and nervous system. This article is a valuable tool that can be widely utilized in theStudy of synaptic development. The synapse assay described in this protocol can be applied to any neural tissue or culture preparation for which you have selective pre- and postsynaptic markers. We define synapses as points of colocalization between the signals generated by these markers. The number of these colocalizations is quantified using a plug in Puncta Analyzer (written by Bary Wark, available upon request, c.eroglu@cellbio.duke.edu) under the ImageJ analysis software platform.ptic terminals as well as sites of post synaptic specialization. The hippocampus can be extracted quickly and easily from Rats and mice and slices remain viable for hours in oxygenated artificial cerebrospinal fluid. Basic electrophysisologic techniques are easily applied to the investigation of synaptic function in hippocampusal slices and have provided some of the best biomarkers for cognitive impairments. The hippocampal slice is especially popular for thestudy of synaptic plasticity mechanisms involved in learning and memory. Changes in the induction of long-term potentiation and depression (LTP and LTD) of synaptic efficacy are frequently used to describe the neurologic phenotype of cognitively-impaired animals and/ or to evaluate the mechanism of action of nootropic compounds. While the focus of this article is on the use of aged Rats and Transgenic mice, novices to slice physiology should find enough detail here to get started on their own studies, using a variety of rodent models. The possible impact of other methodological factors onaptic function in these animal models (e.g. recording solution components, stimulation parameters) are also discussed. It is hoped that this article will provide enough detail to help new researchers get started with their own Studies of the Hippocampal Slices of Rats and Mice in the Study of Alzheimer’s Disease (ASD) and AD1 and AD2 and ASD1 and 3. It also provides examples of synaptic responses acquired in “healthy” and “unhealthy’ slices fromrats and mice. The article also provides representative examples of ""healthy"" and ‘unhealthy"" slices from rodents. In utero electroporation (IUE) has become a powerful technique to study the development of different regions of the embryonic nervous system. IUE provides a useful tool to identify functional interactions between genes involved in dendrite, spine and/or synapse development. In contrast to other gene transfer methods such as virus, it is straightforward to combine multiple RNAi or transgenes in the same population of cells. In summary, IUE is a powerful method that has already contributed to the characterization of molecular mechanisms underlying brain function and disease and it should also be useful in the study of dendrites and spines. It is important to develop a new culture system that allows separation of neuronal compartments and specifically examines the interaction between glia and neuronal axons/dendrites. Back to Mail Online home. Back To the page you came from. READ: In utero Electroporation, we electroporate in utero the cerebral cortex and the hippocampus and provide evidence that this approach can be used to study d Endrites and Spines in these two cerebral regions. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential. support on suicide matters call the suicide prevention Lifeline at 1-856-788-7070 or visithttp:// www.suicidesprevention.org.uk/. For information on suicide prevention in the UK, contact The Samaritans at 08457 909090. For information about suicide Prevention in the United States, call the Suicide Prevention Line on 0800-909090 or visit www. suicideprevention Lifeline.uk. For details on suicide in the EU, call 08457 89 90 90 or visit the Suicideprevention helpline on 080-908-7090. For more information on how to get involved in suicide prevention, visit the suicide Prevention Helpline at http:// www-suicide Prevention.org/suicide-prevention-in-the-EU. For the UK and Europe, go to www. Suicide Prevention helplines.org or www.SuicidePreventionHelpline.uk for more information. for the UK or the European Union, visit http:www.skeptic.uk/ Suicide Prevention in Europe and the UK. For. the UK/Europe/The Netherlands, see http:/www.SkepticPrevention in the Netherlands and the Netherlands for more details on how. to getinvolved in Suicide Prevention. in Europe. for. the EU and the rest of the world. The conventional mixed culture system is not capable of differentiating the soluble factors and direct membrane contact signals between neuron and glia. In this microfluidic culture platform, only neuronal processes (especially axons) can enter the glial side through the central channels. This system allows direct examination of signaling pathways between axonal/dendritic and glial interactions, such as axon-mediated transcriptional regulation in glia, glia-mediated receptor trafficking in neuronal terminals, and gla-mediated axon growth. This video shows the preparation of primary neuronal cultures from the brains of late stage Drosophila pupae. The procedure begins with the removal of brains from animals at 70-78 hrs after puparium formation. The isolated brains are shown after brief incubation in papain followed by several washes in serum-free growth medium. The process of mechanical dissociation of each brain in a 5 ul drop of media on a coverslip is illustrated. The axons and dendrites of the post-mitotic neurons are sheered off near the soma during dissociation but the neurons begin to regenerate processes within a few hours of plating. A short video segment illustrates calcium dynamics in the cultured neurons using Fura-2 as a calcium indicator dye to monitor spontaneous calcium transients and nicotine evoked calcium responses in a dish of cultured neurons. These pupal brain cultures are a useful model system in gene gene gene therapy. The video shows live cultures at 2 days. Neurons continue to elaborate processes during the first week in culture. The cultured neurons form functional, spontaneously active cholinergic and GABAergic synapses. This technique could be used to characterize protein mobility and turnover rate. This FRAP protocol shows how to perform a basic FRAP experiment as well as how to analyze the data. The fluorescence of a region recovers when the unbleached GFP-tagged protein from outside of the region diffuses into the region of interest. The mobility of the protein is then analyzed by measuring the fluorescence recovery rate. Using a strong excitation laser, the fluorescent of a GFP/RFP tagged protein is bleached in the area of interest to measure the mobility of that protein in the region. The method could beused to quantify the mobility and rate of protein turnover in the hippocampus and other brain regions. The technique could also be used in the development of new treatments for cancer and other diseases. It can be found at the University of California, Irvine (UCI) and the National Institutes of Health (NIH) in the Neurobiology of Cancer section of the Journal of the American Society for Cell Biology (JACB) (http://www.jacb.org/news/2013/09/07/neuronal-cultures-from-the-brains-of-late-stage-drosophilla-pupae-in-a-new-study-and-technique-to-quantify-protein-mobility.html). ic and pharmacological tools can be used to identify intrinsic and extrinsic factors that influence formation and function of central synapses."
How many people attend the 233rd ACS national meeting?,"The major actions taken by the board of directors and council during the national meeting in Chicago were reported in C&EN, April 30 (page 32).
The Society Committee on Budget & Finance met on Saturday, March 24, to review the society's 2006 financial performance. The society ended 2006 with a net contribution from operations of $12.2 million, on revenues of $424.0 million and expenses of $411.8 million. This was $7.8 million favorable to the approved budget.
After including the results of the Member Insurance Program and new ventures, the society's overall net contribution for 2006 was $11.5 million, which was $7.4 million favorable to the approved budget. The favorable variance was primarily attributable to higher than budgeted electronic services revenue and investment income, as well as expense savings from lower than budgeted health care costs and reduced IT spending. In addition, the society ended the year in compliance with the board-established financial guidelines.
The Society Committee on Education (SOCED) received an update from President Catherine Hunt on the thematic programming featured in Chicago focusing on the sustainability of energy, food, and water. President-Elect Bruce Bursten solicited input from the committee pertaining to the central role of education in his agenda. SOCED received a presentation from the Membership Affairs Committee on its white paper on membership requirements.
Committee members strongly support the proposal to include undergraduates as members of the society, but they requested that financial arrangements be clearly spelled out in the petition to ensure that the highly successful Student Affiliates program remains intact. The committee discussed the Education Division programs that were reviewed in 2006 and those that will be reviewed in 2007, under the auspices of the Program Review Advisory Group. SOCED received an update from the Committee on Professional Training regarding the draft ACS guidelines for approval of bachelor's degree programs in chemistry.
Committee members discussed the report prepared by the Globalization Task Force, focusing on those sections relevant to education. The committee suggested initiatives related to the new ACS strategic plan, including a potential program that would engage retired chemists in the K-12 classroom. SOCED created a task force to consider the role of online, or ""virtual,"" simulations in the chemistry laboratory, recognizing the value of online/virtual experiences as a supplement to, but not a replacement for, hands-on laboratory experiments.
The committee ratified two interim actions taken since the Dec. 3, 2006, meeting: to remove the financial restriction of the ACS Petroleum Research Fund (ACS PRF) Supplement for Underrepresented Minority Research Programs (SUMR) and to contact nominators whose nominations for the Volunteer Service Award had expired and to invite them to reactivate their nomination packet for the 2008 Volunteer Service Award.
Acting under delegated authority, the committee voted to accept the recommendations of the ACS Petroleum Research Fund Advisory Board (February 2007 meeting) for funding grants totaling $5.2 million; voted to recommend to the board a screened list of six nominees (due to a two-way tie for fifth place) for the 2008 Priestley Medal; voted to recommend to the board a screened list of five nominees for the 2008 Award for Volunteer Service to ACS; on the recommendation of the ACS Committee on Frasch Foundation Grants, voted to recommend to the board that it recommend to the trustee (US Trust) of the Frasch Foundation 12 grants for research in agricultural chemistry for the period of 2007–12; voted to recommend to the ACS Board of Directors that a new national award be established, the ""ACS Award for Affordable Green Chemistry,"" sponsored by Rohm and Haas; and voted to recommend to the ACS Board of Directors that a new endowment be established, the ""Affordable Green Chemistry Endowment Fund,"" to support the award.
The committee also reviewed the final report from the Special Board Task Force on the Review of the ACS National Awards Program, chaired by Ronald Breslow; established a Canvassing & Selection Subcommittee; and reviewed a list of external awards for which ACS may want to nominate candidates. The committee agreed to include the list of significant external awards in the awards locator database that is being developed.
The committee was updated on efforts to reconcile ACS's technical divisions' desires to leverage national meeting content using the Internet with our journal editors' concerns about prior publication issues. A conference call on this issue was scheduled for April 21, 2007.
The committee received a presentation on the recent actions of the ACS Board of Directors International Strategy Group (ISG). The group's charge is to develop recommendations for a short- and long-term international strategy for the society.
The committee was updated on the status of the activities of the Board Oversight Group on Leadership Development (BOG). Potential solutions for the unexpectedly high cost of facilitator training and transitioning from the current Leaders Conference format to the newly designed curriculum were presented to the committee.
The committee reviewed plans for conducting the 2007 Membership Satisfaction Survey. Preliminary results are expected in May or June with a final report to be delivered to the board at the 2007 Boston national meeting.
The committee received a briefing on the status of the MORE Project: Multidisciplinary Opportunities though Resource Enhancement. Twenty-eight proposals were received, and a decision on which proposals to support will be made in early May.
The chair led a discussion on draft 2007 committee goals, and committee members offered several suggestions related to successfully meeting them. One suggestion was to modify a communications goal to make it more completely reflect the duties of the committee outlined in the board regulations. The chair and committee members will examine the suggestion and revisit the question after the board retreat where board committee duties will be examined.
ACS President Hunt discussed her 2007-08 Presidential Task Force on Enhancing Science & Technology, which is charged with developing advocacy best practices that can enhance ACS's attainment of its public policy priorities. The task force is composed of a diverse set of ACS members as well as former U.S. Representative and chairman of the House Science Committee, Sherwood Boehlert, who will cochair the task force.
• Results of the 2007 Public Policy Priorities Survey, which resulted in a four-tiered ranking of ACS's 2007 public policies. The ranking will help focus staff resources in conducting outreach and advocacy on behalf of ACS members.
• The hiring of a communications consulting firm for 2007 to assist ACS in implementing the initial phase of the ACS Strategic Communications Plan.
• Creation of a pilot ACS state government affairs advocacy program. Committee members agreed to the creation of a pilot, and staff will propose an initial list of states, policy focus, and a budget to carry out the program.
The committee met in executive session on March 23 and in open session jointly with the Joint Board-Council Committee on Publications and the Division of Chemical Information on March 26.
The committee heard from Chemical Abstracts Service (CAS) management on a range of issues including a report on continuing database building efforts, product enhancements, and CAS's centennial celebration plans.
The Committee on Chemical Safety (CCS) provides advice on the handling of chemicals and seeks to ensure safe facilities, designs, and operations by calling attention to potential hazards and stimulating education in safe practices.
CCS has several publications (many downloadable), including the flagship publication, ""Safety in Academic Chemistry Labs"" (SACL). Work has recently started on the translation of SACL into Arabic. This is in addition to the online Spanish version of SACL. Also online are the ""Student Lab Code of Conduct for Secondary Science Programs"" and a security vulnerability analysis checklist. A K-12 restricted hazardous substances list is under development. The third edition of the ""Chemical Safety Manual for Small Businesses"" will be ready soon.
The committee's Task Force on Laboratory Environment, Health & Safety is working on a new edition of ""Laboratory Waste Management."" Task force members also commented on the recent Environmental Protection Agency Proposed Rule for Hazardous Waste in Academic Laboratories. Our Video Safety Resources Task Force is developing video resources to be distributed over the Web.
CCS has been involved in collaborations for the updating of publications like ""Prudent Practices in the Laboratory"" and ""ACS Guidelines for the Teaching of High School Chemistry."" Along with other ACS units, CCS is exploring participating in the EPA's School Chemicals Cleanout Campaign.
The Committee on Chemists with Disabilities (CWD) met at the 233rd ACS national meeting, Chicago, on Monday, March 26. Judy Summers-Gates reported on the Joint Subcommittee on Diversity meeting. This subcommittee is made up of representatives of the five committees that support people in chemistry (as opposed to a category of the profession): CWD, Committee on Minority Affairs, Committee on Technician Affairs, Women Chemists Committee, and Younger Chemists Committee, and its goal is to develop ways to coordinate the efforts of the five groups.
The CWD Ambassador Program that was announced at CWD's 25th anniversary celebration at the Washington, D.C., meeting was discussed. Zelda Wasserman reported on the status of the letter from CWD to the ACS Board regarding captioning of ACS video materials. Janelle Kasper-Wolf, of ACS staff, discussed adding new questions to the ACS annual employment salary survey to obtain information for the committee.
At the Chicago national meeting, the Committee on Community Activities (CCA) partnered with the ACS Education Division and the Office of the President to host ""Chemistry In Action—It's Easy Being Green"" at the Peggy Notebaert Nature Museum on Saturday, March 24. More than 250 children participated in the hands-on activities focused on recycling. ACS President Hunt presented a Salutes to Excellence plaque to the museum for its dedication to community outreach.
The Chemists Celebrate Earth Day celebration occurred in 120 local sections with 138 coordinators leading the efforts within their communities. This represents an increase of more than 30% in local section and coordinator participation from 2006.
CCA was featured in C&EN's April 16th issue on page 53. A shortcut to CCA's homepage was created: chemistry.org/committees/cca.html.
During the Boston national meeting, CCA and the Office of Community Activities will celebrate National Chemistry Week's 20th Anniversary and its theme, ""The Many Faces of Chemistry."" A special outreach event is being planned for Sunday, Aug. 19. Hands-on activities will focus on health and wellness.
The Committee on Corporation Associates (CCA) advises and influences ACS to ensure that its products and services are of value to industrial members and their companies. CCA vice chair, Roslyn White (SC Johnson), provided an overview of recent interactions between Corporation Associates and the U.K.-based Society of Chemical Industry (SCI).
CCA gave feedback to a recommendations report from the ACS Board Committee on Professional & Member Relations Task Force on Globalization. Presentations were also received from the ACS Green Chemistry Institute and SCI.
Staff reported on the Department of Industry Member Programs' activities since the San Francisco meeting. The report covered the Regional Industrial Innovation Awards, the World Congress on Industrial Biotechnology, the Analytical Pavilion sponsored by C&EN, and the ACS/Pharma Leaders Meeting.
The Awards/Finance & Grants Subcommittee reported that CCA received two funding proposals that total $7,500. Funding was provided to the following: The Committee on Economic & Professional Affairs at $3,000 for the Chicago symposium on ""Benefits Trends for the Chemical Workforce"" and the Office of Graduate Education and the Department of Career Development & Management at $4,500 for a workshop on ""Preparing for Life after Graduate School,"" to be held in conjunction with the 39th Central Regional Meeting.
The subcommittee also requested that ACS staff provide CCA with an official annual statement of Corporation Associates' financial reserves as of Jan. 1 of each year.
The Programs Subcommittee reported on planned programming activities in 2007 and beyond between CCA and SCI. The subcommittee gave an update on a Boston symposium cosponsored by Corporation Associates and the Medicinal Chemistry Division featuring past ACS Heroes of Chemistry from the pharmaceutical industry.
By request of the subcommittee, representatives from Chemical Abstracts Service gave an overview on AnaVist—a tool with potential applications for CCA's efforts to provide member companies with industry-relevant information and reports. The subcommittee also requested that CCA earmark approximately $20,000 of Corporation Associates funds in 2008 for its activities.
The Educational Outreach Subcommittee reported on its decision to collaborate with the Graduate Student Symposium Programming Committee of the Chemical Education Division on a graduate student industry roundtable program in Boston.
The subcommittee requested $5,000 in support of this effort. The subcommittee also discussed a request for corporate executive support of an American Association of Physics Teachers initiative to promote undergraduate research.
The Committee on Environmental Improvement (CEI) continues to be focused on the sustainability of the chemical enterprise. In Chicago, the committee introduced a multiyear effort to make ACS meetings more sustainable. This effort is designed to take advantage of the large size and diverse programs of the society to lead in the sustainability arena by ""walking the talk.""
The committee held a dialogue with representatives of the U.S. Environmental Protection Agency who are trying to ""green"" federal conferences and work with the travel and tourism industry to change practices and shrink the environmental footprint of meetings. The committee also used advertising and student volunteers to engage individual meeting participants in a campaign to increase recycling by asking ""Are you sustainable?"" Moving forward, CEI looks forward to working closely with the Committee on Meetings & Expositions to advance this agenda.
CEI was also pleased to participate in the meeting theme on sustainability through the ACS presidential programming. CEI cohosted the Monday presidential luncheon to discuss sustainability issues with the Committee on Science and is leading the follow-up to that luncheon, which will include recommendations on advancing sustainability in the three focal areas of the meeting—energy, food, and water.
The committee also continued its dialogue with the Committee on Corporation Associates about a collaborative workshop. This activity, tentatively slated for the New Orleans meeting, will seek additional insights from chemical and allied products companies about public policy barriers that limit adoption of more sustainable products and practices as well as policy incentives that would lead to increased sustainability in the chemical enterprise.
At its Chicago meeting, the committee welcomed the president of the Jordanian Chemical Society and the past-president of the Arab Union of Chemists.
The committee was briefed on Pittcon 2007, where, with financial support from the Society of Analytical Chemists of Pittsburgh, ACS coordinated participation of a scientific delegation from Adriatic nations.
The committee heard reports on the 2007 Frontiers of Chemical Science III: Research & Education in the Middle East meeting; the 2007 Transatlantic Frontiers of Chemistry meeting, which was jointly sponsored by ACS, the German Chemical Society, and the Royal Society of Chemistry; planned workshops to engage U.S. and Chinese early-career scientists in chemical biology, supramolecular, and new materials chemistry; and ACS Discovery Corps U.S./Brazil Research Collaboration Project in Biomass Conversion to Biofuels, Biomaterials & Chemicals.
The committee discussed Latin American engagement opportunities created through Puerto Rico's involvement in three key chemical science events there: the 2009 ACS Southeast Regional Meeting, the 2008 Federation of Latin American Chemical Associations (FLAQ) meeting, and the proposed IUPAC 2011 Congress & General Assembly.
The committee heard reports on letter-writing efforts by the ACS president to government officials in Libya and Mexico expressing concerns about challenges to the scientific freedom and human rights of scientists there.
The Committee on Minority Affairs (CMA) approved new vision, mission, and values statements at the Chicago national meeting. The mission of CMA is to increase the participation of minority chemical scientists and influence policy on behalf of minorities in ACS and the chemical enterprise.
An aggressive new strategic plan was approved by CMA to guide its activities over the next three years. By the end of 2009, CMA will increase the number of ACS Scholars that graduate to 100 per year, add 100 new minorities to leadership positions in ACS, engage in several collaborations, and increase the number of minority members of ACS by 5,000. CMA will focus initially on increasing minorities in ACS leadership. In working toward this goal, CMA began work on two new leadership-development programs for minority chemists.
CMA continues to support the work of the Joint Subcommittee on Diversity (JSD) in developing programs, products, and services to ensure full participation of all members in ACS. In Chicago, JSD premiered a diversity booth at the meeting exposition hall and cosponsored symposia.
The Committee on Patents & Related Matters (CPRM) discussed proposed legislative and regulatory changes to the U.S. patent system as well as open-access legislation and the potential effects such matters might have on industry and academia as well as on ACS.
CPRM also continued its work on several new educational tools to assist and inform members on patent issues and other intellectual property matters important to a successful career in the chemical enterprise. Many of these tools are now available on the committee's expanded website, membership.acs.org/C/CPRM/.
At the March 2007 meeting, the Committee on Professional Training (CPT) reviewed 42 new and additional information reports from ACS-approved chemistry programs. CPT held conferences with four schools seeking approval, discussed three updates and five site visit reports, and approved three new schools. The total number of ACS-approved chemistry programs is now 642.
The committee released the full draft of the ACS guidelines for review and comment. Copies of the draft were distributed to the department chairs at all ACS-approved schools, the chairs of all ACS committees, and the chairs of all ACS technical divisions.
Several CPT members met with the ACS technical divisions during the Chicago meeting to present an overview of the draft and obtain feedback. The draft guidelines document is available on the CPT website, and the committee invites any comments to be sent to cpt@acs.org.
In other business, the committee continued development of the two workshops with minority-serving institutions that will be held in 2007. The committee reviewed the editorial policies for the 2007 edition of the ACS Directory of Graduate Research, which is using a new protocol for retrieving research publication titles in an effort to improve the accuracy of the directory.
C&EN finished 2006 with an exceptionally strong editorial package. The first months of 2007 are proving to be equally successful in fulfilling the magazine's mission of keeping its readers informed. On the advertising side, revenues in 2006 increased for the second year in a row, and the early months of 2007 show continuing positive signs. The most significant editorial achievement was the successful launch of the redesign of the print edition of C&EN with the Oct. 16, 2006, issue.
The Subcommittee on Copyright has successfully updated the Copyright Module on the ACS Publications website. The subcommittee is looking into the possibility of conducting copyright programs at future ACS national and regional meetings.
The final monitoring reports for Chemistry of Materials, Journal of Agricultural & Food Chemistry, and Molecular Pharmaceutics were presented and accepted by the committee. Journal of Chemical Information & Modeling, Organic Letters, Accounts of Chemical Research, and the Journal of Chemical Theory & Computation will be monitored next.
3. Examining the scientific basis of public policies related to the chemical sciences and making recommendations to the appropriate ACS units.
In the first of these areas, ComSci partnered with President Hunt and the Committee on Environmental Improvement in planning and hosting a sustainability luncheon that featured roundtable discussions centering on a key sustainability question. At the Boston national meeting, ComSci will deliver a full-day program on the subject of ""Partnerships in Innovation & Competitiveness.""
Regarding the second thrust, ComSci will present two programs in Boston: a box lunch that will feature two speakers taking opposing sides on the subject of ""Genetic Screening & Diagnostic Testing: Do You Really Want to Know?"" and a symposium titled ""Creating & Sustaining International Research Collaborations.""
In support of the last thrust, ComSci is planning two events for 2008: ""Balancing Security & Openness"" will gather data to determine if the recent emphasis on security is hindering scientific progress and ""Transitioning Chemical Science to Commercially Successful Products.""
The Women Chemists Committee (WCC) hosted more than 70 attendees at its open meeting recently in Chicago, where representatives from Iota Sigma Pi, Women in Science & Engineering, the Association of Women in Science, and the Chicago local section helped WCC celebrate the committee's 80th anniversary.
The Women in Industry Breakfast was also highly successful with a new format of speed networking. More than 100 participants had the opportunity to practice their elevator speeches and make several professional connections. A related workshop will be offered by WCC in Boston.
In Chicago, WCC sponsored two symposia, ""Women Achieving Success: The ACS as a Platform in Leadership Development"" in honor of Madeleine Joullié's 80th birthday and the ACS Award for Encouraging Women into Careers in the Chemical Sciences: Symposium in Honor of Bojan H. Jennings.
More than 225 ACS meeting attendees were present for the biannual WCC Luncheon and heard the keynote speaker Laura Kiessling, 2007 Francis P. Garvan-John Olin Medal Recipient. Twelve women presented their research at this meeting with funding by the WCC/Eli Lilly Travel Grant Award. WCC members also spent time educating expo attendees on programs offered by the ACS Office of Diversity Programs at its new booth.
In Chicago, the Younger Chemists Committee (YCC) welcomed its new committee members with an information session centered on YCC's charter as well as on its strategic plan: to make ACS relevant to younger chemists, to involve younger chemists in all levels of the society, and to integrate younger chemists into the profession.
In January, YCC again hosted a Leadership Development Workshop during the ACS Leaders Conference. There were more than 80 applications for the 15 awards, which covered travel and registration for the conference. YCC plans to again fund the travel awards and provide leadership training for young chemists in 2008. YCC also solicited applications and selected a new graduate student representative on the Graduate Education Advisory Board.
During the Chicago meeting, YCC programs included ""Starting a Successful Research Program at a Predominantly Undergraduate Institution,"" ""Career Experiences at the Interface of Chemistry & Biology,"" and ""Chemistry Pedagogy 101.""
In addition to these programs, YCC cosponsored five programs with various committees and divisions. YCC continues to reach out to ACS committees and divisions and has initiated liaisonships with 11 technical divisions to encourage technical programming that highlights the contributions of younger chemists. Looking forward to Boston, YCC is planning symposia including ""The Many Faces of Chemistry: International Opportunities for Chemists""; ""Being a Responsible Chemist: Ethics, Politics & Policy""; and ""Changing Landscapes of the Bio-Pharma Industry.""
The Committee on Committees (ConC) conducted its annual training session for new national committee chairs at the ACS Leaders Conference in January 2007. ConC's interactive session for committee chairs in Chicago served as an opportune follow-on and a forum for informative interchange among seasoned and new chairs.
ConC began developing its recommendations for the 2008 committee chair appointments for consideration by the president-elect and chair of the board. ConC continues to focus efforts to identify members with the skills and expertise specified by the committee chairs using the councilor preference form.
The form will be sent to councilors in May. ConC also seeks the names of noncouncilor members for consideration for service on council-related committees, especially those with no prior appointment.
As part of ongoing activities with the joint CPC-Board Governance Review Task Force, ConC has collected data on committee liaisons to other committees. This information will be distributed to committee chairs. The number of liaisons indicates that unofficial but strong communication channels exist within the ACS committee structure.
On Sunday evening, the Committee on Nominations & Elections (N&E) sponsored its fifth successful Town Hall Meeting for President-Elect Nominees. An estimated 200 people attended this session. This forum facilitated communication among the 2008 president-elect nominees, councilors, and other members. N&E will hold another Town Hall Meeting featuring the candidates for director-at-large at the fall meeting in Boston.
Now that voting over the Internet has become an accepted procedure for ACS national elections, the ACS technical divisions and local sections have expressed strong interest in using this method for their elections. N&E has developed protocols for elections for local sections and divisions. This document will be forwarded to the appropriate committees for their review and distribution.
N&E is responsible for reviewing annually the distribution of member populations within the six electoral districts to ensure that the districts have equitable representation. According to bylaw V, section 4(a), the member population of each electoral district must be within 10% of the result of dividing by six the number of members whose addresses lie within these districts. The committee is happy to report that the six electoral districts are in compliance.
The committee has developed a petition on election procedures for president-elect and district director. The proposed election mechanism provides for a preferential (ranked) ballot and an ""instant runoff."" N&E continues to address the areas of campaigning and the timing of our national election process. Between the Chicago and Boston meetings, the committee plans to sponsor online forums for input from councilors and other interested members on these issues.
In response to member concerns regarding the collection of signatures for petition candidates, N&E reviewed the society's bylaws. The bylaws state that an endorsement is required, but does not stipulate the method of endorsement. N&E has determined that original or electronic signatures are acceptable and will establish appropriate procedures for receipt of electronic signatures.
The Committee on Constitution & Bylaws (C&B), acting for the council, issued new certified bylaws to the Corning Section, the Portland Section, the Division of Colloid & Surface Chemistry, and the Division of Chemical Education. The committee reviewed new proposed amendments for the Division of Medicinal Chemistry, the Columbus Section, the Detroit Section, and the Southern Arizona Section.
Three petitions were presented to council for action at this meeting. Regarding the ""Petition on Election Procedures 2006,"" a motion to separate the petition was approved, and the petition was divided. Provisions affecting bylaw V, sec. 2d, bylaw V, sec. 3c, and bylaw V, sec. 4f, which deal with election procedures and the timing of run-off elections, were approved by council and will become effective following confirmation by the board of directors.
The second part of the petition regarding bylaw V, sec. 2c, and bylaw V, sec. 3b, which deal with signature requirements for petition candidates for president-elect and director-at-large respectively, was recommitted to the Committee on Nominations & Elections, which has primary substantive responsibility for the petition.
The Committee on Nominations & Elections was asked to reconsider the signature requirements, procedures for acceptance of electronic signatures, and recommendations from the Governance Review Task Force on election procedures.
The second petition presented to council for action was the ""Petition on Rules for Nominating Members of N&E for National Offices."" This petition was not approved by council. The third petition, the ""Petition on Multiyear Dues,"" was amended by incidental motion on the council floor, calling for the petition to become effective when technical components are instituted to track payments, but no later than Jan. 1, 2010. Council approved the incidental motion and then approved the petition.
The committee reviewed one petition for consideration, the ""Petition on Local Section Affiliations,"" which will be submitted to council for action at the fall 2007 meeting in Boston.
The committee met with representatives of the Committee on Membership Affairs and the Governance Review Task Force to continue discussions on proposals currently being formulated on membership requirements and student membership. In addition, the committee discussed election issues of concern to the Southern California Section.
We hope you enjoyed the presidential and division thematic program, ""Sustainability of Energy, Food & Water"" in Chicago. A small, dedicated group of volunteers and staff labored tirelessly to create and coordinate this programming; to them the Committee on Divisional Activities (DAC) offers sincere thanks.
DAC has committed to transfer the process of choosing and organizing future national meeting themes to a body that represents all divisions. We made substantial progress in Chicago, where division, secretariat, and committee representatives convened to discuss national meeting program concepts. They proposed themes for the 2008 Philadelphia national meeting as well as a framework for a future national programming group.
Divisions have successfully served their members fortunate enough to attend national meetings. To maximize benefits to division members, DAC encourages divisions to consider extending the reach of the content they deliver at national meetings through Internet-based distribution channels and will support worthy efforts in this direction via Innovative Program Grants.
The committee voted in Chicago to propose modifications to the division funding formula that will more greatly reward interdisciplinary programming. The new formula will also be simpler and more transparent to divisions. DAC will present the revised plan to council for action in Boston.
The Committee on Economic & Professional Affairs (CEPA), working with ACS staff in the Departments of Career Management & Development and Member Research & Technology, continues to update and implement its strategic plan to address the career needs of society members.
Specifically, the committee reviewed and revised existing workshops and materials to help ACS members get jobs. CEPA is developing new programs to address the needs of mid- and late-career chemists to ensure their continued competitiveness in the workplace and to ease their career transitions. New initiatives in these areas include the development of workshops, online training, surveys to assess member needs, suggested changes to public policies, and updates to professional and ethical workplace guidelines. As a result of discussions at the Public Policy Roundtable, which was held in San Francisco, a background paper is being developed on trends in health care issues.
The newly revised ""Chemical Professional's Code of Conduct"" was presented to council, which approved it. The Standards & Ethics Subcommittee is preparing a revision of the ""Academic Professional Guidelines"" to be presented to council for consideration in Boston.
CEPA reviewed the Globalization Task Force Report. As our science diffuses around the globe, we want to make sure that our members are aware of the economic and professional challenges they will face and that they have the tools they need to succeed. Therefore, CEPA made a commitment to work with other committees, divisions, and ACS staff to develop programs and policies that position our membership to compete in the global workforce.
CEPA heard and discussed a presentation on the proposal from the Membership Affairs Committee on broadening the requirements of membership. CEPA supports the spirit of this proposal and encourages further detailed studies to assess financial impacts on local sections and student affiliates chapters.
The Local Section Activities Committee (LSAC) recognized local sections celebrating significant anniversaries in 2007, including Savannah River (50 years), Northeast Tennessee (75 years), and the St. Louis and Syracuse local sections (both celebrating 100 years).
LSAC hosted the local section leaders track in conjunction with the ACS Leaders Conference in Baltimore on Jan. 26–28. A total of 135 delegates from 124 local sections participated in the weekend leadership conference.
LSAC also hosted a Local Section Summit on March 2–4 in Arlington, Va. The summit focused on practical operational issues that will support local sections' long-term success. Specific areas that were discussed include the development of a multiyear plan to expand or develop programming for local sections, opportunities to encourage innovation and experimentation within and among local sections, and capitalizing on existing opportunities to facilitate partnerships between local sections and other ACS groups.
Following the San Francisco national meeting, LSAC launched a local section Science Café minigrant program. Fifty-five local sections accepted LSAC invitation to host Science Cafés in 2007.
A DVD entitled ""ACS Close to Home: Local Sections Connecting Chemistry & the Community"" was released earlier this year. The video provides a seven-minute overview of the many outreach and educational programs sponsored by local sections and the critical role they play in positively influencing the public's perception of chemistry and its practitioners. Copies of the DVD were sent to all local section officers.
The Committee on Meetings & Expositions (M&E) reported that the 233rd ACS national meeting hosted 14,520 attendees. This included 7,152 chemical scientists, 5,059 students, 1,283 exhibitors, 119 precollege teachers, 573 exposition visitors, and 453 guests. The exposition had 424 booths with 268 companies.
The 10 2006 regional meetings set a new standard for excellence with attendance exceeding 8,000, a 30% increase in average meeting attendance compared to the 2005 meetings. A total of 4,717 abstracts were submitted. A region summit was held in February at which the final report of the ReACT study group was reviewed.
The practice of tracking the number of presenter no-shows continues. M&E will collaborate with the Committee on Divisional Activities to study options for addressing this problem. Suggestions will be presented at the Boston meeting for implementation in 2008.
It is the intent of M&E to pursue the goal of making our meetings ""greener."" We will communicate with staff and governance units to identify actions for both the short and long term.
The American Institute of Chemical Engineers (AIChE) and ACS will hold their 2008 spring meetings simultaneously in New Orleans. An ad hoc working group consisting of members from M&E, DAC, and AIChE are actively exploring joint programming opportunities for this meeting.
The Committee on Membership Affairs (MAC) met in executive session on Saturday and Sunday in Chicago and reported that the ACS closed 2006 with 160,491 members, our highest year-end membership count since 2002. Of the 17,857 applications processed in 2006, more than 1,000 came from the Member-Get-a-Member campaign in which many councilors participated. The society's retention rate in 2006 remained strong at 92%. The committee also reported that recruitment for the first two months of 2007 netted 2,844 new applications—729 more than for the same time period last year.
MAC continues to work with deliberate speed on the proposed new bylaw language for members, student members, and society affiliates-the three ways to connect to the society. The committee received input from the Governance Review Task Force and its action teams, the Council Policy Committee, the board of directors, the Committee on Constitution & Bylaws, and several other committees between the San Francisco and Chicago meetings. These interactions have resulted in the current bylaw change recommendations.
In Chicago, representatives from MAC attended several committee meetings and all seven councilor caucuses to summarize the current proposal for membership changes, answer questions, and seek input. In addition, all committee chairs were invited to have their respective committees review these bylaw changes and respond to MAC—if possible—before council met on Wednesday. MAC received 11 responses: eight supported the proposed changes as is, and three supported the proposed language with specified changes or considerations.
The comprehensive petition will likely represent the most significant and voluminous change in the ACS bylaws that has occurred in decades, and MAC is proud to be among the leaders in its development and in efforts to get it right the first time. Hundreds of individuals have contributed to this major effort, since MAC began such discussions at the spring 2004 national meeting.
The Committee on Ethics met in Chicago and discussed the possibility of organizing and scheduling a committee retreat in the near future to enable the committee to move from the current stage of exploring the needs and interests of ACS members to setting priorities for the next few years.
The Project SEED program offers summer research opportunities for high school students from economically disadvantaged families. Since its inception in 1968, the program has had a significant impact on the lives of more than 8,400 students. At the selection meeting in March, the committee approved research projects for 340 SEED I students and 98 SEED II students for this summer in more than 100 institutions.
The 2006 annual assessment surveys from 300 students indicate that 78% of the Project SEED participants are planning to major in a chemistry-related science, and 66% aspire to continue to graduate education. This program is made possible by contributions from industry, academia, local sections, ACS friends and members, the ACS Petroleum Research Fund, and the Project SEED Endowment.
The committee formally submitted a request to ConC to amend the Project SEED acronym and the committee duties described in the Supplementary Information of the ""ACS Charter, Constitution, Bylaws & Regulations.""
In Chicago, the committee's agenda focused on the ACS Strategic Plan and how Project SEED fits into it, the Program Review Advisory Group (PRAG) review of the Project SEED program, the committee's review of an online application form, and planning of the 40th anniversary celebration to be held at the Philadelphia meeting in the fall of 2008. The committee selected a task force to review the criteria for selection of the Project SEED ChemLuminary Award.
3. Making ACS relevant to technicians.
Last year, CTA, along with the Division of Chemical Technicians, the Committee on Economic & Professional Affairs, and ChemTechLinks, started the Equipping the 2015 Chemical Technology Workforce initiative. This year, the initiative awarded six $500 minigrants to activities and programs that support the educational and professional development of chemical technicians.
We are pleased to announce that the winners of the minigrants are the ACS Division of Environmental Chemistry; the Chemical Technician Program Chair for the 39th ACS Central Regional Meeting in Covington, Ky.; Delta College, University Center, Mich.; Grand Rapids Community College, in Michigan; Mount San Antonio College, Walnut, Calif.; and Southwestern College in Chula Vista, Calif.
The winners are collaborating with industry, academia, and ACS local sections on such activities as chemical technology career fairs for high school students, discussion panels on employability skills for technicians, and technical programming at regional and national meetings on the vital role technicians have in the chemical enterprise.
Because of the enthusiastic response to the minigrants, Equipping the 2015 Chemical Technology Workforce will be supporting another round of minigrants to be distributed in the fall. Details will be available on the website. For more information, go to www.ChemTechLinks.org and click on ""Equipping the 2015 Chemical Technology Workforce.""
CTA has also joined with the Joint Subcommittee on Diversity, formerly known as the Collaboration of Committees Working Group. Because this group is focused on increasing diversity in ACS and the chemical enterprise, we believe that this is an opportunity to raise awareness of the value of technicians. CTA looks forward to collaborating on the promotion of traditionally underrepresented chemical professionals.
In 2007, CTA will be placing renewed focus on distribution of the ACS Chemical Technology Student Recognition Award. The award recognizes academic excellence in students preparing for careers as chemical technicians. For more information on the award, please visit the CTA website at chemistry.org/committees/cta.","['There are 14,520 attendees, including 7,152 chemical scientists, 5,059 students, 1,283 exhibitors, 119 precollege teachers, 573 exposition visitors, and 453 guests.']",6444,multifieldqa_en,en,,ddfed65e5495d0257b22a9537993c3453d8590cc94b8e0d6," The major actions taken by the board of directors and council during the national meeting in Chicago were reported in C&EN, April 30 (page 32). The society ended 2006 with a net contribution from operations of $12.2 million, on revenues of $424.0 million and expenses of $411.8 million. The committee discussed the Education Division programs that were reviewed in 2006 and those that will be reviewed in 2007, under the auspices of the Program Review Advisory Group. Committee members strongly support the proposal to include undergraduates as members of the society, but they requested that financial arrangements be clearly spelled out in the petition to ensure that the highly successful Student Affiliates program remains intact. Committee voted to accept the recommendations of the ACS Petroleum Research Fund Advisory Board (February 2007 meeting) for funding grants totaling $5.2million. voted to recommend to the ACS Board of Directors that a new national award be established, the ""ACS Award for Affordable Green Chemistry,"" sponsored by Rohm and Haas; and voted to create a new endowment to support research in agricultural chemistry for the period of 2007–12. The Committee on Professional Training discussed draft ACS guidelines for approval of bachelor's degree programs in chemistry. It also voted to contact nominators whose nominations for the Volunteer Service Award had expired and to invite them to reactivate their nomination packet for the 2008 Volunteer Service Awards. The Society Committee on Education received an update from President Catherine Hunt on the thematic programming featured in Chicago focusing on the sustainability of energy, food, and water. President-Elect Bruce Bursten solicited input from the committee pertaining to the central role of education in his agenda. It created a task force to consider the role of online, or ""virtual,"" simulations in the chemistry laboratory, recognizing the value of online/virtual experiences as a supplement to, but not a replacement for, hands-on laboratory experiments. It voted to propose to the board a screened list of six nominees (due to a two-way tie for fifth place) for the2008 Priestley Medal, and to recommend five nominees for the 2007 Award for Volunteer Service to ACS. For confidential support call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support on suicide matters call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or see www.samaritans.org. For support in the U.S., call the national suicide prevention Lifeline on 1-877-847-8457 or visithttp://www.-samaritan.org/pages/sophia. The committee met in executive session on March 23 and in open session jointly with the Joint Board-Council Committee on Publications and the Division of Chemical Information on March 26. The committee received a briefing on the status of the MORE Project: Multidisciplinary Opportunities though Resource Enhancement. Twenty-eight proposals were received, and a decision on which proposals to support will be made in early May. The chair led a discussion on draft 2007 committee goals, and committee members offered several suggestions related to successfully meeting them. Committee members agreed to the creation of a pilot ACS state government affairs advocacy program. The 2007 Public Policy Priorities Survey resulted in a four-tiered ranking of ACS's 2007 public policies. The ranking will help focus staff resources in conducting outreach and advocacy on behalf of ACS members. The Committee on Chemical Safety (CCS) provides advice on the handling of chemicals and seeks to ensure safe facilities, designs, and operations by calling attention to potential hazards and stimulating education in safe practices. Work has recently started on the tricameral ""Safety in Academic Chemistry Labs"" (SACL) The committee also reviewed the final report from the Special Board Task Force on the Review of the ACS National Awards Program, chaired by Ronald Breslow; established a Canvassing & Selection Subcommittee; and reviewed a list of external awards for which ACS may want to nominate candidates. the committee agreed to include the list of significant external awards in the awards locator database that is being developed. The final report will be delivered to the board at the 2007 Boston national meeting. Preliminary results are expected in May or June with a final report to be delivered at the national meeting in July or August. The board will decide on the 2007 membership survey results in the spring of 2007. A conference call on this issue was scheduled for April 21, 2007. The group's charge is to develop recommendations for a short- and long-term international strategy for the society. The panel also discussed the 2007-08 Presidential Task force on Enhancing Science & Technology, which is charged with developing advocacy best practices that can enhance ACS's attainment of its public policy priorities. One suggestion was to modify a communications goal to make it more completely reflect the duties of the committee outlined in the board regulations. The subcommittee agreed to examine the suggestion and revisit the question after the board retreat where board committee duties will be examined. The meeting was adjourned at 10:30 p.m. ET on March 28. The next meeting will be held on March 29. The members of the board will meet again on March 30. CCS has been involved in collaborations for the updating of publications like ""Prudent Practices in the Laboratory"" and ""ACS Guidelines for the Teaching of High School Chemistry"" The Committee on Chemists with Disabilities (CWD) met at the 233rd ACS national meeting, Chicago, on March 26. CWD Ambassador Program that was announced at CWD's 25th anniversary celebration at the Washington, D.C., meeting was discussed. CCA gave feedback to a recommendations report from the ACS Board Committee on Professional & Member Relations Task Force on Globalization. The third edition of the ""Chemical Safety Manual for Small Businesses"" will be ready soon. The Chemists Celebrate Earth Day celebration occurred in 120 local sections with 138 coordinators leading the efforts within their communities. This represents an increase of more than 30% in local section and coordinator participation from 2006. A special outreach event is being planned for Sunday, Aug. 19. Hands-on activities will focus on health and wellness. A shortcut to CCA's homepage was created: chemistry.org/committees/cca.html. The A was featured in C&EN's April 16th issue on page 53 of the ACS&EN magazine, ""Chemistry in Action"" (C&EN, April 16, issue 53, pages 1-3). The A is the acronym for the American Chemical Society's Association of Chemists and Technologists (ACS) and the American Institute of Chemical Engineers (AICE) (April 16, issues 1-2). The C was featured on the cover of the April 16 issue of the magazine, and the A was the headline, ""The Many Faces of Chemistry"" (April 17, issue 1, pages 3-4 of the ACS/CICE magazine, April 17, issues 3-5 of the CICE journal, April 18, issues 6-7 of theACS/ACE magazine. The C is the logo for the ACS/C ICE magazine (April 18, issue 6-8 of the CCI magazine, May 1, issue 9-10 of theCCI magazine), and the C was the cover for the May 1 issue of CICET (May 2, issue 10 of the ICCE magazine) (CICET, May 8, issue 11-12 of the CIET magazine, June 6, issue 12-13 of theCIET journal, June 14, issue 14-15 of theCCC. The CITC is the trade name for the Society of Chemical Industry (SCI), which is a member of the International Council of Chemical Scientists (ICCS) (May 6, 2012, issue 7). The CICT is the official trade name of the American College of Chemistry (ACC). CCA received two funding proposals that total $7,500. Funding was provided to the following: The Committee on Economic & Professional Affairs at $3,000 for the Chicago symposium on ""Benefits Trends for the Chemical Workforce"" and the Office of Graduate Education and the Department of Career Development & Management at $4,500 for a workshop on ""Preparing for Life after Graduate School"" The subcommittee also requested that CCA earmark approximately $20,000 of Corporation Associates funds in 2008 for its activities. The subcommittee gave an update on a Boston symposium cosponsored by Corporation Associates and the Medicinal Chemistry Division featuring past ACS Heroes of Chemistry from the pharmaceutical industry. The Educational Outreach Subcommittee reported on its decision to collaborate with the Graduate Student Symposium Programming Committee of the Chemical Education Division on a graduate student industry roundtable program in Boston. The committee heard reports on the 2007 Frontiers of Chemica meeting in Chicago, and the committee welcomed the president of the Jordanian Chemical Society and the past-president of the Arab Union of Chemists to the meeting. The Subcommittee on Environmental Improvement (CEI) continues to be focused on the sustainability of the chemical enterprise. CEI cohosted the Monday presidential luncheon to discuss sustainability issues with the Committee on Science and is leading the follow-up to that luncheon, which will include recommendations on advancing sustainability in the three focal areas of the meeting—energy, food, and water. This activity, tentatively slated for the New Orleans meeting, will seek additional insights from chemical and allied products companies about public policy barriers that limit adoption of more sustainable products and practices as well as policy incentives that would lead to increased sustainability inThe committee was briefed on Pittcon 2007, where, with financial support from the Society of Analytical Chemists of Pittsburgh, ACS coordinated participation of a scientific delegation from Adriatic nations. In Chicago, the committee introduced a multiyear effort to make ACS meetings more sustainable. This effort is designed to take advantage of the large size and diverse programs of the society to lead in the sustainability arena by ""walking the talk"" The committee held a dialogue with representatives of the U.S. Environmental Protection Agency who are trying to ""green"" federal conferences and work with the travel and tourism industry to change practices and shrink the environmental footprint of meetings. It also used advertising and student volunteers to engage individual meeting participants in a campaign to increase recycling by asking ""Are you sustainable?"" Moving forward, CEI looks forward to working closely with the committee on Meetings & Expositions to advance this agenda. The Committee on Minority Affairs (CMA) approved new vision, mission, and values statements at the Chicago national meeting. Committee on Patents & Related Matters (CPRM) discussed proposed legislative and regulatory changes to the U.S. patent system. Committee reviewed 42 new and additional information reports from ACS-approved chemistry programs. The committee reviewed the editorial policies for the 2007 edition of the ACS Directory of Graduate Research, which is using a new protocol for retrieving research publication titles. The Committee on Professional Training (CPT) held conferences with four schools seeking approval, discussed three updates and five site visit reports, and approved three new schools. The total number of ACS- approved chemistry programs is now 642. The CMA continues to support the work of the Joint Subcommittee on Diversity (JSD) in developing programs, products, and services to ensure full participation of all members in ACS. It also continued its work on several new educational tools to assist and inform members on patent issues and other intellectual property matters important to a successful career in the chemical enterprise. In other business, the committee continued development of the two workshops with minority-serving institutions that will be held in 2007. It discussed Latin American engagement opportunities created through Puerto Rico's involvement in three key chemical science events there: the 2009 ACS Southeast Regional Meeting, the 2008 Federation of Latin American Chemical Associations (FLAQ) meeting, and the proposed IUPAC 2011 Congress & General Assembly. It heard reports on letter-writing efforts by the ACS president to government officials in Libya and Mexico expressing concerns about challenges to the scientific freedom and human rights of scientists there. It was also discussed the 2007 Transatlantic Frontiers of Chemistry meeting, which was jointly sponsored by ACS, the German Chemical Society and the Royal Society of Chemistry; planned workshops to engage early-career scientists in chemical biology, supramolecular, and new materials chemistry; and ACS Discovery Corps U. s./Brazil Research Collaboration Project in Biomass Conversion to Biofuels, Biomaterials & Chemicals. It concluded that the committee released the full draft of the 2007 ACS guidelines for review and comment. The draft guidelines document is available on the CPT website and the committee invites any comments to be sent to cpt@acs.org. It finished 2006 with an exception, with the exception of the 2006 edition of C&EN, which had an exception to be published in the fall of that year. It is now available on its expanded website, membership.ACS.org/C/CPRm/. It is available to the public on the website and can be downloaded from the website for free. It can also be downloaded as a PDF from the ACS website. The Subcommittee on Copyright has successfully updated the Copyright Module on the ACS Publications website. ComSci is planning two events for 2008: ""Balancing Security & Openness"" and ""Transitioning Chemical Science to Commercially Successful Products"" The Women Chemists Committee (WCC) hosted more than 70 attendees at its open meeting recently in Chicago, where representatives from Iota Sigma Pi, Women in Science & Engineering, the Association of Women in science, and the Chicago local section helped WCC celebrate the committee's 80th anniversary. More than 225 ACS meeting attendees were present for the biannual WCC Luncheon and heard the keynote speaker Laura Kiessling, 2007 Francis P. Garvan-John Olin Medal Recipient. There were more than 80 applications for the 15 awards, which covered travel and registration for the conference. The subcommittee is looking into the possibility of conducting copyright programs at future ACS national and regional meetings. The final monitoring reports for Chemistry of Materials, Journal of Agricultural & Food Chemistry, and Molecular Pharmaceutics were presented and accepted by the committee. In Chicago, WCC sponsored two symposia, ""Women Achieving Success: The ACS as a Platform in Leadership Development"" and the ACS Award for Encouraging Women into Careers in the Chemical Sciences: Symposium in Honor of Bojan H. Jennings. YCC plans to again fund the travel awards and prov.ly strong editorial package for the ACS Office of Diversity Programs at its new booth at the 2007 ACS Leaders Conference in New York. The conference will also feature a Women in Industry Breakfast, with a new format of speed networking. A related workshop will be offered by WCC in Boston in 2008. In January, YCC again hosted a Leadership Development Workshop during the ACS leaders Conference. There was a strong turnout for the Leadership Development workshop, and more than 100 participants had the opportunity to practice their elevator speeches and make several professional connections. It was also the first time the YCC has hosted a leadership development workshop during the conference, which was a first for the group. The WCC will also hold a leadership workshop at the 2008 ACS Leaders conference in New Orleans, Louisiana, which will feature a panel discussion on the role of women in the chemical sciences in the 21st century. It will also be the first year that the WCC has sponsored a leadershipdevelopment workshop at a national ACS meeting. The first months of 2007 are proving to be equally successful in fulfilling the magazine's mission of keeping its readers informed. The most significant editorial achievement was the successful launch of the redesign of the print edition of C&EN with the Oct. 16, 2006, issue. The magazine's revenues in 2006 increased for the second year in a row, and early months of2007 show continuing positive signs. The Committee on Committees (ConC) conducted its annual training session for new national committee chairs at the ACS Leaders Conference in January 2007. ConC began developing its recommendations for the 2008 committee chair appointments for consideration by the president-elect and chair of the board. The Committee on Nominations & Elections (N&E) sponsored its fifth successful Town Hall Meeting for President-Elect Nominees. N&E is responsible for reviewing annually the distribution of member populations within the six electoral districts to ensure that the districts have equitable representation. The number of liaisons indicates that unofficial but strong communication channels exist within the ACS committee structure. The committee has developed a petition on election procedures for president-Elect and district director. The proposed election mechanism provides for a preferential (ranked) ballot and an ""instant runoff"" for the national election process. The group is planning symposia including ""The Many Faces of Chemistry: International Opportunities for Chemists""; ""Being a Responsible Chemist: Ethics, Politics & Policy""; and ""Changing Landscapes of the Bio-Pharma Industry"" in Boston, 2008. The ACS will hold another Town Hall meeting featuring the candidates for director-at-large at the fall meeting in Boston. It will also sponsor online forums for input from councilors and other interested members on these issues. The society's bylaws state that a petition for petition candidates can only be collected by members of the society. The bylaws also state that the bylaws are to be reviewed and updated every five years. It is hoped that this year will be the first year that the society will hold a national meeting that is open to the general public, not just the ACS members. The national meeting will be held in Washington, D.C., on September 14-15, 2008, and will be followed by a national conference in New York, New York and Washington, DC, on September 16-17. The conference will be sponsored by the American Society of Chemists (ASC) and the American Chemical Society (ACS). The ACS is a not-for-profit organization that promotes the study of chemistry, biology, and other related fields. It was founded in 1903. The ASC is a member of the American Philosophical Society (ASPS) and is the world's oldest society of chemists. It has been chartered by the Royal Society of Chemistry (R.A.C.S). It was established to promote the study and practice of chemistry and biology in the United States in the 1950s and 1960s. The Society was founded to encourage the growth and development of young chemists in the field. The Committee on Constitution & Bylaws (C&B), acting for the council, issued new certified bylaws. The committee reviewed new proposed amendments for the Division of Medicinal Chemistry, the Columbus Section, the Detroit Section, and the Southern Arizona Section. A motion to separate the ""Petition on Election Procedures 2006"" was approved. The Committee on Nominations & Elections was asked to reconsider the signature requirements, procedures for acceptance of electronic signatures, and recommendations from the Governance Review Task Force on election procedures. The second petition was not approved by council. The third petition was amended by incidental motion, calling for the petition to become effective when technical components are instituted to track payments, but no later than Jan. 1, 2010. The fourth petition was submitted to council for consideration, which will be submitted at the fall 2007 meeting in Boston. The fifth petition was presented to council to discuss election issues of concern to the Southern California Section of the C&B, which was approved for consideration. The final petition was to discuss the election issues for the Northern California Section, which were approved for discussion at the spring 2007 meeting, but not for consideration until after the spring 2008 meeting in New York. The sixth petition to be discussed was to consider the election of a president-at-large for the C & B Section. The seventh petition to consider was the nomination of a vice-president for the N&B Section, to be submitted for consideration at the autumn 2007 meeting. The last petition to discuss was the nominating process for a director-in-residence, to which the committee of directors agreed to refer the matter to the council of directors. The petition was approved by the council and will become effective after the fall 2008 meeting, when it will be considered for consideration by the full C &B Section. It will be known as the ""N&B Nominating Process for the President-At-Large"" petition, which is expected to be approved in the spring 2009 meeting. It is the first time the committee has considered the nomination process for the position of director in the past two years. It was the first of its kind, and will be the only time in the history of the organization that the process has been considered. The proposal will be discussed at a meeting of the Council of Directors, which also includes the Board of Directors of the University of California, San Diego. The council will also consider a proposal for a division-by-division program for the 2008-2009 season. The division-wide program will be called ""Sustainability of Energy, Food & Water"" The Committee on Economic & Professional Affairs (CEPA) continues to update and implement its strategic plan to address the career needs of society members. The committee reviewed and revised existing workshops and materials to help ACS members get jobs. The newly revised ""Chemical Professional's Code of Conduct"" was presented to council, which approved it. The Standards & Ethics Subcommittee is preparing a revision of the ""Academic Professional Guidelines"" to be presented for consideration in Boston. The Committee on Meetings & Expositions (M&E) reported that the 233rd ACS national meeting hosted 14,520 attendees. This included 7,152 chemical scientists, 5,059 students, 1,283 exhibitors, 119 precollege teachers, 573 exposition visitors, and 453 guests. The 10 2006 regional meetings set a new standard for excellence with attendance exceeding 8,000, a 30% increase in averag. The Local Section Activities Committee (LSAC) recognized local sections celebrating significant anniversaries in 2007, including Savannah River (50 years), Northeast Tennessee (75 years), and the St. Louis and Syracuse local sections (both celebrating 100 years). The LSAC hosted the local section leaders track in conjunction with the ACS Leaders Conference in Baltimore on Jan. 26–28. A total of 135 delegates from 124 local sections participated in the weekend leadership conference. The summit focused on practical operational issues that will support local sections' long-term success. Specific areas that were discussed include the development of a multiyear plan to expand or develop programming for local sections, opportunities to encourage innovation and experimentation within and among local sections. TheLSAC also hosted a Local Section Summit on March 2–4 in Arlington, Va. A video entitled ""ACS Close to Home: Local Sections Connecting Chemistry & the Community"" was released earlier this year. Copies of the DVD were sent to all local section officers. The video provides a seven-minute overview of the many outreach and educational programs sponsored by local sections and the critical role they play in positively influencing the public's perception of chemistry and its practitioners. The DVD also provides an overview of some of the issues facing local sections in 2007. The LSC launched a local section Science Café minigrant program. Fifty-five local sections accepted LSAC invitation to host Science Cafés in 2007 for the first time in the U.S. The program is open to members of the LSC and is available to the public through the ACS Web site. It is available in English, Spanish, French, German, and Italian. It has been available for a few months and is free to all members. ACS closed 2006 with 160,491 members, our highest year-end membership count since 2002. Of the 17,857 applications processed in 2006, more than 1,000 came from the Member-Get-a-Member campaign. The Project SEED program offers summer research opportunities for high school students from economically disadvantaged families. At the selection meeting in March, the committee approved research projects for 340 SEED I and 98 SEED II students for this summer in more than 100 institutions. The committee also reported that recruitment for the first two months of 2007 netted 2,844 new applications—729 more than for the same time period last year. The Committee on Ethics met in Chicago and discussed the possibility of organizing and scheduling a committee retreat in the near future to enable the committee to move from the current stage of exploring the needs and interests of ACS members to setting priorities for the next few years. It is the intent of M&E to pursue the goal of making our meetings ""greener."" We will communicate with staff and governance units to identify actions for both the short and long term. The American Institute of Chemical Engineers (AIChE) and ACS will hold their 2008 spring meetings simultaneously in New Orleans. Since its inception in 1968, the program has had a significant impact on the lives of more than 8,400 students. The 2006 annual assessment surveys from 300 students indicate that 78% of the Project SEed participants are planning to major in a chemistry-related science, and 66% aspire to continue to graduate education. The society's retention rate in 2006 remained strong at 92%. The committee received 11 responses: eight support the proposed changes as is, and three supported the proposed language with specified changes or considerations. The comprehensive petition will likely represent the most significant and voluminous change in the ACS bylaws that has occurred in decades, and MAC is proud to be among the leaders in its development and in efforts to get it right the first time. In addition, all committee chairs were invited to have their respective committees review these bylaw changes and respond to MAC—if possible—before council met on Wednesday. The meeting attendance at this year's ACS meeting was higher than the 2005 meeting compared to the 2005 meetings. A total of 4,717 abstracts were submitted. A region summit was held in February at which the final report of the ReACT study group was reviewed. The practice of tracking the number of presenter no-shows continues. M&e will collaborate with the Committee on Divisional Activities to study options for addressing this problem. Suggestions will be presented at the Boston meeting for implementation in 2008 for implementation of this problem in 2008. ittee formally submitted a request to ConC to amend the Project SEED acronym. The committee selected a task force to review the criteria for selection of the ProjectSEED ChemLuminary Award. CTA will be placing renewed focus on distribution of the ACS Chemical Technology Student Recognition Award. The award recognizes academic excellence in students preparing for careers as chemical technicians. For more information on the award, please visit the CTA website at chemistry.org/committees/cta-recognition-award. The CTA has also joined with the Joint Subcommittee on Diversity, formerly known as the Collaboration of Committees Working Group, to promote diversity in the field of chemical technology. The JSCWG will be supporting another round of minigrants to be distributed in the fall. The 2015 Equipping the 2015 Chemical Technology Workforce initiative will be available on the website at www.ChemTechLinks.org and click on ""Equipping the2015 Chemical Workforce"" for more information. The program will provide information on how to get involved in the program, as well as how to submit an application for the program. The website will be updated with the latest information about the program in the coming weeks and months. It will also provide information about how to apply for the Program Review Advisory Group (PRAG) and how to send an application to the PRAG. The PRAg will also be available to the public for the first time in a number of ways, such as by sending an email to pra.research@acsca.org or by mailing it to: p.r.news@acsm.org. The project will be held at the 39th ACS Central Regional Meeting in Covington, Kentucky, on September 26-28, 2015. The event will be open to the general public, with a limited number of tickets available for those who wish to attend."
What did Justice Kennedy argue about Quill in Direct Marketing Ass'n v. Brohl?,"South Dakota v. Wayfair, Inc. - Harvard Law Review
Fourth Circuit Invalidates Maryland Statute Regulating Price Gouging in the Sale of Generic Drugs.
South Dakota Supreme Court Holds Unconstitutional State Law Requiring Internet Retailers Without In-State Physical Presence to Remit Sales Tax.
Judicial junk, the Court has long thought, is easier to scrap when the erroneous precedent cannot be fixed by Congress, as in constitutional cases.1× 1. See Burnet v. Coronado Oil & Gas Co., 285 U.S. 393, 405–10 (1932) (Brandeis, J., dissenting); Lee Epstein, William M. Landes & Adam Liptak, The Decision to Depart (or Not) from Constitutional Precedent: An Empirical Study of the Roberts Court, 90 N.Y.U. L. Rev. 1115, 1116 (2015) (“[Justice Brandeis’s] dissenting opinion . . . now has the status of black letter law.”). On the flip side, whenever a bad precedent can be corrected by Congress, stare decisis applies with “special force.”2× 2. See Patterson v. McLean Credit Union, 491 U.S. 164, 172–73 (1989). The Court, following Justice Brandeis, usually articulates the rule as distinguishing between “constitutional” and “statutory” precedents. See, e.g., id. But the distinction is occasionally said to be between “constitutional” and “nonconstitutional cases.” See, e.g., Glidden Co. v. Zdanok, 370 U.S. 530, 543 (1962) (plurality opinion). Nomenclature aside, the Court has — until now — adhered to Justice Brandeis’s key insight that the important factor is whether or not the mistake may be legislatively corrected. Last Term, in South Dakota v. Wayfair, Inc.,3× 3. 138 S. Ct. 2080 (2018). the Court tinkered with this thinking in overruling an outdated dormant commerce clause precedent. Dormant commerce clause decisions technically produce constitutional holdings, but Congress may override them at will.4× 4. See Prudential Ins. Co. v. Benjamin, 328 U.S. 408, 421–27 (1946). Under the usual logic of stare decisis, it should take special force to dislodge such precedents. But Wayfair applied the weakened stare decisis of constitutional cases, asserting that the Court must “address a false constitutional premise . . . . whether or not Congress can or will act.”5× 5. Wayfair, 138 S. Ct. at 2096–97.
Emerging from Wayfair is an odd and ominous development in stare decisis doctrine. Odd, because it turns on a formal classification instead of on Congress’s practical ability to fix the problem. Ominous, because the Court’s logic leads far past the dormant commerce clause. Wayfair grants only feeble stare decisis to precedents that set a “constitutional default rule,”6× 6. Id. at 2096 (“While . . . Congress has the authority to change the physical presence rule, Congress cannot change the constitutional default rule.”). meaning constitutional decisions that allow for legislative adjustment or override. This new stare decisis analysis makes other precedents setting constitutional default rules more vulnerable — including, perhaps, mainstays of criminal procedure like Miranda v. Arizona7× 7. 384 U.S. 436 (1966). and Mapp v. Ohio.8× 8. 367 U.S. 643 (1961).
Since its 1967 decision in National Bellas Hess, Inc. v. Department of Revenue,9× 9. 386 U.S. 753 (1967). the Court has held that, under the “dormant” or “negative” implication of the Commerce Clause,10× 10. The dormant or negative commerce clause is a judicial derivation from the Commerce Clause “prohibiting States from discriminating against or imposing excessive burdens on interstate commerce without congressional approval,” which “strikes at one of the chief evils that led to the adoption of the Constitution, namely, state tariffs and other laws that burdened interstate commerce.” Comptroller of the Treasury of Md. v. Wynne, 135 S. Ct. 1787, 1794 (2015). states may not compel remote sellers with no physical presence in the state to collect and remit sales taxes.11× 11. See Bellas Hess, 386 U.S. at 759–60. In Quill Corp. v. North Dakota,12× 12. 504 U.S. 298 (1992). the Court refused to overrule the “bright-line, physical-presence requirement” of Bellas Hess, leaning heavily on stare decisis.13× 13. Id. at 317–18. Three Justices joined a concurrence explaining that their decision rested solely “on the basis of stare decisis.” Id. at 320 (Scalia, J., concurring in part and concurring in the judgment). So the physical presence test remained the law of the land while the internet conquered the earth. Justice Kennedy had joined the Quill majority and Justice Scalia’s concurring opinion emphasizing stare decisis, but by 2015 he had second thoughts. Writing separately in Direct Marketing Ass’n v. Brohl,14× 14. 135 S. Ct. 1124 (2015). Justice Kennedy acknowledged that “[t]he Internet has caused far-reaching systemic and structural changes in the economy” and therefore “Quill now harms States to a degree far greater than could have been anticipated earlier.”15× 15. Id. at 1135 (Kennedy, J., concurring). He concluded with the wish that “[t]he legal system should find an appropriate case for this Court to reexamine Quill and Bellas Hess.”16× 16. Id.
Seldom has a concurring opinion signed by a lone Justice prompted a state to officially declare an emergency. Yet in 2016, in response to Justice Kennedy’s overture, the South Dakota legislature passed a law, S.B. 106, “to provide for the collection of sales taxes from certain remote sellers . . . and to declare an emergency.”17× 17. 2016 S.D. Sess. Laws ch. 70 pmbl. 217 (codified at S.D. Codified Laws § 10-64 (2017)). It required every remote seller to collect and remit sales tax if the seller’s business in South Dakota comprised either a “gross revenue” greater than $100,000 or at least 200 “separate transactions” within one calendar year.18× 18. Id. § 1. Significantly, the law did not apply retroactively.19× 19. Id. § 5. The “emergency” declaration was necessary to give the law immediate effect, for the purpose of “permitting the most expeditious possible review of the constitutionality of this law” by the U.S. Supreme Court.20× 20. Id. § 8(8). As Justice Alito put it, the “South Dakota law [was] obviously a test case.”21× 21. Transcript of Oral Argument at 27, Wayfair, 138 S. Ct. 2080 (No. 17-494), https://www.supremecourt.gov/oral_arguments/argument_transcripts/2017/17-494_7lho.pdf [https://perma.cc/8HYH-VU8N].
Expeditiously, a group of remote sellers challenged the law. After being sued by South Dakota for refusing to register for the newly required sales tax license, Wayfair, Inc., Overstock.com, Inc., and Newegg, Inc. moved for summary judgment in South Dakota circuit court on the grounds that S.B. 106 was unconstitutional under Quill and Bellas Hess — a point South Dakota conceded, indicating that it was seeking review by the U.S. Supreme Court to overturn Quill.22× 22. State v. Wayfair Inc., 2017 SD 56, ¶¶ 9–11, 901 N.W.2d 754, 759–60. Accordingly, the South Dakota circuit court granted the motion for summary judgment and South Dakota appealed to the state’s highest court.23× 23. Id. ¶ 12, 901 N.W.2d at 760. The South Dakota Supreme Court unanimously affirmed, recognizing that South Dakota’s “arguments on the merits” may be “persuasive” but “Quill remains the controlling precedent.”24× 24. Id. ¶ 18, 901 N.W.2d at 761. See generally Recent Case, State v. Wayfair Inc., 2017 SD 56, 901 N.W.2d 754 (S.D. 2017), 131 Harv. L. Rev. 2089 (2018).
The U.S. Supreme Court vacated and remanded.25× 25. Wayfair, 138 S. Ct. at 2100. Writing for the Court one last time, Justice Kennedy26× 26. Justices Thomas, Ginsburg, Alito, and Gorsuch joined Justice Kennedy’s opinion. pilloried Quill’s physical presence rule as “arbitrary, formalistic,” “anachronistic,” and “unfair and unjust” to both states and brick-and-mortar retailers.27× 27. Wayfair, 138 S. Ct. at 2092, 2095. After all, the rationale of Quill was that remote sellers lacked a sufficiently “substantial nexus” with the state to justify imposing a duty of tax collection.28× 28. Quill Corp. v. North Dakota, 504 U.S. 298, 311 (1992) (quoting Complete Auto Transit, Inc. v. Brady, 430 U.S. 274, 279 (1977)). This was wrong even in the mail-order catalog days of 1967 and 1992, but “the Internet revolution has made [Quill’s] earlier error all the more egregious and harmful.”29× 29. Wayfair, 138 S. Ct. at 2097; see also id. at 2092. The rule deprived the states of billions of dollars, since they could not force remote sellers to collect the tax and consumers hardly ever paid it on their own.30× 30. Id. at 2088 (“[C]onsumer compliance rates are notoriously low.”). Quill “serve[d] as a judicially created tax shelter” for remote retailers who do a great deal of business online.31× 31. Id. at 2094.
Satisfied that Bellas Hess and Quill were wrongly decided, the Court then jumped the hurdle of stare decisis. The Quill Court had feared upsetting reliance interests.32× 32. Quill, 504 U.S. at 317 (“Bellas Hess . . . has engendered substantial reliance and has become part of the basic framework of a sizable industry.”). Wayfair shrugged off this concern, noting that “stare decisis accommodates only ‘legitimate reliance interest[s]’”; by contrast, reliance on the physical presence rule was largely due to consumers evading their use-tax obligations.33× 33. Wayfair, 138 S. Ct. at 2098 (alteration in original) (quoting United States v. Ross, 456 U.S. 798, 824 (1982)). Quill had also appealed to Congress’s ultimate authority over interstate commerce as a reason to abide by a precedent, even if wrongly decided.34× 34. See Quill, 504 U.S. at 318–19; id. at 320 (Scalia, J., concurring in part and concurring in the judgment) (“Congress . . . can change the rule of Bellas Hess by simply saying so.”). But Wayfair denied that Congress’s ability to change the law was a proper consideration:
While it can be conceded that Congress has the authority to change the physical presence rule, Congress cannot change the constitutional default rule. It is inconsistent with the Court’s proper role to ask Congress to address a false constitutional premise of this Court’s own creation. Courts have acted as the front line of review in this limited sphere; and hence it is important that their principles be accurate and logical, whether or not Congress can or will act in response.35× 35. Wayfair, 138 S. Ct. at 2096–97.
Having dispensed with the physical presence rule, the Court remanded the case to the South Dakota courts to determine in the first instance “whether some other principle in the Court’s Commerce Clause doctrine might invalidate the Act.”36× 36. Id. at 2099. But the Court listed “several features [of South Dakota law] that appear[ed] designed to prevent discrimination against or undue burdens upon interstate commerce.” Id.
Justices Thomas and Gorsuch each filed concurring opinions. Justice Thomas wistfully likened himself to Justice White — who voted for Bellas Hess but against Quill a quarter-century later — and confessed that he “should have joined [Justice White’s dissenting] opinion.”37× 37. Id. at 2100 (Thomas, J., concurring). Justice Thomas added that the “Court’s entire negative Commerce Clause jurisprudence” is wrong and should be abandoned.38× 38. Id. Justice Gorsuch also wrote separately to express skepticism of the Court’s dormant commerce clause jurisprudence, raising “questions for another day” of whether the doctrine “can be squared with the text of the Commerce Clause, justified by stare decisis, or defended as misbranded products of federalism or antidiscrimination imperatives flowing from Article IV’s Privileges and Immunities Clause.”39× 39. Id. at 2100–01 (Gorsuch, J., concurring).
Chief Justice Roberts dissented.40× 40. Justices Breyer, Sotomayor, and Kagan joined the Chief Justice’s dissent. Surprisingly, the dissenting Justices “agree[d] that Bellas Hess was wrongly decided, for many of the reasons given by the Court.”41× 41. Wayfair, 138 S. Ct. at 2101 (Roberts, C.J., dissenting). The dispute between the majority and the dissent turned entirely on the principles and application of stare decisis. Chief Justice Roberts argued that whether or how to reverse Quill should be left to Congress, which “has the flexibility to address these questions in a wide variety of ways” and “can focus directly on current policy concerns rather than past legal mistakes.”42× 42. Id. at 2104. He also pointed to the “baffling” burdens of compliance with the idiosyncratic tax codes of “[o]ver 10,000 jurisdictions,” particularly for small businesses, and doubted that new “software” — the majority’s proposed solution to this mess43× 43. Id. at 2098 (majority opinion) (“Eventually, software that is available at a reasonable cost may make it easier for small businesses to cope with these problems.”). — would soon solve the problem.44× 44. Id. at 2103–04 (Roberts, C.J., dissenting). In Bellas Hess, the Court reasoned that the dormant commerce clause protects interstate business from being “entangle[d] . . . in a virtual welter of complicated obligations to local jurisdictions.” Nat’l Bellas Hess, Inc. v. Dep’t of Revenue, 386 U.S. 753, 759–60 (1967). The dissent replied that the Court “vastly underestimate[d] the skill of contemporary man and his machines.” Id. at 766 (Fortas, J., dissenting). The dispute in Wayfair over whether software is up to the task effectively reprised the old debate from Bellas Hess, only this time couched as part of the stare decisis inquiry’s concern for reliance interests rather than as a matter of dormant commerce clause doctrine. While Wayfair acknowledged that “[c]omplex state tax systems could have the effect of discriminating against interstate commerce,” 138 S. Ct. at 2099, the Court remarked that “[t]he physical presence rule is a poor proxy” for an inquiry into any actual burdens imposed on interstate commerce, id. at 2093.
Chief Justice Roberts emphasized that a “heightened form of stare decisis”45× 45. Wayfair, 138 S. Ct. at 2102 (Roberts, C.J., dissenting). applies when “Congress . . . can, if it wishes, override this Court’s decisions with contrary legislation.”46× 46. Id. at 2101 (first citing Michigan v. Bay Mills Indian Cmty., 134 S. Ct. 2024, 2036 (2014) (tribal sovereign immunity); then citing Kimble v. Marvel Entm’t, LLC, 135 S. Ct. 2401, 2409 (2015) (statutory interpretation); and then citing Halliburton Co. v. Erica P. John Fund, Inc., 134 S. Ct. 2398, 2411 (2014) (judicially created doctrine implementing a judicially created cause of action)). In Quill, the Chief Justice noted, the Court had taken to heart that “Congress may be better qualified” and “has the ultimate power to resolve” the question47× 47. Id. at 2102 (quoting Quill Corp. v. North Dakota, 504 U.S. 279, 318 (1992)). while Justice Scalia had “recogniz[ed] that stare decisis has ‘special force’ in the dormant Commerce Clause context due to Congress’s ‘final say over regulation of interstate commerce.’”48× 48. Id. (quoting Quill, 504 U.S. at 320 (Scalia, J., concurring in part and concurring in the judgment)). Moreover, “[i]f stare decisis applied with special force in Quill, it should be an even greater impediment” afterward since Quill effectively “tossed [the ball] into Congress’s court.”49× 49. Id. (alteration in original) (quoting Kimble, 135 S. Ct. at 2409); cf. Bay Mills, 134 S. Ct. at 2039 n.12 (“When we inform Congress that it has primary responsibility over a sphere of law, and invite Congress to consider a specific issue within that sphere, we cannot deem irrelevant how Congress responds.”). Because the Court invited Congress to act and then “suddenly chang[ed] the ground rules, the Court may have waylaid Congress’s consideration of the issue.”50× 50. Wayfair, 138 S. Ct. at 2102–03 (Roberts, C.J., dissenting).
In Wayfair, the Court applied the flimsier form of stare decisis to a precedent that could have been overruled by Congress. It did so in the context of a dormant commerce clause case, but Wayfair’s logic extends to all constitutional default rules — that is, constitutional decisions that Congress remains free to change. Not only does Wayfair deviate from the Court’s decades-old stare decisis analysis, it also imperils other precedents that set constitutional default rules.
The Court’s reasoning in Wayfair departs from its prior stare decisis analysis. In 1932, Justice Brandeis posited that stare decisis must bend “in cases involving the Federal Constitution, where correction through legislative action is practically impossible.”51× 51. Burnet v. Coronado Oil & Gas Co., 285 U.S. 393, 406–07 (1932) (Brandeis, J., dissenting). The Court has long since adopted his argument,52× 52. See, e.g., Smith v. Allwright, 321 U.S. 649, 665 (1944). as well as its corollary — that stare decisis commands “special force in the area of statutory interpretation” where “Congress remains free to alter what [the Court has] done.”53× 53. Patterson v. McLean Credit Union, 491 U.S. 164, 172–73 (1989). For normative evaluations of heightened stare decisis for statutory precedents, see generally Einer Elhauge, Statutory Default Rules: How to Interpret Unclear Legislation 211–23 (2008); and William N. Eskridge, Jr., Overruling Statutory Precedents, 76 Geo. L.J. 1361, 1364–1409 (1988). Justice Brandeis’s logic demands that dormant commerce clause cases, where Congress is free to act, be granted the weightier stare decisis.54× 54. Scholars have noted the curious fact that Justice Brandeis included many dormant commerce clause cases as examples of overruled constitutional precedents. See, e.g., Earl M. Maltz, Commentary, Some Thoughts on the Death of Stare Decisis in Constitutional Law, 1980 Wis. L. Rev. 467, 468–469, 469 n.11. One explanation for this is that Justice Brandeis sought the authority of Chief Justice Taney’s dictum that the Court’s “opinion upon the construction of the Constitution is always open to discussion” — which referred to the dormant commerce clause. See Burnet, 285 U.S. at 408 n.3 (Brandeis, J., dissenting) (quoting The Passenger Cases, 48 U.S. (7 How.) 283, 470 (1849) (Taney, C.J., dissenting)). In Chief Justice Taney’s time, it was thought that Congress could not override the Court’s dormant commerce clause decisions, see Cooley v. Bd. of Wardens, 53 U.S. (12 How.) 299, 321 (1852), so the context of Chief Justice Taney’s dictum does not conflict with Justice Brandeis’s theory of stare decisis. The Court applied this reasoning in Quill, as Chief Justice Roberts underscored.55× 55. Wayfair, 138 S. Ct. at 2102 (Roberts, C.J., dissenting).
Yet the Wayfair majority refused to consider Congress’s authority to legislate as a relevant factor for stare decisis.56× 56. Even Justice Kennedy’s earlier opinion in Direct Marketing contemplated judicially overruling Quill, conspicuously neglecting a possible legislative solution. See supra p. 278. The Court even insisted that to do so “is inconsistent with the Court’s proper role,” since Quill embodied “a false constitutional premise of th[e] Court’s own creation.”57× 57. Wayfair, 138 S. Ct. at 2096 (emphasis added). This refusal breaks from the practical Brandeisian wisdom that has guided the Court’s treatment of precedent for the better part of a century. The point is not that stare decisis should have ultimately propped up Bellas Hess yet again, as Wayfair’s dissenting Justices maintained. After all, a realistic approach that is alert to each branch’s institutional capacities might have led to the conclusion that Congress was actually ill-equipped to overrule Quill. In this vein, the Court could have sensibly pointed out that Congress is unlikely to stick its neck out with a tax hike (or a look-alike) from which only the states would benefit.58× 58. For two practical arguments to this effect, see Brian Galle, Essay, Kill Quill, Keep the Dormant Commerce Clause: History’s Lessons on Congressional Control of State Taxation, 70 Stan. L. Rev. Online 158, 160–62 (2018), https://review.law.stanford.edu/wp-content/uploads/sites/3/2018/03/70-Stan.-L.-Rev.-Online-158-Galle.pdf [https://perma.cc/22YP-P4V5]; Edward A. Zelinsky, The Political Process Argument for Overruling Quill, 82 Brook. L. Rev. 1177, 1191–92 (2017). Indeed, South Dakota advanced such practical arguments in its brief.59× 59. See Petitioner’s Brief at 54, Wayfair, 138 S. Ct. 2080 (No. 17-494) (“Congress has little incentive to act here because it would be (or appear to be) authorizing new or greater tax collections from its constituents, while receiving none of the revenue in return.”). More generally, the Court might have discussed the limits of the states’ influence in the federal system as a reason not to wait for congressional intervention, a topic it has debated on other occasions.60× 60. See Richard H. Pildes, Institutional Formalism and Realism in Constitutional and Public Law, 2013 Sup. Ct. Rev. 1, 30–32; see also Galle, supra note 58, at 159 (“Congress is not a trustworthy guardian of state fiscal power, making continuing judicial involvement a more appealing prospect.”). Or it could have argued that new facts on the ground — namely, the blast of e-commerce that hit like a comet after Quill — overpowered stare decisis of any force, special or plain.61× 61. Two recent studies of stare decisis highlighted the physical presence rule as exemplifying a precedent that may reasonably be overruled due to changed facts. See Bryan A. Garner et al., The Law of Judicial Precedent 364–65 (2016); Randy J. Kozel, Settled Versus Right: A Theory of Precedent 112–13 (2017). It should be noted that the authors of The Law of Judicial Precedent classify the physical presence rule as a constitutional precedent for stare decisis purposes, thus anticipating the Court’s misstep in Wayfair. Garner et al., supra, at 354–65. Because even statutory precedents may sometimes be overruled,62× 62. See Patterson v. McLean Credit Union, 491 U.S. 164, 173–74 (1989) (discussing justifications for overruling statutory precedents). Contra Lawrence C. Marshall, “Let Congress Do It”: The Case for an Absolute Rule of Statutory Stare Decisis, 88 Mich. L. Rev. 177 (1989). the Court could have killed Quill without first planting its constitutional kiss of death.63× 63. Cf. Thomas R. Lee, Stare Decisis in Historical Perspective: From the Founding Era to the Rehnquist Court, 52 Vand. L. Rev. 647, 704 (1999) (“Justice Brandeis’ . . . memorable prose has since become a mandatory part of the burial rite for any constitutional precedent.”).
The Court resisted such arguments. Instead, Wayfair reasoned that Congress’s total ability to correct an erroneous decision counts for nothing when the Court gets the Constitution wrong. That such a theory sprouts from a case like Wayfair, which repudiated a “formalistic distinction,”64× 64. Wayfair, 138 S. Ct. at 2092. is ironic. Wayfair’s stare decisis analysis resorts to the formalism of making constitutional a “magic” word65× 65. See Transcript of Oral Argument, supra note 21, at 12. rather than asking whether Congress can step in.
Moreover, the Court’s new thinking on stare decisis threatens other constitutional default rules. Wayfair now stands for the proposition that a “constitutional default rule” — a term the Court apparently lifted from South Dakota’s reply brief on the merits66× 66. Reply Brief at 22, Wayfair, 138 S. Ct. 2080 (No. 17-494) (“Congress is polarized, which makes it critical . . . to get the constitutional default rule right.”). — gets only weakened stare decisis. To appreciate why this holding matters, it is worth exploring the concept and scope of constitutional default rules. Contract theory describes default rules as legal rules that the parties may “contract around.”67× 67. See, e.g., Ian Ayres & Robert Gertner, Filling Gaps in Incomplete Contracts: An Economic Theory of Default Rules, 99 Yale L.J. 87, 87 (1989). Although “constitutional default rule” could be read broadly to include a variety of actors and contracting mechanisms,68× 68. See John Ferejohn & Barry Friedman, Toward a Political Theory of Constitutional Default Rules, 33 Fla. St. U. L. Rev. 825, 826 (2006) (“When we speak of default rules in constitutional law, we typically are talking about specifications of ways the government can act (or modify its behavior) to get around a constitutional prohibition.”). the Court’s use of the term for purposes of stare decisis may be narrowly defined as judicial precedents of constitutional law that “are ultimately subject to congressional control.”69× 69. Gillian E. Metzger, Congress, Article IV, and Interstate Relations, 120 Harv. L. Rev. 1468, 1525 (2007) (describing judicially enforceable “constitutional default rules imposing obligations on the states in the name of union [that] are ultimately subject to congressional control”). The dormant commerce clause is a paradigmatic constitutional default rule because what the Court does today Congress may undo tomorrow. Justice Scalia declared this fact “[t]he clearest sign that the negative Commerce Clause is a judicial fraud,” for “[h]ow could congressional consent lift a constitutional prohibition?”70× 70. Comptroller of the Treasury of Md. v. Wynne, 135 S. Ct. 1787, 1808 (2015) (Scalia, J., dissenting). But that’s what a constitutional default rule is. The Court has allowed Congress to overturn its dormant commerce clause cases since 1891.71× 71. See In re Rahrer, 140 U.S. 545, 560–62 (1891).
Dormant commerce clause cases are not the only constitutional default rules. Professor Laurence Tribe’s treatise identifies two others.72× 72. 1 Laurence H. Tribe, American Constitutional Law § 6-35 (3d ed. 2000). And in a groundbreaking article, Professor Henry Monaghan revealed “a substructure of substantive, procedural, and remedial rules” forming “a constitutional common law subject to amendment, modification, or even reversal by Congress.”73× 73. Henry P. Monaghan, The Supreme Court, 1974 Term — Foreword: Constitutional Common Law, 89 Harv. L. Rev. 1, 2–3 (1975); see also Susan R. Klein, Identifying and (Re)Formulating Prophylactic Rules, Safe Harbors, and Incidental Rights in Constitutional Criminal Procedure, 99 Mich. L. Rev. 1030 (2001) (further developing Monaghan’s theory in criminal procedure context). What follows is a list of six lines of cases beyond the dormant commerce clause that may be fairly described as constitutional default rules. The first two are drawn from Tribe’s treatise while the next four are found in Monaghan’s article:
(1) State Taxation of Federal Instrumentalities: States may not tax instrumentalities of the federal government74× 74. McCulloch v. Maryland, 17 U.S. (4 Wheat.) 316, 436 (1819). — unless Congress consents.75× 75. See, e.g., Helvering v. Gerhardt, 304 U.S. 405, 411 n.1 (1938) (“Congress may curtail an immunity which might otherwise be implied or enlarge it beyond the point where, Congress being silent, the Court would set its limits.” (citations omitted)). One court has described such judicial decisions as setting a “constitutional default rule.” United States v. Delaware, 958 F.2d 555, 560 n.9 (3d Cir. 1992) (“[W]e must decide the constitutional default rule for this type of tax, fully aware that Congress could decide at any time to reverse our decision statutorily.”). (2) Article I, Section 10 Cases: Article I, Section 10 provides that certain prohibitions on the states may be waived by Congress.76× 76. See U.S. Const. art. I, § 10, cls. 2–3. The Court has taken note of this when considering whether to overrule, for instance, an Import-Export Clause precedent.77× 77. See Hooven & Allison Co. v. Evatt, 324 U.S. 652, 668 (1945) (“In view of the fact that the Constitution gives Congress authority to consent to state taxation of imports and hence to lay down its own test for determining when the immunity ends, we see no convincing practical reason for abandoning the test which has been applied for more than a century . . . .”), overruled on other grounds by Limbach v. Hooven & Allison Co., 466 U.S. 353 (1984). In Michelin Tire Corp. v. Wages, 423 U.S. 276 (1976), the Court left open the question whether “Congress may authorize, under the Import-Export Clause, an exaction that it could not directly impose under the Tax Clause.” Id. at 301 n.13. Metzger, however, argues that the Import-Export Clause is free of other clauses’ limits on congressional power. See Metzger, supra note 69, at 1500 & n.120. (3) Bivens Cases: In Bivens v. Six Unknown Named Agents of Federal Bureau of Narcotics,78× 78. 403 U.S. 388 (1971). the Court held that a violation of the Fourth Amendment gives rise to a right to sue for damages.79× 79. Id. at 397. But the Court has also held that “[s]uch a cause of action may be defeated . . . when . . . Congress has provided an alternative remedy which it explicitly declared to be a substitute for recovery directly under the Constitution and viewed as equally effective.”80× 80. Carlson v. Green, 446 U.S. 14, 18–19 (1980). (4) Miranda Cases: The Miranda Court famously “encourage[d]” Congress and the states to explore alternative “procedures which are at least as effective in apprising accused persons of their right of silence and in assuring a continuous opportunity to exercise it.”81× 81. Miranda v. Arizona, 384 U.S. 436, 467 (1966). In Dickerson v. United States, 530 U.S. 428 (2000), the Court struck down a congressional attempt to effectively abolish Miranda, holding that “Miranda announced a constitutional rule that Congress may not supersede legislatively.” Id. at 444. But Dickerson also stood by Miranda’s “invitation for legislative action” to replace Miranda with an adequate substitute. Id. at 440; see also Michael C. Dorf & Barry Friedman, Shared Constitutional Interpretation, 2000 Sup. Ct. Rev. 61 (discussing legislative alternatives to Miranda). (5) The Police Lineup Case: In United States v. Wade,82× 82. 388 U.S. 218 (1967). the Court created an exclusionary rule for evidence obtained from a police lineup in violation of the Sixth Amendment right to counsel but acknowledged that it could be replaced by “[l]egislative or other regulations . . . which eliminate the risks of abuse.”83× 83. Id. at 239. (6) The Exclusionary Rule Cases: Mapp v. Ohio made the Fourth Amendment “exclusionary rule” binding on the states,84× 84. 367 U.S. 643, 655 (1961). yet Congress is thought to have the power to replace it.85× 85. See Bivens v. Six Unknown Named Agents of Fed. Bureau of Narcotics, 403 U.S. 388, 422–24 (1971) (Burger, C.J., dissenting) (inviting Congress to replace the Fourth Amendment exclusionary rule); Harold J. Krent, How to Move Beyond the Exclusionary Rule: Structuring Judicial Response to Legislative Reform Efforts, 26 Pepp. L. Rev. 855, 864–71 (1999).
All of the above are arguably constitutional default rules set by the Court that remain, to one degree or another, open to congressional revision. The list could be longer or shorter, depending on which default rules the Court will view as constitutional86× 86. A shorter list could be produced by whittling away at the constitutional status of the cases identified by Monaghan. While the Court has held that Miranda is a constitutional decision, Dickerson, 530 U.S. at 444, some of the other cases may be viewed as nonconstitutional. See, e.g., Collins v. Virginia, 138 S. Ct. 1663, 1675–80 (2018) (Thomas, J., concurring) (arguing that Mapp is “nonconstitutional,” id. at 1678 n.5); Richard H. Fallon, Jr. et al., Hart and Wechsler’s The Federal Courts and the Federal System 775–77 (7th ed. 2015) (discussing whether Bivens is constitutionally required). Conversely, a longer list might include any constitutional right that can be waived by a party. See, e.g., Daniel A. Farber, Another View of the Quagmire: Unconstitutional Conditions and Contract Theory, 33 Fla. St. U. L. Rev. 913, 918 (2006) (describing the Eleventh Amendment as “just a contractual default rule that the states are free to barter away”). Such a list might also include various constitutionally inspired judicial presumptions. See, e.g., Jack Goldsmith & John F. Manning, The President’s Completion Power, 115 Yale L.J. 2280, 2299 (2006) (describing the Chevron presumption of delegated interpretive power to administrative agencies as “a constitutionally inspired default rule”); Nicholas Quinn Rosenkranz, Federal Rules of Statutory Interpretation, 115 Harv. L. Rev. 2085, 2097–98 (2002) (describing clear statement rules as “constitutional default rules” reversible by Congress). Many other decisions could likely be characterized as constitutional default rules; the list above is only an initial stab. and on how it will answer open questions about congressional authority over certain constitutional provisions.87× 87. See, e.g., Thomas v. Wash. Gas Light Co., 448 U.S. 261, 272 n.18 (1980) (plurality opinion) (leaving unresolved whether Congress may limit constitutional full faith and credit obligations); White v. Mass. Council of Constr. Emp’rs, Inc., 460 U.S. 204, 215 n.1 (1983) (Blackmun, J., concurring in part and dissenting in part) (leaving unresolved “whether Congress may authorize . . . what otherwise would be a violation” of the Privileges and Immunities Clause); 1 Tribe, supra note 72, § 6-35, at 1243–44 (arguing that Congress cannot override judicial constructions of the Privileges and Immunities Clause); Metzger, supra note 69, at 1486–89 (arguing the opposite). But the takeaway is clear: weaker stare decisis for constitutional default rules. Pre-Wayfair, one would have thought that stare decisis applies with special force to such precedents, given congressional power to set them straight. Not anymore. Why? Because it is improper to “ask Congress to address a false constitutional premise of th[e] Court’s own creation.”88× 88. Wayfair, 138 S. Ct. at 2096. The Latin for Wayfair’s doctrine is not stare decisis, which should reflect a realistic, working relationship between the legislative and judicial branches. It is mea culpa.
In its zeal to update the Constitution for “the Cyber Age,”89× 89. Id. at 2097. the Court deleted Congress from stare decisis doctrine in constitutional cases. The Court had better options. It could have left Quill on Congress’s doorstep, as the dissent argued. Or it could have justified overruling Quill notwithstanding the special force of stare decisis. Instead, the Court reasoned that it doesn’t matter whether Congress is willing and able to do the job: a constitutional mess calls for a judicial clean-up crew. For constitutional default rules — a category of decisions embracing the dormant commerce clause and sweeping far beyond — Wayfair’s new theory of stare decisis makes the Court’s precedents less sticky and Congress less relevant.",['Quill harmed states more than anticipated due to the Internet.'],5429,multifieldqa_en,en,,e4725ed7b6df9933d5336f79d5c9d1d803a3e475831f1017," South Dakota v. Wayfair, Inc. is an odd and ominous development in stare decisis doctrine, writes Julian Zelizer. Zelizer: The Court has long thought that judicial junk is easier to scrap when the erroneous precedent cannot be fixed by Congress, as in constitutional cases. But Wayfair applied the weakened stare dec is of constitutional cases, Zelizer says. He says the Court's logic leads far past the dormant commerce clause in Wayfair to other precedents that set a “constitutional default rule” for state laws, such as the law in Quill Corp. v. North Dakota, 1260–1260 (1992) Zelizer writes: Other precedents setting constitutional default rules may be more vulnerable, including mainstays of criminal procedure like Miranda v. Arizona, 384 U.S. 436 (1966) and Mapp v. Ohio, 643 (1961), and the Court has held that, under the “dormant” or “negative” implication of the Commerce Clause, it should take special force to dislodge such precedents, he says. The Court refused to overrule the Maryland Comptroller of the Treasury of the United States v. Wynne, 135 S. Ct. 1787, 1794 (2015), Zelizer adds. The court refused to rule over the Maryland law, he writes, and the case was thrown out of the Fourth Circuit Court of Appeals. The Supreme Court is expected to rule on the case in the coming weeks, and Zelizer predicts the ruling will be appealed to the Supreme Court itself, which has the power to overturn precedents in this type of case. It will be the first time the court has overruled a dormant Commerce Clause precedent in more than 40 years. The decision will be heard in the Court of Appeal in the spring, and it will be closely watched by the American Bar Association and the American Civil Liberties Union, which have sued the State of South Dakota over the law. The case will be decided in the fall, and if the ruling is upheld, it could have a major impact on the future of online retailing in the U.K. and other states, says Zelizer, who says the ruling would be a blow to the state’s ability to compete with online retailers in the digital age. It would also be a setback for the Obama administration, which is trying to crack down on illegal online sales of prescription drugs and other drugs. The ruling will likely be appealed in the federal courts, where it would be difficult to get a ruling on the question of whether or not the state can collect sales taxes from online retailers. It could also have a significant impact on other states that have similar laws that require online retailers to have physical presence in the state to collect and remit sales taxes, he adds, and this could have an impact on how the federal government deals with the issue of online sales tax collection in the future. The opinion will be published in a forthcoming issue of the Harvard Law Review, which will be available in print and online in the Spring of next year. For more information, go to: http://www.harvardlawreview.com/news/features/2015/01/26/south-dakota-v-wayfair-inc.html#storylink=cpy. The article has been updated to reflect that the ruling was overturned in the fourth circuit court of appeals, as well as the decision in South Dakota v Wayfair,. It will also appear in the next issue of The New England Journal of Law and Science, published in May. South Dakota law required remote sellers to collect and remit sales tax if the seller’s business in South Dakota comprised either a “gross revenue” greater than $100,000 or at least 200 “separate transactions” within one calendar year. After being sued by South Dakota for refusing to register for the newly required sales tax license, Wayfair, Inc., Overstock.com, and Newegg, Inc. moved for summary judgment. The South Dakota Supreme Court unanimously affirmed, recognizing that South Dakota's “arguments on the merits” may be “persuasive” but “Quill remains the controlling precedent.” The U.S. Supreme Court vacated the ruling and remanded the case for the last time for the Court to rule on the issue. Justices Thomas, Ginsburg, Alito and Ginsburg joined Justice Kennedy in the opinion vacated by the Supreme Court in the Wayfair v. State v. Wayfair Inc., 2017 SD 56, ¶¶ 9–11, 901 N.W.2d 754, 759–60 (2017) (S.C.L.A. Rev. 2089), 2089-2089 (Rev. L.A.) (2018) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (b) (c) (a) (B) (A) (C) (d) (D) (E) (e) (h) (H) (f) (g) (w) (m) (N) (n) (k) (i) (j) (r) ( A) (K) (M) (W) (G) (J) (s) (t) (F) (R) (un) (z) (l) (u) (p) ( b) (o) (x) (v) (y) (P) (ii) (U) (Z) (iii) (q) (xi) (etz) (ch) (zh) (), (b), (c), (d), (e), (j), (w), (g), (h), (k), (m), (z), (f), (i), (l), (r), (a), (n), (t), (p), (q), (s), (v), (1), (2), (3), (4), (7), (9,8,9,7,8) & (b). (c,j)(a)(b)(h,r, (a,j,r)(c)(d)(e)(g)(k) (#1)(j) (#2)(r) (#3) (#4) (#5) (#6) (#7) (#8) (#9) (#10) (#11) (#12) (#13) (#14) (#15) (#16) (#17) (#18) (#19) (#20) (#21) (#22) (#23) (#24) (#25) (#26) (#28) (#29) (#30) (#31) (#32) (#34) (#39) (#38) (#40) (#41) (#42) (#43) (#44) (#45) (#46) (#47) (#48) (#49) (#50) (#51) (#52) (#53) (#54) (#55) (#56) (#58) (#59) (#60) (#61) Quill “serve[d] as a judicially created tax shelter’ for remote retailers. The rule deprived the states of billions of dollars, since they could not force remote sellers to collect the tax and consumers hardly ever paid it on their own. The Court remanded the case to the South Dakota courts to determine in the first instance “whether some other principle in the Court’s Commerce Clause doctrine might invalidate the Act’” Justices Thomas and Gorsuch each filed concurring opinions. Chief Justice Roberts dissented, and Justice Justices Breyer, Sotomayor and Kagan joined the Chief Justice's dissent. The dissent turned entirely on the principles and application of the dispute between the majority and the dissent, Justice Breyer said. The Justices agreed that Bellas Hess was wrongly decided, but “the Internet revolution has made [Quill’S] earlier error all the more egregious and harmful,” he said. “The entire negative Commerce Clause jurisprudence” is wrong and should be abandoned, Justice Thomas added, adding that the “Court” “should have joined [Justice White”‘s dissenting] opinion’. ‘It is inconsistent with the Court's proper role to ask Congress to address a false constitutional premise,’ Justice Thomas said, “and it is important that their principles be accurate and logical, whether or not Congress can or will act in response.’ “ ‘Stare decisis accommodates only ‘legitimate reliance interest[s]’, he added, ‘and that is what this case is about.‘ ‘‘“Stare Decisis’ is a term used by the Supreme Court to express its view of the meaning of the Commerce Clause.” – Justice Scalia, in a dissent to Wayfair, 138 S. Ct. at 2098 (alteration in original) (quoting United States v. Ross, 456 U.S. 798, 824 (1982). ‘ ”Staredecis” means “to apply the law to the limited sphere of review in this limited sphere’ – Justice Thomas, J. at 2100 (Thomas, J., concurring in the judgment). “”. ”. . . . has engendered substantial reliance and has become part of the basic framework of a sizable industry’ — Id. at 2101 (Roberts, CJ., dissenting). ”“.  ”: “While it can be conceded that Congress has the authority to change the physical presence rule, Congress cannot change the constitutional default rule” — id. at 2021. � “[C]onsumer compliance rates are notoriously low” at 2088 (“[c]onsumptive compliance rates] are notoriouslyLow”).   “. . “It is. important that [the] courts have acted as the front line of review’ in thislimited sphere; and hence it is needed to be clear and logical.�” Id. At 2100–2101 (Gorsuch, J,. concurring, concurring), Justice Just, Justices Just and Breyer joined the dissent. ’ “The Court” said the doctrine “can be squared with the text of the. Commerce Clause, justified by stare dec isisis, or defended as branded products of antidiscrimination imperatives flowing from Article IV of the Privileges and Immunities Clause” Chief Justice Roberts argued that whether or how to reverse Quill should be left to Congress. In Bellas Hess, the Court reasoned that the dormant commerce clause protects interstate business from being “entangle[d] . . . in a virtual welter of complicated obligations to local jurisdictions.” In Wayfair, Chief Justice Roberts emphasized that a “heightened form of stare decisis’ applies when “Congress can, if it wishes, override this Court’s decisions with contrary legislation’” The Court acknowledged that “[c]omplex state tax systems could have the effect of discriminating against interstate commerce,” 138 S. Ct. at 2099 (Roberts, C.J., dissenting). The dispute in Wayfair over whether software is up to the task effectively reprised the old debate from Bellas. Hess, only this time couched as part of the stare dec isis inquiry's concern for reliance interests rather than as a matter of dormant Commerce clause doctrine. The Court could have overruled a precedent that could have been overruled by Congress by ruling that Congress could not override a precedent set by the Supreme Court. It could also have ruled that Congress can’t “change the ground rules’ in a way that the Court may have way have way of dealing with” the issue of the Wayfair ruling. The decision could have “laid the groundwork’ for future challenges to the tax code in the future,’ Justice Scalia said in a dissent. The ruling could also “have the impact of forcing Congress to consider a specific issue within a specific sphere of law, and invite it to consider how Congress responds” to that sphere of issue. The opinion could also be “a greater impediment” afterward since Quill effectively “tossed [the ball] into Congress’S court.�” (alteration in original) n.12 (quoting Bay Mills Indian Cmty. v. Bay Mills, 134 S.. Ct., 2024, 2036 (2014) (tribal sovereign immunity. at 2409); cf. Kimble v. Marvel Entm’T, LLC, 135 S. C. 2401, 2409 (2015) (statutory interpretation) (judicially created doctrine implementing a judicially. created cause of action) (in original) at 2039 n. 12 (quoted n.13 (quotes Kimble n. 13) (“When we inform Congress that it has primary responsibility over a specific Sphere of Law, we cannot deem how it responds irrelevant”). The opinion also said that the ruling could have a ‘flimsier’ effect if Congress responds and then “suddenly chang[ed] the groundrules’ of the law.’ (alter n. 14) (Quill v. North Dakota, 504 U.S. 279, 318 (1992) (quying Quill). The dissent replied that the. Court “vastly underestimate [d] the skill of contemporary man and his machines” Id. at 766 (Fortas, J., dissenting) (In original) and in the opinion, Roberts said “the Court cannot deem [Quill] irrelevant’ when it is applied to the flimsier act of Congress to act on a specific matter within a sphere of Law.“ Id. At 2098 (majority opinion) ( “Eventually, software that is available at a reasonable cost may make it easier for small businesses to cope with these problems’). The Court’s reasoning in Wayfair departs from its prior stare decisis analysis. In 1932, Justice Brandeis posited that stare Decisis must bend “in cases involving the Federal Constitution, where correction through legislative action is practically impossible” The Court has long since adopted his argument, e.g., Smith v. Allwright, 321 U.S. 649, 665 (1944). Yet the Wayfair majority refused to consider Congress”s authority to legislate as a relevant factor for stare dec isis. This refusal breaks from the practical Brande isian wisdom that has guided the Court” s treatment of precedent for the better part of a century, says Brian Galle, Killle, and Essay, Dormant Commerce Clause: Lessons on Congressional Control of State Taxation, 70–62 L. Rev. L.J. (158–62), 158–3 L.Rev. L, 158–62, 160–3, 158-3, 160-4, 158,158,158-3. For two practical arguments to this effect, see Killle and Killle. For a practical argument to keep Quill, see Essay and Killay, Keep Dormants Commerce Clause, 70-62 L Rev L. J., 158–4,158–3 (158-4), 158-5,158. For an argument to stop the tax hike, see Galle andkillle, Keep Killle,. Keep Killant, Killay. For another, see Brian Galley, Killon, Killin, Killan, Killen, Killian, Killam, Killain, Killman, Killion, Killall, Killland, Killant and Killian. For more, see: Killall and Killain. For. a look-alike at how to stick out with a tax hike from which only the states would benefit, see. Killall & Killain: How to Stick Out With a Tax Hike from Which Only the States Would benefit, See. the. Lookalike. At the bottom of the page, please share your thoughts on this case and how you think the Court should have dealt with the tax issue in the case of Wayfair, 138 S. Ct. at 2102 (Roberts, C.J., dissenting). For a look at how the Court could have handled the issue in Direct Marketing, see supra p. 278 (emphasis added). For more on this issue, see The Taxpayer's Guide to the Case, by Brian Gallego, Killalike, Killand, and Killan. at the. bottom of this article, see the. The Taxpayers' Guide to The Case, which is available on the University of California, Berkeley's website and on the. University of the West Coast's. The. West Coast Legal Center's. website. For the. West coast Legal Center, see www.thetaxpayer's.org. for more. information on how to stay out of the tax tax hike and how to avoid it, visit the. Taxpayer’S. Taxpayers’ Taxpayer Guide, by the. way of the way. of the law. to change. it. at 2096 (emphasis. added). for more information, visit. the Taxpayer Taxpayer Information Center, by clicking here. for the. ways-to-stay-out-with-a-tax hike, and the Taxpayers Taxpayer Initiative, by going to: www.taxpayers.org/taxpayer-information-the-law. Wayfair, 138 S. Ct. 2080 (No. 17-494), ruled that Congress can’t step in when a court rules in favor of a state. David Frum: The Court could have discussed the limits of the states’ influence in the federal system. He says it could have argued that new facts on the ground — namely, the blast of e-commerce that hit like a comet after Quill — overpowered stare decisis of any force, special or plain.Frum: Wayfair now stands for the proposition that a “constitutional default rule’ — a term the Court apparently lifted from South Dakota’s reply brief on the merits — gets only weakened in this case. It is ironic that such a theory sprouts from a case like Wayfair, which repudiated a ‘formalistic distinction,’ Frum says, and that it threatens other constitutional default rules as well as the ‘magic’ word ‘constitutional.’ ‘Constitutional default rules’ may be read broadly to include a variety of actors and contracting mechanisms, Frum writes, and could be read to include ‘contract’ and ‘constitution’ in the same way as ‘legislation’ or ‘law’ can be read as meaning ‘the law of the land’, he says. “Constitution is not a magic word,” he writes, “but it can be a very powerful one.” “The Constitution is not the only law, but it is the most important,“ Frum adds, � “and it is important to get it right’. ‘Stare Decisis’ is “a rule that can be applied to all laws, not just a few, and can be used to set the stage for future ones,�’ he says, ‘‘” and � � ‘stare decis is the rule that must be applied in all circumstances.‘ ‘The Constitution does not have to be written in stone, but can be amended to make it so,‘ Frum argues. ’’‘The Court has the power to change the law, not the other way around’ – and it can do it by changing the language of the Constitution, he adds. ”‘Constitutionality’ does not mean the same thing in every case, and it does not require it to be the same as in every other case.�.'’ '’Constitution' is a ’magic word’ - Frum. ' Constitution’ means ‘that which can be changed by the will of the people’.' 'Constitution” is ‘a law that can and should be applied by the people, not by the courts, and not by Congress.' ’'Constitution's’ meaning can be ‘translated’ to mean ‘to the extent that the parties can agree to change it, and to the extent they can, the law can be altered by the parties, and so on.' '‘constitutional default rules' are ‘legal rules that apply to all parties, not only to the parties around them' '    ’ The dormant commerce clause is a paradigmatic constitutional default rule. What the Court does today Congress may undo tomorrow. Professor Laurence Tribe’s treatise identifies two other constitutional default rules. The Court has taken note of this when considering whether to overrule, for instance, an Import-Export Clause precedent. See, e.g., Helvering v. Gerhardt, 304 U.S. 405, 411 n.1 (1938) (“Congress may curtail an immunity which might otherwise be implied or enlarge it beyond the point where, Congress being silent, the Court would set its limits”). See In re Rahrer, 140 U. s. 545, 560–62 (1891). See McCulloch v. Maryland, 17 U.s. (4 Wheat.) 316, 436 (1819). — unless Congress consents. (2) Article I, Section 10 Cases: Article I,. Section 10 provides that certain prohibitions on the states may be waived by Congress. See Hooven & Allison Co. v. Evatt, 652, 653 (1945) — the Court left open the fact that the Constitution gives Congress authority to lay down its own test for determining immunity. (3) State Taxation of Federal Instrumentalities: States may not tax instrumentalities of the federal government74× 74. In Michelin Tire Corp. v Wages, 423 U. S. 276, 423 (1976), the court left the open Court left the test for deciding immunity of imports and exports open. (5) The Court must decide the constitutional default. rule for this type of tax, fully aware that Congress could decide at any time to reverse our decision statutorily. (6) The Supreme Court, 1974 Term — Foreword: Constitutional Common Law, 89 Harv. L. Rev. 1, 2–3 (1975); see also Susan R. Klein, Identifying and (Re)Formulating Prophylactic Rules, Safe Harbors, and Incidental Rights in Constitutional Criminal Procedure, 99 Mich. L Rev. 1030 (2001) (further developing Monaghan’S theory in criminal procedure context) (7) The First Amendment to the United States Constitution, which protects the right to bear arms, can be amended by Congress at any point in time, and can be reversed by the Supreme Court at the discretion of the Congress. (8) The Constitution can be modified by the Congress at a later date, and the Congress can reverse a decision made by the Court at that time, if it so chooses. (9) The Second Amendment, which prohibits the government from imposing obligations on the States in the name of union that are ultimately subject to congressional control, can also be amended at a future date by the House of Representatives or the Senate. (10) The first two are drawn from Tribe's treatise while the next four are found in Monaghan's article. (11) The last is a list of six lines of cases that may be fairly described asconstitutional default rules that can be fairly called “constitutional. default rules’ (12) The list is based on the American Constitutional Law § 6-35 (3d ed. 2000) (13), 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 47 and 48, 48. 78× 78. Metzger argues that the Import-Export Clause is free of other clauses’ limits on congressional power. (3) Bivens Cases: The Miranda Court famously “encourage[d]” Congress and the states to explore alternative “procedures which are at least as effective” in apprising accused persons of their right of silence. (4) The Police Lineup Case: In United States v. Wade, the Court created an exclusionary rule for evidence obtained from a police lineup in violation of the Sixth Amendment right to counsel. (5) The Exclusionary Rule Cases: Mapp v. Ohio made the Fourth Amendment “exclusionary rule” binding on the states,84× 84. Yet Congress is thought to have the power to replace it.85× 85. The list could be longer or shorter, depending on which default rules the Court will view as constitutional86× 86. A shorter list couldbe produced by whittling away at the constitutional status of the cases identified by Monaghan. Conversely, a longer list might include any constitutional right required by a party that can be waived by a judicial ruling. See, e.g., Jack Goldsmith, The Unconstitutional Conditions and Theory, 33 Fla. St. L. Rev. Rev., 913, 918 (2006) (describing the Eleventh Amendment as “just a contractual rule’). See also John F. Manning, Another View of the Quagmire: Unconstitutional conditions and Contract, 33 Florida St.L.Rev. Rev, 775–7th ed. (2015) (discing whether Biven is constitutionally required). Conversely, eg., Daniel A. Farber, Daniel Farg., John Manning, Jack. Goldsmith & Jack. Farg, The Quaggy’s Guide to the Federal Courts and Federal System, 774-775 (2015), 775-776 (2018) ( describing the Federal courts and Federal system as ‘just’ and ‘unconstitutional’) See also The Federal Courts & Federal System and Federal Courts, 776-774 (2015, 2018) (Describing the federal courts and federal courts as just and constitutional). See eg. The Federal Court and Federal courts, 778-778 (2015). See id. at 1500 & n.120. See also Jack Goldmith, Another view of the quaggy. The Quoggy”, 33Fla. Stl. L Rev.rev. 1663, 1675–80 (2018), (describes the federal court and federal system as just, constitutional and contractual. See id.) at 1500 and n.12. See the U.S. Court of Appeals for the Fifth Circuit, 772-772 (2015); see also the Federal Court of appeals, 773-773 (2015; see also id.) See also the United States Court of Appeal, 782-777 (2013) and 783-788 (2013). See Id. at 1400 and 784-769 (2014) for a list of constitutional default rules set by the Court that remain, to one degree or another, open to congressional revision. See Also: The Eleventh. Amendment as just a ‘contractor’ that states are free to barter away from judicial presumptions that can’t be presumptively be waived. See. id. At 1500 n.13. In Wayfair, the Court deleted Congress from stare decisis doctrine in constitutional cases. The Latin for Wayfair’s doctrine is not stareDecisis, it is mea culpa. The Court had better options. It could have left Quill on Congress's doorstep, as the dissent argued. Instead, it reasoned that it doesn’t matter whether Congress is willing and able to do the job: a constitutional mess calls for a judicial clean-up crew. For constitutional default rules — a category of decisions embracing the dormant commerce clause and sweeping far beyond — Wayfair's new theory of stare Decisis makes the Court's precedents less sticky and Congress less relevant. It is improper to “ask Congress to address a false constitutional premise of th[e] Court's own creation,” Id. at 2097. It should reflect a realistic, working relationship between the legislative and judicial branches. The takeaway is clear: weaker staredecis forconstitutional default rules. The court could have justified overruling Quill notwithstanding the special force of stare dec isis. But instead, it said it doesn't matter if Congress is able or willing to set things straight. It said it would be better to leave it to the courts to do its job. It was a mistake, and the Court should not have done it. The decision is a blow to the rule of law, and to the Constitution as we know it, and it is a setback for the future of the U.S. Supreme Court. It will be hard to predict how the court will rule in the future, and how it will answer open questions about congressional authority over certain constitutional provisions. It's a step in the right direction, but it's a long way from where we are now, and a long, long way away from the end of the current Supreme Court term. It may be too late to turn back the clock, however, and we'll have to wait until the next term to find out what the Court will rule on the issue. It'll be a long time before we know what the next step will be, and that will be a big step in our understanding of the Constitution."
What factors control the reliance of artificial organisms on plasticity?,"Paper Info

Title: Environmental variability and network structure determine the optimal plasticity mechanisms in embodied agents
Publish Date: Unkown
Author List: Sina Khajehabdollahi (from Department of Computer Science, University of Tübingen)

Figure

Figure2: An outline of the network controlling the foraging agent.The sensor layer receives inputs at each time step (the ingredients of the nearest food), which are processed by the plastic layer in the same way as the static sensory network, Fig.1.The output of that network is given as input to the motor network, along with the distance d and angle α to the nearest food, the current velocity v, and energy E of the agent.These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent
Figure4: The evolved parameters θ = (θ 1 , . . ., θ 8 ) of the plasticity rule for the reward prediction (a.) and the decision (b.) tasks, for a variety of parameters (p tr = 0.01, d e ∈ 0, 0.1, . . ., 1, and σ ∈ 0, 0.1, . . ., 1 in all 100 combinations).Despite the relatively small difference between the tasks, the evolved learning rules differ considerably.For visual guidance, the lines connect θs from the same run.
Figure5: a.The trajectory of an agent (blue line) in the 2D environment.A well-trained agent will approach and consume food with positive values (green dots) and avoid negative food (red dots).b.The learning rate of the plastic sensory network eta p grows with the distance between environments d e c. and decreases with the frequency of environmental change.d.The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network.e.The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red).In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food.

abstract

The evolutionary balance between innate and learned behaviors is highly intricate, and different organisms have found different solutions to this problem. We hypothesize that the emergence and exact form of learning behaviors is naturally connected with the statistics of environmental fluctuations and tasks an organism needs to solve.
Here, we study how different aspects of simulated environments shape an evolved synaptic plasticity rule in static and moving artificial agents. We demonstrate that environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity. Interestingly, the form of the emerging plasticity rule is additionally determined by the details of the task the artificial organisms are aiming to solve.
Moreover, we show that coevolution between static connectivity and interacting plasticity mechanisms in distinct sub-networks changes the function and form of the emerging plasticity rules in embodied agents performing a foraging task. One of the defining features of living organisms is their ability to adapt to their environment and incorporate new information to modify their behavior.
It is unclear how the ability to learn first evolved , but its utility appears evident. Natural environments are too complex for all the necessary information to be hardcoded genetically and more importantly, they keep changing during an organism's lifetime in ways that cannot be anticipated ; . The link between learning and environmental uncertainty and fluctuation has been extensively demonstrated in both natural ; , and artificial environments .
Nevertheless, the ability to learn does not come without costs. For the capacity to learn to be beneficial in evolutionary terms, a costly nurturing period is often required, a phenomenon observed in both biological , and artificial organisms . Additionally, it has been shown that in some complex environments, hardcoded behaviors may be superior to learned ones given limits in the agent's lifetime and envi-ronmental uncertainty ; ; .
The theoretical investigation of the optimal balance between learned and innate behaviors in natural and artificial systems goes back several decades. However, it has recently found also a wide range of applications in applied AI systems ; . Most AI systems are trained for specific tasks, and have no need for modification after their training has been completed.
Still, technological advances and the necessity to solve broad families of tasks make discussions about life-like AI systems relevant to a wide range of potential application areas. Thus the idea of open-ended AI agents that can continually interact with and adapt to changing environments has become particularly appealing.
Many different approaches for introducing lifelong learning in artificial agents have been proposed. Some of them draw direct inspiration from actual biological systems ; . Among them, the most biologically plausible solution is to equip artificial neural networks with some local neural plasticity , similar to the large variety of synaptic plasticity mechanisms ; ; that performs the bulk of the learning in the brains of living organisms .
The artificial plasticity mechanisms can be optimized to modify the connectivity of the artificial neural networks toward solving a particular task. The optimization can use a variety of approaches, most commonly evolutionary computation. The idea of meta-learning or optimizing synaptic plasticity rules to perform specific functions has been recently established as an engineering tool that can compete with stateof-the-art machine learning algorithms on various complex tasks ; ; Pedersen and Risi (2021); .
Additionally, it can be used to reverse engineer actual plasticity mechanisms found in biological neural networks and uncover their functions ; . Here, we study the effect that different factors (environ-arXiv:2303.06734v1 [q-bio.NC] 12 Mar 2023 mental fluctuation and reliability, task complexity) have on the form of evolved functional reward-modulated plasticity rules.
We investigate the evolution of plasticity rules in static, single-layer simple networks. Then we increase the complexity by switching to moving agents performing a complex foraging task. In both cases, we study the impact of different environmental parameters on the form of the evolved plasticity mechanisms and the interaction of learned and static network connectivity.
Interestingly, we find that different environmental conditions and different combinations of static and plastic connectivity have a very large impact on the resulting plasticity rules. We imagine an agent who must forage to survive in an environment presenting various types of complex food particles. Each food particle is composed of various amounts and combinations of N ingredients that can have positive (food) or negative (poison) values.
The value of a food particle is a weighted sum of its ingredients. To predict the reward value of a given resource, the agent must learn the values of these ingredients by interacting with the environment. The priors could be generated by genetic memory, but the exact values are subject to change. To introduce environmental variability, we stochastically change the values of the ingredients.
More precisely, we define two ingredient-value distributions E 1 and E 2 and switch between them, with probability p tr for every time step. We control how (dis)similar the environments are by parametrically setting E 2 = (1 − 2d e )E 1 , with d e ∈ [0, 1] serving as a distance proxy for the environments; when d e = 0, the environment remains unchanged, and when d e = 1 the value of each ingredient fully reverses when the environmental transition happens.
For simplicity, we take values of the ingredients in E 1 equally spaced between -1 and 1 (for the visualization, see Fig. ). The static agent receives passively presented food as a vector of ingredients and can assess its compound value using the linear summation of its sensors with the (learned or evolved) weights, see Fig. .
The network consists of N sensory neurons that are projecting to a single post-synaptic neuron. At each time step, an input X t = (x 1 , . . . , x N ) is presented, were the value x i , i ∈ {1, . . . , N } represents the quantity of the ingredient i. We draw x i independently form a uniform distribution on the [0, 1] interval (x i ∼ U (0, 1)).
The value of each ingredient w c i is determined by the environment (E 1 or E 2 ). The postsynaptic neuron outputs a prediction of the food X t value as y t = g(W X T t ). Throughout the paper, g will be either the identity function, in which case the prediction neuron is linear, or a step-function; however, it could be any other nonlinearity, such as a sigmoid or ReLU.
After outputting the prediction, the neuron receives feedback in the form of the real value of the input R t . The real value is computed as R t = W c X T t + ξ, where W c = (w c 1 , . . . , w c N ) is the actual value of the ingredients, and ξ is a term summarizing the noise of reward and sensing system ξ ∼ N (0, σ).
Figure : An outline of the static agent's network. The sensor layer receives inputs representing the quantity of each ingredient of a given food at each time step. The agent computes the prediction of the food's value y t and is then given the true value R t ; it finally uses this information in the plasticity rule to update the weight matrix.
For the evolutionary adjustment of the agent's parameters, the loss of the static agent is the sum of the mean squared errors (MSE) between its prediction y t and the reward R t over the lifetime of the agent. The agent's initial weights are set to the average of the two ingredient value distributions, which is the optimal initial value for the case of symmetric switching of environments that we consider here.
As a next step, we incorporate the sensory network of static agents into embodied agents that can move around in an environment scattered with food. To this end, we merge the static agent's network with a second, non-plastic motor network that is responsible for controlling the motion of the agent in the environment.
Specifically, the original plastic network now provides the agent with information about the value of the nearest food. The embodied agent has additional sensors for the distance from the nearest food, the angle between the current velocity and the nearest food direction, its own velocity, and its own energy level (sum of consumed food values).
These inputs are processed by two hidden layers (of 30 and 15 neurons) with tanh activation. The network's outputs are angular and linear acceleration, Fig. . The embodied agents spawn in a 2D space with periodic boundary conditions along with a number of food particles that are selected such that the mean of the food value distribution is ∼ 0. An agent can eat food by approaching it sufficiently closely, and each time a food particle is eaten, it is The sensor layer receives inputs at each time step (the ingredients of the nearest food), which are processed by the plastic layer in the same way as the static sensory network, Fig. .
The output of that network is given as input to the motor network, along with the distance d and angle α to the nearest food, the current velocity v, and energy E of the agent. These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent re-spawned with the same value somewhere randomly on the grid (following the setup of ).
After 5000 time steps, the cumulative reward of the agent (the sum of the values of all the food it consumed) is taken as its fitness. During the evolutionary optimization, the parameters for both the motor network (connections) and plastic network (learning rule parameters) are co-evolved, and so agents must simultaneously learn to move and discriminate good/bad food.
Reward-modulated plasticity is one of the most promising explanations for biological credit assignment . In our network, the plasticity rule that updates the weights of the linear sensor network is a rewardmodulated rule which is parameterized as a linear combination of the input, the output, and the reward at each time step:
Additionally, after each plasticity step, the weights are normalized by mean subtraction, an important step for the stabilization of Hebbian-like plasticity rules . We use a genetic algorithm to optimize the learning rate η p and amplitudes of different terms θ = (θ 1 , . . . , θ 8 ). The successful plasticity rule after many food presentations must converge to a weight vector that predicts the correct food values (or allows the agent to correctly decide whether to eat a food or avoid it).
To have comparable results, we divide θ = (θ 1 , . . . , θ 8 ) by We then multiply the learning rate η p with θ max to maintain the rule's evolved form unchanged, η norm p = η p • θ max . In the following, we always use normalized η p and θ, omitting norm . To evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism .
The agents' parameters are initialized at random (drawn from a Gaussian distribution), then the sensory network is trained by the plasticity rule and finally, the agents are evaluated. After each generation, the bestperforming agents (top 10 % of the population size) are selected and copied into the next generation.
The remaining 90 % of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (σ = 0.1) to its parameters. To start with, we consider a static agent whose goal is to identify the value of presented food correctly. The static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task.
We first look at the evolved learning rate η p , which determines how fast (if at all) the network's weight vector is updated during the lifetime of the agents. We identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition.
The first natural factor is the distance d e between the two environments, with a larger distance requiring a higher learning rate, Fig. . This is an expected result since the convergence time to the ""correct"" weights is highly dependent on the initial conditions. If an agent is born at a point very close to optimality, which naturally happens if the environments are similar, the distance it needs to traverse on the fitness landscape is small.
Therefore it can afford to have a small learning rate, which leads to a more stable convergence and is not affected by noise. A second parameter that impacts the learning rate is the variance of the rewards. The reward an agent receives for the plasticity step contains a noise term ξ that is drawn from a zero mean Gaussian distribution with standard deviation σ.
This parameter controls the unreliability of the agent's sensory system, i.e., higher σ means that the information the agent gets about the value of the foods it consumes cannot be fully trusted to reflect the actual value of the foods. As σ increases, the learning rate η p decreases, which means that the more unreliable an environment becomes, the less an agent relies on plasticity to update its weights, Fig. .
Indeed for some combinations of relatively small distance d e and high reward variance σ, the EA converges to a learning rate of η p ≈ 0. This means that the agent opts to have no adaptation during its lifetime and remain at the mean of the two environments. It is an optimal solution when the expected loss due to ignoring the environmental transitions is, on average, lower than the loss the plastic network will incur by learning via the (often misleading because of the high σ) environmental cues.
A final factor that affects the learning rate the EA will converge to is the frequency of environmental change during an agent's lifetime. Since the environmental change is modeled as a simple, two-state Markov process (Fig. ), the control parameter is the transition probability p tr . When keeping everything else the same, the learning rate rapidly rises as we increase the transition probability from 0, and after reaching a peak, it begins to decline slowly, eventually reaching zero (Fig. ).
This means that when environmental transition is very rare, agents opt for a very low learning rate, allowing a slow and stable convergence to an environment-appropriate weight vector that leads to very low losses while the agent remains in that environment. As the rate of environmental transition increases, faster learning is required to speed up convergence in order to exploit the (comparatively shorter) stays in each environment.
Finally, as the environmental transition becomes too fast, the agents opt for slower or even no learning, which keeps them ) and the decision (b.) tasks, for a variety of parameters (p tr = 0.01, d e ∈ 0, 0.1, . . . , 1, and σ ∈ 0, 0.1, . . . , 1 in all 100 combinations). Despite the relatively small difference between the tasks, the evolved learning rules differ considerably.
For visual guidance, the lines connect θs from the same run. near the middle of the two environments, ensuring that the average loss of the two environments is minimal (Fig. ). The form of the evolved learning rule depends on the task: Decision vs. Prediction The plasticity parameters θ = (θ 1 , . . . , θ 8 ) for the rewardprediction task converge on approximately the same point, regardless of the environmental parameters (Fig. ).
In particular, θ 3 → 1, θ 5 → −1, θ i → 0 for all other i, and thus the learning rule converges to: Since by definition y t = g(W t X T t ) = W t X T t (g(x) = x in this experiment) and R t = W c X T t + ξ we get: Thus the distribution of ∆W t converges to a distribution with mean 0 and variance depending on η p and σ and W converges to W c .
So this learning rule will match the agent's weight vector with the vector of ingredient values in the environment. We examine the robustness of the learning rule the EA discovers by considering a slight modification of our task. Instead of predicting the expected food value, the agent now needs to decide whether to eat the presented food or not.
This is done by introducing a step-function nonlinearity (g(x) = 1 if x ≥ 1 and 0 otherwise). Then the output y(t) is computed as: Instead of the MSE loss between prediction and actual value, the fitness of the agent is now defined as the sum of the food values it chose to consume (by giving y t = 1). Besides these two changes, the setup of the experiments remains exactly the same.
The qualitative relation between η p and parameters of environment d e , σ and p tr is preserved in the changed experiment. However, the resulting learning rule is significantly different (Fig. ). The evolution converges to the following learning rule: In both cases, the rule has the form ∆W t = η p X t [α y R t + β y ].
Thus, the ∆W t is positive or negative depending on whether the reward R t is above or below a threshold (γ = −β y /α y ) that depends on the output decision of the network (y t = 0 or 1). Both learning rules (for the reward-prediction and decision tasks) have a clear Hebbian form (coordination of preand post-synaptic activity) and use the incoming reward signal as a threshold.
These similarities indicate some common organizing principles of reward-modulated learning rules, but their significant differences highlight the sensitivity of the optimization process to task details. We now turn to the moving embodied agents in the 2D environment. To optimize these agents, both the motor network's connections and the sensory network's plasticity parameters evolve simultaneously.
Since the motor network is initially random and the agent has to move to find food, the number of interactions an agent experiences in its lifetime can be small, slowing down the learning. However, having the larger motor network also has benefits for evolution because it allows the output of the plastic network to be read out and transformed in different ways, resulting in a broad set of solutions.
The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. e. The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red).
In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food. The agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights (Fig. ).
After ∼ 100 evolutionary steps (Fig. ), the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values. We compare the dependence of the moving and the static agents on the parameters of the environment: d e and the state transition probability p tr .
At first, in order to simplify the experiment, we set the transition probability to 0, but fixed the initial weights to be the average of E 1 and E 2 , while the real state is E 2 . In this experiment, the distance between states d e indicates twice the distance between the agent's initial weights and the optimal weights (the environment's ingredient values) since the agent is initialized at the mean of the two environment distributions.
Same as for the static agent, the learning rate increases with the distance d e (Fig. ). Then, we examine the effect of the environmental transition probability p tr on the evolved learning rate η p . In order for an agent to get sufficient exposure to each environment, we scale down the probability p tr from the equivalent experiment for the static agents.
We find that as the probability of transition increases, the evolved learning rate η p decreases (Fig. ). This fits with the larger trend for the static agent, although there is a clear difference when it comes to the increase for very small transition probabil-ities that were clearly identifiable in the static but not the moving agents.
This could be due to much sparser data and possibly the insufficiently long lifetime of the moving agent (the necessity of scaling makes direct comparisons difficult). Nevertheless, overall we see that the associations observed in the static agents between environmental distance d e and transition probability p tr and the evolved learning rate η p are largely maintained in the moving agents.
Still, more data would be needed to make any conclusive assertions about the exact effect of these environmental parameters on the emerging plasticity mechanisms. A crucial difference between the static and the moving agents is the function the plasticity has to perform. While in the static agents, the plasticity has to effectively identify the exact value distribution of the environment in order to produce accurate predictions, in the embodied agents, the plasticity has to merely produce a representation of the environment that the motor network can evolve to interpret adequately enough to make decisions about which food to consume.
To illustrate the difference, we plot the Pearson correlation coefficient between an agent's weights and the ingredient values of the environment it is moving in (Fig. ). We use the correlation instead of the MSE loss (which we used for the static agents in Fig. ) because the amplitude of the vector varies a lot for different agents and meaningful The evolved parameters of moving agents' plasticity rule for the g(s) = x, identity (a.) and the step function (Eq.
4) (b.) sensory networks (the environmental parameters here are d e ∈ [0, 1], σ = 0 and p tr = 0.001). The step function (binary output) network evolved a more structured plasticity rule (e.g., θ 3 > 0 for all realizations) than the linear network. Moreover, the learned weights for the identity network (c.) have higher variance and correlate significantly less with the environment's ingredient distribution compared to the learned weights for the thresholded network (d.)
conclusions cannot be drawn from the MSE loss. For many agents, the learned weights are consistently anti-correlated with the actual ingredient values (an example of such an agent is shown in Fig. ). This means that the output of the sensory network will have the opposite sign from the actual food value.
While in the static network, this would lead to very bad predictions and high loss, in the foraging task, these agents perform exactly as well as the ones where the weights and ingredients values are positively correlated, since the motor network can simply learn to move towards food for which it gets a negative instead of a positive sensory input.
This additional step of the output of the plastic network going through the motor network before producing any behavior has a strong effect on the plasticity rules that the embodied agents evolve. Specifically, if we look at the emerging rules the top performing agents have evolved (Fig. ), it becomes clear that, unlike the very well-structured rules of the static agents (Fig. ), there is now virtually no discernible pattern or structure.
The difference becomes even clearer if we look at the learned weights (at the end of a simulation) of the best-performing agents (Fig. ). While there is some correlation with the environment's ingredient value distribution, the variance is very large, and they do not seem to converge on the ""correct"" values in any way.
This is to some extent expected since, unlike the static agents where the network's output has to be exactly correct, driving the evolution of rules that converge to the precise environmental distribution, in the embodied networks, the bulk of the processing is done by the motor network which can evolve to interpret the scalar value of the sensory network's output in a variety of ways.
Thus, as long as the sensory network's plasticity rule co-evolves with the motor network, any plasticity rule that learns to produce consistent information about the value of encountered food can potentially be selected. To further test this assumption, we introduce a bottleneck of information propagation between the sensory and motor networks by using a step-function nonlinearity on the output of the sensory network (Eq.
4). Similarly to the decision task of the static network, the output of the sensory network now becomes binary. This effectively reduces the flow of information from the sensory to the motor network, forcing the sensory network to consistently decide whether food should be consumed (with the caveat that the motor network can still interpret the binary sign in either of two ways, either consuming food marked with 1 or the ones marked with 0 by the sensory network).
The agents perform equally well in this variation of the task as before (Fig. ), but now, the evolved plasticity rules seem to be more structured (Fig. ). Moreover, the variance of the learned weights in the bestperforming agents is significantly reduced (Fig. ), which indicates that the bottleneck in the sensory network is in-creasing selection pressure for rules that learn the environment's food distribution accurately.
We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental change that is neither too large nor too small are necessary conditions for an agent to be able to effectively adapt via synaptic plasticity.
Additionally, we find that minor variations of the task an agent has to solve or the parametrization of the network can give rise to significantly different plasticity rules. Our results partially extend to embodied artificial agents performing a foraging task. We show that environmental variability also pushes the development of plasticity in such agents.
Still, in contrast to the static agents, we find that the interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. We propose a potential cause of this degeneracy; as the relatively complex motor network is allowed to read out and process the outputs from the plastic network, any consistent information coming out of these outputs can be potentially interpreted in a behaviorally useful way.
Reducing the information the motor network can extract from the sensory system significantly limits learning rule variability. Our findings on the effect of environmental variability concur with the findings of previous studies that have identified the constraints that environmental variability places on the evolutionary viability of learning behaviors.
We extend these findings in a mechanistic model which uses a biologically plausible learning mechanism (synaptic plasticity). We show how a simple evolutionary algorithm can optimize the different parameters of a simple reward-modulated plasticity rule for solving simple prediction and decision tasks.
Reward-modulated plasticity has been extensively studied as a plausible mechanism for credit assignment in the brain ; ; and has found several applications in artificial intelligence and robotics tasks ; . Here, we demonstrate how such rules can be very well-tuned to take into account different environmental parameters and produce optimal behavior in simple systems.
Additionally, we demonstrate how the co-evolution of plasticity and static functional connectivity in different subnetworks fundamentally changes the evolutionary pressures on the resulting plasticity rules, allowing for greater diversity in the form of the learning rule and the resulting learned connectivity.
Several studies have demonstrated how, in biological networks, synaptic plasticity heavily interacts with and is driven by network topology . Moreover, it has been recently demonstrated that biological plasticity mechanisms are highly redundant in the sense that any observed neural connectivity or recorded activity can be achieved with a variety of distinct, unrelated learning rules .
This observed redundancy of learning rules in biological settings complements our results and suggests that the function of plasticity rules cannot be studied independently of the connectivity and topology of the networks they are acting on. The optimization of functional plasticity in neural networks is a promising research direction both as a means to understand biological learning processes and as a tool for building more autonomous artificial systems.
Our results suggest that reward-modulated plasticity is highly adaptable to different environments and can be incorporated into larger systems that solve complex tasks. This work studies a simplified toy model of neural network learning in stochastic environments. Future work could be built on this basic framework to examine more complex reward distributions and sources of environmental variability.
Moreover, a greater degree of biological realism could be added by studying more plausible network architectures (multiple plastic layers, recurrent and feedback connections) and more sophisticated plasticity rule parametrizations. Additionally, our foraging simulations were constrained by limited computational resources and were far from exhaustive.
Further experiments can investigate environments with different constraints, food distributions, multiple seasons, more complex motor control systems and interactions of those systems with different sensory networks as well as the inclusion of plasticity on the motor parts of the artificial organisms.",['Environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity.'],5339,multifieldqa_en,en,,79ffdceb9859803e365c3de5d24c187ed06b15f04d04ae6a," Environmental variability and network structure determine the optimal plasticity mechanisms in embodied agents. A well-trained agent will approach and consume food with positive values (green dots) and avoid negative food (red dots) The learning rate of the plastic sensory network eta p grows with the distance between environments d e c and decreases with the frequency of environmental change. The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red) is not an issue for performance since the motor network can interpret the inverted signs of food. For the capacity to learn to be beneficial in evolutionary terms, a costly nurturing period is often required, a phenomenon observed in both biological , and artificial organisms. It is unclear how the ability to learn first evolved, but its utility appears evident. The evolutionary balance between innate and learned behaviors is highly intricate, and different organisms have found different solutions to this problem. We hypothesize that the emergence and exact form of learning behaviors is naturally connected with the statistics of environmental fluctuations and tasks an organism needs to solve. We demonstrate that environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity. The link between learning and environmental uncertainty and fluctuation has been extensively demonstrated in both natural ;, and artificial environments. We show that coevolution between static connectivity and interacting plasticity mechanism in distinct sub-networks changes the function and form of the emerging plasticity rules in embodied agent performing a foraging task. The study was conducted by Sina Khajehabdollahi (from Department of Computer Science, University of Tübingen) and his co-authors. The paper was published in the journal Computer Science and Engineering (CSE). It was published by the German Academic Press (CAS) and the German Society for Computer Science (SAS) on November 14, 2013. It can be ordered by clicking here for a copy of the paper and its contents, or clicking here to see the online version. It includes the abstract, the first page, the second page, and the third page. It was written by the authors, Sina Khajiabdollahis and their co-researchers, and it is published by CSE on November 13, 2013, at the request of the CSE. For more information about the study, visit the CSA website. It also includes the second part of the article, the Introduction, the Third Page, the Methods, the Discussion, the Results and the Discussion. It has been translated from the German into English and the fourth page, as well as the English version. In some complex environments, hardcoded behaviors may be superior to learned ones given limits in the agent's lifetime and envi-ronmental uncertainty. The idea of meta-learning or optimizing synaptic plasticity rules to perform specific functions has been recently established as an engineering tool that can compete with state-of-the-art machine learning algorithms on various complex tasks. The plasticity mechanisms can be optimized to modify the connectivity of the artificial neural networks toward solving a particular task. We find that different environmental conditions and different combinations of static and plastic connectivity have a very large impact on the resulting plasticityRules. We imagine an agent who must forage to survive in an environment presenting various types of complex food particles. Each food particle is composed of various amounts and combinations of N ingredients that can have positive (food) or negative (poison) values. The priors could be generated by genetic memory, but the exact values are subject to change. To introduce environmental variability, we stochastically change the values of the ingredients. The value of a food particles is a weighted sum of its ingredients. To predict the rewardvalue of a given resource, the agent must learn the value of these ingredients by interacting with the environment. We control how (dis)similar the environments are by parametrically setting E 2 = (1 − 2d e )E 1 , with d e serving as a distance proxy for the environments. When d e = 0, the environment remains unchanged, and when D e = 1 the value. of each ingredient fully reverses when the environmental transition happens. The static agents perform a complex foraging task. Then we increase the complexity by switching to moving agents performing acomplex foragingtask. In both cases, we study the impact of different environmental parameters on the form of the evolved plasticity mechanism and the interaction of learned and static network connectivity. We study the effect that different factors (environ-arXiv:2303.06734v1 [q-bio.NC] 12 Mar 2023 mental fluctuation and reliability, task complexity) have on the. form of evolved functional reward-modulated plasticity Rules. We conclude by showing that the static agents are able to adapt to changing environments more easily than the moving ones, and that this can be used to create life-like AI systems that can continually interact with and adapt to new environments. The. idea of open-ended AI agents has become particularly appealing to a wide range of potential application areas, such as medicine, finance, medicine, and other fields. It can also be used as a tool to reverse engineer actual Plasticity mechanisms found in biological neural networks and uncover their functions. gent receives passively presented food as a vector of ingredients and can assess its compound value using the linear summation of its sensors with the (learned or evolved) weights. The network consists of N sensory neurons that are projecting to a single post-synaptic neuron. The agent computes the prediction of the food's value y t and is then given the true value R t. An agent can eat food by approaching it sufficiently closely, and each time a food particle is eaten, it is given a reward. The embodied agent has additional sensors for the distance from the nearest food, the angle between the current velocity and the nearby food direction, its own velocity, and its own energy level. These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent re-spawned with the same value somewhere randomly on the grid (following the setup of).After 5000 time steps, the cumulative reward of theAgent (the sum of the values of all the food it consumed) is taken as its fitness. During the evolutionary optimization, the parameters for both network (connections) and plastic network (learning rule parameters) are co-evolved, a-olved, the motor network and the plastic network are coevolved. The motor network's outputs are angular and linear acceleration, Fig. . The embodied agents spawn in a 2D space with periodic boundary conditions along with a number of food particles that are selected such that the mean of theFood value distribution is ∼ 0. The sensor layer receives inputs at each time step, which are processed by the plastic layer in the same way as the static sensory network. The output of that network is given as input to the motorNetwork, along with the distance d and angle α to the nearestFood, the currentVelocity v, and energy E of the Agent. These inputs areprocessed by twohidden layers (of 30 and 15 neurons) with tanh activation. The neural network's output is the speed and direction of the movement of an embodied agent in an environment scattered with food. The sensory network is then merged with a non-plastic motor network that is responsible for controlling the motion of the acting agent in the environment. The resulting motor network is called the embodied motor network, and it is used to control the embodied agent's movement in a variety of ways, such as the direction and speed of movement in which it is moving in a given environment, for example, the direction of travel in a straight line or the speed in a circle, or the angle in which the agent's velocity is in relation to the food direction. It is also used to train the embodied agents to move in a particular direction in which they can eat the same amount of food at a given time step. For example, to eat a piece of bread, the agent would move in such a way that the distance between the food and the next food direction is the same as the distance in which its velocity is at the start of a straightline. This is known as a ‘straightline’ movement. Reward-modulated plasticity is one of the most promising explanations for biological credit assignment. To evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism. We identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition. After each generation, the bestperforming agents (top 10 % of the population size) are selected and copied into the next generation. The remaining 90% of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (σ = 0.1) to its parameters. The reward an agent receives contains a noise term ξ that is drawn from a zero mean Gaussian distribution with standard deviation σ. This parameter controls the unreliability of the agent's sensory system, i.e., higher σ means that the information the agent gets about the value of the foods it consumes cannot be fully trusted to reflect the actual value. The more unreliable an environment becomes, the less an agent relies on plasticity to update its weights, Fig. . The EA converged to a learning rate of η p ≈ 0.d so agents must simultaneously learn to move and discriminate good/bad food. It is an optimal solution when the expected loss due to ignoring the environmental transitions is, on average, lower than the loss th th th for the agent to ignore the environment. The static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task of eating or avoiding a food. To start with, we consider a static agent whose goal is to identify thevalue of presented food correctly. We use a genetic algorithm to optimize thelearning rate of the EA to converge to a weight vector that predicts the correct food values. This is an expected result since the convergence time to the ""correct"" weights is highly dependent on the initial conditions. The distance d e between the two environments, with a larger distance requiring a higher learning rate, is expected to lead to a faster convergence time than the distance d a between the 2 environments. The agent opts to have no adaptation during its lifetime and remain at the mean of the two environment. This means that it can afford to have a small learning rate which leads to a more stable convergence and is not affected by noise. We then multiply the learned rate with θ max to maintain the rule's evolved form unchanged, η norm p = (θ 1 , . . . , θ 8 ) and omitting norm . In the following, we always use normalizedη p and θ, omittingnorm . The learning rate is the rate at which the network's weight vector is updated during the lifetime of the agents. As σ increases, the learningRate p decreases, which means the more unreliable the environment becomes. A final factor that affects the learning rate the EA will converge to is the frequency of environmental change during an agent's lifetime. When environmental transition is very rare, agents opt for a very low learning rate, allowing a slow and stable convergence to an environment-appropriate weight vector. As the rate of environmental transition increases, faster learning is required to speed up convergence in order to exploit the (comparatively shorter) stays in each environment. The form of the evolved learning rule depends on the task: Decision vs. Prediction. We examine the robustness of the learning rule the EA discovers by considering a slight modification of our task. Instead of predicting the expected food value, the agent now needs to decide whether to eat the presented food or not. The evolution converges to the following learning rule: In both cases, the rule has the form ∆W t = η p X t [α y R t + β y ]. The similarities indicate some common organizing principles of reward-modulated learning, but their significant differences highlight the sensitivity of the optimization process to task details. We turn to the moving agents in the 2D environment. To optimize these, both the motor network's connections and the sensory network's p. plastic network will incur by learning via the (often misleading because of the high σ) environmental cues. For visual guidance, the lines connect θs from the same run. near the middle of the two environments, ensuring that the average loss of theTwo environments is minimal (Fig. 1). Besides these two changes, the setup of the experiments remains exactly the same. The plasticity parameters. for the rewardprediction task converge on approximately the same point, regardless of the environmental parameters. The resulting learning rule is significantly different ( Fig. 2D). However, the resulting learning rules differ considerably. The rules have a clear Hebbian form (coordination of preand post-synaptic activity) and use the incoming reward signal as a threshold. For example, the output y(t) is computed as: Instead of the MSE loss between prediction and actual value,. the fitness of the agent is now defined as the sum of the food values it chose to consume (by giving y t = 1). The rules converge to a distribution with mean 0 and variance depending on the environment. So this learning rule will match the agent's weight vector with the vector of ingredient values in the environment and thus converge to W c X T t (g(x) = x in this experiment) and R t = W c x T t + ξ we get: Thus the distribution of w t is positive or negative depending on whether the reward R t is above or below a threshold (γ = −β y /α y ) that depends onthe output decision of the network (y t = 0 or 1). In this experiment, the rules converged to a positive distribution, which is the same as the one in the previous section of the study. We show that the learning rules (for the reward-prediction and decision tasks) have a similar form to those in the earlier section of this study. The results are shown in the next section. asticity parameters evolve simultaneously. The agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights. After 100 evolutionary steps, the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values. We compare the dependence of the moving and the static agents on the parameters of the environment: d e and the state transition probability p tr . We find that as the probability of transition increases, the evolved learning rate η p decreases in the moving agents. However, there is a clear difference when it comes to the increase for very small transition probabil-ities that were clearly identifiable in the static but not the moving agent. More data would be needed to make any conclusive assertions about the exact effect of these environmental parameters on the emerging plasticity mechanisms, we say. We conclude that the associations observed in thestatic agents between environmental distance d e. and transition probabilities p tr are largely maintained in theMoving agents. The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. The agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food. We use the correlation instead of the MSE loss because the amplitude of the vector varies a lot for different agents and   for different agent and  for different agents. We find the association between environmental distances and the agent's learning rate increases with the distance d E (Fig. 1). The agent is initialized at the mean of the two environment distributions. In order for an agent to get sufficient exposure to each environment, we scale down the probability p Tr from the equivalent experiment for the static agent. We see that the associated learning rate decreases as the distance between the agent and the environment increases. The association between the environmental distance and the evolving learning rate p tr increases as the agent gets more exposure to the environment. We show that this is the case for moving agents as well as static agents in Fig. 1. We also show that the correlation coefficient of an evolved agent’s weights with the ingredientvalue vector of the current environment (E 1 -blue, E 2 -red) is higher in moving agents than it is for static agents. In moving agents, the plasticity has to effectively identify the exact value distribution of the environments in order to produce accurate predictions. In the embodied agents, it has to merely produce a representation of the. environment that the motorNetwork can evolve to interpret adequately enough to make decisions about which food to consume. The step function (binary output) network evolved a more structured plasticity rule than the linear network. For many agents, the learned weights are consistently anti-correlated with the actual ingredient values. This effectively reduces the flow of information from the sensory to the motor network, forcing the sensory network to consistently decide whether food should be consumed. We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental changings are key to the evolution of plasticity rules in the embodied agents. We conclude by showing that the best-performing agents are able to learn the environment's food distribution more accurately than the static agents, but that this is not a fully-fledged plasticity process. The results are published in the open-source version of the book, ""Evolutionary Plasticity in the Embodied Brain"" (Simon & Schuster, 2013). For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit www.suicidepreventionlifeline.org. For confidential. support, contact the National suicide Prevention Helpline at 1-877-788-7255 or go to http://www.suicidespreventionhelpline.org/. For confidential help in the UK, call 08457 909090 or visit http:// www.sophistication.org/suicide-prevention-helplines/ or  the Samaritans in the UK on 0800-909090. For the UK version of this article, see http://sophistories.co.uk/sophism/features/features-of-your-life-in-the-trends-and-researchers-that-can-help-you-be-more-vulnerable-to-suicide.html. For more information, visit the Samaritans  website or the Sophisticated Psychology and Behavior and Cognition Online (http://www sophistics.com/blog/2014/01/09/sop/features of your-behavior and-behaviours-and behaviour in the-stark-structure-of evolutionary-plasticity-rules-that can-be taken-on-a-sensory-networks and/or from a trend to-a sensational-netway. The interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. The optimization of functional plasticity in neural networks is a promising research direction both as a means to understand biological learning processes and as a tool for building more autonomous artificial systems. The results suggest that reward-modulated plasticity is highly adaptable to different environments and can be incorporated into larger systems that solve complex tasks. Future work could be built on this basic framework to examine more complex reward distributions and sources of environmental variability. Further experiments can investigate environments with different constraints, food diaspora, and other factors that can affect learning in stochastic environments. The study was published in the journal Proceedings of the National Academy of Sciences of the United States of America (PNAS) (http://www.pnas.org/content/early/2014/09/29/151515/npsa.full) It is the first time the authors have used a mechanistic model which uses a biologically plausible learning mechanism (synaptic plasticity) to study the development of plasticity rules in the brain. The findings suggest that biological plasticity mechanisms are highly redundant in the sense that any observed neural connectivity or recorded activity can be achieved with a variety of distinct, unrelated learning rules . This observed redundancy of learning rules in biological settings complements our results and suggests that the function of Plasticity rules cannot be studied independently of the connectivity and topology of the networks they are acting on. It is hoped that the study will help develop a more realistic model of neural network learning in a more complex and realistic way than the one used in this study. It has been previously shown that plasticity can be very well-tuned to take into account different environmental parameters and produce optimal behavior in simple systems. This work studies a simplified toy model of Neural Network learning in the form of a simple reward-Modulated Plasticity rule for solving simple prediction and decision tasks. It also shows that the co-evolution of plasticITY and static functional connectivity in different subnetworks fundamentally changes the evolutionary pressures on the resulting plasticity Rules. The work could also be used as a basis for developing more sophisticated network architectures (multiple plastic layers, recurrent and feedback connections) and more sophisticated plasticity rule parametrizations. It could also lead to the creation of more realistic models of neural networks. tributions, multiple seasons, more complex motor control systems and interactions of those systems with different sensory networks as well as the inclusion of plasticity on the motor parts."
What is the problem encountered when building the fuselage sides?,"Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go ""perfectly."" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane.
This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes.
While building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying ""banana"" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built.
First understand that the plans show the finished form of the plane. They show the ""projected"" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat.
Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel ""undevelopable"" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition ""developable"". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a ""compounded"" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain.
Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home), the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock.
This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later.
Layout to ensure a fair and true fuselage starts by drawing a reference line (baseline) on the building surface. Refer to figures 2 & 3 and use a wire guide to draw a very straight baseline. About 500 lbs. Of tension should be adequate. One could use a chalk line, but we're talking airplanes here, not house framing.
The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Although the baseline will still be the reference, the top and bottom longerons will be laid separately.
Layout differences don't end there. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. Another major difference is that the bottom & side panels are applied after forming the fuselage box section. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape.
Refer to figure 1 for the layout of the new developed side panel. The firewall (station a) is layed out perpendicular to the baseline. Longitudinal (station) measurements are given along the length of the baseline from the firewall. Vertical dimensions are given to reference the angle and breadths of the station at the baseline.
Notice that the top longeron is bowed outward and that the stations are spaced slightly greater than called out in the plans. When the panels are formed into the box frame section ,they will work into the dimensions specified in the plans.
Strike a centerline, longer than is needed on the building surface using a wire guide. Draw off the firewall line perpendicular to the centerline at one end.
Using the distances listed in the balloons, mark them off on the centerline. Distances are measured to the nearest sixteenth of an inch. Take time to mark them off carefully. Don't mark off the distances in a cumulative fashion. Use the firewall as a common reference.
Using the angles listed at each station, mark off a station line longer than is needed. The angles are measured to the nearest hundredth of a degree. Take time to mark them off carefully.
At each station, start by marking off each short (bottom longeron) line distance from the centerline. Use your set of trammels or beam compass for doing this. Mark the intersection of the short line with the station line.
At each station, mark off each long (top longeron) line distance from the intersection of the short line distance and the station line. Again the trammels or beam compass is best for completing this step. Mark the intersection of the long line distance with the station line.
Using the longeron as a batten, trace out the inside and outside curves of the longeron. After the batten is secure, in between each station, fasten a keeper block inside and outside to preserve the shape of the longeron taking care to avoid potential future interference with the diagonal members to be installed later. The fairing blocks can be removed or left in place if they won't interfere with building. The vertical station members and their diagonals can now be measured and positioned. Remember to refer to the plans for the material thickness direction.
After vertical and diagonal members are cut and fitted, take time to draw their outlines on the building surface to cut down on time and confusion when laying out the opposite side.
Finishing the side panel is accomplished in a manner similar to that called for in the handbook with the exception that the side and bottom skin panels will be attached later.
The next article in the series will discuss jigging and building techniques to ensure alignment and straightness of the flat built side panels. Also covered will be building a ""strongback"" jig to assure alignment of the side panels when they are formed into their final shape.
Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the side and bottom ply skins.
U.S. Mail: Densmore Associates, inc.
ANSI ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand.
""Scarfing"" is the practice of splicing plywood so that short pieces of plywood can be used to span long distances. On the KR, it is required on both the fuselage skins and spar webs. The angle of the splice should be 10 to 12 degrees to maintain strength across the joint. Also, joints should coincide with structural members, such as spar webs or fuselage truss members.
This scarfer is made by mating a regular plunge router (this one costs about $50) to a table saw. Obviously, you really only need a table saw to cut the chamfer, but it does make a nice heavy table for scarfing. You could just as easily use a large work table as the base.First, set the table saw for a 5.5 degree cut (for a 1:12 joint, or 6.5 degree cut for a 10:1 joint), and run a 1 x 6 through on edge to chamfer a corner on the board. Then drill the board for three router mounting holes (two are countersunk) and connect the assembly to the table saw with two 1/4 inch bolts. Use a long (2-3 inch) straight cutting bit to do the cutting. Adjust the bit so it doesn't interfere with your table top, and go to town. Keep pressure on the plywood to ensure contact with the table while you're scarfing. Make sure you feed your material from the same end as you would if you were sawing, or the router will take your plywood away from you and put a big dent in your garage door.
In the late 60's Ken Rand and Stuart Robinson were working as flight system engineers for Douglas Avionics. Ken was working as an electrical engineer, having previously worked for Sperry as an autopilots project engineer, while Stu's degree was in aeronautical engineering from Northrop University. They were two of the guys at the end of the DC-8,9, and 10 assembly lines responsible for correcting some of the nits and picks in various systems before delivery to the customer.
They both wanted to build a fast, inexpensive airplane which was also economical to maintain. Several designs were considered, and plans were bought first for the Jeanie's Teenie and then the Taylor Monoplane. The Monoplane was more to their liking, but would require some modification to fit their needs. A cooperative redesign effort ensued, with virtually no dimensions left untouched. Only the basic fuselage structure, airfoil, and powerplant were retained. The tail shape was Stu's, and came directly from the big DC-8s parked on the ramp outside his office window. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport.
Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Ken's wife Jeanette became owner of RR overnight, and stepped up to keep the plans and parts coming. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79.
To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5 KR2Ss now in the air. Much of the development work done on KR's is now done by the builders themselves. KR builders tend to be innovative, which leads to some interesting modifications. Some of the mods that work eventually creep into the plans. The KR2S is a case in point. Many builders who'd heard of the pitch sensitivity and tight cabin of the KR2 began to build an enlarged version, with the length determined by the most commonly available longeron material. The result is a KR2 that is stretched 2"" between firewall and main spar, and 14"" behind the main spar. Higher gross weights dictated more wing area, with the new standard becoming the Diehl wing skin. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts.
Mike Stearns addresses the KR Forum crowd.
This year's KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Mike Stearns spoke on several topics, including the many sources for KR and homebuilding information available on the Internet. He also mentioned KRNet, the list server devoted entirely to KR aircraft, as well as several notable World Wide Web home pages. He also brought a sample of the new Rand Robinson wing skins with him, and discussed their high temperature core prepreg construction. His KR2S will receive the first set, which is currently being installed at Hinson Composites.
Steve Trentman spoke on his turbine installation. It uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus. sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. More on this exciting development in next month's issue of KROnline.
Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. After researching the KR series, and reading Neil Bingham's ""A Critical Analysis of the KR2"" (Jan 88 Sport Aviation), Les decided to build his as a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. His fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. It is skinned with 1/8"" birch plywood. Spars are covered with plywoood on both fore and aft sides, ala KR2S. Diehl wing skins provide the lift. Horizontal stabilizer and elevator were stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. . The fuselage to cowling junction was made more graceful by adding 1.5 inches to the height of the firewall end of the fuselage sides.
Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. The canopy frame fits snugly into a recess in the foward deck, providing an excellent wind and water seal. The fiberglass work is exemplary.
Seating is luxurious for one.
The cowling is also a work of art, and uses NACA ducts for efficiency. Female molds were made for all the fiberglass parts on Les's plane, so he could proabably be persuaded to make more, if demand dictates. Les also machines a multitude of KR aluminum and steel parts which he now offers for sale.
The firewall was reinforced with aluminum brackets and angles bolted between the longerons in anticipation of the 200 lb Subaru EA-81 engine installation. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy.
Originally built as a taildragger, the fixed gear is made from 4130 steel tubing. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. An early taxi test accident damaged the main gear, and prompted Les to change to tricycle gear. Again, he designed his own fiberglass main gear, and uses a Diehl nose wheel fork with a 4130 strut and 6"" wheel up front.
Early tests revealed cooling problems, which prompted a radiator move from the firewall to a lower cowling location.
The first flight was almost a disaster, as test pilot Randy Smith lost power right after takeoff. He managed a 180 with a safe downwind landing with only minor nosewheel pant damage. The culprit proved to be a spark plug with too much reach, which was quickly remedied. Subsequent flights have shown water temp to be about 210 degrees, oil temp is 220-230, and airspeed is about 180 mph.
Shopping for the Partially Built KR.
This story starts about twenty years ago when I first started looking at the KR-2 as the plane I'd like to build. The only problem at that time was a lack of money, lack of knowledge, and a lack of job stability. I liked the design, except for the low ground clearance of the retractable gear and that a KR was going to be a tight fit for me to fly.
Over the past twenty years I've owned a number of planes, but still always wanted to build my own. I needed one that would fit me, my budget requirements, and have the speed and performance that I wanted. When ""KITPLANES"" published the article featuring Roy Marsh's new KR-2S, it was the first I had heard of any major modifications or improvements to the same old KR design. I believe that article and Roy Marsh's workmanship have probably been the greatest boon to Rand Robinson (RR) in the last twenty years. It certainly caught my eye! Here was the same design I had decided I wanted to build twenty years ago, with all of the improvements I wanted. It was sitting on fixed gear with some reasonable ground clearance. It had the capability to be built large enough to accommodate me. It has enough prefab parts available that it didn't have to be 100% scratch built if I decided to hurry the project along. And it had the speed I wanted. I knew that Roy's published speeds were probably not realistic expectations for the average KR, but after knocking around for the last three years in my Champ, anything over 90 mph seems pretty fast to me.
After purchasing the info kit and the sales video from Rand Robinson, the next step after deciding for sure to build this plane was to order the KR-2 plans and the KR-2S addendum. I finally got my plans and was putting together my first order to start the plane, when my partner in the Champ pointed out that there was a partially completed KR-2S for sale in Trade-a-plane. My initial answer was ""No, I don't even want to look at it. I want to build my own from scratch."" My partner insisted that for the advertised price and the fact that it wasn't too far away, I ought to at least give the guy a call and investigate it. ""No, I don't think I want to buy someone else's problems,"" I persisted. That night I went home and crunched up some numbers on the calculator and finally came to the conclusion that for the sake of my budget for the next several years, I really should give this guy a call.
Three days later, I flew to his place about 400 miles away to take a look at his project. At this point I should probably mention that I consider myself to be fairly knowledgeable about airplane construction, although the vast majority of my experience is with tube and fabric. The rest of this article deals with what I looked for and more importantly what I missed and have had to repair in the last year since I purchased the project.
When we went to the seller's house, I found that the left wing was built using the Dan Diehl wing skins and the right wing skins were leaning against the wall inside the house. Also the canopy was in the house with the canopy covered with paper and tape. I wanted to inspect the fuselage first, so off we went to the shop.
There I found a fuselage sitting on it's gear painted in primer gray. The first step was to inspect the quality of workmanship of what could be seen as it sat. The interior of the fuselage looked as if it had been built with a great deal of care. The fit and finish of all of the interior wood was very nice. Even the gussets looked like they had been painstakingly perfectly fitted. The glass work on the turtle back also looked very precise and clean. It was evenly faired into the vertical and horizontal stabs. The tail also appeared to be well built with the exception of a depression directly over the front and rear spars in the horizontal stabs. He explained that when he moved recently, that he had shot the plane with gray primer to protect it from the weather since he wouldn't have ready access to a shop to put it in right away. It ended up sitting out in the hot south Texas summer sun for a few weeks before he got a shop rented to work in. That caused the glass (or possibly the foam inside the horizontal stab) to swell, except that it held onto the spar, so it was slightly ballooned in front of and behind the spars. His recommendation was to fill it back smooth with micro.
I also found a small linear crack in the lower left wing spar cap on the left wing stub. It appeared to be from over tightening the rear spar wing attach fitting bolts. His explanation was that the crack wasn't important because the rear spars only job is to keep the wings from folding back. I also noticed that the holes for attaching the outer wing to the wing stub were badly rounded out on the rear spar. He explained that the Diehl wing skins require the rear spar to be swept slightly more forward than the stock wings. This won't allow you to use the rear spar attach fittings from RR and that I would need to fabricate a new set of rear spar attach fittings.
I also found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. I couldn't check for function since the right bellcrank and sheeve wasn't installed, the left wing also wasn't installed, and the right wing didn't exist yet.
Next we pulled the inspection panels off of the fuselage and tail and looked at everything I could see with a good flashlight. I didn't find anything else that might be questionable about the fuselage except for a cracked elevator trim tab that was damaged when it fell off it's hanging place on the wall.
Next we spent some time going over his builders log and builders photo album. I still hadn't seen anything that would dissuade me from buying this project.
At this point it was starting to get late and my ride down needed to get airborne for the flight home. I needed to make a decision about whether I wanted this project or not, but I hadn't inspected the wings and canopy yet. I took a cursory look at the left wing and saw lots on micro built up on it and some bubbles in the leading edge, but nothing that looked seriously wrong to my amateur eye. The right wing was only a set of spars in the shop and the Diehl wing skins in the house, so there wasn't much to look at there. The canopy was wrapped in paper and tape, so there wasn't much to look at there either. I decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the project. For the advertised price, I could build a new set of wings and still be way ahead financially. We negotiated a final price, shook hands, took my ride to the airport, and started off in search of a U-haul to haul the project home.
Now, at this point, some of you are thinking about what I surely must have forgotten to inspect and why didn't I take a local A & P or EAA member along for the ride. First of all, I don't know any mechanics locally that have any experience with glass and our EAA chapter of which I am VP is woefully lacking in fiberglass knowledge. Secondly, as you will see, I missed plenty. Some by ignorance, some by just not looking close enough.
Now for a list of the problems that I found over the last year and a few of the fixes that I came up with.
I found that the lower set of rear spar attach fittings on the left rear spar were installed backwards with the longer spaced hole towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Also in the same area he had drilled through the rear spar with a hole saw to create a place for the aileron cable to pass through and managed to cut out the second from the outside vertical brace in the spar. Then he chose to install the aileron bellcranks in front of the rear spar, and cut another hole through the rear spar for the aileron push rod. He also managed to cut out the outside vertical brace in the spar. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in, cut the whole rear spar carrythrough out of the fuselage including ruining the left lower wing skin, or do something else creative to reinforce the spar cap and install a custom built set of attach fittings.
I also found that after I built and installed the right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. The cable that crosses between the two bellcranks had a sharp uphill from the sheeve to the bellcrank in the last 12 inches on either side. This combined with the radius that the bellcranks turn caused the cross cable to pull up tight when the ailerons were pushed to either end of their travel, but allowed the cables to go very slack when the ailerons were centered. Also the Aileron pushrods needed to pass directly through the lower set of rear wing attach fittings to attach to the aileron. This whole rear spar and aileron bellcrank setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it.
I decided that I had to remove the rear fittings from the left wing to be replaced with the new set that my neighborhood machinist was cutting out for me. When I put the wing on the work bench to start removing the rear fittings, I thought I had better take a closer look at the bubbles in the leading edge. I found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leading edge. What I found was that the leading edge had been floxed together and glassed over, but the mold release had never been scrubbed off the leading edge of the wing. It peeled apart for rebuild quite easily.
When I got back to removing the rear spar attach fittings, I noticed that the woodwork inside the wing looked awfully dull. The reason was that the wing had been closed up without varnishing any of the woodwork. This was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer.
I also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. Retapping the fitting the right direction seemed to be a good fix for that problem.
When I finally got around to attaching the wing to the fuselage, I found that the front spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts.
On the fuselage I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. I must complement the manufacturers for taking care of that problem. One call to the Cleveland factory and they shipped me a new set of wheels and brakes even though the receipt for this set was over four years old and in the original builders name. Their only concern was that this set had never been placed in service yet.
I chose to sand the load of micro off the left wing to see what it was covering. When I got down to the glass, I found that there was no glass for the aft inch and a half of the underside of the wing in front of the aileron hinge. With the Diehl wing skins, you build the wings, then cut the ailerons out of trailing edge of the wing. He had mismeasured and cut too much material off the bottom side of the trailing edge in front of the aileron. It was filled by floxing a piece of spruce into the gap to fill the space between the back edge of the fiberglass and the aileron mount. I chose to wrap the trailing edge of that wing, and the other wing to match with a couple of lay-ups of glass.
When I sanded the primer off the aforementioned damaged trim tab, I found that the hinge was floxed to the leading edge of the foam insides of the tab, but not the glass. I also chose to wrap the front of the trim tab with a lay-up of glass.
I decided to pull the paper off the canopy and take a look at it before I'm ready to bolt it on and fly. The original builder had blown his own canopy and after some of the previous problems, I was beginning to have some concerns about not having looked it over closely enough. The canopy turned out to have been blow a little too large. It ended up with a little larger bubble for headroom, which I didn't object to. However, it had more headroom on the right side than the left. Yes, it was just a little bit lopsided. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm.. My fear was that this is just thin enough that it may decide to lay on my head or in my lap when flying on a warm day. It will have to be replaced.
I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and I'm sure that there could have been many other problems that didn't but could have existed on this project. This is also not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. This article is intended to feature some of the problems that you may run into in buying someone else's project.
The final question is, knowing what I have found over the past year, would I have still purchased this project. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. There are a few things that I would have done differently, but nothing that I can't live with. Although I won't be able to say that I built it all from scratch, I have built and rebuild enough of the plane that I should have no problem qualifying under the 51% rule.
You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"".
Here is an brief explanation of how I built my turtledecks. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo.
After the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade.
After the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out.
At this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the ""backbone"" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees.","['The longerons bow up from the building surface, forming a ""banana"" shape.']",6240,multifieldqa_en,en,,9ac60cf60d4bae1dc93b09f374b74b3dec8b1b333397d7cd," This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 &KR-2 series planes. The solution starts by understanding the three dimensional relationship of the assembled parts being built. The problem starts when the side panels are bent and sloped to form the fuselage box section. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat. This is important when laying out the side and bottom panels onto flat plywood. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, andSloped to Form a Conical section, it takes on an elliptical shape firewall to tailstock. It should be stressed that although this method borrows heavily from proven techniques used in the marine trades, it should not be stressed at this point in the process that it is not a complete solution to the problem of building an airplane with pre-fabricated parts. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to anydegree of satisfaction. If the sides were not sloped, the section formed would be cylINDrical and the longons would lie flat. Since they were not tumbled home, the part formed is now conical. If you want to build an experimental aircraft, you need to start with a minimum of pre- fabricated parts, especially when using the preformed fiberglass parts. You need to build the plane with a basic understanding of the materials you are using. The best way to start is to start by understanding that the parts you are building are not going to last for long periods of time, and that they will need to be replaced with new parts that will last for longer periods of the construction process. The next step is to learn how to use the parts that you have already built to make the plane more reliable. The most important thing is to understand that the plane will not last longer than a few months or even a few years before it is ready to fly. The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape. The next article in the series will discuss jigging and building techniques to ensure alignment and strai. The layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later. Back to Mail Online home. back to the page you came from. Back To the pageyou came from, Back to the pages you came From. The fuselage layout guide is available in English, French, German, Italian, Spanish, and Portuguese. For more information on how to layout a fuselage, visit the fuselage. layoutguide.org. For the layout guide in English and French, visit www.fuselage.com/fuselage-laid-in-the-world-for-aircraft-builders-and-engine-builders. For a guide to layout in Spanish and Portuguese, visit http://www. fuselageguide.co.uk/ fuselage-layout-guide.html. For an overview of the layout procedure in German and Italian, visit: fuselagelayoutguide.com/. For a breakdown of layout procedures in the U.S., visit: www.u.S.A. Aircraft Building Guide. For details on layout in the United States, see: http:/www.uS/Aircraft-Building-Guide.html/. For more details on the layout in Europe and the rest of the world, go to: http:// www.australia.org/airplane-building-guide/faselage- layout-guide-1-2-3-4-5-6-7-7. For information on layout for Russia and the Middle East, see www.aircraftbuilder.com%. For information about layout in other countries, visit  the fuselage building guide. for the Middle Eastern and South American countries, such as Russia and South Africa, see http://aircraftbuilding.co/home.uk/. for more information about the layout process in South Africa and the West African and South America, see  http:www.aastralia-building.org/. For the South American and South African versions of this article, click on the link to the home page for a detailed layout guide. Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the sides and bottom ply skins. ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand. To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5KR2Ss now in the air. KR2 builders tend to be innovative, which leads to interesting mods that the KR2 has. The KR2 is a case in point: Many who'd heard of the pitch and tightness of the cabin of theKR2 began to modify the plans to suit their needs. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport. Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79. Some of the work on KR2's is now done on some modifications on the KR1 and KR2. Some modifications eventually creep into some of the mods that KR2 builder's have done on their KR2 aircraft. The most recent modification is the addition of ailerons to the fuselage to make it easier for the plane to take off and land on a tight angle, such as with aileron-only landing gear, or with a fixed landing gear instead of a roll-up landing gear. It is also the case in which the landing gear came directly from the big DC-8s parked on the ramp outside his office window. The tail shape was Stu's, and came from thebig DC-9s parked in the parking lot outside hisoffice window. It was also the tail shape that led to the design of the KR-2, which is now the world's most popular single-engine fighter jet. It's also the most popular fighter jet in the world, followed by the F/A-18 Hornet and the F-18 Super Hornet. The F-16 Hornet is the most advanced fighter jet on the market today, with a top speed of nearly 100 mph. It has a range of more than 1,200 miles (1,600 km) and can carry a crew of up to 20,000 pounds (9,000 km) on a single engine. It also has the largest wingspan of any U.S. fighter jet, and is the only one of its kind that can take off from a standstill and land vertically. It can also be easily converted into a glider by using a special frame that can be attached to the front of the plane with a piece of plywood, or by attaching it to the back of it with a screwdriver. The fuselage can be made to look more like a conventional fuselage by cutting it in half.  KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. The KR2 is stretched 2"" between firewall and main spar, and 14"" behind the main spar. The fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy Hegy. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. The cowling is also a work of art, and uses NACA ducts for efficiency. The fiberglass work is exemplary. The cockpit is a fiberglass main, with a Diehl nose nose and a Diee nosecone. The instrument panel is made of fiberglass, with an aluminum frame. The landing gear is a tricycle gear, and he designed his own test accident test accident gear. He also offers a multitude of KR aluminum and steel parts which he now offers for sale. He uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. More on this exciting development in next month's issue of KROnline. It is also possible to build a KR2 with a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. The result is aKR2 that is stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts. It will receive the first set of the new Rand Robinson wing skins, which are currently being installed at Hinson Composites. The first set is expected to arrive in the next few weeks, and the KR2S will get the second set later this year. It has a high temperature core prepreg construction, and will be made of high-performance polyethylene terephthalate (HETT) material. It also has a large number of parts that can be pre-assembled by hand. The KR-2S was designed by Rand Robinson in the 1980s. The plane was designed to fly at speeds of up to 180 mph. It was designed with low ground clearance and a large fuselage. It has been partially built by a team of four people. The project has been in the works for more than 20 years and is still in the planning stages. It is not yet clear if the plane will ever be fully built by Robinsons' team. For more information on the project, visit www.kitplans.com. For information on building your own KR, visit http://www.kitspans.co.uk/kits-pans/KR-2s-partially-built-by-randy-robertson-and-the-team-of-four-who-have-been-working-on-it-for-20-years. For details on building a KR, go to kitspanners.com/kickspanners/kips-panners-partial-built.html. For the rest of the story, visit kitsplanners.org/kps-kips/kpinners/kpp-kinspinners-part-1-kickspinners. For all the information on how to build your own kips, visit the kipspanners site.com/. For the full story on the kpinners site, visit kipspaners.org/. For more info on the Kipspinners website, visit: http://kipsbaners.com%. For the complete story on kipspanners.com, visit  www. kipskinners.org. for the complete list of kipspinner's kipsons.For more information about the Kinspanners, visit their website: http:www.kspinner.com/?p=kipspiners. For a full list of the kpsons' kipspins, see http: www.skipspin.org%. For a list of all the kenspinners, see www.kspins.gov.uk/. For a complete list, visithttp:http: http:\/www.skype.com /skyps/skype/skypes/skypiners/skypaners/skips/skypins/sky-paners-skypin-skyps-sky-pins-skypan-skypans-skype-skypes-skyPins-SkyPins. For an overview of the world's most popular kipspeners, see:http://skyps.com/""skyps""/. For an in-depth look at the best of the best kipspunters, see ""skyps"", ""skype"" or ""skypin"" . For a short time, I was the only person in the world who could fly the KR, so I didn't know what to do with it. I decided to build it. CNN.com's John Sutter sat down with a group of friends to try to find a way to get to the top of the list. Sutter started out by looking at the glass work on the turtle back. He found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. He decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the new set of wings and still be way ahead of the rest of the group. Now for a list of the problems that I found over the last year and a few of the fixes that I came up with that I didn't know any of the people at the center of the story about. The top 10 list: ""Some by ignorance, some by just not looking close enough. Some by ignorance and some by not looking very close enough."" The bottom 10 list includes: ""I found a small linear crack in the lower left wing cap on the left wing stub,"" and ""I didn't find anything else that might be questionable about the right-hand side of the wall"" and ""It was evenly faired into the vertical and horizontal stabs"" on the right wing. The bottom ten list includes ""the top 10 things that I could see with a good flashlight, but nothing that looked seriously wrong to my eye"" and the bottom 10 items that I couldn't see with my own eyes, but that I thought would be nice to look at at some point. The list ends at the top with ""The top ten things that you can see with your own eyes,"" and the top 10 items are ""the things you didn't see that you should have seen that you wished you had seen more of"" and a ""top 10 list of things to do with the time you spent on this project"" and someplace else that you wish you had spent more time on it, but didn't have the time to do it on the other side of it. It's time to move on to the next part of the show. The next part is ""The Top 10 Things You Need To Know About The Top 10 List,"" which will be posted at the bottom of the page on Monday. The final part is the ""Top 10 Things to Know about the Top Ten List"" which will feature the most important things you learned from this week's episode of CNN.com iReporters and iReporter photos and videos from around the world. It will also feature some of the best photos of the U.S. from the last few weeks of the year, as well as a look back at some of our favorite moments from the past year. The show will be available on CNN.co.uk and CNN.uk on Monday, July 25. The episode will also be posted on iReport.com on Tuesday, July 26, and Wednesday, July 27, and Thursday, July 28, at 10 a.m. and 7 p.m., July 28. Le towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in. Or do something else creative to reinforce the spar cap. Also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. Retapping the fitting the right direction seemed to be a good fix for that problem. I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. One call to the Cleveland factory and they shipped me a new set of wheels andbrakes even though the set for this set was four years old and in the original builders name. Their only receipt was over receipt was a receipt for the set that was over four years ago and in their co-owner's name. The problem was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer. The rear spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts. I also found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leads edge. It peeled apart for rebuild quite easily. The leading edge had been floxing together and glassed over, but the mold release had never been scrubbed off the lead edge of the wing. It was rectify with asmall hole saw and modified undercoat sprayer to get it back to its original state. The right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. Also the Aileron pushrods needed to pass directly through the lower set of rear wingattach fittings to attach to the ailerson. This whole rear spar and bellcranks setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it. It needed to be replaced with the new set that my neighborhood mach inist was cutting out for me. It had to be removed the rear fittings from theleft wing to be replacing with thenew set that his new set. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. It also had a sharp uphill from the sheeve to the bellcranking in the last 12 inches on either side. This combined with the radius that the bell cranks turn caused the cross cable to pull up tight when the aiderons were pushed to either end of their travel, but allowed the cables to go very slack when they were centered. This article is intended to feature some of the problems that you may run into in buying someone else's project. The canopy turned out to have been blow a little too large. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. This will give you an idea of how tall your aft bulkhead need to be. As far as location, I placed my aft Bulkhead just forward of the lower/front of my vertical fin. I constructed the fuselage, it is glued together with automotive bondoed to the jig. I used the stringers I ripped from the 1x 4s and gave me a male poster form to cover with thin plastic or plastic. This was the only form of cover I could find that was thin enough to cover. The fuselage was glued together. I was able to cover it with a thin plastic form to give it a male cover. It is now ready to fly. You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"". You can also send them to jscott.gov@mailonline.co.uk or call the FAA at 1-800-447-8255. The FAA has no comment on this article or any of the other questions you may have about the Turtledecks. The author has no plans to fly the aircraft in the near future. He plans to continue to build his own turtledecaks in the coming months. He has built and rebuild enough of the plane that he should have no problem qualifying under the 51% rule. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and there could have been many other problems that didn't but could have existed on this project. It's not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. Yes, it was just a little bit lopsided. It had more headroom on the right side than the left. It will have to be replaced. The final question is, would I have still purchased this project? I don't know if I'll ever be able to fly it in the first place, but I'll still live with the knowledge that I've built it all from scratch, even if it's not in service yet. I can't live with that. I'll live with it until I fly it. I stapled two layers of posterboard to the jig(thin plastic would work better) The posterboard wraps down two inches onto the fuselage. I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fu fuselage) and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed and 2 inch tapes added to the Bulkheads inside and out. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each sides of the ""backbone"" half the distance to the edge of the turtlingeck. I scuff sanded and glued the foam stringers in with micro. board @ 45degrees. I also covered the foamstringers with one layer. of 8oz bid @ 45 Degrees. When covered with a layer of bid @45degrees there would be a nice transition from the turledecks skin up onto the foam and then back onto the tourledeks skin. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-up 4 to 6 hours after wetout while theLay-up is still soft enough to cut with a razorblade. (be careful, the bondO sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass. skin that tends to hold the shape of theJig but is still flexible enough to work with. I made two inches wide strips that would run the entire length, forward and aft inside theTurtledEck."
When did Goodwin become a Naval aviator?,"Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War.

Following the War, he remained in the Navy and rose to the flag rank and held several important commands including Vice Commander, Military Air Transport Service, Commander, Carrier Division Two and Commander, Naval Air Forces, Continental Air Defense Command.

Early life and career

Hugh H. Goodwin was born on December 21, 1900, in Monroe, Louisiana and attended Monroe High School there (now Neville High School). Following the United States' entry into World War I in April 1917, Goodwin left the school without receiving the diploma in order to see some combat and enlisted the United States Navy on May 7, 1917. He completed basic training and was assigned to the battleship . Goodwin participated in the training of armed guard crews and engine room personnel as the Atlantic Fleet prepared to go to war and in November 1917, he sailed with the rest of Battleship Division 9, bound for Britain to reinforce the Grand Fleet in the North Sea.

Although he did not complete the last year of high school, Goodwin was able to earn an appointment to the United States Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname ""Huge"" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Robert E. Blick Jr., Herbert S. Duckworth, Clayton C. Jerome, James P. Riseley, James A. Stuart, Frank Peak Akers, Sherman Clark, Raymond P. Coffman, Delbert S. Cornwell, Frederick J. Eckhoff, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy, William S. Parsons, Harold R. Stevens, John P. Whitney, Lyman G. Miller and George J. O'Shea.

Goodwin graduated with Bachelor of Science degree on June 3, 1922, and was commissioned Ensign in the United States Navy. He was subsequently assigned to the battleship  and took part in the voyage to Rio de Janeiro, Brazil, before he was ordered to the Naval Torpedo Station at Newport, Rhode Island for submarine instruction in June 1923. Goodwin completed the training several weeks later and was attached to the submarine . He then continued his further training aboard submarine  and following his promotion to Lieutenant (junior grade) on June 3, 1925, he qualified as submariner.

He then served aboard submarine  off the coast of California, before he was ordered for the recruiting duty to San Francisco in September 1927. While in this capacity, Goodwin applied for naval aviation training which was ultimately approved and he was ordered to the Naval Air Station Pensacola, Florida in August 1928. Toward the end of the training, he was promoted to lieutenant on December 11, 1928, and upon the completion of the training in January 1929, he was designated Naval aviator.

Goodwin was subsequently attached to the Observation Squadron aboard the aircraft carrier  and participated in the Fleet exercises in the Caribbean. He was transferred to the Bureau of Aeronautics in Washington, D.C. in August 1931 and served consecutively under the architect of naval aviation William A. Moffett and future Chief of Naval Operations Ernest J. King.

In June 1933, Goodwin was ordered to the Naval War College at Newport, Rhode Island, where he completed junior course in May of the following year. He subsequently joined the crew of aircraft carrier  and served under Captain Arthur B. Cook and took part in the Fleet exercises in the Caribbean and off the East Coast of the United States.

He was ordered back to the Naval Air Station Pensacola, Florida in June 1936 and was attached to the staff of the Base Commandant, then-Captain Charles A. Blakely. When Blakely was succeeded by William F. Halsey in June 1937, Goodwin remained in Halsey's staff and was promoted to Lieutenant Commander on December 1, 1937. He also completed correspondence course in International law at the Naval War College.

Goodwin was appointed Commanding officer of the Observation Squadron 1 in June 1938 and attached to the battleship  he took part in the patrolling of the Pacific and 
West Coast of the United States until September 1938, when he assumed command of the Observation Squadron 2 attached to the battleship .

When his old superior from Lexington, now Rear Admiral Arthur B. Cook, was appointed Commander Aircraft, Scouting Force in June 1939, he requested Goodwin as his Aide and Flag Secretary. He became Admiral Cook's protégé and after year and half of service in the Pacific, he continued as his Aide and Flag Secretary, when Cook was appointed Commander Aircraft, Atlantic Fleet in November 1940.

World War II

Following the United States' entry into World War II, Goodwin was promoted to the temporary rank of Commander on January 1, 1942, and assumed duty as advisor to the Argentine Navy. His promotion was made permanent two months later and he returned to the United States in early 1943 for duty as assistant director of Planning in the Bureau of Aeronautics under Rear admiral John S. McCain. While still in Argentina, Goodwin was promoted to the temporary rank of Captain on June 21, 1942.

By the end of December 1943, Goodwin was ordered to Astoria, Oregon, where he assumed command of newly commissioned escort carrier USS Gambier Bay. He was responsible for the initial training of the crew and was known as a strict disciplinarian, but the crew appreciated the skills he taught them that prepared them for combat. Goodwin insisted that everyone aboard has to do every job right every time and made us fight our ship at her best.

During the first half of 1944, Gambier Bay was tasked with ferrying aircraft for repairs and qualified carrier pilots from San Diego to Pearl Harbor, Hawaii, before departed on May 1, 1944, to join Rear admiral Harold B. Sallada's Carrier Support Group 2, staging in the Marshalls for the invasion of the Marianas.

The air unit, VC-10 Squadron, under Goodwin's command gave close air support to the initial landings of Marines on Saipan on June 15, 1944, destroying enemy gun emplacements, troops, tanks, and trucks. On the 17th, her combat air patrol (CAP) shot down or turned back all but a handful of 47 enemy planes headed for her task group and her gunners shot down two of the three planes that did break through to attack her.

Goodwin's carrier continued in providing of close ground support operations at Tinian during the end of July 1944, then turned her attention to Guam, where she gave identical aid to invading troops until mid-August that year. For his service during the Mariana Islands campaign, Goodwin was decorated with Bronze Star Medal with Combat ""V"".

He was succeeded by Captain Walter V. R. Vieweg on August 18, 1944, and appointed Chief of Staff, Carrier Division Six under Rear admiral Arthur W. Radford. The Gambier Bay was sunk in the Battle off Samar on October 25, 1944, during the Battle of Leyte Gulf after helping turn back a much larger attacking Japanese surface force.

Goodwin served with Carrier Division Six during the Bonin Islands raids, the naval operations at Palau and took part in the Battle of Leyte Gulf and operations supporting Leyte landings in late 1944. He was later appointed Air Officer of the Philippine Sea Frontier under Rear admiral James L. Kauffman and remained with that command until the end of hostilities. For his service in the later part of World War II, Goodwin was decorated with Legion of Merit with Combat ""V"". He was also entitled to wear two Navy Presidential Unit Citations and Navy Unit Commendation.

Postwar service

Following the surrender of Japan, Goodwin assumed command of Light aircraft carrier  on August 24, 1945. The ship was tasked with air missions over Japan became mercy flights over Allied prisoner-of-war camps, dropping food and medicine until the men could be rescued. She was also present at Tokyo Bay for the Japanese surrender on September 2, 1945.

Goodwin returned with San Jacinto to the United States in mid-September 1945 and he was detached in January 1946. He subsequently served in the office of the Chief of Naval Operations until May that year, when he entered the instruction at National War College. Goodwin graduated in June 1947 and served on Secretary's committee for Research on Reorganization. Upon promotion to Rear admiral on April 1, 1949, Goodwin was appointed Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet under Admiral William H. P. Blandy.

Revolt of the Admirals

In April 1949, the budget's cuts and proposed reorganization of the United States Armed Forces by the Secretary of Defense Louis A. Johnson launched the wave of discontent between senior commanders in the United States Navy. Johnson proposed the merging of the Marine Corps into the Army, and reduce the Navy to a convoy-escort force.

Goodwin's superior officer, Admiral Blandy was call to testify before the House Committee on Armed Services and his harsh statements for the defense of the Navy, costed him his career. Goodwin shared his views and openly criticized Secretary Johnson for having power concentrated in a single civilian executive, who is an appointee of the Government and not an elected representative of the people. He also criticized aspects of defense unification which permitted the Joint Chiefs of Staff to vote on arms policies of individual services, and thus ""rob"" the branches of autonomy.

The outbreak of the Korean War in summer 1950 proved the proposal of Secretary Johnson as incorrect and he resigned in September that year. Also Secretary of the Navy, Francis P. Matthews resigned one month earlier.

Later service

Due to the Revolts of the admirals, Blandy was forced to retire in February 1950 and Goodwin was ordered to Newport, Rhode Island for temporary duty as Chief of Staff and Aide to the President of the Naval War College under Vice admiral Donald B. Beary in April 1950. Goodwin was detached from that assignment two months and appointed member of the General Board of the Navy. He was shortly thereafter appointed acting Navy Chief of Public Information, as the substitute for Rear Admiral Russell S. Berkey, who was relieved of illness, but returned to the General Board of the Navy in July that year. Goodwin served in that capacity until February 1951, when he relieved his Academy class, Rear admiral John P. Whitney as Vice Commander, Military Air Transport Service (MATS).

While in this capacity, Goodwin served under Lieutenant general Laurence S. Kuter and was co-responsible for the logistical support of United Nations troops fighting in Korea. The MATS operated from the United States to Japan and Goodwin served in this capacity until August 1953, when he was appointed Commander Carrier Division Two. While in this assignment, he took part in the Operation Mariner, Joint Anglo-American exercise which encountered very heavy seas over a two-week period in fall 1953.

Goodwin was ordered to the Philippines in May 1954 and assumed duty as Commander, U.S. Naval Forces in the Philippines with headquarters at Naval Station Sangley Point near Cavite. He held that command in the period of tensions between Taiwan and China and publicly declared shortly after his arrival, that any attack on Taiwan by the Chinese Communists on the mainland would result in US participation in the conflict. The naval fighter planes under his command also provided escort for passing commercial planes. Goodwin worked together with retired Admiral Raymond A. Spruance, then-Ambassador to the Philippines, and accompanied him during the visits to Singapore, Bangkok and Saigon in January 1955.

On December 18, 1955, Goodwin's classmate Rear admiral Albert K. Morehouse, then serving as Commander, Naval Air Forces, Continental Air Defense Command (CONAD), died of heart attack and Goodwin was ordered to CONAD headquarters in Colorado Springs, Colorado to assume Morehouse's position. While in this capacity, he was subordinated to Army General Earle E. Partridge and was responsible for the Naval and Marine Forces allocated to the command designated for the defense of the Continental United States.

Retirement

Goodwin retired on June 1, 1957, after 40 years of active service and was advanced to the rank of Vice admiral on the retired list for having been specially commended in combat. A week later, he was invited back to his Monroe High School (now Neville High School) and handed a diploma showing that he had been graduated with the class of 1918. He then settled in Monterey, California where he taught American history at Stevenson school and was a member of the Naval Order of the United States.

Vice admiral Hugh H. Goodwin died at his home on February 25, 1980, aged 79. He was survived by his wife, Eleanor with whom he had two children, a daughter Sidney and a son Hugh Jr., who graduated from the Naval Academy in June 1948, but died one year later, when the Hellcat fighter he was piloting collided with another over the Gulf of Mexico during training.

Decorations

Here is the ribbon bar of Vice admiral Hugh H. Goodwin:

References

1900 births
1980 deaths
People from Monroe, Louisiana
Military personnel from Louisiana
United States Naval Academy alumni
Naval War College alumni
United States Naval Aviators
United States Navy personnel of World War I
United States Navy World War II admirals
United States Navy vice admirals
United States submarine commanders
Recipients of the Legion of Merit",['Goodwin became a Naval aviator in January 1929.'],2294,multifieldqa_en,en,,0e2b2bc81542b95fb86a7fe76cf11c52316937c9aac9123e," Hugh Hilton Goodwin (December 21, 1900 – February 25, 1980) was a decorated officer in the United States Navy with the rank of Vice Admiral. A veteran of both World Wars, he commanded escort carrier  during the Mariana Islands campaign. Goodwin then served consecutively as Chief of Staff, Carrier Strike Group 6 and as Air Officer, Philippine Sea Frontier and participated in the Philippines campaign in the later part of the War. Although he did not complete the last year of high school, Goodwin was able to earn an appointment to the U.S. Naval Academy at Annapolis, Maryland in June 1918. While at the academy, he earned a nickname ""Huge"" and among his classmates were several future admirals and generals including: Hyman G. Rickover, Milton E. Miles, Herbert S. Duckworth, James A. Riseley, Frank Peak Akers, Raymond P. Coffman, Ralph B. DeWitt, John Higgins, Vernon Huber, Albert K. Morehouse, Harold F. Pullen, Michael J. Malanaphy. Goodwin was ordered to the Naval Air Station at Newport, Rhode Island, where he completed junior course in May of the following year. He then continued his further training aboard submarine  and following his promotion to Lieutenant (junior grade) on June 3, 1925, he qualified as submariner. He was subsequently assigned to the battleship  and took part in the voyage to Rio de Janeiro, Brazil, before he was ordered for the recruiting duty to San Francisco in September 1927. Goodwin applied for naval aviation training which was ultimately approved and he was order to the naval air station at Newport in August 1928. After the end of the training, he was promoted to lieutenant on December 11, 1928, and upon the completion of theTraining in January 1929, he became designated Naval aviator. Goodwin completed the training several weeks later and was attached to the. Naval Torpedo Station at. Newport in June 1923. He subsequently joined the crew of aircraft carrier  and served under Captain Arthur B. Cook and took parts in the Fleet exercises in the Caribbean and off the East Coast of the United United States. Goodwin went on to become a director of a number of other companies including a bank, an insurance company, a hedge fund, a bank and a real estate company. He also served as the president of a company that went bankrupt in 2008. He is now the chairman and CEO of a non-profit organization that helps young people through their college and university degrees. He has been awarded the title of “Distinguished Alumnus” for his contributions to society and has received numerous awards for his service to the community. Goodwin has also been awarded a “C” grade for his work in the community, including the “A’s”, “B” and “D” grades for his “F” “G” awards. Goodwin served with Carrier Division Six during the Bonin Islands raids, the naval operations at Palau and operations supporting Leyte landings in late 1944. For his service in the later part of World War II, Goodwin was decorated with Legion of Merit with Combat ""V"". He was later appointed Air Officer of the Philippine Sea Frontier under Rear admiral James L. Kauffman and remained with that command until the end of hostilities. He was responsible for the initial training of the crew and was known as a strict disciplinarian, but the crew appreciated the skills he taught them that prepared them for combat. Goodwin was awarded the Bronze Star Medal for his service during the Mariana Islands campaign, and the Navy Commendation Medal for bravery in the Battle of Leyte Gulf in October 1944. He died of a heart attack on December 31, 1945, in San Diego, California. He is survived by his wife, two children and a step-grandchild. He has a son and a daughter, both of whom were born in the U.S. during the Second World War. He also has a grandson, a son-in-law, a grandson and a great-grandson. He served in the Navy as a Lieutenant Commander and was promoted to Lieutenant Commander on December 1, 1937. Goodwin also served as an Aide and Flag Secretary to Rear Admiral Arthur B. Cook in the Atlantic Fleet during World War Two. He later served as the Chief of Staff of the Bureau of Aeronautics under Rear Admiral John S. McCain in the early 1940s. Goodwin served as a Commander of the USS Gambier Bay in the Pacific Ocean during the war. He helped turn back a much larger attacking Japanese surface force at the Battle off Samar in October, 1944. His ship was sunk by the Japanese during the Battle for Leyte in November, 1944, after helping turn back an attacking Japanese force. Goodwin died in a car crash on December 30, 1945 in San Jose, California, while on his way back to the United States from the Philippines. He will be buried in the San Diego National Cemetery in San Francisco, California on January 25, 1967. He had served in World War I and the First World War as a member of the staff of the Base Commandant, then-Captain Charles A. Blakely. He worked for the US Navy until he retired in 1946. Goodwin is the author of the book, ""The American Navy: The First World Wars and the Second Half of the War"" published by Simon & Schuster, Ltd. (1998). He is also the co-founder of the Naval War College, which is based at the University of California, San Diego. He currently lives in California with his wife and their three children. His great-great-granddaughter, Emily, was born on December 26, 1936, and is married to Captain Walter V. R. Vieweg, a former Navy officer. Goodwin worked as a captain in the battleship USS Gambiers Bay, which was sunk off Samar on October 25, 1944 by a Japanese surface-attack force. Goodwin served in the U.S. Navy during the Second World War. He was a member of the General Board of the Navy from 1950 to 1954. He served as Commander, U.s. Naval Forces in the Philippines from 1954 to 1955. Goodwin served as the Chief of Staff and Aide to Commander-in-Chief, Atlantic Fleet from 1949 to 1950. He also served as Vice Commander, Military Air Transport Service (MATS) from 1951 to 1953. Goodwin retired from the Navy in 1954 after the outbreak of the Korean War. Goodwin was the first member of his class to graduate from the Naval War College. He died on December 31, 1958. He is buried in the National Cemetery of the Pacific in San Diego, California. He has a son, Rear Admiral William P. Blandy, and a daughter, Rear admiral Mary P. Goodwin. The Goodwin family has lived in Newport, Rhode Island, since the 1950s. Goodwin is survived by his wife, Mary, and his son, Admiral William H. B. Goodwin, who served as a Rear Admiral in the United States Air Force from 1958 to 1961. Goodwin also has a grandson, Vice Admiral John P. Whitney, who is a retired Rear Admiral and served in World War II as a Commander in the Air Force. Goodwin died on January 25, 2011. He had a son and daughter, Admiral Russell S. Berkey, who was a Rear Admiral in the Navy until he retired in 1961. He later served as Chief of the Naval Staff at the United Nations in New York City. Goodwin has a daughter and a son- in-law, Admiral Michael B. Bower, who also served in U.N. forces in the Pacific. Goodwin's son was killed in action during the First World War in the Battle of the Sino-Japanese War in December 1945. He went on to serve in the Army Air Force and the Navy as a Lieutenant Commander. He retired in 1966. He wrote a book about his time in the US Navy, ""The Great War: A Memoirs of a Navy Captain"" (1980). He has also written several other books, including ""The Navy Remembered"" (1989) and ""The Naval Remembrance Project: A History of the American Navy"" (1990). He is also the author of a book, ""Navy Remembered: A Naval Remembered Story"" (1991). He also wrote a play about his experiences in the war, ""A Man Walks Among the Stars"" (1992). He was also the co-founder of the National War College and served as its president from 1993 to 1994. He currently serves as the President of Naval War University. He lives in Newport with his wife in Rhode Island and has two children, Michael and Mary Bower. Hugh H. Goodwin was a vice admiral in the United States Navy. He retired in 1957 after 40 years of active service. He died on February 25, 1980, aged 79. He had two children, a daughter Sidney and a son Hugh Jr., who died in a plane crash one year after he graduated from the Naval Academy in June 1948. He was a member of the Naval Order of the U.S. and the American Academy of Naval Sciences. He also served in the Army Air Force and the Air Force Reserve. He served in World War II and the Korean War. He is survived by his wife, Eleanor, and a daughter, Sidney, who he had with his wife when she died in 1980. He leaves behind a son, Hugh Jr, who was also a naval officer and a grandson, Hugh III, who served with him in the Navy and Air Force. He has also a son who was killed in a helicopter crash in 1967. He worked as a teacher in California and in Louisiana. He lived in Monroe, Louisiana, and died in Monterey, California, in 1986. He left behind a wife and two children. He never remarried and died at the age of 79. His son Hugh III was a marine officer. He went on to serve in the Air Corps and the Army National Guard. His daughter Sidney was a teacher at the University of Louisiana at Lafayette."
对于PD3.0协议，FS312BH支持的最高诱骗电压是多少？,"'无锡速芯微电子有限公司是一家集芯片 研发，销售和服务于一体的国家高新技 术企业，为客户提供高性能，高集成 度，极致体验的全协议快充芯片。 无锡速芯微电子有限公司 FastSOC Microelectronics Co.,Ltd. 销售联系方式： 联系人：顾先生 手机：1800 185 3071 邮箱：gpp@fastsoc.com 网址：www.fastsoc.com 地址：无锡市新吴区菱湖大道200号中国物联网国际创新园E-503室 顾工微信号 速芯微公众号 免责声明：本文所述方法、方案均供客户参考，用于提示或者展示芯片应用的一种或者多种方式，不作为最终产品的实际方案。文中所描述的功能和性能指标在实 验室环境下测试得到，部分可以提供第三方测试报告，但是不保证客户产品上能获得相同的数据。本文信息只作为芯片使用的指导，不授权用户使用本公司或者其 他公司的知识产权。本文信息只作为芯片使用的指导，不承担因为客户自身应用不当而造成的任何损失。 **文中信息仅供参考，详情请联系我司获取最新资料” 无锡速芯微电子有限公司 FastSOC Microelectronics Co.,Ltd. 产品手册 2023年 
新品快览 FS312A：PD3.0 诱骗- FS312A支持PD2.0/PD3.0最高诱骗电压：20V - FS312AE支持PD2.0/PD3.0 最高诱骗电压：20V支持Emarker模拟功能 - 封装：SOT23-5 VBUS CC1 CC2 DM DP 用电电路 4.7K 0.47uF R C C 1 V D D F U N C C C 2F S 3 1 2 B D M D P EP GND 应用图 FS8628：A+C快充协议CC2 CC1 VBUS CC2 CC1 FS312A FUNC GND VDD 4.7K GND R 用电电路 1uF GND 应用图 多口极简方案 FS8611SP*2+CCM-8611SP-A+7533B-T 双C智能降功率方案 FS8611S USB-C AC-DC 双变压器 7533B-T CCM-8611SP-A FS8611S USB-C 采用2颗FS8611SP搭配CCM-8611SP-A （MCU），7533B-T配合工作 - 支持多种协议 - 支持I2C控制 - 任意单 C 的为 35W - 双 插 降 功 率 ， 三 档 功 率 智 能 配 置：27.4W+7.4W；17.4W+17.4W； 27.4W - BOM极简，成本低 FS312B：PD3.1 诱骗FS8611K*2+CCM-8611K-A+7550B-T 双C方案 - FS312BL支持PD2.0/PD3.0/PD3.1/第三方协议最高诱骗电压：20V - FS312BLE支持PD2.0/PD3.0/PD3.1/第三方协议最高诱骗电压：20V支持Emarker模拟功能 - FS312BH支持PD2.0/PD3.0/PD3.1/第三方协议最高诱骗电压：48V - FS312BHE支持PD2.0/PD3.0/PD3.1/第三方协议最高诱骗电压：48V 支持Emarker模拟功能 - 封装：DFN2x2-6L - 兼容兼容BC1.2、Apple2.4A、 QC2.0 Class A、QC3.0 Class A/B、 FCP、SCP、AFC、低压直充等 - 兼容Type-C PD2.0、Type-C PD3.0、 Type-C PD3.0 PPS、QC4.0协议 - 支持两路DP/DM - 支持CV/CC（分段CC）功能 - 支持定制PDO - 支持A+C双口工作，电压自动回5V - 支持FB/OPTO反馈 - 封装：QFN3x3-20L VPWR FB PowerSystem 100K GND R1 GND 19 VIN 17 FB FUNC1 FUNC2 20 15 18 13 PLUGIND VFB FS8628 QFN3x3-20L AGATE 47K 7.5K 47K 7.5K 1 16 8 7 3 4 5 6 10 9 11 CGATE CVBUS CC2 CC1 CDP CDM AVBUS DM DP ISP ISN 12 应用图 2 V3P3 100Ω 1u EP GND GND CVBUS TYPE- C CC2 CC1 CDP CDM CGND TYPE-A AVBUS DM DP 10n 200 AGND 5mΩ GND FS8611K USB-C AC-DC DC-DC 7550B-T CCM-8611K-A FS8611K USB-C 采用2颗FS8611K搭配CCM-8611K-A （MCU）工作，7550B-T配合工作 - 支持PD2.0/PD3.0/QC2.0/AFC/FCP - 支持PDO定制 - 任意单 C 的为 35W(可定制) - 双插18W（可定制15W/20W） - BOM极简，成本低 FS212C+ACM-212C-A+7550B-T 双C方案 FS212C USB-C AC-DC DC-DC 7550B-T ACM-212C-A FS8623B-A+C方案 AC-DC DC-DC FS8623B USB-A USB-C USB-A 采 用 1 颗 F S 2 1 2 C 搭 配 ACM-212C-A 工 作，7550B-T配合工作 - 支持PD2.0/PD3.0 - 支持PDO定制 - 任意单 C 的为20W - 双插7.5W回5V - BOM极简，成本低 采用一颗FS8623B实现A+C方案 - 兼容兼容Apple2.4A/低压直充 QC2.0 Class A/QC3.0 Class A/B/ FCP/SCP等 - 兼 容Type -C PD2.0 / PD3.0 / PD3.0PPS/QC4.0协议 - 支持PDO定制 - 双插回5V 
多口方案选型 产品选型 受电端芯片选型 速芯微现有多种多口的方案选择：A+C，C+C，C+C+A，C+C+C，C+C+A+A等方案。对于 A+C的方案，可使用1颗芯片实现，也可用多颗芯片来实现。 速芯微现有多种受电端诱骗芯片，客户可根据应用需求进行选择。 受电端诱骗芯片应用领域 筋膜枪 无线充 线材 无人机 产品型号 PD2.0 PD3.0 PD3.1 第三方协议 诱骗电压(V) 控制方式 内置Emarker 定制 封装 FS312A √ √ 5/9/12/15/20 电阻阻值 可变电压策略 SOT23-5 FS312AE √ √ 5/9/12/15/20 电阻阻值 √ (公头专用) 可变电压策略 SOT23-5 FS312BL √ √ √ √ 5/9/12/15/20 电阻阻值 可变电压策略 DFN2x2-6 FS312BLE √ √ √ √ 5/9/12/15/20 电阻阻值 √ (公头专用) 可变电压策略 DFN2x2-6 FS312BH √ √ √ √ 5/20/28/36/48 电阻阻值 可变电压策略 DFN2x2-6 FS312BHE √ √ √ √ 5/20/28/36/48 电阻阻值 √ (公头专用) 可变电压策略 DFN2x2-6 FS312LC √ √ √ 5/9/12 电阻阻值 可变第三方 协议 SSOP10 FS312HC √ √ √ 5/9/12/15/20 电阻阻值 可变第三方 协议 SSOP10 FS2711Q √ √ √ 任意设置 I2C √ QFN3x3-16 FS2711P √ √ √ 任意设置 I2C √ QFN3x3-16 FS2711PA √ √ 全协议 任意设置 I2C √ SSOP10 FS2711SW √ √ 全协议 SSOP10 FS512 √ √ 全协议 任意设置 I2C √ SSOP10 方案 类型 产品型号 单C 单A 双插 A+C方案 FS8623 20W（PPS）（可定制） A口全协议18w 5V共享3A FS8623B 20W（PPS）(可定制) A口全协议18w 5V共享3A FS8628 20W（PPS）(可定制) A口全协议18w 5V共享3A FS8611RPC+FS116DB 65W（PPS）(可定制) A口全协议18w A口：5V/2.4A C口：45W FS8628RC+FS116DB 35W(可定制) A口全协议18w A口：5V（BC1.2，Apple 2.4） C口：20W 方案类型 产品型号 单C1 单C2 C1/C2 C+C方案 FS8611RPB*2 30W(可定制) 30W(可定制) C1/C2：5V/3A(或5V/2.4A) FS8611GH*2 35W（可定制） 35W（可定制） C1/C2:18W（可定制） FS8628P*2 35W（可定制） 35W（可定制） C1/C2：17.4W可定制） FS8611KL*2 20W（可定制） 20W（可定制） C1/C2：5V/1.5 A FS8611PC*2 35W 35W C1/C2：18W FS8611BH*2 65W（可定制） 65W（可定制） C1：45W（可定制）C2：20W（可定制） FS8628RPC+FS8611RB 45W（可定制）） 36W （可定制）） C1：30W（可定制）C2：5V/1.5A（可定制） 方案类型 产品型号 单C1 单C2 单A C1+C2 C1/C2+A C1+C2+A C+C+A FS8611S*2+FS116DB 65W（可定制） 65W（ 可定制）） A口全协议18w 智能分配功率 45W+18W C1/C2：智能分配功率 A：18W（或5V1.5A） FS8612C+FS8628P 100W（可定制） 35W （可定制）） 20W C1：65W C2：20W C1+A：65W+20W C2+A：7.5W+7.5W C1：65W C2：7.5W A：7.5W 其他 
Source-TYPE C协议芯片选型 Source-TYPE A协议芯片选型 速芯微现有多种TYPE-C的快充协议芯片，支持多种协议，支持客户定制，多样化，满 足客户对TYPE C的各种快充需求。 速芯微现有多种TYPE A快充协议芯片，支持全协议，支持定制，满足客户对A口协议的各种需 求。速芯微的TYPE-A快充协议芯片的协议丰富，FS112系列拥有多种的型号；FS116D 系列带插入指示，可搭配TYPE-C快充协议芯片，实现A+C，A+C+C，A+A+C+C等多口方 案，协议丰富，其中FS116A一般用于插入指示使用 Source-TYPE A协议芯片引脚封装图 D+ VSS FB 1 2 3 FS112 6 5 4 D- VDD FUNC GATE VIN FUNC FB LED/PLUG_IN 1 2 3 4 5 FS116D 10 DM 9 8 7 6 DP CSP CSN VSS速芯微的各TYPE-C快充协议芯片之间可搭配使用，实现多口方案，更多详情请咨 询我司工作人员。 多口降功率专用快充协议芯片：FS8611RB，FS8611RC，FS8611RPB，FS8611RPC， FS8612CP。 带I2C快充协议芯片：FS8611S，FS8611SP 产品型号 BC1.2 Apple 2.4 QC2.0 QC3.0 AFC FCP SCP HISCP 大电流直充 封装 FS112 √ √ √ √ √ √ √ SOT23-6 FS112H √ √ √ √ √ √ √ √ √ SOT23-6 FS113 √ v √ √ √ √ √ √ √ SOT23-6 FS116DP √ √ √ √ √ √ √ √ SSOP10 FS116DB √ √ √ √ √ √ √ √ SSOP10 FS116E √ √ √ √ √ √ √ √ √ SSOP10 FS116A √ √ SSOP10 其他 可定制 PD2.0 PD3.0 PD3.0 PPS 第三方协议 反馈方式 MOS CV/CC 定制 封装 FS212C √ √ FB √ SOT23-6 FS212CM √ √ FB PMOS(可省) √ SOT23-6 FS212D √ √ √ FB √ SOT23-6 FS212DH √ √ √ FB √ SOT23-6 FS212DP √ √ √ FB PMOS √ SOT23-6 FS212DG √ √ √ FB PMOS √ SOT23-6 FS8611G √ √ FB PMOS(可省) √ SOP-8 FS8611K √ √ QC2.0/AFC/FCP FB PMOS(可省) √ SOP8 FS8611J √ √ √ 全协议 FB PMOS(可省) √ SOP8 FS8611B √ √ √ 全协议 FB PMOS(可省) √ SSOP10 FS8611RB √ √ 全协议 FB PMOS √ SSOP10 FS8611RC √ √ 全协议 FB PMOS √ SSOP10 FS8611S √ √ √ 全协议 FB PMOS √ SSOP10 FS8611PP √ √ √ 全协议 FB PMOS √ SSOP10 FS8611BP √ √ √ 全协议 FB PMOS(可省) √ SSOP10 FS8611RPB √ √ √ 全协议 FB PMOS √ SSOP10 FS8611RPC √ √ √ 全协议 FB PMOS √ SSOP10 FS8611SP √ √ √ 全协议 FB PMOS(可省) SSOP10 FS8612 √ √ √ 全协议 OPTO PMOS √ √ SSOP16 FS8612B √ √ √ 全协议 FB PMOS √ √ SSOP16 FS8612BP √ √ √ 全协议 FB PMOS √ √ SSOP16 FS8612C √ √ √ 全协议 FB/OPTO PMOS √ √ QFN4x4-16 FS8612CP √ √ √ 全协议 FB/OPTO PMOS √ √ QFN4x4-16 
'",['48V.'],898,multifieldqa_en,en,,910c9a02ee857c1019702818b6fa2d5c25ed432d08385ba8," FastSOC Microelectronics Co. Co.,Ltd. is the owner of FastSOC.com. The company's website is located at www.fastsoc.com and its Twitter account is @FastsocMicroelectronics. Its Facebook page is @FastSocMicroElectronics. It has a Twitter account called @FastingSOC and a Facebook page with the name FastSoc. It is also known as FastS OC Microelectronic Co., Ltd. and has a blog called @fastSOCMicroelectronic. It also has a YouTube channel where it posts videos of its products under the hashtag #fastsocmicroelectronics and #fastscore. It's not clear if the company has any plans to go public with its new product line, but it has been in the works for a few months. For more information, visit www.fastsOCmicroelectronicco.com/. For more on FastSoco, visit their Facebook page or their Twitter account at @fastscomicroelectrics. For confidential support, call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here for details. In the U.S., call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential. support in the UK, visit the Samaritans’ website at www.samaritans.org. For further information on suicide prevention in the United States, contact the National Suicide Prevention Line at 800 273-TALK (8255). For confidential help in the Philippines, call the National Suicide prevention Lifeline on 1-844-788-8457 or  visit  http:// www.suicidesprevention.org /. For information on how to help a child in need in the developing world, please visit  www.nhs.uk. For information about how to donate to the Children’s Emergency Fund, visit: www.acs.org/pages/Children-In-Need-of-Service-and-Music-Lifeline. For details on donating to a local charity, see www.crowdrise.com/crowd-sourcing/Child-Serving-Emergency-Fund-Lights-Initiatives. For help in developing countries, visit http://crowdsourcing.cnn.it/2013/09/07/30/38/39/3938/3838/40/3939/3840/3839/40/. For help with a project of this type, contact the National Institutes of Health (NIH) at http: http:/www.nh.gov/news/article/child-serving-in-the-developing-world-and researcher/childs-service-initiative/childhood-enrichment-enabling-innovations.html#storylink=cpy.html. For support in developing nations, visit the National Institute of Health and Human Services (NICE) at http:://www.-nih.gov/. For info on a project in the Developed World, see: http://www-www.nc.gov.uk/News/story/stories/childish-envisioning-innovation-in the developing world/child innovative-innovements-in the-developed world.html?story=false, title=Child-Envisioning in-Developing World, article included in this article, page 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 28P 100W） 35W ） 20W C1：65W C2３20W C 1+AＬ65W+20WC2+A  7.5W+7.5w C1+C+C，A+A+C-C+ C+C 等多 1911.2 Apple 2.4 QC2.0 QC3.0 AFC-CP 2.0.0 FC-CP 3.0 F-CP 4.0 C-CP 5.0 D-CP 6.0 E-CP 7.0 S-CP 8.0 P-CP 9.0 R-CP 10.0 T-CP 11.0 V-CP 12.0 W-CP 13.0 X-CP 14.0 Y-CP 15.0 Z-CP 16.5 X-P 17.5 Y-P 18.5 Z-P 19.5 W-P 20.5 T-P 21.5 D-P 22.5 E-P 23.5 F-P 24.5 A-P 25.5 B-P 26.5 C-P 27.5 G-P 28.5 H-P 29.5 L-P 30.5 M-P 31.5 R-P 34.5 S-P 35.5 N-P 36.5 V-P 37.5 Q-P 38.5 U-P 39.5 P-P 40.5 K-P 50.5.0 A-W-P 100.0 M-W 1.0 B-W 5.1 D-W 6.5 J-W 7.2 R-W 8.5 DD-W 9.5 DM-W 10.5 DP-W 11.5 RP-W 12.5RP-W 13.5 TP-W 14.5 DB-W 15.5DP-W 16.0 DP-DP-DP 17.0 RP-DP 18.0 DB-DP 19.0 BD-DP 20.0 DS-DP 21.0 DD-DP 22.0 DF-DP 23.0 FD-DP 25.0 DL-DP 26.0 G-DP 30.0 DA-DP 27.0 DE-DP 28.0 DH-DP 29.0 DM-DP 32.5 DF-DD-DP 33.5 DS-DD 3.5 FS-DP 4.5 FD-DB-DP 3.6 DP-DB 4.6DP-DB 5.5 TD-DP 6.6 D-VDD-DB 6.7 D- VDD-DD 5.7 DP-DD 6.8 DP-D-DP 7.7 DM-DD 7.8 DM-D7.7 RP-D8.5 BD-D9.7 DB-D6.7DP-D10.8 D-DD 4.7 V-V DD-DD 9.7 DF-D3.7 DD-D5.7.6 V-DD 1.8 V-DDD-D4.6 DD-V5.6 DF-V6.6 DB-V7.8 DB-DD7.9 D-VD-DP 8.6 DM-V8.7 BD-V9.8DP-V1.8 DD-2.7 DS-V3.6 FS-V4.7 FS-D1.6 BD-6.8 DF-3.8 BD-4.8."
When did Born resign as chairperson of the CFTC?,"Brooksley Elizabeth Born (born August 27, 1940) is an American attorney and former public official who, from August 26, 1996, to June 1, 1999, was chair of the Commodity Futures Trading Commission (CFTC), the federal agency which oversees the U.S. futures and commodity options markets. During her tenure on the CFTC, Born lobbied Congress and the President to give the CFTC oversight of off-exchange markets for derivatives, in addition to its role with respect to exchange-traded derivatives, but her warnings were ignored or dismissed, and her calls for reform resisted by other regulators.<ref name=""nytimes"">Goodman, Peter S. The Reckoning - Taking Hard New Look at a Greenspan Legacy, The New York Times, October 9, 2008.</ref> Born resigned as chairperson on June 1, 1999, shortly after Congress passed legislation prohibiting her agency from regulating derivatives.

In 2009, Born received the John F. Kennedy Profiles in Courage Award, along with Sheila Bair of the Federal Deposit Insurance Corporation, in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis.

Early life and education
Born graduated from Abraham Lincoln High School (San Francisco, California) at the age of 16. She then attended Stanford University, where she majored in English and was graduated with the class of 1961.  She initially wanted to become a doctor, but a guidance counsellor at Stanford advised her against medicine, so she majored in English literature instead.

She then attended Stanford Law School, one of only seven women in her class.  She was the first female student ever to be named president of the Stanford Law Review. She received the ""Outstanding Senior"" award and graduated as valedictorian of the class of 1964.

Legal career
Immediately after law school Born was selected as a law clerk to judge Henry Edgerton of the U.S. Court of Appeals for the District of Columbia Circuit. It was during this time that she met her first husband, Jacob C. Landau, who was a journalist covering the Federal courts at the time. Following her clerkship, she became an associate at the Washington, D.C.-based international law firm of Arnold & Porter. Born was attracted to Arnold & Porter because it was one of the few major law firms to have a woman partner at that time, Carolyn Agger, who was the head of the tax practice. Born took a two-year leave of absence from Arnold & Porter to accompany her first husband to Boston, where he had received a fellowship. During that time she worked as a research assistant to law professor Alan Dershowitz.

Born's early career at Arnold & Porter focused on international trade law, in which she represented a number of Swiss industries and the government of Switzerland. She developed a practice representing clients in numerous complex litigation and arbitration cases involving financial market transactions. Among her high-profile cases was the matter of the Hunt Brothers attempt to corner the market in silver in the 1970s. She made partner at Arnold & Porter, after moving to a three-day schedule to help raise her second child, and eventually rose to be the head of the firm's derivatives practice.

Born was among the first female attorneys to systematically address inequities regarding how the laws treated women. Born and another female lawyer, Marna Tucker, taught what is considered to have been the first ""Women and the Law"" course at Catholic University’s Columbus School of Law. The class exclusively concerned prejudicial treatment of women under the laws of the United States, past and present.  Born and Tucker were surprised to discover that there was no textbook on the issue at the time.  Born is also one of the co-founders of the National Women's Law Center. Born also helped rewrite the American Bar Association rules to make it possible for more women and minorities to sit on federal bench.

During her long legal career, and into her retirement, Born did much pro bono and other types of volunteer work. She was active in the American Bar Association, the largest professional organization of lawyers in the United States.  Initially Born was named a member of the governing council of the ABA's Individual Rights Section, eventually becoming chairperson.  Born and Tucker founded the ABA Women's Caucus, the first organization of female lawyers in the ABA.  She held several other senior positions in the ABA, including being named the first woman member of the ABA's Standing Committee on the Federal Judiciary.  As a member of the Judiciary Committee, Born provided testimony and opinion on persons nominated for federal judgeships. In 1980 she was named chair of the committee.  As chair of the committee, Born was invited to address the U.S. Congress regarding the nomination of Judge Sandra Day O'Connor to the U.S. Supreme Court.

In 1993, Born's name was floated as a possible candidate for Attorney General of the United States, but Janet Reno was nominated.

In July 2009, Nancy Pelosi appointed Brooksley Born as a commissioner to the Financial Crisis Inquiry Commission (FCIC).

Born and the OTC derivatives market
Born was appointed to the CFTC on April 15, 1994, by President Bill Clinton. Due to litigation against Bankers Trust Company by Procter and Gamble and other corporate clients, Born and her team at the CFTC sought comments on the regulation of over-the-counter derivatives, a first step in the process of writing CFTC regulations to supplement the existing regulations of the Federal Reserve System,  the Options Clearing Corporation, and the National Association of Insurance Commissioners. Born was particularly concerned about swaps, financial instruments that are traded over the counter between banks, insurance companies or other funds or companies, and thus have no transparency except to the two counterparties and the counterparties' regulators, if any.  CFTC regulation was strenuously opposed by Federal Reserve chairman Alan Greenspan, and by Treasury Secretaries Robert Rubin and Lawrence Summers. On May 7, 1998, former SEC Chairman Arthur Levitt joined Rubin and Greenspan in objecting to the issuance of the CFTC's concept release. Their response dismissed Born's analysis and focused on the hypothetical possibility that CFTC regulation of swaps and other OTC derivative instruments could create a ""legal uncertainty"" regarding such financial instruments,  hypothetically reducing the value of the instruments. They argued that the imposition of regulatory costs would ""stifle financial innovation"" and encourage financial capital to transfer its  transactions offshore. The disagreement between Born and the Executive Office's top economic policy advisors has been described not only as a classic Washington turf war, but also a war of ideologies,  insofar as it is possible to argue that Born's actions were consistent with Keynesian and neoclassical economics while Greenspan, Rubin, Levitt, and Summers consistently espoused neoliberal, and neoconservative policies.

In 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse.  Using mathematical models to calculate debt risk, LTCM used derivatives to leverage $5 billion into more than $1 trillion, doing business with fifteen of Wall Street's largest financial institutions.  The derivative transactions were not regulated, nor were investors able to evaluate LTCM's exposures.  Born stated, ""I thought that LTCM was exactly what I had been worried about"".  In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance.  After intervention by the Federal Reserve, the crisis was averted.  In congressional hearings into the crisis, Greenspan acknowledged that language had been introduced into an agriculture bill that would prevent CFTC from regulating the derivatives which were at the center of the crisis that threatened the US economy.  U.S. Representative Maurice Hinchey (D-NY) asked ""How many more failures do you think we'd have to have before some regulation in this area might be appropriate?""  In response, Greenspan brushed aside the substance of Born's warnings with the simple assertion that ""the degree of supervision of regulation of the over-the-counter derivatives market is quite adequate to maintain a degree of stability in the system"". Born's warning was that there wasn't any regulation of them.  Born's chief of staff, Michael Greenberger summed up Greenspan's position this way: ""Greenspan didn't believe that fraud was something that needed to be enforced, and he assumed she probably did. And of course, she did.""  Under heavy pressure from the financial lobby, legislation prohibiting regulation of derivatives by Born's agency was passed by the Congress.  Born resigned on June 1, 1999.

The derivatives market continued to grow yearly throughout both terms of George W. Bush's administration. On September 15, 2008, the bankruptcy of Lehman Brothers forced a broad recognition of a financial crisis in both the US and world capital markets.  As Lehman Brothers' failure temporarily reduced financial capital's confidence, a number of newspaper articles and television programs suggested that the failure's possible causes included the conflict between the CFTC and the other regulators.Faiola, Anthony, Nakashima, Ellen and Drew, Jill. The Crash: Risk and Regulation - What Went Wrong, The Washington Post, October 15, 2008.

Born declined to publicly comment on the unfolding 2008 crisis until March 2009, when she said: ""The market grew so enormously, with so little oversight and regulation, that it made the financial crisis much deeper and more pervasive than it otherwise would have been."" She also lamented the influence of Wall Street lobbyists on the process and the refusal of regulators to discuss even modest reforms.

An October 2009 Frontline documentary titled ""The Warning""  described Born's thwarted efforts to regulate and bring transparency to the derivatives market, and the continuing opposition thereto. The program concluded with an excerpted interview with Born sounding another warning: ""I think we will have continuing danger from these markets and that we will have repeats of the financial crisis -- may differ in details but there will be significant financial downturns and disasters attributed to this regulatory gap, over and over, until we learn from experience.""

In 2009 Born, along with Sheila Bair of the FDIC, was awarded the John F. Kennedy Profiles in Courage Award in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis.  According to Caroline Kennedy, ""Brooksley Born recognized that the financial security of all Americans was being put at risk by the greed, negligence and opposition of  powerful and well connected interests.... The catastrophic financial events of recent months have  proved them [Born and Sheila Bair] right.""  One member of the President's working group had a change of heart about Brooksley Born.  SEC Chairman Arthur Levitt stated ""I've come to know her as one of the most capable, dedicated, intelligent and committed public servants that I have ever come to know"", adding that ""I could have done much better. I could have made a difference"" in response to her warnings.

In 2010, a documentary film Inside Job further alleged that derivatives regulation was ineffective from the Clinton administration on. Along with fellow whistleblower, former IMF Chief Economist Raghuram Rajan, who was also scorned by the economic establishment, Brooksley Born was cited as one of the authorities arguing that financial derivatives increase economic risk.

 Personal life 
Born is married to Alexander E. Bennett (also retired from Arnold & Porter).  She has five adult children - two from a previous marriage to Jacob Landau and three stepchildren. Notably, Born was named a partner at Arnold & Porter while working part-time so she could raise her two young children.  When both of her children were school-age, Born returned to practice full-time.

References

External links
Attorney profile at Arnold & Porter
Brooksley Born (2009 Winner) of the Profiles in Courage Award, with acceptance speech transcript and NECN video

Profile at MarketsWiki
Speeches and statements
""Testimony Of Brooksley Born Chairperson of the CFTC Concerning The Over-The-Counter Derivatives Market"", before the House Committee On Banking And Financial Services, July 24, 1998.
""The Lessons of Long Term Capital Management L.P."", Remarks of Brooksley Born, Chairperson of the CFTC, Chicago-Kent-IIT Commodities Law Institute, Chicago, Illinois, October 15, 1998.
 Interview: Brooksley Born for ""PBS Frontline: The Warning"", PBS, (streaming VIDEO 1 hour), October 20, 2009.
Articles
Manuel Roig-Franzia. ""Credit Crisis Cassandra:Brooksley Born's Unheeded Warning Is a Rueful Echo 10 Years On"", The Washington Post, May 26, 2009
 Taibbi, Matt. ""The Great American Bubble Machine"", Rolling Stone'', July 9–23, 2009

1940 births
American women lawyers
Arnold & Porter people
Clinton administration personnel
Columbus School of Law faculty
Commodity Futures Trading Commission personnel
Heads of United States federal agencies
Lawyers from San Francisco
Living people
Stanford Law School alumni
21st-century American women
Stanford University alumni","['June 1, 1999.']",2088,multifieldqa_en,en,,2d48bbc2bd6052c5e67824752a9296cbbc26351616cd6d7e," Brooksley Elizabeth Born was chair of the Commodity Futures Trading Commission (CFTC) from August 26, 1996, to June 1, 1999. During her tenure on the CFTC, Born lobbied Congress and the President to give the agency oversight of off-exchange markets for derivatives. Her warnings were ignored or dismissed, and her calls for reform resisted by other regulators. In 2009, Born received the John F. Kennedy Profiles in Courage Award, along with Sheila Bair of the Federal Deposit Insurance Corporation, in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis. Born is one of the co-founders of the National Women's Law Center. She helped rewrite the American Bar Association rules to make it possible for more women and minorities to sit on federal bench. She is married to Jacob C. Landau, who was a journalist covering the Federal courts at the time. She has two children, a son and a daughter. She was the first female student ever to be named president of the Stanford Law Review and graduated as valedictorian of the class of 1964. She also taught the first ""Women and the Law"" course at Catholic University’s Columbus School of Law, which exclusively concerned prejudicial treatment of women under the laws of the United States, past and present. Born was among the first women to systematically address inequities regarding how the laws treated women. She made partner at Arnold & Porter, after moving to a three-day schedule to help raise her second child, and eventually rose to be the head of the firm's derivatives practice. She represented a number of Swiss industries and the government of Switzerland. Among her high-profile cases was the matter of the Hunt Brothers attempt to corner the market in silver in the 1970s. Born took a two-year leave of absence to accompany her first husband to Boston, where he had received a fellowship. During that time she worked as a research assistant to law professor Alan Dershowitz. She became an associate at the Washington, D.C.-based international law firm of Arnold and Porter. She developed a practice representing clients in numerous complex litigation and arbitration cases involving financial market transactions. She later became a partner at the law firm, and represented clients in several international trade law cases, including the Swiss government and the Swiss canton of Basel. She retired from the law practice in 1999, shortly after Congress passed legislation prohibiting her agency from regulating derivatives. She currently lives in San Francisco with her husband, Jacob Landau. Her son, son and daughter live in California, and she has a daughter and a son-in-law who lives in New York, New York and Washington, DC. Her husband and daughter have three children, both of whom were born in the U.S. and live in the San Francisco area. Brooksley Born was appointed to the CFTC on April 15, 1994, by President Bill Clinton. Born was particularly concerned about swaps, financial instruments that are traded over the counter between banks, insurance companies or other funds or companies. In 1998, a trillion-dollar hedge fund called Long Term Capital Management (LTCM) was near collapse. In the last weekend of September 1998, the President's working group was told that the entire American economy hung in the balance. After intervention by the Federal Reserve, the crisis was averted. In July 2009, Nancy Pelosi appointed Brooksley Born as a commissioner to the Financial Crisis Inquiry Commission (FCIC). Born and the OTC derivatives market were at the center of the crisis that threatened the U.S. economy. She was named the first woman member of the ABA's Standing Committee on the Federal Judiciary in 1980. In 1980 she was named chair of the committee. In 1993, Born's name was floated as a possible candidate for Attorney General of the United States, but Janet Reno was nominated instead. In 1995, Born was invited to address the Congress regarding the nomination of Judge Sandra Day O'Connor to the Supreme Court. In 1997, Born and Tucker founded the A BA Women's Caucus, the first organization of female lawyers inThe ABA. She held several other senior positions in the American Bar Association, including being named the head of the Individual Rights Section. In 2000, she became chairperson of the Criminal Defense Section of the American Lawyer's Association, the largest professional organization of lawyers in the US. In 2002, Born became the first female member of the ABA Governing Council, the governing council of which she was the chairperson for two years. In 2003, she was elected to the American Bar Association's Board of Trustees, the highest level of membership for women in the association. In 2008, she was named the chair of its Individual Rights Committee, a position she held until her retirement in 2010. In 2010, she served as the chairwoman of the ABA's Criminal Defense Committee, the council's governing council for the first time. In 2011, she resigned from the board to take up a post at the University of California, Los Angeles. In 2012, she wrote a book about her experiences as a CFTC commissioner, ""Brooke Born: A Memoir of a Regulator and a Lawyer"" The book was published by Simon & Schuster, a division of Penguin Books, and is available on Amazon.com for $20.99. For more information, or to order a copy of the book, go to: http://www.samaritans.com/Brooke-Born-A-Memoirs-of-a-Regulator. Brooksley Born was the head of the CFTC, which regulated the over-the-counter derivatives market. In 1999, she warned that there wasn't any regulation of them. The derivatives market continued to grow yearly throughout both terms of George W. Bush's administration. On September 15, 2008, the bankruptcy of Lehman Brothers forced a broad recognition of a financial crisis in both the US and world capital markets. Born declined to publicly comment on the unfolding 2008 crisis until March 2009, when she said: ""The market grew so enormously, with so little oversight and regulation, that it made the financial crisis much deeper and more pervasive than it otherwise would have been"" In 2009 Born, along with Sheila Bair of the FDIC, was awarded the John F. Kennedy Profiles in Courage Award in recognition of the ""political courage she demonstrated in sounding early warnings about conditions that contributed"" to the 2007-08 financial crisis. In 2010, a documentary film alleged that derivatives regulation was ineffective from the Clinton administration on. Along with fellow whistleblower, former IMF Chief Economist Raghuram Rajan, who was also scorned by the economic establishment, Born was cited as one of the authorities arguing that financial derivatives increase economic risk. She is married to Alexander E. Bennett (also retired from Arnold & Porter).  She has five adult children - two from a previous marriage to Jacob Landau and three stepchild stepchildren. She also has two stepchildren from her first marriage to Alexander Landau. The SEC Chairman Arthur Levitt stated ""I could have done much better. I could have made a difference"" in response to her warnings. In an excerpt of an October 2009 Frontline documentary titled ""The Warning""  described Born's thwarted efforts to regulate and bring transparency to the derivatives market, and the continuing opposition thereto. The program concluded with an excerpted interview with Born sounding another warning: ""I think we will have continuing danger from these markets ... There will be significant financial downturns and disasters attributed to this regulatory gap, over and over, until we learn from experience"" She is survived by her husband, three stepchildren and a daughter from her second marriage. She has a son from her third marriage, and a stepdaughter from her fourth marriage. The CFTC is now headed by former CFTC Chairman Michael Greenberger, who is also a former FDIC chief. The agency is based in Washington, D.C. and has offices in New York, Washington, and San Francisco. It was created in 1998 by President George H.W. Bush to oversee the creation of the Federal Deposit Insurance Corporation (FDIC) and the Securities and Exchange Commission (SEC). The agency was created by President Bill Clinton to protect the U.S. financial system. Brooksley Born was named a partner at Arnold & Porter while working part-time so she could raise her two young children. When both of her children were school-age, Born returned to practice full-time. Born is the winner of the 2009 Profiles in Courage Award. He is married to the former head of the U.S. Commodity Futures Trading Commission. He has two daughters and a son. He lives in San Francisco with his wife and two daughters. Born has served as Chair of the CFTC since 1998. He was a member of the House Committee on Banking and Financial Services from 1997 to 2001. He also served as Chairman of the Securities and Exchange Commission from 2001 to 2007. He currently serves as the chairman of the Federal Reserve Bank of San Francisco. He previously served as the chair of the New York Mercantile Bank from 1995 to 1998. For more information on Brooksley Born, visit his website: http://www.brooksley-born.com/. For more on the Credit Crisis Cassandra, visit: www.credit-crisis-c Cassandra.org. For information on the Financial Crisis Cassandra Project, visit www.financialcrisis Cassandra.com. for more information about the Financial crisis Cassandra Project."
