{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cb26350-3e60-4fcc-97fa-483663a798b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stp 1 Dependecis instal\n",
    "#!pip install transformers==4.41.2\n",
    "# Important to install that transformer version, unles it occur error\n",
    "#See link: https://github.com/THUDM/CogVLM2/issues/181#issuecomment-2381807778\n",
    "# !pip install transformers sentencepiece datasets evaluate\n",
    "# !pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bc4697-f92f-4838-92ab-a9dd306dd236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "#from LLMLingua.experiments.llmlingua2.evaluation.metrics import qa_f1_score\n",
    "from metrics import qa_f1_score, f1_score\n",
    "if 'model' in globals():\n",
    "    del model \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#######################################\n",
    "print('cuda is',torch.cuda.is_available()) \n",
    "#################################################\n",
    "# 3 model & dataset \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcaafff-a6de-4b15-b4ef-be30536c8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is True\n"
     ]
    }
   ],
   "source": [
    "print('cuda is',torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41da0400-1574-4695-8eb0-4a8d2910d847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0b591f59fd4049bfe3ab62f00e9eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ChatGLM-6B\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"THUDM/chatglm2-6b-32k\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True\n",
    ").to(device)\n",
    "\n",
    "model=model.eval()\n",
    "#print('model config is:',model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b03872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b-32k\", trust_remote_code=True)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36cd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patch the tokenizer's _pad method to handle the padding_side argument\n",
    "original_pad = tokenizer._pad\n",
    "def new_pad(self, encoded_inputs, max_length=None, padding_strategy=\"longest\", pad_to_multiple_of=None, return_attention_mask=None, **kwargs):\n",
    "    return original_pad(encoded_inputs, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask)\n",
    "tokenizer._pad = new_pad.__get__(tokenizer, type(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e2ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the four datasets\n",
    "datasets = [\"multifieldqa_en\", \"hotpotqa\", \"triviaqa\", \"narrativeqa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe10a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM token PRUNE\n",
    "def random_prune(text, compression_ratio=0.5):\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)#.to(device)  # e.g., [\"The\", \" quick\", \" brown\"...]\n",
    "    \n",
    "    num_keep = int(len(tokens) * (compression_ratio)) # compression ratio means remaining \n",
    "    \n",
    "    # Step 3: Randomly pick tokens (like lottery balls)\n",
    "    kept_indices = sorted(  # Maintain original order\n",
    "        np.random.choice(  # Random selection\n",
    "            len(tokens), \n",
    "            num_keep, \n",
    "            replace=False  # No duplicates\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Step 4: Rebuild text from kept tokens\n",
    "    pruned_tokens = [tokens[i] for i in kept_indices]\n",
    "    return tokenizer.convert_tokens_to_string(pruned_tokens)  # Tokens âž” text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9adaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "\n",
    "def no_pruning(context: str) -> str:\n",
    "    \"\"\"Returns the full context (no pruning).\"\"\"\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, compression_func, ratio=None):\n",
    "    f1_scores = []\n",
    "    latencies = []\n",
    "    memory_usages = []\n",
    "    peak_memory_usages = []\n",
    "\n",
    "    # Model constraints\n",
    "    max_context_length = 32768  # ChatGLM2-6B-32k's limit\n",
    "    max_new_tokens = 256  # Allow generating up to 256 tokens\n",
    "    max_input_length = max_context_length - max_new_tokens  # 32768 - 256 = 32512\n",
    "\n",
    "    #for idx in range(20): \n",
    "    for idx in range(len(data)):  # Test on all examples\n",
    "        example = data[idx] \n",
    "        context = example[\"context\"]\n",
    "        question = example[\"input\"]\n",
    "        answers =example[\"answers\"][0]\n",
    "        if ratio is not None:\n",
    "            compressed_context = compression_func(context, ratio)\n",
    "        else:\n",
    "            compressed_context = compression_func(context)  #for no.prune & compressr\n",
    "\n",
    "        tokenized = tokenizer.encode(\n",
    "            compressed_context, \n",
    "            truncation=True, \n",
    "            max_length=max_input_length\n",
    "        )\n",
    "        truncated_context = tokenizer.decode(tokenized, skip_special_tokens=True)\n",
    "            \n",
    "        # prompt = f\"Context: {compressed_context}\\nQuestion: {question}\"\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_time = time.time()\n",
    "        #response, _ = model.chat(tokenizer, prompt)\n",
    "        response, _ = model.chat(\n",
    "            tokenizer,\n",
    "            question,\n",
    "            history=[(truncated_context, \"Answer:\")],\n",
    "            max_new_tokens=max_new_tokens  # Critical parameter\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Measure memory\n",
    "        memory = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
    "        \n",
    "        #Measure peak memory?\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = qa_f1_score(prediction=response, ground_truth=answers)\n",
    "        f1_scores.append(f1)\n",
    "        latencies.append(latency)\n",
    "        peak_memory_usages.append(peak_memory)\n",
    "    \n",
    "    return {\n",
    "        \"avg_f1\": np.mean(f1_scores),\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_peak_memory\": np.mean(peak_memory_usages)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f58e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable function to print and save results\n",
    "def log_result_no_prune(message):\n",
    "    #print(message)\n",
    "    with open(\"results_no_prune.txt\", \"a\") as f:  # 'a' = append mode\n",
    "        f.write(message + \"\\n\")\n",
    "\n",
    "def log_result_random_prune(message):\n",
    "    #print(message)\n",
    "    with open(\"results_random_prune.txt\", \"a\") as f:  # 'a' = append mode\n",
    "        f.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "363c34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===multifieldqa_en  Random Token Pruning (25%) ===\n",
      "F1: 0.15, Latency: 1.58s, Peak Memory: 24579.09 MB\n",
      "\n",
      "===multifieldqa_en  Random Token Pruning (50%) ===\n",
      "F1: 0.26, Latency: 1.30s, Peak Memory: 24955.47 MB\n",
      "\n",
      "===multifieldqa_en  Random Token Pruning (75%) ===\n",
      "F1: 0.33, Latency: 1.81s, Peak Memory: 25336.37 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[0]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Randn pruning\n",
    "for ratio in pruning_ratios:\n",
    "    random_pruning_results = evaluate(data, random_prune, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_random_prune(f\"\\n==={dataset_name} Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    log_result_random_prune(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===hotpotqa  Random Token Pruning (25%) ===\n",
      "F1: 0.10, Latency: 1.21s, Peak Memory: 12586.63 MB\n",
      "\n",
      "===hotpotqa  Random Token Pruning (50%) ===\n",
      "F1: 0.07, Latency: 1.50s, Peak Memory: 13224.21 MB\n",
      "\n",
      "===hotpotqa  Random Token Pruning (75%) ===\n",
      "F1: 0.14, Latency: 2.16s, Peak Memory: 13865.97 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[1]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Randn pruning\n",
    "for ratio in pruning_ratios:\n",
    "    random_pruning_results = evaluate(data, random_prune, ratio=ratio)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_random_prune(f\"\\n==={dataset_name} Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    log_result_random_prune(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f00702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===triviaqa  Random Token Pruning (25%) ===\n",
      "F1: 0.46, Latency: 0.65s, Peak Memory: 12567.37 MB\n",
      "\n",
      "===triviaqa  Random Token Pruning (50%) ===\n",
      "F1: 0.50, Latency: 1.13s, Peak Memory: 13059.76 MB\n",
      "\n",
      "===triviaqa  Random Token Pruning (75%) ===\n",
      "F1: 0.41, Latency: 1.70s, Peak Memory: 13554.41 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[2]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Randn pruning\n",
    "for ratio in pruning_ratios:\n",
    "    random_pruning_results = evaluate(data, random_prune, ratio=ratio-0.1)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_random_prune(f\"\\n==={dataset_name} Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    log_result_random_prune(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e2f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===narrativeqa  Random Token Pruning (25%) ===\n",
      "F1: 0.11, Latency: 1.74s, Peak Memory: 12668.78 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===narrativeqa  Random Token Pruning (50%) ===\n",
      "F1: 0.15, Latency: 2.75s, Peak Memory: 13865.31 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===narrativeqa  Random Token Pruning (75%) ===\n",
      "F1: 0.14, Latency: 4.54s, Peak Memory: 14929.30 MB\n"
     ]
    }
   ],
   "source": [
    "pruning_ratios = [0.25, 0.5, 0.75]\n",
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[3]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Randn pruning\n",
    "for ratio in pruning_ratios:\n",
    "    random_pruning_results = evaluate(data, random_prune, ratio=ratio-0.1)\n",
    "    \n",
    "    print(f\"\\n==={dataset_name}  Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    print(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")\n",
    "    log_result_random_prune(f\"\\n==={dataset_name} Random Token Pruning ({int(ratio*100)}%) ===\")\n",
    "    log_result_random_prune(f\"F1: {random_pruning_results['avg_f1']:.2f}, Latency: {random_pruning_results['avg_latency']:.2f}s, Peak Memory: {random_pruning_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "129d8364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===multifieldqa_en  No Pruning (Full Context) ===\n",
      "F1: 0.53, Latency: 1.88s,  Peak Memory: 25723.81 MB\n"
     ]
    }
   ],
   "source": [
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[0]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Evaluate no pruning (full context)\n",
    "full_context_results = evaluate(data, no_pruning)\n",
    "print(f\"\\n==={dataset_name}  No Pruning (Full Context) ===\")\n",
    "print(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s,  Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")\n",
    "log_result_no_prune(f\"\\n==={dataset_name} No Pruning (Full Context) ===\")\n",
    "log_result_no_prune(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s, Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58896e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===hotpotqa  No Pruning (Full Context) ===\n",
      "F1: 0.28, Latency: 2.99s,  Peak Memory: 26766.88 MB\n"
     ]
    }
   ],
   "source": [
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[1]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Evaluate no pruning (full context)\n",
    "full_context_results = evaluate(data, no_pruning)\n",
    "print(f\"\\n==={dataset_name}  No Pruning (Full Context) ===\")\n",
    "print(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s,  Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")\n",
    "log_result_no_prune(f\"\\n==={dataset_name} No Pruning (Full Context) ===\")\n",
    "log_result_no_prune(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s, Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd1c0b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===triviaqa  No Pruning (Full Context) ===\n",
      "F1: 0.44, Latency: 2.30s,  Peak Memory: 26306.77 MB\n"
     ]
    }
   ],
   "source": [
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[2]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Evaluate no pruning (full context)\n",
    "full_context_results = evaluate(data, no_pruning)\n",
    "print(f\"\\n==={dataset_name}  No Pruning (Full Context) ===\")\n",
    "print(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s,  Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")\n",
    "log_result_no_prune(f\"\\n==={dataset_name} No Pruning (Full Context) ===\")\n",
    "log_result_no_prune(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s, Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f037b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=32768) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===narrativeqa  No Pruning (Full Context) ===\n",
      "F1: 0.17, Latency: 6.04s,  Peak Memory: 29049.48 MB\n"
     ]
    }
   ],
   "source": [
    "# for dataset_name in datasets:\n",
    "dataset_name = datasets[3]\n",
    "data = load_dataset(\"THUDM/LongBench\", dataset_name, split=\"test\", cache_dir=\"custom_cache_dir\")\n",
    "# Evaluate no pruning (full context)\n",
    "full_context_results = evaluate(data, no_pruning)\n",
    "print(f\"\\n==={dataset_name}  No Pruning (Full Context) ===\")\n",
    "print(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s,  Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")\n",
    "log_result_no_prune(f\"\\n==={dataset_name} No Pruning (Full Context) ===\")\n",
    "log_result_no_prune(f\"F1: {full_context_results['avg_f1']:.2f}, Latency: {full_context_results['avg_latency']:.2f}s, Peak Memory: {full_context_results['avg_peak_memory']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98b2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seongjae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
